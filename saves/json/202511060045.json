[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2511.02761v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02761v1",
                "updated": "2025-11-04T17:40:31Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    17,
                    40,
                    31,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T17:40:31Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    17,
                    40,
                    31,
                    1,
                    308,
                    0
                ],
                "title": "Non-Contact Manipulation of Induced Magnetic Dipoles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-Contact Manipulation of Induced Magnetic Dipoles"
                },
                "summary": "Extending the field of magnetic manipulation to conductive, non-magnetic\nobjects opens the door for a wide array of applications previously limited to\nhard or soft magnetic materials. Of particular interest is the recycling of\nspace debris through the use of oscillating magnetic fields, which represent a\ncache of raw materials in an environment particularly suited to the low forces\ngenerated from inductive magnetic manipulation. Building upon previous work\nthat demonstrated 3D open-loop position control by leveraging the opposing\ndipole moment created from induced eddy currents, this work demonstrates\nclosed-loop position control of a semi-buoyant aluminum sphere in lab tests,\nand the efficacy of varying methods for force inversion is explored. The\nclosed-loop methods represent a critical first step towards wider applications\nfor 3-DOF position control of induced magnetic dipoles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extending the field of magnetic manipulation to conductive, non-magnetic\nobjects opens the door for a wide array of applications previously limited to\nhard or soft magnetic materials. Of particular interest is the recycling of\nspace debris through the use of oscillating magnetic fields, which represent a\ncache of raw materials in an environment particularly suited to the low forces\ngenerated from inductive magnetic manipulation. Building upon previous work\nthat demonstrated 3D open-loop position control by leveraging the opposing\ndipole moment created from induced eddy currents, this work demonstrates\nclosed-loop position control of a semi-buoyant aluminum sphere in lab tests,\nand the efficacy of varying methods for force inversion is explored. The\nclosed-loop methods represent a critical first step towards wider applications\nfor 3-DOF position control of induced magnetic dipoles."
                },
                "authors": [
                    {
                        "name": "Seth Stewart"
                    },
                    {
                        "name": "Joseph Pawelski"
                    },
                    {
                        "name": "Steve Ward"
                    },
                    {
                        "name": "Andrew J. Petruska"
                    }
                ],
                "author_detail": {
                    "name": "Andrew J. Petruska"
                },
                "author": "Andrew J. Petruska",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02761v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02761v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02749v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02749v1",
                "updated": "2025-11-04T17:22:49Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    17,
                    22,
                    49,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T17:22:49Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    17,
                    22,
                    49,
                    1,
                    308,
                    0
                ],
                "title": "Using Span Queries to Optimize for Cache and Attention Locality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Span Queries to Optimize for Cache and Attention Locality"
                },
                "summary": "Clients are evolving beyond chat completion, and now include a variety of\ninnovative inference-time scaling and deep reasoning techniques. At the same\ntime, inference servers remain heavily optimized for chat completion. Prior\nwork has shown that large improvements to KV cache hit rate are possible if\ninference servers evolve towards these non-chat use cases. However, they offer\nsolutions that are also optimized for a single use case, RAG. In this paper, we\nintroduce the span query to generalize the interface to the inference server.\nWe demonstrate that chat, RAG, inference-time scaling, and agentic workloads\ncan all be expressed as span queries. We show how the critical distinction that\nhad been assumed by prior work lies in whether the order of the inputs matter\n-- do they commute? In chat, they do not. In RAG, they often do. This paper\nintroduces span queries, which are expression trees of inference calls, linked\ntogether with commutativity constraints. We describe span query syntax and\nsemantics. We show how they can be automatically optimized to improve KV cache\nlocality. We show how a small change to vLLM (affecting only 492 lines) can\nenable high-performance execution of span queries. Using this stack, we\ndemonstrate that span queries can achieve 10-20x reductions in TTFT for two\ndistinct non-chat use cases. Finally, we show that span queries can also be\noptimized to improve attention locality, so as to avoid the so-called\nlost-in-the-middle problem. We demonstrate that an attention-optimized span\nquery on a 2b parameter model vastly outperforms the accuracy of a stock\ninference server using an 8b model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clients are evolving beyond chat completion, and now include a variety of\ninnovative inference-time scaling and deep reasoning techniques. At the same\ntime, inference servers remain heavily optimized for chat completion. Prior\nwork has shown that large improvements to KV cache hit rate are possible if\ninference servers evolve towards these non-chat use cases. However, they offer\nsolutions that are also optimized for a single use case, RAG. In this paper, we\nintroduce the span query to generalize the interface to the inference server.\nWe demonstrate that chat, RAG, inference-time scaling, and agentic workloads\ncan all be expressed as span queries. We show how the critical distinction that\nhad been assumed by prior work lies in whether the order of the inputs matter\n-- do they commute? In chat, they do not. In RAG, they often do. This paper\nintroduces span queries, which are expression trees of inference calls, linked\ntogether with commutativity constraints. We describe span query syntax and\nsemantics. We show how they can be automatically optimized to improve KV cache\nlocality. We show how a small change to vLLM (affecting only 492 lines) can\nenable high-performance execution of span queries. Using this stack, we\ndemonstrate that span queries can achieve 10-20x reductions in TTFT for two\ndistinct non-chat use cases. Finally, we show that span queries can also be\noptimized to improve attention locality, so as to avoid the so-called\nlost-in-the-middle problem. We demonstrate that an attention-optimized span\nquery on a 2b parameter model vastly outperforms the accuracy of a stock\ninference server using an 8b model."
                },
                "authors": [
                    {
                        "name": "Paul Castro"
                    },
                    {
                        "name": "Nick Mitchell"
                    },
                    {
                        "name": "Nathan Ordonez"
                    },
                    {
                        "name": "Thomas Parnell"
                    },
                    {
                        "name": "Mudhakar Srivatsa"
                    },
                    {
                        "name": "Antoni Viros i Martin"
                    }
                ],
                "author_detail": {
                    "name": "Antoni Viros i Martin"
                },
                "author": "Antoni Viros i Martin",
                "arxiv_comment": "12 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02749v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02749v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02651v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02651v1",
                "updated": "2025-11-04T15:17:43Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    15,
                    17,
                    43,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T15:17:43Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    15,
                    17,
                    43,
                    1,
                    308,
                    0
                ],
                "title": "Apriel-H1: Towards Efficient Enterprise Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Apriel-H1: Towards Efficient Enterprise Reasoning Models"
                },
                "summary": "Large Language Models (LLMs) achieve remarkable reasoning capabilities\nthrough transformer architectures with attention mechanisms. However,\ntransformers suffer from quadratic time and memory complexity in the attention\nmodule (MHA) and require caching key-value states during inference, which\nseverely limits throughput and scalability. High inference throughput is\ncritical for agentic tasks, long-context reasoning, efficient deployment under\nhigh request loads, and more efficient test-time compute scaling.\n  State Space Models (SSMs) such as Mamba offer a promising alternative with\nlinear inference complexity and a constant memory footprint via recurrent\ncomputation with fixed-size hidden states. In this technical report we\nintroduce the Apriel-H1 family of hybrid LLMs that combine transformer\nattention and SSM sequence mixers for efficient reasoning at 15B model size.\nThese models are obtained through incremental distillation from a pretrained\nreasoning transformer, Apriel-Nemotron-15B-Thinker, progressively replacing\nless critical attention layers with linear Mamba blocks.\n  We release multiple post-distillation variants of Apriel-H1-15B-Thinker with\ndifferent SSM-to-MHA ratios and analyse how reasoning performance degrades as\nmore Mamba layers replace MHA. Additionally, we release a 30/50 hybrid variant\nof Apriel-H1, further fine-tuned on a supervised dataset of reasoning traces,\nachieving over 2x higher inference throughput when deployed in the\nproduction-ready vLLM environment, with minimal degradation in reasoning\nperformance. This shows that distilled hybrid SSM-Transformer architectures can\ndeliver substantial efficiency gains over the pretrained transformer equivalent\nwithout substantially compromising the reasoning quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) achieve remarkable reasoning capabilities\nthrough transformer architectures with attention mechanisms. However,\ntransformers suffer from quadratic time and memory complexity in the attention\nmodule (MHA) and require caching key-value states during inference, which\nseverely limits throughput and scalability. High inference throughput is\ncritical for agentic tasks, long-context reasoning, efficient deployment under\nhigh request loads, and more efficient test-time compute scaling.\n  State Space Models (SSMs) such as Mamba offer a promising alternative with\nlinear inference complexity and a constant memory footprint via recurrent\ncomputation with fixed-size hidden states. In this technical report we\nintroduce the Apriel-H1 family of hybrid LLMs that combine transformer\nattention and SSM sequence mixers for efficient reasoning at 15B model size.\nThese models are obtained through incremental distillation from a pretrained\nreasoning transformer, Apriel-Nemotron-15B-Thinker, progressively replacing\nless critical attention layers with linear Mamba blocks.\n  We release multiple post-distillation variants of Apriel-H1-15B-Thinker with\ndifferent SSM-to-MHA ratios and analyse how reasoning performance degrades as\nmore Mamba layers replace MHA. Additionally, we release a 30/50 hybrid variant\nof Apriel-H1, further fine-tuned on a supervised dataset of reasoning traces,\nachieving over 2x higher inference throughput when deployed in the\nproduction-ready vLLM environment, with minimal degradation in reasoning\nperformance. This shows that distilled hybrid SSM-Transformer architectures can\ndeliver substantial efficiency gains over the pretrained transformer equivalent\nwithout substantially compromising the reasoning quality."
                },
                "authors": [
                    {
                        "name": "Oleksiy Ostapenko"
                    },
                    {
                        "name": "Luke Kumar"
                    },
                    {
                        "name": "Raymond Li"
                    },
                    {
                        "name": "Denis Kocetkov"
                    },
                    {
                        "name": "Joel Lamy-Poirier"
                    },
                    {
                        "name": "Shruthan Radhakrishna"
                    },
                    {
                        "name": "Soham Parikh"
                    },
                    {
                        "name": "Shambhavi Mishra"
                    },
                    {
                        "name": "Sebastien Paquet"
                    },
                    {
                        "name": "Srinivas Sunkara"
                    },
                    {
                        "name": "Valérie Bécaert"
                    },
                    {
                        "name": "Sathwik Tejaswi Madhusudhan"
                    },
                    {
                        "name": "Torsten Scholak"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Scholak"
                },
                "author": "Torsten Scholak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02651v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02651v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02647v1",
                "updated": "2025-11-04T15:14:58Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    15,
                    14,
                    58,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T15:14:58Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    15,
                    14,
                    58,
                    1,
                    308,
                    0
                ],
                "title": "Federated Attention: A Distributed Paradigm for Collaborative LLM\n  Inference over Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Attention: A Distributed Paradigm for Collaborative LLM\n  Inference over Edge Networks"
                },
                "summary": "Large language models (LLMs) are proliferating rapidly at the edge,\ndelivering intelligent capabilities across diverse application scenarios.\nHowever, their practical deployment in collaborative scenarios confronts\nfundamental challenges: privacy vulnerabilities, communication overhead, and\ncomputational bottlenecks. To address these, we propose Federated Attention\n(FedAttn), which integrates the federated paradigm into the self-attention\nmechanism, creating a new distributed LLM inference framework that\nsimultaneously achieves privacy protection, communication efficiency, and\ncomputational efficiency. FedAttn enables participants to perform local\nself-attention over their own token representations while periodically\nexchanging and aggregating Key-Value (KV) matrices across multiple Transformer\nblocks, collaboratively generating LLM responses without exposing private\nprompts. Further, we identify a structural duality between contextual\nrepresentation refinement in FedAttn and parameter optimization in FL across\nprivate data, local computation, and global aggregation. This key insight\nprovides a principled foundation for systematically porting federated\noptimization techniques to collaborative LLM inference. Building on this\nframework, we theoretically analyze how local self-attention computation within\nparticipants and heterogeneous token relevance among participants shape error\npropagation dynamics across Transformer blocks. Moreover, we characterize the\nfundamental trade-off between response quality and communication/computation\nefficiency, which is governed by the synchronization interval and the number of\nparticipants. Experimental results validate our theoretical analysis, and\nreveal significant optimization opportunities through sparse attention and\nadaptive KV aggregation, highlighting FedAttn's potential to deliver\nscalability and efficiency in real-world edge deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are proliferating rapidly at the edge,\ndelivering intelligent capabilities across diverse application scenarios.\nHowever, their practical deployment in collaborative scenarios confronts\nfundamental challenges: privacy vulnerabilities, communication overhead, and\ncomputational bottlenecks. To address these, we propose Federated Attention\n(FedAttn), which integrates the federated paradigm into the self-attention\nmechanism, creating a new distributed LLM inference framework that\nsimultaneously achieves privacy protection, communication efficiency, and\ncomputational efficiency. FedAttn enables participants to perform local\nself-attention over their own token representations while periodically\nexchanging and aggregating Key-Value (KV) matrices across multiple Transformer\nblocks, collaboratively generating LLM responses without exposing private\nprompts. Further, we identify a structural duality between contextual\nrepresentation refinement in FedAttn and parameter optimization in FL across\nprivate data, local computation, and global aggregation. This key insight\nprovides a principled foundation for systematically porting federated\noptimization techniques to collaborative LLM inference. Building on this\nframework, we theoretically analyze how local self-attention computation within\nparticipants and heterogeneous token relevance among participants shape error\npropagation dynamics across Transformer blocks. Moreover, we characterize the\nfundamental trade-off between response quality and communication/computation\nefficiency, which is governed by the synchronization interval and the number of\nparticipants. Experimental results validate our theoretical analysis, and\nreveal significant optimization opportunities through sparse attention and\nadaptive KV aggregation, highlighting FedAttn's potential to deliver\nscalability and efficiency in real-world edge deployments."
                },
                "authors": [
                    {
                        "name": "Xiumei Deng"
                    },
                    {
                        "name": "Zehui Xiong"
                    },
                    {
                        "name": "Binbin Chen"
                    },
                    {
                        "name": "Dong In Kim"
                    },
                    {
                        "name": "Merouane Debbah"
                    },
                    {
                        "name": "H. Vincent Poor"
                    }
                ],
                "author_detail": {
                    "name": "H. Vincent Poor"
                },
                "author": "H. Vincent Poor",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02770v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02770v5",
                "updated": "2025-11-04T12:04:06Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    12,
                    4,
                    6,
                    1,
                    308,
                    0
                ],
                "published": "2025-02-04T23:26:10Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    23,
                    26,
                    10,
                    1,
                    35,
                    0
                ],
                "title": "Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$ Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$ Pruning"
                },
                "summary": "Leveraging attention sparsity to accelerate long-context large language\nmodels (LLMs) has been a hot research topic. However, current algorithms such\nas sparse attention or key-value (KV) cache compression tend to use a fixed\nbudget, which presents a significant challenge during deployment because it\nfails to account for the dynamic nature of real-world scenarios, where the\noptimal balance between accuracy and efficiency can vary greatly. In this\npaper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse\nattention can surprisingly achieve adaptive budgeting. Based on this, we\npropose Twilight, a framework to bring adaptive sparsity to any existing sparse\nattention algorithm without sacrificing their accuracy. Empirical results show\nthat Twilight can adaptively prune at most 98% of redundant tokens, leading to\n$15.4\\times$ acceleration in self-attention operations and $3.9\\times$\nacceleration in end-to-end per token latency in long context LLM decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging attention sparsity to accelerate long-context large language\nmodels (LLMs) has been a hot research topic. However, current algorithms such\nas sparse attention or key-value (KV) cache compression tend to use a fixed\nbudget, which presents a significant challenge during deployment because it\nfails to account for the dynamic nature of real-world scenarios, where the\noptimal balance between accuracy and efficiency can vary greatly. In this\npaper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse\nattention can surprisingly achieve adaptive budgeting. Based on this, we\npropose Twilight, a framework to bring adaptive sparsity to any existing sparse\nattention algorithm without sacrificing their accuracy. Empirical results show\nthat Twilight can adaptively prune at most 98% of redundant tokens, leading to\n$15.4\\times$ acceleration in self-attention operations and $3.9\\times$\nacceleration in end-to-end per token latency in long context LLM decoding."
                },
                "authors": [
                    {
                        "name": "Chaofan Lin"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Hanshuo Wang"
                    },
                    {
                        "name": "Tian Tang"
                    },
                    {
                        "name": "Boyu Tian"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Mingyu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Mingyu Gao"
                },
                "author": "Mingyu Gao",
                "arxiv_comment": "To appear on NeurIPS 2025 (spotlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02770v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02770v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02381v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02381v1",
                "updated": "2025-11-04T09:03:09Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    9,
                    3,
                    9,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T09:03:09Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    9,
                    3,
                    9,
                    1,
                    308,
                    0
                ],
                "title": "Laser diagnostics for negative ion source optimization: insights from\n  SPIDER at the ITER Neutral Beam Test Facility",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Laser diagnostics for negative ion source optimization: insights from\n  SPIDER at the ITER Neutral Beam Test Facility"
                },
                "summary": "The ITER Heating Neutral Beams (HNBs) require large, high-energy H/D atom\nbeams (285/330 A/m^2 extracted current density, and 1/0.87 MeV acceleration\nenergy, respectively for H and D). To address the associated challenges, the\nSPIDER negative ion RF beam source at the Neutral Beam Test Facility (NBTF) in\nPadova (Italy) serves as a full-scale source prototype with a 100 kV triode\naccelerator, for design validation and performance verification. SPIDER is\nequipped with two advanced laser diagnostics to monitor key plasma parameters;\nCavity Ring-Down Spectroscopy (CRDS) is used to measure H$^-$\\slash D$^-$ ion\ndensities, while Laser Absorption Spectroscopy (LAS) tracks caesium neutral\ndensity in the source. These measurements are essential for optimizing negative\nion production and meeting ITER source targets. We present diagnostic upgrade\ndetails, recent experimental results, and correlations with other machine\nparameters. Since CRDS relies on a single 4.637-meter-long optical cavity, the\nlongest used in such sources, it has demonstrated sensitivity to alignment.\nBased on recent experimental experience, structural improvements are being\nimplemented to enhance both stability and measurement reliability. LAS has\nmainly been employed as a tool to monitor the caesium conditioning status of\nSPIDER. Additionally, due to a distributed measurement over four lines of\nsight, LAS has proven effective in monitoring the caesium distribution within\nthe source. This work demonstrates the essential role of laser diagnostics in\ndeveloping ITER-relevant plasma sources and informs ongoing efforts to improve\nmeasurement accuracy in challenging environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ITER Heating Neutral Beams (HNBs) require large, high-energy H/D atom\nbeams (285/330 A/m^2 extracted current density, and 1/0.87 MeV acceleration\nenergy, respectively for H and D). To address the associated challenges, the\nSPIDER negative ion RF beam source at the Neutral Beam Test Facility (NBTF) in\nPadova (Italy) serves as a full-scale source prototype with a 100 kV triode\naccelerator, for design validation and performance verification. SPIDER is\nequipped with two advanced laser diagnostics to monitor key plasma parameters;\nCavity Ring-Down Spectroscopy (CRDS) is used to measure H$^-$\\slash D$^-$ ion\ndensities, while Laser Absorption Spectroscopy (LAS) tracks caesium neutral\ndensity in the source. These measurements are essential for optimizing negative\nion production and meeting ITER source targets. We present diagnostic upgrade\ndetails, recent experimental results, and correlations with other machine\nparameters. Since CRDS relies on a single 4.637-meter-long optical cavity, the\nlongest used in such sources, it has demonstrated sensitivity to alignment.\nBased on recent experimental experience, structural improvements are being\nimplemented to enhance both stability and measurement reliability. LAS has\nmainly been employed as a tool to monitor the caesium conditioning status of\nSPIDER. Additionally, due to a distributed measurement over four lines of\nsight, LAS has proven effective in monitoring the caesium distribution within\nthe source. This work demonstrates the essential role of laser diagnostics in\ndeveloping ITER-relevant plasma sources and informs ongoing efforts to improve\nmeasurement accuracy in challenging environments."
                },
                "authors": [
                    {
                        "name": "R. Agnello"
                    },
                    {
                        "name": "M. Barbisan"
                    },
                    {
                        "name": "R. Pasqualotto"
                    },
                    {
                        "name": "B. Pouradier-Duteil"
                    },
                    {
                        "name": "E. Sartori"
                    },
                    {
                        "name": "A. Tiso"
                    },
                    {
                        "name": "B. Zaniol"
                    }
                ],
                "author_detail": {
                    "name": "B. Zaniol"
                },
                "author": "B. Zaniol",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02381v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02381v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02230v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02230v1",
                "updated": "2025-11-04T03:43:05Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    3,
                    43,
                    5,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T03:43:05Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    3,
                    43,
                    5,
                    1,
                    308,
                    0
                ],
                "title": "Continuum: Efficient and Robust Multi-Turn LLM Agent Scheduling with KV\n  Cache Time-to-Live",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuum: Efficient and Robust Multi-Turn LLM Agent Scheduling with KV\n  Cache Time-to-Live"
                },
                "summary": "Agentic LLM applications interleave LLM generation requests with tool calls.\nThese tool calls break the continuity of the workflow by creating pauses\nbetween LLM requests, bringing many challenges for the serving system,\nespecially under multi-turn scenarios. Each pause potentially causes KV cache\neviction and extra waiting time before entering the continuous batch for the\nfollowing LLM request. Since these pauses happen for each call, this problem\nbecomes increasingly severe as turn number grow for agentic programs. Previous\nworks either fail to incorporate information from the tool call, evicting KV\ncache that leads to repetitive prefill or loading, or ignore the continuity of\na multi-turn program, creating waiting time between turns that increases\nper-request latency.\n  We present Continuum, a serving system to optimize job completion time for\nmulti-turn agent workloads by combining tool-aware KV cache timeout with\nprogram-level scheduling. By predicting tool call durations in agentic\nworkflows, Continuum selectively pins the KV cache in GPU memory with a\ntime-to-live value based on total turn number. When combined with program-level\nfirst-come-first-serve, Continuum prevents scheduling bubbles, preserves\nmulti-turn continuity, and optimizes for throughput for complex agentic\nworkflows. By modeling the variability of tool call and agent program\ncontinuity, Continuum outperforms state-of-the-art baselines. Our evaluation on\nreal-world agentic workloads (SWE-Bench and BFCL) with Llama-3.1 8B/70B models\nshows that Continuum significantly improves the average job completion times,\nand remains performant across different hardware setups and DRAM offloading\nschemes. Preview code is available at:\nhttps://github.com/Hanchenli/vllm-continuum",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic LLM applications interleave LLM generation requests with tool calls.\nThese tool calls break the continuity of the workflow by creating pauses\nbetween LLM requests, bringing many challenges for the serving system,\nespecially under multi-turn scenarios. Each pause potentially causes KV cache\neviction and extra waiting time before entering the continuous batch for the\nfollowing LLM request. Since these pauses happen for each call, this problem\nbecomes increasingly severe as turn number grow for agentic programs. Previous\nworks either fail to incorporate information from the tool call, evicting KV\ncache that leads to repetitive prefill or loading, or ignore the continuity of\na multi-turn program, creating waiting time between turns that increases\nper-request latency.\n  We present Continuum, a serving system to optimize job completion time for\nmulti-turn agent workloads by combining tool-aware KV cache timeout with\nprogram-level scheduling. By predicting tool call durations in agentic\nworkflows, Continuum selectively pins the KV cache in GPU memory with a\ntime-to-live value based on total turn number. When combined with program-level\nfirst-come-first-serve, Continuum prevents scheduling bubbles, preserves\nmulti-turn continuity, and optimizes for throughput for complex agentic\nworkflows. By modeling the variability of tool call and agent program\ncontinuity, Continuum outperforms state-of-the-art baselines. Our evaluation on\nreal-world agentic workloads (SWE-Bench and BFCL) with Llama-3.1 8B/70B models\nshows that Continuum significantly improves the average job completion times,\nand remains performant across different hardware setups and DRAM offloading\nschemes. Preview code is available at:\nhttps://github.com/Hanchenli/vllm-continuum"
                },
                "authors": [
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Qiuyang Mang"
                    },
                    {
                        "name": "Runyuan He"
                    },
                    {
                        "name": "Qizheng Zhang"
                    },
                    {
                        "name": "Huanzhi Mao"
                    },
                    {
                        "name": "Xiaokun Chen"
                    },
                    {
                        "name": "Alvin Cheung"
                    },
                    {
                        "name": "Joseph Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02230v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02230v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02132v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02132v1",
                "updated": "2025-11-03T23:48:39Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    23,
                    48,
                    39,
                    0,
                    307,
                    0
                ],
                "published": "2025-11-03T23:48:39Z",
                "published_parsed": [
                    2025,
                    11,
                    3,
                    23,
                    48,
                    39,
                    0,
                    307,
                    0
                ],
                "title": "Optimizing Attention on GPUs by Exploiting GPU Architectural NUMA\n  Effects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Attention on GPUs by Exploiting GPU Architectural NUMA\n  Effects"
                },
                "summary": "The rise of disaggregated AI GPUs has exposed a critical bottleneck in\nlarge-scale attention workloads: non-uniform memory access (NUMA). As\nmulti-chiplet designs become the norm for scaling compute capabilities, memory\nlatency and bandwidth vary sharply across compute regions, undermining the\nperformance of traditional GPU kernel scheduling strategies that assume uniform\nmemory access. We identify how these NUMA effects distort locality in\nmulti-head attention (MHA) and present Swizzled Head-first Mapping, a\nspatially-aware scheduling strategy that aligns attention heads with GPU NUMA\ndomains to exploit intra-chiplet cache reuse. On AMD's MI300X architecture, our\nmethod achieves up to 50% higher performance over state-of-the-art attention\nalgorithms using conventional scheduling techniques and sustains consistently\nhigh L2 cache hit rates of 80-97%. These results demonstrate that NUMA-aware\nscheduling is now fundamental to achieving full efficiency on next-generation\ndisaggregated GPUs, offering a path forward for scalable AI training and\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of disaggregated AI GPUs has exposed a critical bottleneck in\nlarge-scale attention workloads: non-uniform memory access (NUMA). As\nmulti-chiplet designs become the norm for scaling compute capabilities, memory\nlatency and bandwidth vary sharply across compute regions, undermining the\nperformance of traditional GPU kernel scheduling strategies that assume uniform\nmemory access. We identify how these NUMA effects distort locality in\nmulti-head attention (MHA) and present Swizzled Head-first Mapping, a\nspatially-aware scheduling strategy that aligns attention heads with GPU NUMA\ndomains to exploit intra-chiplet cache reuse. On AMD's MI300X architecture, our\nmethod achieves up to 50% higher performance over state-of-the-art attention\nalgorithms using conventional scheduling techniques and sustains consistently\nhigh L2 cache hit rates of 80-97%. These results demonstrate that NUMA-aware\nscheduling is now fundamental to achieving full efficiency on next-generation\ndisaggregated GPUs, offering a path forward for scalable AI training and\ninference."
                },
                "authors": [
                    {
                        "name": "Mansi Choudhary"
                    },
                    {
                        "name": "Karthik Sangaiah"
                    },
                    {
                        "name": "Sonali Singh"
                    },
                    {
                        "name": "Muhammad Osama"
                    },
                    {
                        "name": "Lisa Wu Wills"
                    },
                    {
                        "name": "Ganesh Dasika"
                    }
                ],
                "author_detail": {
                    "name": "Ganesh Dasika"
                },
                "author": "Ganesh Dasika",
                "arxiv_comment": "11 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02132v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02132v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.01815v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.01815v1",
                "updated": "2025-11-03T18:20:35Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    18,
                    20,
                    35,
                    0,
                    307,
                    0
                ],
                "published": "2025-11-03T18:20:35Z",
                "published_parsed": [
                    2025,
                    11,
                    3,
                    18,
                    20,
                    35,
                    0,
                    307,
                    0
                ],
                "title": "KV Cache Transform Coding for Compact Storage in LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Cache Transform Coding for Compact Storage in LLM Inference"
                },
                "summary": "Serving large language models (LLMs) at scale necessitates efficient\nkey-value (KV) cache management. KV caches can be reused across conversation\nturns via shared-prefix prompts that are common in iterative code editing and\nchat. However, stale caches consume scarce GPU memory, require offloading, or\nforce recomputation. We present KVTC, a lightweight transform coder that\ncompresses KV caches for compact on-GPU and off-GPU storage. Drawing on\nclassical media compression, KVTC combines PCA-based feature decorrelation,\nadaptive quantization, and entropy coding. It requires only a brief initial\ncalibration and leaves model parameters unchanged. By exploiting redundancies\nin KV caches, KVTC achieves up to 20$\\times$ compression while maintaining\nreasoning and long-context accuracy, and 40$\\times$ or higher for specific use\ncases. We test KVTC with Llama 3, Mistral NeMo, and R1-Qwen 2.5 models across\nbenchmarks including AIME25, LiveCodeBench, GSM8K, MMLU, Qasper, RULER, and\nMATH-500. It consistently outperforms inference-time baselines such as token\neviction, quantization, and SVD-based methods, while achieving higher\ncompression ratios. These results support KVTC as a practical building block\nfor memory-efficient LLM serving with reusable KV caches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving large language models (LLMs) at scale necessitates efficient\nkey-value (KV) cache management. KV caches can be reused across conversation\nturns via shared-prefix prompts that are common in iterative code editing and\nchat. However, stale caches consume scarce GPU memory, require offloading, or\nforce recomputation. We present KVTC, a lightweight transform coder that\ncompresses KV caches for compact on-GPU and off-GPU storage. Drawing on\nclassical media compression, KVTC combines PCA-based feature decorrelation,\nadaptive quantization, and entropy coding. It requires only a brief initial\ncalibration and leaves model parameters unchanged. By exploiting redundancies\nin KV caches, KVTC achieves up to 20$\\times$ compression while maintaining\nreasoning and long-context accuracy, and 40$\\times$ or higher for specific use\ncases. We test KVTC with Llama 3, Mistral NeMo, and R1-Qwen 2.5 models across\nbenchmarks including AIME25, LiveCodeBench, GSM8K, MMLU, Qasper, RULER, and\nMATH-500. It consistently outperforms inference-time baselines such as token\neviction, quantization, and SVD-based methods, while achieving higher\ncompression ratios. These results support KVTC as a practical building block\nfor memory-efficient LLM serving with reusable KV caches."
                },
                "authors": [
                    {
                        "name": "Konrad Staniszewski"
                    },
                    {
                        "name": "Adrian Łańcucki"
                    }
                ],
                "author_detail": {
                    "name": "Adrian Łańcucki"
                },
                "author": "Adrian Łańcucki",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.01815v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.01815v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.01633v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.01633v1",
                "updated": "2025-11-03T14:42:53Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    14,
                    42,
                    53,
                    0,
                    307,
                    0
                ],
                "published": "2025-11-03T14:42:53Z",
                "published_parsed": [
                    2025,
                    11,
                    3,
                    14,
                    42,
                    53,
                    0,
                    307,
                    0
                ],
                "title": "Scaling Graph Chain-of-Thought Reasoning: A Multi-Agent Framework with\n  Efficient LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Graph Chain-of-Thought Reasoning: A Multi-Agent Framework with\n  Efficient LLM Serving"
                },
                "summary": "Graph Chain-of-Thought (Graph-CoT) enables large language models (LLMs) to\nperform step-by-step reasoning over graph-structured knowledge, but existing\npipelines suffer from low accuracy, excessive token usage, high latency, and\nlow throughput due to single-agent monolithic prompts, repeated context\nre-encoding, and inefficient serving execution. We present GLM, the first\nmulti-agent Graph-CoT system co-designed with an optimized LLM serving\narchitecture. GLM decomposes reasoning into specialized agents for\nclassification, reasoning, action generation, and graph retrieval, enabling\nbranching and selective context sharing to reduce prompt length and reasoning\niterations while preserving reasoning quality, thereby improving accuracy and\nreducing overall token consumption. To scale inference, we introduce a\nGraph-CoT-aware LLM inference mechanism with graph-specific KV-cache\nmanagement, priority-based eviction, and pipelined execution to improve serving\nefficiency. Experiments demonstrate that GLM improves answer accuracy by up to\n38%, reduces token cost by up to 95.7%, lowers inference latency by 90.3%, and\nachieves up to 15.1x higher throughput compared to state-of-the-art Graph-CoT\nbaselines, enabling efficient adoption for complex real-world reasoning at\nscale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Chain-of-Thought (Graph-CoT) enables large language models (LLMs) to\nperform step-by-step reasoning over graph-structured knowledge, but existing\npipelines suffer from low accuracy, excessive token usage, high latency, and\nlow throughput due to single-agent monolithic prompts, repeated context\nre-encoding, and inefficient serving execution. We present GLM, the first\nmulti-agent Graph-CoT system co-designed with an optimized LLM serving\narchitecture. GLM decomposes reasoning into specialized agents for\nclassification, reasoning, action generation, and graph retrieval, enabling\nbranching and selective context sharing to reduce prompt length and reasoning\niterations while preserving reasoning quality, thereby improving accuracy and\nreducing overall token consumption. To scale inference, we introduce a\nGraph-CoT-aware LLM inference mechanism with graph-specific KV-cache\nmanagement, priority-based eviction, and pipelined execution to improve serving\nefficiency. Experiments demonstrate that GLM improves answer accuracy by up to\n38%, reduces token cost by up to 95.7%, lowers inference latency by 90.3%, and\nachieves up to 15.1x higher throughput compared to state-of-the-art Graph-CoT\nbaselines, enabling efficient adoption for complex real-world reasoning at\nscale."
                },
                "authors": [
                    {
                        "name": "Chengying Huan"
                    },
                    {
                        "name": "Ziheng Meng"
                    },
                    {
                        "name": "Yongchao Liu"
                    },
                    {
                        "name": "Zhengyi Yang"
                    },
                    {
                        "name": "Yun Zhu"
                    },
                    {
                        "name": "Yue Yun"
                    },
                    {
                        "name": "Shipeng Li"
                    },
                    {
                        "name": "Rong Gu"
                    },
                    {
                        "name": "Xiabao Wu"
                    },
                    {
                        "name": "Haitao Zhang"
                    },
                    {
                        "name": "Chuntao Hong"
                    },
                    {
                        "name": "Shaonan Ma"
                    },
                    {
                        "name": "Guihai Chen"
                    },
                    {
                        "name": "Chen Tian"
                    }
                ],
                "author_detail": {
                    "name": "Chen Tian"
                },
                "author": "Chen Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.01633v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.01633v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21590v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21590v2",
                "updated": "2025-11-03T11:32:13Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    11,
                    32,
                    13,
                    0,
                    307,
                    0
                ],
                "published": "2025-06-18T05:07:47Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    5,
                    7,
                    47,
                    2,
                    169,
                    0
                ],
                "title": "Representation Consistency for Accurate and Coherent LLM Answer\n  Aggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Representation Consistency for Accurate and Coherent LLM Answer\n  Aggregation"
                },
                "summary": "Test-time scaling improves large language models' (LLMs) performance by\nallocating more compute budget during inference. To achieve this, existing\nmethods often require intricate modifications to prompting and sampling\nstrategies. In this work, we introduce representation consistency (RC), a\ntest-time scaling method for aggregating answers drawn from multiple candidate\nresponses of an LLM regardless of how they were generated, including variations\nin prompt phrasing and sampling strategy. RC enhances answer aggregation by not\nonly considering the number of occurrences of each answer in the candidate\nresponse set, but also the consistency of the model's internal activations\nwhile generating the set of responses leading to each answer. These activations\ncan be either dense (raw model activations) or sparse (encoded via pretrained\nsparse autoencoders). Our rationale is that if the model's representations of\nmultiple responses converging on the same answer are highly variable, this\nanswer is more likely to be the result of incoherent reasoning and should be\ndown-weighted during aggregation. Importantly, our method only uses cached\nactivations and lightweight similarity computations and requires no additional\nmodel queries. Through experiments with four open-source LLMs and four\nreasoning datasets, we validate the effectiveness of RC for improving task\nperformance during inference, with consistent accuracy improvements (up to 4%)\nover strong test-time scaling baselines. We also show that consistency in the\nsparse activation signals aligns well with the common notion of coherent\nreasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling improves large language models' (LLMs) performance by\nallocating more compute budget during inference. To achieve this, existing\nmethods often require intricate modifications to prompting and sampling\nstrategies. In this work, we introduce representation consistency (RC), a\ntest-time scaling method for aggregating answers drawn from multiple candidate\nresponses of an LLM regardless of how they were generated, including variations\nin prompt phrasing and sampling strategy. RC enhances answer aggregation by not\nonly considering the number of occurrences of each answer in the candidate\nresponse set, but also the consistency of the model's internal activations\nwhile generating the set of responses leading to each answer. These activations\ncan be either dense (raw model activations) or sparse (encoded via pretrained\nsparse autoencoders). Our rationale is that if the model's representations of\nmultiple responses converging on the same answer are highly variable, this\nanswer is more likely to be the result of incoherent reasoning and should be\ndown-weighted during aggregation. Importantly, our method only uses cached\nactivations and lightweight similarity computations and requires no additional\nmodel queries. Through experiments with four open-source LLMs and four\nreasoning datasets, we validate the effectiveness of RC for improving task\nperformance during inference, with consistent accuracy improvements (up to 4%)\nover strong test-time scaling baselines. We also show that consistency in the\nsparse activation signals aligns well with the common notion of coherent\nreasoning."
                },
                "authors": [
                    {
                        "name": "Junqi Jiang"
                    },
                    {
                        "name": "Tom Bewley"
                    },
                    {
                        "name": "Salim I. Amoukou"
                    },
                    {
                        "name": "Francesco Leofante"
                    },
                    {
                        "name": "Antonio Rago"
                    },
                    {
                        "name": "Saumitra Mishra"
                    },
                    {
                        "name": "Francesca Toni"
                    }
                ],
                "author_detail": {
                    "name": "Francesca Toni"
                },
                "author": "Francesca Toni",
                "arxiv_comment": "Accepted at NeurIPS 2025. Camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21590v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21590v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.01385v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.01385v1",
                "updated": "2025-11-03T09:36:11Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    9,
                    36,
                    11,
                    0,
                    307,
                    0
                ],
                "published": "2025-11-03T09:36:11Z",
                "published_parsed": [
                    2025,
                    11,
                    3,
                    9,
                    36,
                    11,
                    0,
                    307,
                    0
                ],
                "title": "Memory-Efficient Training with In-Place FFT Implementation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-Efficient Training with In-Place FFT Implementation"
                },
                "summary": "Fast Fourier Transforms (FFT) are widely used to reduce memory and\ncomputational costs in deep learning. However, existing implementations,\nincluding standard FFT and real FFT (rFFT), cannot achieve true in-place\ncomputation. In particular, rFFT maps an input of size n to a complex output of\nsize n/2+1, causing dimensional mismatch and requiring additional memory\nallocation. We propose the first real-domain, fully in-place FFT framework\n(rdFFT) that preserves input-output memory space consistency. By leveraging\nbutterfly operation symmetry and conjugate properties in the frequency domain,\nwe design an implicit complex encoding scheme that eliminates intermediate\ncache usage entirely. Experiments on multiple natural language understanding\ntasks demonstrate the method effectiveness in reducing training memory cost,\noffering a promising direction for frequency-domain lightweight adaptation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Fourier Transforms (FFT) are widely used to reduce memory and\ncomputational costs in deep learning. However, existing implementations,\nincluding standard FFT and real FFT (rFFT), cannot achieve true in-place\ncomputation. In particular, rFFT maps an input of size n to a complex output of\nsize n/2+1, causing dimensional mismatch and requiring additional memory\nallocation. We propose the first real-domain, fully in-place FFT framework\n(rdFFT) that preserves input-output memory space consistency. By leveraging\nbutterfly operation symmetry and conjugate properties in the frequency domain,\nwe design an implicit complex encoding scheme that eliminates intermediate\ncache usage entirely. Experiments on multiple natural language understanding\ntasks demonstrate the method effectiveness in reducing training memory cost,\noffering a promising direction for frequency-domain lightweight adaptation."
                },
                "authors": [
                    {
                        "name": "Xinyu Ding"
                    },
                    {
                        "name": "Bangtian Liu"
                    },
                    {
                        "name": "Siyu Liao"
                    },
                    {
                        "name": "Zhongfeng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhongfeng Wang"
                },
                "author": "Zhongfeng Wang",
                "arxiv_comment": "Accepted at NeurIPS 2025. Presents a real-domain in-place FFT (rdFFT)\n  operator for memory-efficient fine-tuning of large language models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.01385v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.01385v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; G.1.2; D.1.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.01266v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.01266v1",
                "updated": "2025-11-03T06:37:53Z",
                "updated_parsed": [
                    2025,
                    11,
                    3,
                    6,
                    37,
                    53,
                    0,
                    307,
                    0
                ],
                "published": "2025-11-03T06:37:53Z",
                "published_parsed": [
                    2025,
                    11,
                    3,
                    6,
                    37,
                    53,
                    0,
                    307,
                    0
                ],
                "title": "MotionStream: Real-Time Video Generation with Interactive Motion\n  Controls",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MotionStream: Real-Time Video Generation with Interactive Motion\n  Controls"
                },
                "summary": "Current motion-conditioned video generation methods suffer from prohibitive\nlatency (minutes per video) and non-causal processing that prevents real-time\ninteraction. We present MotionStream, enabling sub-second latency with up to 29\nFPS streaming generation on a single GPU. Our approach begins by augmenting a\ntext-to-video model with motion control, which generates high-quality videos\nthat adhere to the global text prompt and local motion guidance, but does not\nperform inference on the fly. As such, we distill this bidirectional teacher\ninto a causal student through Self Forcing with Distribution Matching\nDistillation, enabling real-time streaming inference. Several key challenges\narise when generating videos of long, potentially infinite time-horizons: (1)\nbridging the domain gap from training on finite length and extrapolating to\ninfinite horizons, (2) sustaining high quality by preventing error\naccumulation, and (3) maintaining fast inference, without incurring growth in\ncomputational cost due to increasing context windows. A key to our approach is\nintroducing carefully designed sliding-window causal attention, combined with\nattention sinks. By incorporating self-rollout with attention sinks and KV\ncache rolling during training, we properly simulate inference-time\nextrapolations with a fixed context window, enabling constant-speed generation\nof arbitrarily long videos. Our models achieve state-of-the-art results in\nmotion following and video quality while being two orders of magnitude faster,\nuniquely enabling infinite-length streaming. With MotionStream, users can paint\ntrajectories, control cameras, or transfer motion, and see results unfold in\nreal-time, delivering a truly interactive experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current motion-conditioned video generation methods suffer from prohibitive\nlatency (minutes per video) and non-causal processing that prevents real-time\ninteraction. We present MotionStream, enabling sub-second latency with up to 29\nFPS streaming generation on a single GPU. Our approach begins by augmenting a\ntext-to-video model with motion control, which generates high-quality videos\nthat adhere to the global text prompt and local motion guidance, but does not\nperform inference on the fly. As such, we distill this bidirectional teacher\ninto a causal student through Self Forcing with Distribution Matching\nDistillation, enabling real-time streaming inference. Several key challenges\narise when generating videos of long, potentially infinite time-horizons: (1)\nbridging the domain gap from training on finite length and extrapolating to\ninfinite horizons, (2) sustaining high quality by preventing error\naccumulation, and (3) maintaining fast inference, without incurring growth in\ncomputational cost due to increasing context windows. A key to our approach is\nintroducing carefully designed sliding-window causal attention, combined with\nattention sinks. By incorporating self-rollout with attention sinks and KV\ncache rolling during training, we properly simulate inference-time\nextrapolations with a fixed context window, enabling constant-speed generation\nof arbitrarily long videos. Our models achieve state-of-the-art results in\nmotion following and video quality while being two orders of magnitude faster,\nuniquely enabling infinite-length streaming. With MotionStream, users can paint\ntrajectories, control cameras, or transfer motion, and see results unfold in\nreal-time, delivering a truly interactive experience."
                },
                "authors": [
                    {
                        "name": "Joonghyuk Shin"
                    },
                    {
                        "name": "Zhengqi Li"
                    },
                    {
                        "name": "Richard Zhang"
                    },
                    {
                        "name": "Jun-Yan Zhu"
                    },
                    {
                        "name": "Jaesik Park"
                    },
                    {
                        "name": "Eli Schechtman"
                    },
                    {
                        "name": "Xun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xun Huang"
                },
                "author": "Xun Huang",
                "arxiv_comment": "Project webpage: https://joonghyuk.com/motionstream-web/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.01266v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.01266v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13544v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13544v3",
                "updated": "2025-11-02T20:27:27Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    20,
                    27,
                    27,
                    6,
                    306,
                    0
                ],
                "published": "2025-05-19T02:09:41Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    2,
                    9,
                    41,
                    0,
                    139,
                    0
                ],
                "title": "Multi-head Temporal Latent Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head Temporal Latent Attention"
                },
                "summary": "While Transformer self-attention offers strong parallelism, the Key-Value\n(KV) cache grows linearly with sequence length and becomes a bottleneck for\ninference efficiency. Multi-head latent attention was recently developed to\ncompress the KV cache into a low-rank latent space. This paper proposes\nMulti-head Temporal Latent Attention (MTLA), which further reduces the KV cache\nsize along the temporal dimension, greatly lowering the memory footprint of\nself-attention inference. MTLA employs a hyper-network to dynamically merge\ntemporally adjacent KV cache vectors. To address the mismatch between the\ncompressed KV cache and processed sequence lengths, a stride-aware causal mask\nis proposed to ensure efficient parallel training and consistency with\ninference behaviour. Experiments across tasks, including speech translation,\nspeech recognition, speech understanding and text summarisation, demonstrate\nthat MTLA achieves competitive performance compared to standard Multi-Head\nAttention (MHA), while greatly improving inference speed and GPU memory usage.\nFor example, on a English-German speech translation task, MTLA achieves a 5.3x\nspeedup and a reduction in GPU memory usage by a factor of 8.3 compared to MHA,\nwhile maintaining translation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformer self-attention offers strong parallelism, the Key-Value\n(KV) cache grows linearly with sequence length and becomes a bottleneck for\ninference efficiency. Multi-head latent attention was recently developed to\ncompress the KV cache into a low-rank latent space. This paper proposes\nMulti-head Temporal Latent Attention (MTLA), which further reduces the KV cache\nsize along the temporal dimension, greatly lowering the memory footprint of\nself-attention inference. MTLA employs a hyper-network to dynamically merge\ntemporally adjacent KV cache vectors. To address the mismatch between the\ncompressed KV cache and processed sequence lengths, a stride-aware causal mask\nis proposed to ensure efficient parallel training and consistency with\ninference behaviour. Experiments across tasks, including speech translation,\nspeech recognition, speech understanding and text summarisation, demonstrate\nthat MTLA achieves competitive performance compared to standard Multi-Head\nAttention (MHA), while greatly improving inference speed and GPU memory usage.\nFor example, on a English-German speech translation task, MTLA achieves a 5.3x\nspeedup and a reduction in GPU memory usage by a factor of 8.3 compared to MHA,\nwhile maintaining translation quality."
                },
                "authors": [
                    {
                        "name": "Keqi Deng"
                    },
                    {
                        "name": "Philip C. Woodland"
                    }
                ],
                "author_detail": {
                    "name": "Philip C. Woodland"
                },
                "author": "Philip C. Woodland",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13544v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13544v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.00868v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.00868v1",
                "updated": "2025-11-02T09:33:12Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    9,
                    33,
                    12,
                    6,
                    306,
                    0
                ],
                "published": "2025-11-02T09:33:12Z",
                "published_parsed": [
                    2025,
                    11,
                    2,
                    9,
                    33,
                    12,
                    6,
                    306,
                    0
                ],
                "title": "FlexiCache: Leveraging Temporal Stability of Attention Heads for\n  Efficient KV Cache Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexiCache: Leveraging Temporal Stability of Attention Heads for\n  Efficient KV Cache Management"
                },
                "summary": "Large Language Model (LLM) serving is increasingly constrained by the growing\nsize of the key-value (KV) cache, which scales with both context length and\ngeneration length. Prior work shows that attention is dominated by a small\nsubset of critical tokens, yet existing systems struggle to exploit this\nefficiently without degrading accuracy, especially in long generation. We make\na key observation: the temporal stability of these critical tokens varies\nsignificantly across KV heads: some heads consistently focus on the same\ntokens, while others shift frequently. Building on this insight, we introduce\nFlexiCache, a hierarchical KV-cache management system that leverages the\ntemporal stability of KV heads to reduce GPU memory usage and computation\noverhead, while preserving model accuracy. FlexiCache classifies KV heads as\nstable or unstable: it retains all KV-cache pages from unstable heads in GPU\nmemory, whereas for stable heads, it keeps only the top-K pages on the GPU and\noffloads the rest to host memory. By exploiting temporal stability, FlexiCache\nperforms periodic reranking for stable heads to fetch newly promoted top pages.\nImplemented atop vLLM, FlexiCache reduces GPU memory footprint for long-context\nrequests by up to 70%, improves offline serving throughput by 1.38-1.55x, and\nlowers online token latency by 1.6-2.1x, all while maintaining accuracy in\nlong-context, long-generation scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) serving is increasingly constrained by the growing\nsize of the key-value (KV) cache, which scales with both context length and\ngeneration length. Prior work shows that attention is dominated by a small\nsubset of critical tokens, yet existing systems struggle to exploit this\nefficiently without degrading accuracy, especially in long generation. We make\na key observation: the temporal stability of these critical tokens varies\nsignificantly across KV heads: some heads consistently focus on the same\ntokens, while others shift frequently. Building on this insight, we introduce\nFlexiCache, a hierarchical KV-cache management system that leverages the\ntemporal stability of KV heads to reduce GPU memory usage and computation\noverhead, while preserving model accuracy. FlexiCache classifies KV heads as\nstable or unstable: it retains all KV-cache pages from unstable heads in GPU\nmemory, whereas for stable heads, it keeps only the top-K pages on the GPU and\noffloads the rest to host memory. By exploiting temporal stability, FlexiCache\nperforms periodic reranking for stable heads to fetch newly promoted top pages.\nImplemented atop vLLM, FlexiCache reduces GPU memory footprint for long-context\nrequests by up to 70%, improves offline serving throughput by 1.38-1.55x, and\nlowers online token latency by 1.6-2.1x, all while maintaining accuracy in\nlong-context, long-generation scenarios."
                },
                "authors": [
                    {
                        "name": "Nazmul Takbir"
                    },
                    {
                        "name": "Hamidreza Alikhani"
                    },
                    {
                        "name": "Nikil Dutt"
                    },
                    {
                        "name": "Sangeetha Abdu Jyothi"
                    }
                ],
                "author_detail": {
                    "name": "Sangeetha Abdu Jyothi"
                },
                "author": "Sangeetha Abdu Jyothi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.00868v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.00868v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.00819v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.00819v1",
                "updated": "2025-11-02T06:15:14Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    6,
                    15,
                    14,
                    6,
                    306,
                    0
                ],
                "published": "2025-11-02T06:15:14Z",
                "published_parsed": [
                    2025,
                    11,
                    2,
                    6,
                    15,
                    14,
                    6,
                    306,
                    0
                ],
                "title": "Optimizing Native Sparse Attention with Latent Attention and Local\n  Global Alternating Strategies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Native Sparse Attention with Latent Attention and Local\n  Global Alternating Strategies"
                },
                "summary": "In this work, we conduct a systematic analysis of Native Sparse Attention\n(NSA) and propose targeted improvements that enhance long-context modeling. A\nkey insight is that alternating between local (sliding-window) and global\n(compression, selective) attention across layers, rather than using fixed\npatterns, enables more effective propagation of long-range dependencies and\nsubstantially boosts performance on long-sequence tasks. Meanwhile, we further\nrefine NSA's branches with Latent Attention that the sliding-window branch is\nenhanced with Multi-head Latent Attention (MLA) while compression and selective\nbranches adopt Group-head Latent Attention (GLA). These changes reduce KV-cache\nmemory by 50\\% versus NSA while improving the model's common-sense reasoning\nand long-text understanding capabilities. Experiments on models from 340M to\n1.3B parameters (trained on 15B and 100B tokens) show our method matches or\nexceeds full attention and native sparse attention in both common-sense\nreasoning and long-context understanding tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we conduct a systematic analysis of Native Sparse Attention\n(NSA) and propose targeted improvements that enhance long-context modeling. A\nkey insight is that alternating between local (sliding-window) and global\n(compression, selective) attention across layers, rather than using fixed\npatterns, enables more effective propagation of long-range dependencies and\nsubstantially boosts performance on long-sequence tasks. Meanwhile, we further\nrefine NSA's branches with Latent Attention that the sliding-window branch is\nenhanced with Multi-head Latent Attention (MLA) while compression and selective\nbranches adopt Group-head Latent Attention (GLA). These changes reduce KV-cache\nmemory by 50\\% versus NSA while improving the model's common-sense reasoning\nand long-text understanding capabilities. Experiments on models from 340M to\n1.3B parameters (trained on 15B and 100B tokens) show our method matches or\nexceeds full attention and native sparse attention in both common-sense\nreasoning and long-context understanding tasks."
                },
                "authors": [
                    {
                        "name": "Yuxuan Hu"
                    },
                    {
                        "name": "Jianchao Tan"
                    },
                    {
                        "name": "Jiaqi Zhang"
                    },
                    {
                        "name": "Wen Zan"
                    },
                    {
                        "name": "Pingwei Sun"
                    },
                    {
                        "name": "Yifan Lu"
                    },
                    {
                        "name": "Yerui Sun"
                    },
                    {
                        "name": "Yuchen Xie"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Jing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jing Zhang"
                },
                "author": "Jing Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.00819v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.00819v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.00745v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.00745v1",
                "updated": "2025-11-02T00:04:54Z",
                "updated_parsed": [
                    2025,
                    11,
                    2,
                    0,
                    4,
                    54,
                    6,
                    306,
                    0
                ],
                "published": "2025-11-02T00:04:54Z",
                "published_parsed": [
                    2025,
                    11,
                    2,
                    0,
                    4,
                    54,
                    6,
                    306,
                    0
                ],
                "title": "High-Power Dual-Channel Field Chamber for High-Frequency Magnetic\n  Neuromodulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Power Dual-Channel Field Chamber for High-Frequency Magnetic\n  Neuromodulation"
                },
                "summary": "Several novel methods, including magnetogenetics and magnetoelectric\nstimulation, use high frequency alternating magnetic fields to precisely\nmanipulate neural activity. To quantify the behavioral effects of such\ninterventions in a freely moving mouse, we developed a dual-channel magnetic\nchamber, specifically designed for rate-sensitive magnetothermal-genetic\nstimulation, and adaptable for other uses of alternating magnetic fields.\nThrough an optimized coil design, the system allows independent control of two\nspatially orthogonal uniform magnetic fields delivered at different frequencies\nwithin a 10 cm x 10 cm x 6 cm chamber. The two channels have nominal\nfrequencies of 50 and 550 kHz with peak magnetic field strengths of 88 and 12.5\nmT, achieved with resonant coil drives having peak voltages of 1.6 and 1.8 kV\nand currents of 1.0 and 0.26 kA, respectively. Additionally, a liquid cooling\nsystem enables magnetic field generation for second-level duration, and an\nobservation port and camera allow video capture of the animal's behavior within\nthe chamber. The system generates high-amplitude magnetic fields across two\nwidely separated frequency channels with negligible interference (< 1%).\nRelatively uniform magnetic field distribution (+/-10% across 94% of the\nchamber volume) is maintained throughout the chamber, and temperature increase\nof the inner side of the coil enclosure during the operation is limited to <\n0.35 {\\deg}C/s to ensure in vivo safety. Using cobalt-doped and undoped iron\noxide nanoparticles, we demonstrate channel-specific heating rates of 3.5\n{\\deg}C/s and 1.5 {\\deg}C/s, respectively, validating frequency-selectivity.\nBoth channels can run continuously for four seconds stably.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several novel methods, including magnetogenetics and magnetoelectric\nstimulation, use high frequency alternating magnetic fields to precisely\nmanipulate neural activity. To quantify the behavioral effects of such\ninterventions in a freely moving mouse, we developed a dual-channel magnetic\nchamber, specifically designed for rate-sensitive magnetothermal-genetic\nstimulation, and adaptable for other uses of alternating magnetic fields.\nThrough an optimized coil design, the system allows independent control of two\nspatially orthogonal uniform magnetic fields delivered at different frequencies\nwithin a 10 cm x 10 cm x 6 cm chamber. The two channels have nominal\nfrequencies of 50 and 550 kHz with peak magnetic field strengths of 88 and 12.5\nmT, achieved with resonant coil drives having peak voltages of 1.6 and 1.8 kV\nand currents of 1.0 and 0.26 kA, respectively. Additionally, a liquid cooling\nsystem enables magnetic field generation for second-level duration, and an\nobservation port and camera allow video capture of the animal's behavior within\nthe chamber. The system generates high-amplitude magnetic fields across two\nwidely separated frequency channels with negligible interference (< 1%).\nRelatively uniform magnetic field distribution (+/-10% across 94% of the\nchamber volume) is maintained throughout the chamber, and temperature increase\nof the inner side of the coil enclosure during the operation is limited to <\n0.35 {\\deg}C/s to ensure in vivo safety. Using cobalt-doped and undoped iron\noxide nanoparticles, we demonstrate channel-specific heating rates of 3.5\n{\\deg}C/s and 1.5 {\\deg}C/s, respectively, validating frequency-selectivity.\nBoth channels can run continuously for four seconds stably."
                },
                "authors": [
                    {
                        "name": "Xiaoyang Tian"
                    },
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Boshuo Wang"
                    },
                    {
                        "name": "Jinshui Zhang"
                    },
                    {
                        "name": "Dong Yan"
                    },
                    {
                        "name": "Jeannette Ingabire"
                    },
                    {
                        "name": "Samantha Coffler"
                    },
                    {
                        "name": "Guillaume Duret"
                    },
                    {
                        "name": "Quoc-Khanh Pham"
                    },
                    {
                        "name": "Gang Bao"
                    },
                    {
                        "name": "Jacob T. Robinson"
                    },
                    {
                        "name": "Stefan M. Goetz"
                    },
                    {
                        "name": "Angel V. Peterchev"
                    }
                ],
                "author_detail": {
                    "name": "Angel V. Peterchev"
                },
                "author": "Angel V. Peterchev",
                "arxiv_comment": "25 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.00745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.00745v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26692v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26692v2",
                "updated": "2025-11-01T12:05:18Z",
                "updated_parsed": [
                    2025,
                    11,
                    1,
                    12,
                    5,
                    18,
                    5,
                    305,
                    0
                ],
                "published": "2025-10-30T16:59:43Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    16,
                    59,
                    43,
                    3,
                    303,
                    0
                ],
                "title": "Kimi Linear: An Expressive, Efficient Attention Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kimi Linear: An Expressive, Efficient Attention Architecture"
                },
                "summary": "We introduce Kimi Linear, a hybrid linear attention architecture that, for\nthe first time, outperforms full attention under fair comparisons across\nvarious scenarios -- including short-context, long-context, and reinforcement\nlearning (RL) scaling regimes. At its core lies Kimi Delta Attention (KDA), an\nexpressive linear attention module that extends Gated DeltaNet with a\nfiner-grained gating mechanism, enabling more effective use of limited\nfinite-state RNN memory. Our bespoke chunkwise algorithm achieves high hardware\nefficiency through a specialized variant of the Diagonal-Plus-Low-Rank (DPLR)\ntransition matrices, which substantially reduces computation compared to the\ngeneral DPLR formulation while remaining more consistent with the classical\ndelta rule.\n  We pretrain a Kimi Linear model with 3B activated parameters and 48B total\nparameters, based on a layerwise hybrid of KDA and Multi-Head Latent Attention\n(MLA). Our experiments show that with an identical training recipe, Kimi Linear\noutperforms full MLA with a sizeable margin across all evaluated tasks, while\nreducing KV cache usage by up to 75% and achieving up to 6 times decoding\nthroughput for a 1M context. These results demonstrate that Kimi Linear can be\na drop-in replacement for full attention architectures with superior\nperformance and efficiency, including tasks with longer input and output\nlengths.\n  To support further research, we open-source the KDA kernel and vLLM\nimplementations, and release the pre-trained and instruction-tuned model\ncheckpoints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Kimi Linear, a hybrid linear attention architecture that, for\nthe first time, outperforms full attention under fair comparisons across\nvarious scenarios -- including short-context, long-context, and reinforcement\nlearning (RL) scaling regimes. At its core lies Kimi Delta Attention (KDA), an\nexpressive linear attention module that extends Gated DeltaNet with a\nfiner-grained gating mechanism, enabling more effective use of limited\nfinite-state RNN memory. Our bespoke chunkwise algorithm achieves high hardware\nefficiency through a specialized variant of the Diagonal-Plus-Low-Rank (DPLR)\ntransition matrices, which substantially reduces computation compared to the\ngeneral DPLR formulation while remaining more consistent with the classical\ndelta rule.\n  We pretrain a Kimi Linear model with 3B activated parameters and 48B total\nparameters, based on a layerwise hybrid of KDA and Multi-Head Latent Attention\n(MLA). Our experiments show that with an identical training recipe, Kimi Linear\noutperforms full MLA with a sizeable margin across all evaluated tasks, while\nreducing KV cache usage by up to 75% and achieving up to 6 times decoding\nthroughput for a 1M context. These results demonstrate that Kimi Linear can be\na drop-in replacement for full attention architectures with superior\nperformance and efficiency, including tasks with longer input and output\nlengths.\n  To support further research, we open-source the KDA kernel and vLLM\nimplementations, and release the pre-trained and instruction-tuned model\ncheckpoints."
                },
                "authors": [
                    {
                        "name": "Kimi Team"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Zongyu Lin"
                    },
                    {
                        "name": "Xingcheng Yao"
                    },
                    {
                        "name": "Jiaxi Hu"
                    },
                    {
                        "name": "Fanqing Meng"
                    },
                    {
                        "name": "Chengyin Liu"
                    },
                    {
                        "name": "Xin Men"
                    },
                    {
                        "name": "Songlin Yang"
                    },
                    {
                        "name": "Zhiyuan Li"
                    },
                    {
                        "name": "Wentao Li"
                    },
                    {
                        "name": "Enzhe Lu"
                    },
                    {
                        "name": "Weizhou Liu"
                    },
                    {
                        "name": "Yanru Chen"
                    },
                    {
                        "name": "Weixin Xu"
                    },
                    {
                        "name": "Longhui Yu"
                    },
                    {
                        "name": "Yejie Wang"
                    },
                    {
                        "name": "Yu Fan"
                    },
                    {
                        "name": "Longguang Zhong"
                    },
                    {
                        "name": "Enming Yuan"
                    },
                    {
                        "name": "Dehao Zhang"
                    },
                    {
                        "name": "Yizhi Zhang"
                    },
                    {
                        "name": "T. Y. Liu"
                    },
                    {
                        "name": "Haiming Wang"
                    },
                    {
                        "name": "Shengjun Fang"
                    },
                    {
                        "name": "Weiran He"
                    },
                    {
                        "name": "Shaowei Liu"
                    },
                    {
                        "name": "Yiwei Li"
                    },
                    {
                        "name": "Jianlin Su"
                    },
                    {
                        "name": "Jiezhong Qiu"
                    },
                    {
                        "name": "Bo Pang"
                    },
                    {
                        "name": "Junjie Yan"
                    },
                    {
                        "name": "Zhejun Jiang"
                    },
                    {
                        "name": "Weixiao Huang"
                    },
                    {
                        "name": "Bohong Yin"
                    },
                    {
                        "name": "Jiacheng You"
                    },
                    {
                        "name": "Chu Wei"
                    },
                    {
                        "name": "Zhengtao Wang"
                    },
                    {
                        "name": "Chao Hong"
                    },
                    {
                        "name": "Yutian Chen"
                    },
                    {
                        "name": "Guanduo Chen"
                    },
                    {
                        "name": "Yucheng Wang"
                    },
                    {
                        "name": "Huabin Zheng"
                    },
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Yibo Liu"
                    },
                    {
                        "name": "Mengnan Dong"
                    },
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Siyuan Pan"
                    },
                    {
                        "name": "Wenhao Wu"
                    },
                    {
                        "name": "Yuhao Wu"
                    },
                    {
                        "name": "Longyu Guan"
                    },
                    {
                        "name": "Jiawen Tao"
                    },
                    {
                        "name": "Guohong Fu"
                    },
                    {
                        "name": "Xinran Xu"
                    },
                    {
                        "name": "Yuzhi Wang"
                    },
                    {
                        "name": "Guokun Lai"
                    },
                    {
                        "name": "Yuxin Wu"
                    },
                    {
                        "name": "Xinyu Zhou"
                    },
                    {
                        "name": "Zhilin Yang"
                    },
                    {
                        "name": "Yulun Du"
                    }
                ],
                "author_detail": {
                    "name": "Yulun Du"
                },
                "author": "Yulun Du",
                "arxiv_comment": "Kimi Linear tech report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26692v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26692v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.00473v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.00473v1",
                "updated": "2025-11-01T09:53:47Z",
                "updated_parsed": [
                    2025,
                    11,
                    1,
                    9,
                    53,
                    47,
                    5,
                    305,
                    0
                ],
                "published": "2025-11-01T09:53:47Z",
                "published_parsed": [
                    2025,
                    11,
                    1,
                    9,
                    53,
                    47,
                    5,
                    305,
                    0
                ],
                "title": "Drinfeld associators and Kashiwara-Vergne associators in higher genera",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Drinfeld associators and Kashiwara-Vergne associators in higher genera"
                },
                "summary": "For $g\\geq 0$, a genus $g$ Kashiwara-Vergne associator, introduced by\nAlekseev-Kawazumi-Kuno-Naef as a solution to the generalised KV equations in\nrelation to the formality problem of the Goldman-Turaev Lie bialgebra on an\noriented surface with a framing, is directly constructed from a genus $g$\nanalogue of a Drinfeld associator formulated by Gonzalez, which we call a\nGonzalez-Drinfeld associator. The proof is based on Massuyeau's work in genus\n0. The framing is automatically determined from the choice of a\nGonzalez-Drinfeld associator, and in the case of genus 1, we show that only one\nparticular framing is realised by our construction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For $g\\geq 0$, a genus $g$ Kashiwara-Vergne associator, introduced by\nAlekseev-Kawazumi-Kuno-Naef as a solution to the generalised KV equations in\nrelation to the formality problem of the Goldman-Turaev Lie bialgebra on an\noriented surface with a framing, is directly constructed from a genus $g$\nanalogue of a Drinfeld associator formulated by Gonzalez, which we call a\nGonzalez-Drinfeld associator. The proof is based on Massuyeau's work in genus\n0. The framing is automatically determined from the choice of a\nGonzalez-Drinfeld associator, and in the case of genus 1, we show that only one\nparticular framing is realised by our construction."
                },
                "authors": [
                    {
                        "name": "Toyo Taniguchi"
                    }
                ],
                "author_detail": {
                    "name": "Toyo Taniguchi"
                },
                "author": "Toyo Taniguchi",
                "arxiv_comment": "40 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.00473v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.00473v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.QA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.QA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.AT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "57K20(primary), 16T05, 16W70, 18M75, 20F36, 20F40, 57M05 (secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19755v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19755v3",
                "updated": "2025-11-01T08:49:20Z",
                "updated_parsed": [
                    2025,
                    11,
                    1,
                    8,
                    49,
                    20,
                    5,
                    305,
                    0
                ],
                "published": "2025-10-22T16:46:05Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    16,
                    46,
                    5,
                    2,
                    295,
                    0
                ],
                "title": "A Survey on Cache Methods in Diffusion Models: Toward Efficient\n  Multi-Modal Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Cache Methods in Diffusion Models: Toward Efficient\n  Multi-Modal Generation"
                },
                "summary": "Diffusion Models have become a cornerstone of modern generative AI for their\nexceptional generation quality and controllability. However, their inherent\n\\textit{multi-step iterations} and \\textit{complex backbone networks} lead to\nprohibitive computational overhead and generation latency, forming a major\nbottleneck for real-time applications. Although existing acceleration\ntechniques have made progress, they still face challenges such as limited\napplicability, high training costs, or quality degradation.\n  Against this backdrop, \\textbf{Diffusion Caching} offers a promising\ntraining-free, architecture-agnostic, and efficient inference paradigm. Its\ncore mechanism identifies and reuses intrinsic computational redundancies in\nthe diffusion process. By enabling feature-level cross-step reuse and\ninter-layer scheduling, it reduces computation without modifying model\nparameters. This paper systematically reviews the theoretical foundations and\nevolution of Diffusion Caching and proposes a unified framework for its\nclassification and analysis.\n  Through comparative analysis of representative methods, we show that\nDiffusion Caching evolves from \\textit{static reuse} to \\textit{dynamic\nprediction}. This trend enhances caching flexibility across diverse tasks and\nenables integration with other acceleration techniques such as sampling\noptimization and model distillation, paving the way for a unified, efficient\ninference framework for future multimodal and interactive applications. We\nargue that this paradigm will become a key enabler of real-time and efficient\ngenerative AI, injecting new vitality into both theory and practice of\n\\textit{Efficient Generative Intelligence}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Models have become a cornerstone of modern generative AI for their\nexceptional generation quality and controllability. However, their inherent\n\\textit{multi-step iterations} and \\textit{complex backbone networks} lead to\nprohibitive computational overhead and generation latency, forming a major\nbottleneck for real-time applications. Although existing acceleration\ntechniques have made progress, they still face challenges such as limited\napplicability, high training costs, or quality degradation.\n  Against this backdrop, \\textbf{Diffusion Caching} offers a promising\ntraining-free, architecture-agnostic, and efficient inference paradigm. Its\ncore mechanism identifies and reuses intrinsic computational redundancies in\nthe diffusion process. By enabling feature-level cross-step reuse and\ninter-layer scheduling, it reduces computation without modifying model\nparameters. This paper systematically reviews the theoretical foundations and\nevolution of Diffusion Caching and proposes a unified framework for its\nclassification and analysis.\n  Through comparative analysis of representative methods, we show that\nDiffusion Caching evolves from \\textit{static reuse} to \\textit{dynamic\nprediction}. This trend enhances caching flexibility across diverse tasks and\nenables integration with other acceleration techniques such as sampling\noptimization and model distillation, paving the way for a unified, efficient\ninference framework for future multimodal and interactive applications. We\nargue that this paradigm will become a key enabler of real-time and efficient\ngenerative AI, injecting new vitality into both theory and practice of\n\\textit{Efficient Generative Intelligence}."
                },
                "authors": [
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Yuqi Lin"
                    },
                    {
                        "name": "Zhikai Wang"
                    },
                    {
                        "name": "Peiru Wang"
                    },
                    {
                        "name": "Peiliang Cai"
                    },
                    {
                        "name": "Qinming Zhou"
                    },
                    {
                        "name": "Zhengan Yan"
                    },
                    {
                        "name": "Zexuan Yan"
                    },
                    {
                        "name": "Zhengyi Shi"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "22 pages,2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19755v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19755v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12872v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12872v2",
                "updated": "2025-11-01T08:26:24Z",
                "updated_parsed": [
                    2025,
                    11,
                    1,
                    8,
                    26,
                    24,
                    5,
                    305,
                    0
                ],
                "published": "2025-10-14T18:00:01Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    18,
                    0,
                    1,
                    1,
                    287,
                    0
                ],
                "title": "KVCOMM: Online Cross-context KV-cache Communication for Efficient\n  LLM-based Multi-agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCOMM: Online Cross-context KV-cache Communication for Efficient\n  LLM-based Multi-agent Systems"
                },
                "summary": "Multi-agent large language model (LLM) systems are increasingly adopted for\ncomplex language processing tasks that require communication and coordination\namong agents. However, these systems often suffer substantial overhead from\nrepeated reprocessing of overlapping contexts across agents. In typical\npipelines, once an agent receives a message from its predecessor, the full\ncontext-including prior turns-must be reprocessed from scratch, leading to\ninefficient processing. While key-value (KV) caching is an effective solution\nfor avoiding redundant computation in single-agent settings where prefixes\nremain unchanged, it cannot be directly reused in multi-agent scenarios due to\ndiverging prefixes introduced by agent-specific context extensions. We identify\nthat the core challenge lies in the offset variance of KV-caches across agents.\nTo address this, we propose KVCOMM, a training-free framework that enables\nefficient prefilling in multi-agent inference by reusing KV-caches and aligning\ncache offsets of overlapping contexts under diverse prefix contexts. KVCOMM\nestimates and adjusts KV-caches for shared content by referencing a pool of\ncached examples-termed anchors-that store observed cache deviations under\nvarying prefixes. The anchor pool is maintained and updated online, allowing\ndynamic adaptation to distinct user requests and context structures. KVCOMM\nachieves over 70% reuse rate across diverse multi-agent workloads, including\nretrieval-augmented generation, math reasoning, and collaborative coding tasks,\nall without quality degradation. Particularly, when each fully-connected agent\nreceives 1K input tokens with 512 prefix tokens and 512 output tokens under a\nfive-agent setting, KVCOMM achieves up to 7.8x speedup compared to the standard\nprefill pipeline, reducing TTFT from ~430 ms to ~55 ms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent large language model (LLM) systems are increasingly adopted for\ncomplex language processing tasks that require communication and coordination\namong agents. However, these systems often suffer substantial overhead from\nrepeated reprocessing of overlapping contexts across agents. In typical\npipelines, once an agent receives a message from its predecessor, the full\ncontext-including prior turns-must be reprocessed from scratch, leading to\ninefficient processing. While key-value (KV) caching is an effective solution\nfor avoiding redundant computation in single-agent settings where prefixes\nremain unchanged, it cannot be directly reused in multi-agent scenarios due to\ndiverging prefixes introduced by agent-specific context extensions. We identify\nthat the core challenge lies in the offset variance of KV-caches across agents.\nTo address this, we propose KVCOMM, a training-free framework that enables\nefficient prefilling in multi-agent inference by reusing KV-caches and aligning\ncache offsets of overlapping contexts under diverse prefix contexts. KVCOMM\nestimates and adjusts KV-caches for shared content by referencing a pool of\ncached examples-termed anchors-that store observed cache deviations under\nvarying prefixes. The anchor pool is maintained and updated online, allowing\ndynamic adaptation to distinct user requests and context structures. KVCOMM\nachieves over 70% reuse rate across diverse multi-agent workloads, including\nretrieval-augmented generation, math reasoning, and collaborative coding tasks,\nall without quality degradation. Particularly, when each fully-connected agent\nreceives 1K input tokens with 512 prefix tokens and 512 output tokens under a\nfive-agent setting, KVCOMM achieves up to 7.8x speedup compared to the standard\nprefill pipeline, reducing TTFT from ~430 ms to ~55 ms."
                },
                "authors": [
                    {
                        "name": "Hancheng Ye"
                    },
                    {
                        "name": "Zhengqi Gao"
                    },
                    {
                        "name": "Mingyuan Ma"
                    },
                    {
                        "name": "Qinsi Wang"
                    },
                    {
                        "name": "Yuzhe Fu"
                    },
                    {
                        "name": "Ming-Yu Chung"
                    },
                    {
                        "name": "Yueqian Lin"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Jianyi Zhang"
                    },
                    {
                        "name": "Danyang Zhuo"
                    },
                    {
                        "name": "Yiran Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yiran Chen"
                },
                "author": "Yiran Chen",
                "arxiv_comment": "Accepted for publication in NeurIPS2025. Code is available at\n  \\url{https://github.com/FastMAS/KVCOMM}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12872v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12872v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22765v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22765v2",
                "updated": "2025-11-01T07:01:00Z",
                "updated_parsed": [
                    2025,
                    11,
                    1,
                    7,
                    1,
                    0,
                    5,
                    305,
                    0
                ],
                "published": "2025-10-26T17:28:05Z",
                "published_parsed": [
                    2025,
                    10,
                    26,
                    17,
                    28,
                    5,
                    6,
                    299,
                    0
                ],
                "title": "Jarvis: Towards Personalized AI Assistant via Personal KV-Cache\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jarvis: Towards Personalized AI Assistant via Personal KV-Cache\n  Retrieval"
                },
                "summary": "The rapid development of Vision-language models (VLMs) enables open-ended\nperception and reasoning. Recent works have started to investigate how to adapt\ngeneral-purpose VLMs into personalized assistants. Even commercial models such\nas ChatGPT now support model personalization by incorporating user-specific\ninformation. However, existing methods either learn a set of concept tokens or\ntrain a VLM to utilize user-specific information. However, both pipelines\nstruggle to generate accurate answers as personalized assistants. We introduce\nJarvis, an innovative framework for a personalized AI assistant through\npersonal KV-Cache retrieval, which stores user-specific information in the\nKV-Caches of both textual and visual tokens. The textual tokens are created by\nsummarizing user information into metadata, while the visual tokens are\nproduced by extracting distinct image patches from the user's images. When\nanswering a question, Jarvis first retrieves related KV-Caches from personal\nstorage and uses them to ensure accuracy in responses. We also introduce a\nfine-grained benchmark built with the same distinct image patch mining\npipeline, emphasizing accurate question answering based on fine-grained\nuser-specific information. Jarvis is capable of providing more accurate\nresponses, particularly when they depend on specific local details. Jarvis\nachieves state-of-the-art results in both visual question answering and\ntext-only tasks across multiple datasets, indicating a practical path toward\npersonalized AI assistants. The code and dataset will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of Vision-language models (VLMs) enables open-ended\nperception and reasoning. Recent works have started to investigate how to adapt\ngeneral-purpose VLMs into personalized assistants. Even commercial models such\nas ChatGPT now support model personalization by incorporating user-specific\ninformation. However, existing methods either learn a set of concept tokens or\ntrain a VLM to utilize user-specific information. However, both pipelines\nstruggle to generate accurate answers as personalized assistants. We introduce\nJarvis, an innovative framework for a personalized AI assistant through\npersonal KV-Cache retrieval, which stores user-specific information in the\nKV-Caches of both textual and visual tokens. The textual tokens are created by\nsummarizing user information into metadata, while the visual tokens are\nproduced by extracting distinct image patches from the user's images. When\nanswering a question, Jarvis first retrieves related KV-Caches from personal\nstorage and uses them to ensure accuracy in responses. We also introduce a\nfine-grained benchmark built with the same distinct image patch mining\npipeline, emphasizing accurate question answering based on fine-grained\nuser-specific information. Jarvis is capable of providing more accurate\nresponses, particularly when they depend on specific local details. Jarvis\nachieves state-of-the-art results in both visual question answering and\ntext-only tasks across multiple datasets, indicating a practical path toward\npersonalized AI assistants. The code and dataset will be released."
                },
                "authors": [
                    {
                        "name": "Binxiao Xu"
                    },
                    {
                        "name": "Junyu Feng"
                    },
                    {
                        "name": "Shaolin Lu"
                    },
                    {
                        "name": "Yulin Luo"
                    },
                    {
                        "name": "Shilin Yan"
                    },
                    {
                        "name": "Hao Liang"
                    },
                    {
                        "name": "Ming Lu"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang",
                "arxiv_comment": "19 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22765v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22765v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16242v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16242v7",
                "updated": "2025-11-01T04:26:03Z",
                "updated_parsed": [
                    2025,
                    11,
                    1,
                    4,
                    26,
                    3,
                    5,
                    305,
                    0
                ],
                "published": "2025-07-22T05:26:28Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    5,
                    26,
                    28,
                    1,
                    203,
                    0
                ],
                "title": "Robustifying Learning-Augmented Caching Efficiently without Compromising\n  1-Consistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robustifying Learning-Augmented Caching Efficiently without Compromising\n  1-Consistency"
                },
                "summary": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce excessive computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_{k-1} + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only O(1) additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce excessive computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_{k-1} + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only O(1) additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice."
                },
                "authors": [
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Hailiang Zhao"
                    },
                    {
                        "name": "Jiaji Zhang"
                    },
                    {
                        "name": "Xueyan Tang"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng",
                "arxiv_comment": "Accepted to NeurIPS 2025.\n  https://neurips.cc/virtual/2025/poster/116615",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16242v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16242v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.00321v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.00321v1",
                "updated": "2025-10-31T23:50:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    23,
                    50,
                    44,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T23:50:44Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    23,
                    50,
                    44,
                    4,
                    304,
                    0
                ],
                "title": "Scalable Processing-Near-Memory for 1M-Token LLM Inference: CXL-Enabled\n  KV-Cache Management Beyond GPU Limits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Processing-Near-Memory for 1M-Token LLM Inference: CXL-Enabled\n  KV-Cache Management Beyond GPU Limits"
                },
                "summary": "The expansion of context windows in large language models (LLMs) to\nmulti-million tokens introduces severe memory and compute bottlenecks,\nparticularly in managing the growing Key-Value (KV) cache. While Compute\nExpress Link (CXL) enables non-eviction frameworks that offload the full\nKV-cache to scalable external memory, these frameworks still suffer from costly\ndata transfers when recalling non-resident KV tokens to limited GPU memory as\ncontext lengths increase. This work proposes scalable Processing-Near-Memory\n(PNM) for 1M-Token LLM Inference, a CXL-enabled KV-cache management system that\ncoordinates memory and computation beyond GPU limits. Our design offloads token\npage selection to a PNM accelerator within CXL memory, eliminating costly\nrecalls and enabling larger GPU batch sizes. We further introduce a hybrid\nparallelization strategy and a steady-token selection mechanism to enhance\ncompute efficiency and scalability. Implemented atop a state-of-the-art CXL-PNM\nsystem, our solution delivers consistent performance gains for LLMs with up to\n405B parameters and 1M-token contexts. Our PNM-only offloading scheme (PNM-KV)\nand GPU-PNM hybrid with steady-token execution (PnG-KV) achieve up to 21.9x\nthroughput improvement, up to 60x lower energy per token, and up to 7.3x better\ntotal cost efficiency than the baseline, demonstrating that CXL-enabled\nmulti-PNM architectures can serve as a scalable backbone for future\nlong-context LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expansion of context windows in large language models (LLMs) to\nmulti-million tokens introduces severe memory and compute bottlenecks,\nparticularly in managing the growing Key-Value (KV) cache. While Compute\nExpress Link (CXL) enables non-eviction frameworks that offload the full\nKV-cache to scalable external memory, these frameworks still suffer from costly\ndata transfers when recalling non-resident KV tokens to limited GPU memory as\ncontext lengths increase. This work proposes scalable Processing-Near-Memory\n(PNM) for 1M-Token LLM Inference, a CXL-enabled KV-cache management system that\ncoordinates memory and computation beyond GPU limits. Our design offloads token\npage selection to a PNM accelerator within CXL memory, eliminating costly\nrecalls and enabling larger GPU batch sizes. We further introduce a hybrid\nparallelization strategy and a steady-token selection mechanism to enhance\ncompute efficiency and scalability. Implemented atop a state-of-the-art CXL-PNM\nsystem, our solution delivers consistent performance gains for LLMs with up to\n405B parameters and 1M-token contexts. Our PNM-only offloading scheme (PNM-KV)\nand GPU-PNM hybrid with steady-token execution (PnG-KV) achieve up to 21.9x\nthroughput improvement, up to 60x lower energy per token, and up to 7.3x better\ntotal cost efficiency than the baseline, demonstrating that CXL-enabled\nmulti-PNM architectures can serve as a scalable backbone for future\nlong-context LLM inference."
                },
                "authors": [
                    {
                        "name": "Dowon Kim"
                    },
                    {
                        "name": "MinJae Lee"
                    },
                    {
                        "name": "Janghyeon Kim"
                    },
                    {
                        "name": "HyuckSung Kwon"
                    },
                    {
                        "name": "Hyeonggyu Jeong"
                    },
                    {
                        "name": "Sang-Soo Park"
                    },
                    {
                        "name": "Minyong Yoon"
                    },
                    {
                        "name": "Si-Dong Roh"
                    },
                    {
                        "name": "Yongsuk Kwon"
                    },
                    {
                        "name": "Jinin So"
                    },
                    {
                        "name": "Jungwook Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jungwook Choi"
                },
                "author": "Jungwook Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.00321v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.00321v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25979v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25979v2",
                "updated": "2025-10-31T18:19:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    18,
                    19,
                    55,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-29T21:26:17Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    21,
                    26,
                    17,
                    2,
                    302,
                    0
                ],
                "title": "AttnCache: Accelerating Self-Attention Inference for LLM Prefill via\n  Attention Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttnCache: Accelerating Self-Attention Inference for LLM Prefill via\n  Attention Cache"
                },
                "summary": "Large Language Models (LLMs) are widely used in generative applications such\nas chatting, code generation, and reasoning. However, many realworld workloads\nsuch as classification, question answering, recommendation, and text embedding\nrely solely on the prefill stage of inference, where the model encodes input\nsequences without performing autoregressive decoding. In these prefill only\nscenarios, the self-attention computation becomes the primary performance\nbottleneck due to its quadratic complexity with respect to sequence length. In\nthis paper, we observe that semantically different sentences often produce\nsimilar attention maps across layers and heads. Building on this insight, we\npropose AttnCache, a framework that accelerates the prefill stage of LLM\ninference by retrieving and reusing similar attention maps. Based on an\nattention map memorization database, AttnCache employs efficient caching and\nsimilarity search techniques to identify and reuse pre-cached attention maps\nduring inference, thereby reducing the computational overhead of\nself-attention. Experimental results show that AttnCache achieves an average of\n1.2x end-to-end and 2x attention speedup on CPU, and 1.6x end-to-end and 3x\nattention speedup on GPU, with negligible accuracy degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely used in generative applications such\nas chatting, code generation, and reasoning. However, many realworld workloads\nsuch as classification, question answering, recommendation, and text embedding\nrely solely on the prefill stage of inference, where the model encodes input\nsequences without performing autoregressive decoding. In these prefill only\nscenarios, the self-attention computation becomes the primary performance\nbottleneck due to its quadratic complexity with respect to sequence length. In\nthis paper, we observe that semantically different sentences often produce\nsimilar attention maps across layers and heads. Building on this insight, we\npropose AttnCache, a framework that accelerates the prefill stage of LLM\ninference by retrieving and reusing similar attention maps. Based on an\nattention map memorization database, AttnCache employs efficient caching and\nsimilarity search techniques to identify and reuse pre-cached attention maps\nduring inference, thereby reducing the computational overhead of\nself-attention. Experimental results show that AttnCache achieves an average of\n1.2x end-to-end and 2x attention speedup on CPU, and 1.6x end-to-end and 3x\nattention speedup on GPU, with negligible accuracy degradation."
                },
                "authors": [
                    {
                        "name": "Dinghong Song"
                    },
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Shangye Chen"
                    },
                    {
                        "name": "Cyril Guyot"
                    },
                    {
                        "name": "Filip Blagojevic"
                    },
                    {
                        "name": "Hyeran Jeon"
                    },
                    {
                        "name": "Pengfei Su"
                    },
                    {
                        "name": "Dong Li"
                    }
                ],
                "author_detail": {
                    "name": "Dong Li"
                },
                "author": "Dong Li",
                "arxiv_comment": "10 pages, 6 figures, submitted to Ninth Annual Conference on Machine\n  Learning and Systems (MLSys'26)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25979v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25979v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27641v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27641v1",
                "updated": "2025-10-31T17:12:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    17,
                    12,
                    34,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T17:12:34Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    17,
                    12,
                    34,
                    4,
                    304,
                    0
                ],
                "title": "SpecAttn: Speculating Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecAttn: Speculating Sparse Attention"
                },
                "summary": "Large Language Models (LLMs) face significant computational bottlenecks\nduring inference due to the quadratic complexity of self-attention mechanisms,\nparticularly as context lengths increase. We introduce SpecAttn, a novel\ntraining-free approach that seamlessly integrates with existing speculative\ndecoding techniques to enable efficient sparse attention in pre-trained\ntransformers. Our key insight is to exploit the attention weights already\ncomputed by the draft model during speculative decoding to identify important\ntokens for the target model, eliminating redundant computation while\nmaintaining output quality. SpecAttn employs three core techniques: KL\ndivergence-based layer alignment between draft and target models, a\nGPU-optimized sorting-free algorithm for top-p token selection from draft\nattention patterns, and dynamic key-value cache pruning guided by these\npredictions. By leveraging the computational work already performed in standard\nspeculative decoding pipelines, SpecAttn achieves over 75% reduction in\nkey-value cache accesses with a mere 15.29% increase in perplexity on the PG-19\ndataset, significantly outperforming existing sparse attention methods. Our\napproach demonstrates that speculative execution can be enhanced to provide\napproximate verification without significant performance degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) face significant computational bottlenecks\nduring inference due to the quadratic complexity of self-attention mechanisms,\nparticularly as context lengths increase. We introduce SpecAttn, a novel\ntraining-free approach that seamlessly integrates with existing speculative\ndecoding techniques to enable efficient sparse attention in pre-trained\ntransformers. Our key insight is to exploit the attention weights already\ncomputed by the draft model during speculative decoding to identify important\ntokens for the target model, eliminating redundant computation while\nmaintaining output quality. SpecAttn employs three core techniques: KL\ndivergence-based layer alignment between draft and target models, a\nGPU-optimized sorting-free algorithm for top-p token selection from draft\nattention patterns, and dynamic key-value cache pruning guided by these\npredictions. By leveraging the computational work already performed in standard\nspeculative decoding pipelines, SpecAttn achieves over 75% reduction in\nkey-value cache accesses with a mere 15.29% increase in perplexity on the PG-19\ndataset, significantly outperforming existing sparse attention methods. Our\napproach demonstrates that speculative execution can be enhanced to provide\napproximate verification without significant performance degradation."
                },
                "authors": [
                    {
                        "name": "Harsh Shah"
                    }
                ],
                "author_detail": {
                    "name": "Harsh Shah"
                },
                "author": "Harsh Shah",
                "arxiv_comment": "Accepted to NeurIPS 2025 Workshop on Structured Probabilistic\n  Inference & Generative Modeling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27641v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27641v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27617v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27617v1",
                "updated": "2025-10-31T16:40:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    16,
                    40,
                    58,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T16:40:58Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    16,
                    40,
                    58,
                    4,
                    304,
                    0
                ],
                "title": "VeriMoA: A Mixture-of-Agents Framework for Spec-to-HDL Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VeriMoA: A Mixture-of-Agents Framework for Spec-to-HDL Generation"
                },
                "summary": "Automation of Register Transfer Level (RTL) design can help developers meet\nincreasing computational demands. Large Language Models (LLMs) show promise for\nHardware Description Language (HDL) generation, but face challenges due to\nlimited parametric knowledge and domain-specific constraints. While prompt\nengineering and fine-tuning have limitations in knowledge coverage and training\ncosts, multi-agent architectures offer a training-free paradigm to enhance\nreasoning through collaborative generation. However, current multi-agent\napproaches suffer from two critical deficiencies: susceptibility to noise\npropagation and constrained reasoning space exploration. We propose VeriMoA, a\ntraining-free mixture-of-agents (MoA) framework with two synergistic\ninnovations. First, a quality-guided caching mechanism to maintain all\nintermediate HDL outputs and enables quality-based ranking and selection across\nthe entire generation process, encouraging knowledge accumulation over layers\nof reasoning. Second, a multi-path generation strategy that leverages C++ and\nPython as intermediate representations, decomposing specification-to-HDL\ntranslation into two-stage processes that exploit LLM fluency in high-resource\nlanguages while promoting solution diversity. Comprehensive experiments on\nVerilogEval 2.0 and RTLLM 2.0 benchmarks demonstrate that VeriMoA achieves\n15--30% improvements in Pass@1 across diverse LLM backbones, especially\nenabling smaller models to match larger models and fine-tuned alternatives\nwithout requiring costly training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automation of Register Transfer Level (RTL) design can help developers meet\nincreasing computational demands. Large Language Models (LLMs) show promise for\nHardware Description Language (HDL) generation, but face challenges due to\nlimited parametric knowledge and domain-specific constraints. While prompt\nengineering and fine-tuning have limitations in knowledge coverage and training\ncosts, multi-agent architectures offer a training-free paradigm to enhance\nreasoning through collaborative generation. However, current multi-agent\napproaches suffer from two critical deficiencies: susceptibility to noise\npropagation and constrained reasoning space exploration. We propose VeriMoA, a\ntraining-free mixture-of-agents (MoA) framework with two synergistic\ninnovations. First, a quality-guided caching mechanism to maintain all\nintermediate HDL outputs and enables quality-based ranking and selection across\nthe entire generation process, encouraging knowledge accumulation over layers\nof reasoning. Second, a multi-path generation strategy that leverages C++ and\nPython as intermediate representations, decomposing specification-to-HDL\ntranslation into two-stage processes that exploit LLM fluency in high-resource\nlanguages while promoting solution diversity. Comprehensive experiments on\nVerilogEval 2.0 and RTLLM 2.0 benchmarks demonstrate that VeriMoA achieves\n15--30% improvements in Pass@1 across diverse LLM backbones, especially\nenabling smaller models to match larger models and fine-tuned alternatives\nwithout requiring costly training."
                },
                "authors": [
                    {
                        "name": "Heng Ping"
                    },
                    {
                        "name": "Arijit Bhattacharjee"
                    },
                    {
                        "name": "Peiyu Zhang"
                    },
                    {
                        "name": "Shixuan Li"
                    },
                    {
                        "name": "Wei Yang"
                    },
                    {
                        "name": "Anzhe Cheng"
                    },
                    {
                        "name": "Xiaole Zhang"
                    },
                    {
                        "name": "Jesse Thomason"
                    },
                    {
                        "name": "Ali Jannesari"
                    },
                    {
                        "name": "Nesreen Ahmed"
                    },
                    {
                        "name": "Paul Bogdan"
                    }
                ],
                "author_detail": {
                    "name": "Paul Bogdan"
                },
                "author": "Paul Bogdan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27617v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27617v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04525v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04525v2",
                "updated": "2025-10-31T05:31:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    5,
                    31,
                    58,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-06T06:30:22Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    6,
                    30,
                    22,
                    0,
                    279,
                    0
                ],
                "title": "Demystifying MaskGIT Sampler and Beyond: Adaptive Order Selection in\n  Masked Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying MaskGIT Sampler and Beyond: Adaptive Order Selection in\n  Masked Diffusion"
                },
                "summary": "Masked diffusion models have shown promising performance in generating\nhigh-quality samples in a wide range of domains, but accelerating their\nsampling process remains relatively underexplored. To investigate efficient\nsamplers for masked diffusion, this paper theoretically analyzes the MaskGIT\nsampler for image modeling, revealing its implicit temperature sampling\nmechanism. Through this analysis, we introduce the \"moment sampler,\" an\nasymptotically equivalent but more tractable and interpretable alternative to\nMaskGIT, which employs a \"choose-then-sample\" approach by selecting unmasking\npositions before sampling tokens. In addition, we improve the efficiency of\nchoose-then-sample algorithms through two key innovations: a partial caching\ntechnique for transformers that approximates longer sampling trajectories\nwithout proportional computational cost, and a hybrid approach formalizing the\nexploration-exploitation trade-off in adaptive unmasking. Experiments in image\nand text domains demonstrate our theory as well as the efficiency of our\nproposed methods, advancing both theoretical understanding and practical\nimplementation of masked diffusion samplers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked diffusion models have shown promising performance in generating\nhigh-quality samples in a wide range of domains, but accelerating their\nsampling process remains relatively underexplored. To investigate efficient\nsamplers for masked diffusion, this paper theoretically analyzes the MaskGIT\nsampler for image modeling, revealing its implicit temperature sampling\nmechanism. Through this analysis, we introduce the \"moment sampler,\" an\nasymptotically equivalent but more tractable and interpretable alternative to\nMaskGIT, which employs a \"choose-then-sample\" approach by selecting unmasking\npositions before sampling tokens. In addition, we improve the efficiency of\nchoose-then-sample algorithms through two key innovations: a partial caching\ntechnique for transformers that approximates longer sampling trajectories\nwithout proportional computational cost, and a hybrid approach formalizing the\nexploration-exploitation trade-off in adaptive unmasking. Experiments in image\nand text domains demonstrate our theory as well as the efficiency of our\nproposed methods, advancing both theoretical understanding and practical\nimplementation of masked diffusion samplers."
                },
                "authors": [
                    {
                        "name": "Satoshi Hayakawa"
                    },
                    {
                        "name": "Yuhta Takida"
                    },
                    {
                        "name": "Masaaki Imaizumi"
                    },
                    {
                        "name": "Hiromi Wakaki"
                    },
                    {
                        "name": "Yuki Mitsufuji"
                    }
                ],
                "author_detail": {
                    "name": "Yuki Mitsufuji"
                },
                "author": "Yuki Mitsufuji",
                "arxiv_comment": "23 pages, fixed cleveref-related issue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04525v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04525v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27171v1",
                "updated": "2025-10-31T04:47:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    4,
                    47,
                    14,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T04:47:14Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    4,
                    47,
                    14,
                    4,
                    304,
                    0
                ],
                "title": "H2-Cache: A Novel Hierarchical Dual-Stage Cache for High-Performance\n  Acceleration of Generative Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "H2-Cache: A Novel Hierarchical Dual-Stage Cache for High-Performance\n  Acceleration of Generative Diffusion Models"
                },
                "summary": "Diffusion models have emerged as state-of-the-art in image generation, but\ntheir practical deployment is hindered by the significant computational cost of\ntheir iterative denoising process. While existing caching techniques can\naccelerate inference, they often create a challenging trade-off between speed\nand fidelity, suffering from quality degradation and high computational\noverhead. To address these limitations, we introduce H2-Cache, a novel\nhierarchical caching mechanism designed for modern generative diffusion model\narchitectures. Our method is founded on the key insight that the denoising\nprocess can be functionally separated into a structure-defining stage and a\ndetail-refining stage. H2-cache leverages this by employing a dual-threshold\nsystem, using independent thresholds to selectively cache each stage. To ensure\nthe efficiency of our dual-check approach, we introduce pooled feature\nsummarization (PFS), a lightweight technique for robust and fast similarity\nestimation. Extensive experiments on the Flux architecture demonstrate that\nH2-cache achieves significant acceleration (up to 5.08x) while maintaining\nimage quality nearly identical to the baseline, quantitatively and\nqualitatively outperforming existing caching methods. Our work presents a\nrobust and practical solution that effectively resolves the speed-quality\ndilemma, significantly lowering the barrier for the real-world application of\nhigh-fidelity diffusion models. Source code is available at\nhttps://github.com/Bluear7878/H2-cache-A-Hierarchical-Dual-Stage-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have emerged as state-of-the-art in image generation, but\ntheir practical deployment is hindered by the significant computational cost of\ntheir iterative denoising process. While existing caching techniques can\naccelerate inference, they often create a challenging trade-off between speed\nand fidelity, suffering from quality degradation and high computational\noverhead. To address these limitations, we introduce H2-Cache, a novel\nhierarchical caching mechanism designed for modern generative diffusion model\narchitectures. Our method is founded on the key insight that the denoising\nprocess can be functionally separated into a structure-defining stage and a\ndetail-refining stage. H2-cache leverages this by employing a dual-threshold\nsystem, using independent thresholds to selectively cache each stage. To ensure\nthe efficiency of our dual-check approach, we introduce pooled feature\nsummarization (PFS), a lightweight technique for robust and fast similarity\nestimation. Extensive experiments on the Flux architecture demonstrate that\nH2-cache achieves significant acceleration (up to 5.08x) while maintaining\nimage quality nearly identical to the baseline, quantitatively and\nqualitatively outperforming existing caching methods. Our work presents a\nrobust and practical solution that effectively resolves the speed-quality\ndilemma, significantly lowering the barrier for the real-world application of\nhigh-fidelity diffusion models. Source code is available at\nhttps://github.com/Bluear7878/H2-cache-A-Hierarchical-Dual-Stage-Cache."
                },
                "authors": [
                    {
                        "name": "Mingyu Sung"
                    },
                    {
                        "name": "Il-Min Kim"
                    },
                    {
                        "name": "Sangseok Yun"
                    },
                    {
                        "name": "Jae-Mo Kang"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Mo Kang"
                },
                "author": "Jae-Mo Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18586v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18586v2",
                "updated": "2025-10-31T04:17:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    4,
                    17,
                    5,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-21T12:39:32Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    12,
                    39,
                    32,
                    1,
                    294,
                    0
                ],
                "title": "Tokencake: A KV-Cache-centric Serving Framework for LLM-based\n  Multi-Agent Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokencake: A KV-Cache-centric Serving Framework for LLM-based\n  Multi-Agent Applications"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed in complex multi-agent\napplications that use external function calls. This workload creates severe\nperformance challenges for the KV Cache: space contention leads to the eviction\nof critical agents' caches and time underutilization leaves the cache of agents\nstalled on long-running tool calls idling in GPU memory. We present Tokencake,\na KV-Cache-centric serving framework that co-optimizes scheduling and memory\nmanagement with an agent-aware design. Tokencake's Space Scheduler uses dynamic\nmemory partitioning to shield critical agents from contention, while its Time\nScheduler employs a proactive offload and predictive upload mechanism to\nrepurpose GPU memory during function call stalls. Our evaluation on\nrepresentative multi-agent benchmarks shows that Tokencake can reduce\nend-to-end latency by over 47.06%, improve effective GPU memory utilization by\nup to 16.9% compared to vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in complex multi-agent\napplications that use external function calls. This workload creates severe\nperformance challenges for the KV Cache: space contention leads to the eviction\nof critical agents' caches and time underutilization leaves the cache of agents\nstalled on long-running tool calls idling in GPU memory. We present Tokencake,\na KV-Cache-centric serving framework that co-optimizes scheduling and memory\nmanagement with an agent-aware design. Tokencake's Space Scheduler uses dynamic\nmemory partitioning to shield critical agents from contention, while its Time\nScheduler employs a proactive offload and predictive upload mechanism to\nrepurpose GPU memory during function call stalls. Our evaluation on\nrepresentative multi-agent benchmarks shows that Tokencake can reduce\nend-to-end latency by over 47.06%, improve effective GPU memory utilization by\nup to 16.9% compared to vLLM."
                },
                "authors": [
                    {
                        "name": "Zhuohang Bian"
                    },
                    {
                        "name": "Feiyang Wu"
                    },
                    {
                        "name": "Teng Ma"
                    },
                    {
                        "name": "Youwei Zhuo"
                    }
                ],
                "author_detail": {
                    "name": "Youwei Zhuo"
                },
                "author": "Youwei Zhuo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18586v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18586v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25977v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25977v2",
                "updated": "2025-10-31T01:52:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    1,
                    52,
                    13,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-29T21:22:08Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    21,
                    22,
                    8,
                    2,
                    302,
                    0
                ],
                "title": "NeuronMM: High-Performance Matrix Multiplication for LLM Inference on\n  AWS Trainium",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeuronMM: High-Performance Matrix Multiplication for LLM Inference on\n  AWS Trainium"
                },
                "summary": "AI accelerators, customized to AI workloads, provide cost-effective and\nhigh-performance solutions for training and inference. Trainium, an AI\naccelerator recently developed by Amazon Web Services (AWS), provides an\nattractive option for LLM training and inference through its heterogeneous\narchitecture. However, leveraging Trainium architecture for high performance\ncan be challenging because of its systolic array architecture and special\nrequirement on data layout. In this paper, we design high-performance matrix\nmultiplication (matmul), a critical compute kernel, for LLM inference on\nTrainium. We introduce a series of techniques customized to Trainium based on\nkernel fusion and novel caching strategies to reduce data movement across the\nsoftware-managed memory hierarchy, maximize SRAM bandwidth, and avoid expensive\nmatrix transpose. Evaluating with nine datasets and four recent LLMs, we show\nthat our system largely outperforms the state-of-the-art matmul implemented by\nAWS on Trainium: at the level of matmul kernel, it achieves an average 1.35x\nspeedup (up to 2.22x), which translates to an average 1.66x speedup (up to\n2.49x) for end-to-end LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI accelerators, customized to AI workloads, provide cost-effective and\nhigh-performance solutions for training and inference. Trainium, an AI\naccelerator recently developed by Amazon Web Services (AWS), provides an\nattractive option for LLM training and inference through its heterogeneous\narchitecture. However, leveraging Trainium architecture for high performance\ncan be challenging because of its systolic array architecture and special\nrequirement on data layout. In this paper, we design high-performance matrix\nmultiplication (matmul), a critical compute kernel, for LLM inference on\nTrainium. We introduce a series of techniques customized to Trainium based on\nkernel fusion and novel caching strategies to reduce data movement across the\nsoftware-managed memory hierarchy, maximize SRAM bandwidth, and avoid expensive\nmatrix transpose. Evaluating with nine datasets and four recent LLMs, we show\nthat our system largely outperforms the state-of-the-art matmul implemented by\nAWS on Trainium: at the level of matmul kernel, it achieves an average 1.35x\nspeedup (up to 2.22x), which translates to an average 1.66x speedup (up to\n2.49x) for end-to-end LLM inference."
                },
                "authors": [
                    {
                        "name": "Dinghong Song"
                    },
                    {
                        "name": "Jierui Xu"
                    },
                    {
                        "name": "Weichu Yang"
                    },
                    {
                        "name": "Pengfei Su"
                    },
                    {
                        "name": "Dong Li"
                    }
                ],
                "author_detail": {
                    "name": "Dong Li"
                },
                "author": "Dong Li",
                "arxiv_comment": "12 pages, 8 figures, submitted to the Proceedings of the Twenty-First\n  European Conference on Computer Systems (EuroSys'26)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25977v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25977v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27070v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27070v1",
                "updated": "2025-10-31T00:39:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    0,
                    39,
                    27,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T00:39:27Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    0,
                    39,
                    27,
                    4,
                    304,
                    0
                ],
                "title": "Descriptor-Based Object-Aware Memory Systems: A Comprehensive Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Descriptor-Based Object-Aware Memory Systems: A Comprehensive Review"
                },
                "summary": "The security and efficiency of modern computing systems are fundamentally\nundermined by the absence of a native architectural mechanism to propagate\nhigh-level program semantics, such as object identity, bounds, and lifetime,\nacross the hardware/software interface. This paper presents a comprehensive\nsurvey of the architectural paradigm designed to bridge this semantic gap:\ndescriptor-based, object-aware memory systems. By elevating the descriptor to a\nfirst-class architectural abstraction, this paradigm enables hardware to\ndynamically acquire and enforce the rich semantics of software-defined objects.\nThis survey systematically charts the evolution and current landscape of this\napproach. We establish the foundational concepts of memory objects and\ndescriptors and introduce a novel taxonomy of descriptor addressing modes,\nproviding a structured framework for analyzing and comparing diverse\nimplementations. Our unified analysis reveals how this paradigm holistically\naddresses the intertwined challenges of memory protection, management, and\nprocessing. As a culminating case study, we re-examine the CentroID model,\ndemonstrating how its hybrid tagged-pointer encoding and descriptor processing\nmechanisms embody the path toward practical and efficient object-aware designs.\nFinally, we outline how the explicit cross-layer communication of object\nsemantics provides a foundational research direction for next-generation cache\nhierarchies, unified virtual memory, and even 128-bit architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The security and efficiency of modern computing systems are fundamentally\nundermined by the absence of a native architectural mechanism to propagate\nhigh-level program semantics, such as object identity, bounds, and lifetime,\nacross the hardware/software interface. This paper presents a comprehensive\nsurvey of the architectural paradigm designed to bridge this semantic gap:\ndescriptor-based, object-aware memory systems. By elevating the descriptor to a\nfirst-class architectural abstraction, this paradigm enables hardware to\ndynamically acquire and enforce the rich semantics of software-defined objects.\nThis survey systematically charts the evolution and current landscape of this\napproach. We establish the foundational concepts of memory objects and\ndescriptors and introduce a novel taxonomy of descriptor addressing modes,\nproviding a structured framework for analyzing and comparing diverse\nimplementations. Our unified analysis reveals how this paradigm holistically\naddresses the intertwined challenges of memory protection, management, and\nprocessing. As a culminating case study, we re-examine the CentroID model,\ndemonstrating how its hybrid tagged-pointer encoding and descriptor processing\nmechanisms embody the path toward practical and efficient object-aware designs.\nFinally, we outline how the explicit cross-layer communication of object\nsemantics provides a foundational research direction for next-generation cache\nhierarchies, unified virtual memory, and even 128-bit architectures."
                },
                "authors": [
                    {
                        "name": "Dong Tong"
                    }
                ],
                "author_detail": {
                    "name": "Dong Tong"
                },
                "author": "Dong Tong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27070v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27070v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00413v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00413v2",
                "updated": "2025-10-30T21:11:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    21,
                    11,
                    33,
                    3,
                    303,
                    0
                ],
                "published": "2025-05-31T06:10:10Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    6,
                    10,
                    10,
                    5,
                    151,
                    0
                ],
                "title": "Accelerating Diffusion LLMs via Adaptive Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion LLMs via Adaptive Parallel Decoding"
                },
                "summary": "The generation speed of LLMs are bottlenecked by autoregressive decoding,\nwhere tokens are predicted sequentially one by one. Alternatively, diffusion\nlarge language models (dLLMs) theoretically allow for parallel token\ngeneration, but in practice struggle to achieve the speed of autoregressive\nmodels without significantly sacrificing quality. We therefore introduce\nadaptive parallel decoding (APD), a novel method that dynamically adjusts the\nnumber of tokens sampled in parallel. We achieve this by defining a\nmultiplicative mixture between the dLLM marginal probabilities and the joint\nprobability of sequences under a small auxiliary autoregressive model. This\ninverts the standard setup of speculative decoding, where the goal is to sample\nfrom a large autoregressive verifier by drafting from a smaller model. We\nfurther optimize APD by enabling KV caching and limiting the size of the masked\ninput. Altogether, our method puts forward three tunable parameters to flexibly\ntradeoff throughput and quality. We show that APD provides markedly higher\nthroughput with minimal quality degradations on downstream benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The generation speed of LLMs are bottlenecked by autoregressive decoding,\nwhere tokens are predicted sequentially one by one. Alternatively, diffusion\nlarge language models (dLLMs) theoretically allow for parallel token\ngeneration, but in practice struggle to achieve the speed of autoregressive\nmodels without significantly sacrificing quality. We therefore introduce\nadaptive parallel decoding (APD), a novel method that dynamically adjusts the\nnumber of tokens sampled in parallel. We achieve this by defining a\nmultiplicative mixture between the dLLM marginal probabilities and the joint\nprobability of sequences under a small auxiliary autoregressive model. This\ninverts the standard setup of speculative decoding, where the goal is to sample\nfrom a large autoregressive verifier by drafting from a smaller model. We\nfurther optimize APD by enabling KV caching and limiting the size of the masked\ninput. Altogether, our method puts forward three tunable parameters to flexibly\ntradeoff throughput and quality. We show that APD provides markedly higher\nthroughput with minimal quality degradations on downstream benchmarks."
                },
                "authors": [
                    {
                        "name": "Daniel Israel"
                    },
                    {
                        "name": "Guy Van den Broeck"
                    },
                    {
                        "name": "Aditya Grover"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Grover"
                },
                "author": "Aditya Grover",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00413v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00413v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26944v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26944v1",
                "updated": "2025-10-30T18:58:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    18,
                    58,
                    2,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T18:58:02Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    18,
                    58,
                    2,
                    3,
                    303,
                    0
                ],
                "title": "Choreographer: A Full-System Framework for Fine-Grained Tasks in Cache\n  Hierarchies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Choreographer: A Full-System Framework for Fine-Grained Tasks in Cache\n  Hierarchies"
                },
                "summary": "In this paper, we introduce Choreographer, a simulation framework that\nenables a holistic system-level evaluation of fine-grained accelerators\ndesigned for latency-sensitive tasks. Unlike existing frameworks, Choreographer\ncaptures all hardware and software overheads in core-accelerator and\ncache-accelerator interactions, integrating a detailed gem5-based hardware\nstack featuring an AMBA coherent hub interface (CHI) mesh network and a\ncomplete Linux-based software stack. To facilitate rapid prototyping, it offers\na C++ application programming interface and modular configuration options. Our\ndetailed cache model provides accurate insights into performance variations\ncaused by cache configurations, which are not captured by other frameworks. The\nframework is demonstrated through two case studies: a data-aware prefetcher for\ngraph analytics workloads, and a quicksort accelerator. Our evaluation shows\nthat the prefetcher achieves speedups between 1.08x and 1.88x by reducing\nmemory access latency, while the quicksort accelerator delivers more than 2x\nspeedup with minimal address translation overhead. These findings underscore\nthe ability of Choreographer to model complex hardware-software interactions\nand optimize performance in small task offloading scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Choreographer, a simulation framework that\nenables a holistic system-level evaluation of fine-grained accelerators\ndesigned for latency-sensitive tasks. Unlike existing frameworks, Choreographer\ncaptures all hardware and software overheads in core-accelerator and\ncache-accelerator interactions, integrating a detailed gem5-based hardware\nstack featuring an AMBA coherent hub interface (CHI) mesh network and a\ncomplete Linux-based software stack. To facilitate rapid prototyping, it offers\na C++ application programming interface and modular configuration options. Our\ndetailed cache model provides accurate insights into performance variations\ncaused by cache configurations, which are not captured by other frameworks. The\nframework is demonstrated through two case studies: a data-aware prefetcher for\ngraph analytics workloads, and a quicksort accelerator. Our evaluation shows\nthat the prefetcher achieves speedups between 1.08x and 1.88x by reducing\nmemory access latency, while the quicksort accelerator delivers more than 2x\nspeedup with minimal address translation overhead. These findings underscore\nthe ability of Choreographer to model complex hardware-software interactions\nand optimize performance in small task offloading scenarios."
                },
                "authors": [
                    {
                        "name": "Hoa Nguyen"
                    },
                    {
                        "name": "Pongstorn Maidee"
                    },
                    {
                        "name": "Jason Lowe-Power"
                    },
                    {
                        "name": "Alireza Kaviani"
                    }
                ],
                "author_detail": {
                    "name": "Alireza Kaviani"
                },
                "author": "Alireza Kaviani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26944v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26944v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26730v1",
                "updated": "2025-10-30T17:29:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    29,
                    27,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T17:29:27Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    29,
                    27,
                    3,
                    303,
                    0
                ],
                "title": "ExpertFlow: Adaptive Expert Scheduling and Memory Coordination for\n  Efficient MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExpertFlow: Adaptive Expert Scheduling and Memory Coordination for\n  Efficient MoE Inference"
                },
                "summary": "The expansion of large language models is increasingly limited by the\nconstrained memory capacity of modern GPUs. To mitigate this,\nMixture-of-Experts (MoE) architectures activate only a small portion of\nparameters during inference, significantly lowering both memory demand and\ncomputational overhead. However, conventional MoE inference approaches, which\nselect active experts independently at each layer, often introduce considerable\nlatency because of frequent parameter transfers between host and GPU memory. In\naddition, current cross-layer prediction strategies, which are typically based\non fixed steps, lack adaptability across different hardware platforms and\nworkloads, thereby reducing their robustness and effectiveness.\n  To address these challenges, we present ExpertFlow, a runtime system for MoE\ninference that combines adaptive expert prefetching and cache-aware routing.\nExpertFlow continuously adjusts its prediction horizon for expert activation by\nleveraging runtime statistics such as transfer bandwidth, parameter\ndimensionality, and model feedback signals. Furthermore, it incorporates a\nhybrid cross-layer prediction scheme that fuses pregating information with\nintermediate computational states to anticipate future expert needs. By\nadaptively refining prefetching decisions and aligning them with actual usage\nbehavior, ExpertFlow effectively decreases cache misses and removes latency\ncaused by expert swap-ins. Our evaluation demonstrates that ExpertFlow reduces\nmodel stall time to less than 0.1% of the baseline, highlighting its capability\nto optimize MoE inference under stringent memory constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expansion of large language models is increasingly limited by the\nconstrained memory capacity of modern GPUs. To mitigate this,\nMixture-of-Experts (MoE) architectures activate only a small portion of\nparameters during inference, significantly lowering both memory demand and\ncomputational overhead. However, conventional MoE inference approaches, which\nselect active experts independently at each layer, often introduce considerable\nlatency because of frequent parameter transfers between host and GPU memory. In\naddition, current cross-layer prediction strategies, which are typically based\non fixed steps, lack adaptability across different hardware platforms and\nworkloads, thereby reducing their robustness and effectiveness.\n  To address these challenges, we present ExpertFlow, a runtime system for MoE\ninference that combines adaptive expert prefetching and cache-aware routing.\nExpertFlow continuously adjusts its prediction horizon for expert activation by\nleveraging runtime statistics such as transfer bandwidth, parameter\ndimensionality, and model feedback signals. Furthermore, it incorporates a\nhybrid cross-layer prediction scheme that fuses pregating information with\nintermediate computational states to anticipate future expert needs. By\nadaptively refining prefetching decisions and aligning them with actual usage\nbehavior, ExpertFlow effectively decreases cache misses and removes latency\ncaused by expert swap-ins. Our evaluation demonstrates that ExpertFlow reduces\nmodel stall time to less than 0.1% of the baseline, highlighting its capability\nto optimize MoE inference under stringent memory constraints."
                },
                "authors": [
                    {
                        "name": "Zixu Shen"
                    },
                    {
                        "name": "Kexin Chu"
                    },
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Dawei Xiang"
                    },
                    {
                        "name": "Runxin Wu"
                    },
                    {
                        "name": "Wei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhang"
                },
                "author": "Wei Zhang",
                "arxiv_comment": "12 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.20499v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.20499v2",
                "updated": "2025-10-30T13:43:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    43,
                    31,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-23T12:39:59Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    12,
                    39,
                    59,
                    3,
                    296,
                    0
                ],
                "title": "GPU-Accelerated Primal Heuristics for Mixed Integer Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU-Accelerated Primal Heuristics for Mixed Integer Programming"
                },
                "summary": "We introduce a fusion of GPU accelerated primal heuristics for Mixed Integer\nProgramming. Leveraging GPU acceleration enables exploration of larger search\nregions and faster iterations. A GPU-accelerated PDLP serves as an approximate\nLP solver, while a new probing cache facilitates rapid roundings and early\ninfeasibility detection. Several state-of-the-art heuristics, including\nFeasibility Pump, Feasibility Jump, and Fix-and-Propagate, are further\naccelerated and enhanced. The combined approach of these GPU-driven algorithms\nyields significant improvements over existing methods, both in the number of\nfeasible solutions and the quality of objectives by achieving 221 feasible\nsolutions and 22% objective gap in the MIPLIB2017 benchmark on a presolved\ndataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a fusion of GPU accelerated primal heuristics for Mixed Integer\nProgramming. Leveraging GPU acceleration enables exploration of larger search\nregions and faster iterations. A GPU-accelerated PDLP serves as an approximate\nLP solver, while a new probing cache facilitates rapid roundings and early\ninfeasibility detection. Several state-of-the-art heuristics, including\nFeasibility Pump, Feasibility Jump, and Fix-and-Propagate, are further\naccelerated and enhanced. The combined approach of these GPU-driven algorithms\nyields significant improvements over existing methods, both in the number of\nfeasible solutions and the quality of objectives by achieving 221 feasible\nsolutions and 22% objective gap in the MIPLIB2017 benchmark on a presolved\ndataset."
                },
                "authors": [
                    {
                        "name": "Akif Çördük"
                    },
                    {
                        "name": "Piotr Sielski"
                    },
                    {
                        "name": "Alice Boucher"
                    },
                    {
                        "name": "Kumar Aatish"
                    }
                ],
                "author_detail": {
                    "name": "Kumar Aatish"
                },
                "author": "Kumar Aatish",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.20499v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.20499v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26486v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26486v1",
                "updated": "2025-10-30T13:39:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    39,
                    8,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T13:39:08Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    39,
                    8,
                    3,
                    303,
                    0
                ],
                "title": "LINK-KG: LLM-Driven Coreference-Resolved Knowledge Graphs for Human\n  Smuggling Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LINK-KG: LLM-Driven Coreference-Resolved Knowledge Graphs for Human\n  Smuggling Networks"
                },
                "summary": "Human smuggling networks are complex and constantly evolving, making them\ndifficult to analyze comprehensively. Legal case documents offer rich factual\nand procedural insights into these networks but are often long, unstructured,\nand filled with ambiguous or shifting references, posing significant challenges\nfor automated knowledge graph (KG) construction. Existing methods either\noverlook coreference resolution or fail to scale beyond short text spans,\nleading to fragmented graphs and inconsistent entity linking. We propose\nLINK-KG, a modular framework that integrates a three-stage, LLM-guided\ncoreference resolution pipeline with downstream KG extraction. At the core of\nour approach is a type-specific Prompt Cache, which consistently tracks and\nresolves references across document chunks, enabling clean and disambiguated\nnarratives for structured knowledge graph construction from both short and long\nlegal texts. LINK-KG reduces average node duplication by 45.21% and noisy nodes\nby 32.22% compared to baseline methods, resulting in cleaner and more coherent\ngraph structures. These improvements establish LINK-KG as a strong foundation\nfor analyzing complex criminal networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human smuggling networks are complex and constantly evolving, making them\ndifficult to analyze comprehensively. Legal case documents offer rich factual\nand procedural insights into these networks but are often long, unstructured,\nand filled with ambiguous or shifting references, posing significant challenges\nfor automated knowledge graph (KG) construction. Existing methods either\noverlook coreference resolution or fail to scale beyond short text spans,\nleading to fragmented graphs and inconsistent entity linking. We propose\nLINK-KG, a modular framework that integrates a three-stage, LLM-guided\ncoreference resolution pipeline with downstream KG extraction. At the core of\nour approach is a type-specific Prompt Cache, which consistently tracks and\nresolves references across document chunks, enabling clean and disambiguated\nnarratives for structured knowledge graph construction from both short and long\nlegal texts. LINK-KG reduces average node duplication by 45.21% and noisy nodes\nby 32.22% compared to baseline methods, resulting in cleaner and more coherent\ngraph structures. These improvements establish LINK-KG as a strong foundation\nfor analyzing complex criminal networks."
                },
                "authors": [
                    {
                        "name": "Dipak Meher"
                    },
                    {
                        "name": "Carlotta Domeniconi"
                    },
                    {
                        "name": "Guadalupe Correa-Cabrera"
                    }
                ],
                "author_detail": {
                    "name": "Guadalupe Correa-Cabrera"
                },
                "author": "Guadalupe Correa-Cabrera",
                "arxiv_comment": "Accepted in ICKG 2025 Conference, 8 Pages, 2 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26486v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26486v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25160v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25160v2",
                "updated": "2025-10-30T08:52:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    8,
                    52,
                    17,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-29T04:29:17Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    4,
                    29,
                    17,
                    2,
                    302,
                    0
                ],
                "title": "Model-Document Protocol for AI Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model-Document Protocol for AI Search"
                },
                "summary": "AI search depends on linking large language models (LLMs) with vast external\nknowledge sources. Yet web pages, PDF files, and other raw documents are not\ninherently LLM-ready: they are long, noisy, and unstructured. Conventional\nretrieval methods treat these documents as verbatim text and return raw\npassages, leaving the burden of fragment assembly and contextual reasoning to\nthe LLM. This gap underscores the need for a new retrieval paradigm that\nredefines how models interact with documents.\n  We introduce the Model-Document Protocol (MDP), a general framework that\nformalizes how raw text is bridged to LLMs through consumable knowledge\nrepresentations. Rather than treating retrieval as passage fetching, MDP\ndefines multiple pathways that transform unstructured documents into\ntask-specific, LLM-ready inputs. These include agentic reasoning, which curates\nraw evidence into coherent context; memory grounding, which accumulates\nreusable notes to enrich reasoning; and structured leveraging, which encodes\ndocuments into formal representations such as graphs or key-value caches. All\nthree pathways share the same goal: ensuring that what reaches the LLM is not\nraw fragments but compact, structured knowledge directly consumable for\nreasoning.\n  As an instantiation, we present MDP-Agent, which realizes the protocol\nthrough an agentic process: constructing document-level gist memories for\nglobal coverage, performing diffusion-based exploration with vertical\nexploitation to uncover layered dependencies, and applying map-reduce style\nsynthesis to integrate large-scale evidence into compact yet sufficient\ncontext. Experiments on information-seeking benchmarks demonstrate that\nMDP-Agent outperforms baselines, validating both the soundness of the MDP\nframework and the effectiveness of its agentic instantiation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI search depends on linking large language models (LLMs) with vast external\nknowledge sources. Yet web pages, PDF files, and other raw documents are not\ninherently LLM-ready: they are long, noisy, and unstructured. Conventional\nretrieval methods treat these documents as verbatim text and return raw\npassages, leaving the burden of fragment assembly and contextual reasoning to\nthe LLM. This gap underscores the need for a new retrieval paradigm that\nredefines how models interact with documents.\n  We introduce the Model-Document Protocol (MDP), a general framework that\nformalizes how raw text is bridged to LLMs through consumable knowledge\nrepresentations. Rather than treating retrieval as passage fetching, MDP\ndefines multiple pathways that transform unstructured documents into\ntask-specific, LLM-ready inputs. These include agentic reasoning, which curates\nraw evidence into coherent context; memory grounding, which accumulates\nreusable notes to enrich reasoning; and structured leveraging, which encodes\ndocuments into formal representations such as graphs or key-value caches. All\nthree pathways share the same goal: ensuring that what reaches the LLM is not\nraw fragments but compact, structured knowledge directly consumable for\nreasoning.\n  As an instantiation, we present MDP-Agent, which realizes the protocol\nthrough an agentic process: constructing document-level gist memories for\nglobal coverage, performing diffusion-based exploration with vertical\nexploitation to uncover layered dependencies, and applying map-reduce style\nsynthesis to integrate large-scale evidence into compact yet sufficient\ncontext. Experiments on information-seeking benchmarks demonstrate that\nMDP-Agent outperforms baselines, validating both the soundness of the MDP\nframework and the effectiveness of its agentic instantiation."
                },
                "authors": [
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Zheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Liu"
                },
                "author": "Zheng Liu",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25160v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25160v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18480v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18480v2",
                "updated": "2025-10-30T08:46:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    8,
                    46,
                    37,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-21T10:00:32Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    10,
                    0,
                    32,
                    1,
                    294,
                    0
                ],
                "title": "How Efficient Are Diffusion Language Models? A Critical Examination of\n  Efficiency Evaluation Practices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Efficient Are Diffusion Language Models? A Critical Examination of\n  Efficiency Evaluation Practices"
                },
                "summary": "Diffusion language models (DLMs) have emerged as a promising alternative to\nthe long-dominant autoregressive (AR) paradigm, offering a parallelable\ndecoding process that could yield greater efficiency. Yet, in practice, current\nopen-source DLMs often underperform their AR counterparts in speed, limiting\ntheir real-world utility. This work presents a systematic study of DLM\nefficiency, identifying key issues in prior evaluation methods. Through\nempirical benchmarking and a roofline-based theoretical analysis, we\ndemonstrate that AR models generally achieve higher throughput, while DLMs\nconsistently lag. We also investigate acceleration strategies, finding that\ntechniques like dual cache and parallel decoding mainly offer gains at small\nbatch sizes, with their benefits diminishing upon scaling. Our findings\nunderscore the necessity of robust evaluation methods and improved acceleration\nstrategies to advance research on DLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models (DLMs) have emerged as a promising alternative to\nthe long-dominant autoregressive (AR) paradigm, offering a parallelable\ndecoding process that could yield greater efficiency. Yet, in practice, current\nopen-source DLMs often underperform their AR counterparts in speed, limiting\ntheir real-world utility. This work presents a systematic study of DLM\nefficiency, identifying key issues in prior evaluation methods. Through\nempirical benchmarking and a roofline-based theoretical analysis, we\ndemonstrate that AR models generally achieve higher throughput, while DLMs\nconsistently lag. We also investigate acceleration strategies, finding that\ntechniques like dual cache and parallel decoding mainly offer gains at small\nbatch sizes, with their benefits diminishing upon scaling. Our findings\nunderscore the necessity of robust evaluation methods and improved acceleration\nstrategies to advance research on DLMs."
                },
                "authors": [
                    {
                        "name": "Han Peng"
                    },
                    {
                        "name": "Peiyu Liu"
                    },
                    {
                        "name": "Zican Dong"
                    },
                    {
                        "name": "Daixuan Cheng"
                    },
                    {
                        "name": "Junyi Li"
                    },
                    {
                        "name": "Yiru Tang"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Wayne Xin Zhao"
                },
                "author": "Wayne Xin Zhao",
                "arxiv_comment": "Withdrawn by the authors to better delineate the related work from\n  the paper's original contributions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18480v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18480v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26234v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26234v1",
                "updated": "2025-10-30T08:12:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    8,
                    12,
                    53,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T08:12:53Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    8,
                    12,
                    53,
                    3,
                    303,
                    0
                ],
                "title": "From req/res to pub/sub: Exploring Media over QUIC Transport for DNS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From req/res to pub/sub: Exploring Media over QUIC Transport for DNS"
                },
                "summary": "The DNS is a key component of the Internet. Originally designed to facilitate\nthe resolution of host names to IP addresses, its scope has continuously\nexpanded over the years, today covering use cases such as load balancing or\nservice discovery. While DNS was initially conceived as a rather static\ndirectory service in which resource records (RR) only change rarely, we have\nseen a number of use cases over the years where a DNS flavor that isn't purely\nbased upon requesting and caching RRs, but rather on an active distribution of\nupdates for all resolvers that showed interest in the respective records in the\npast, would be preferable. In this paper, we thus explore a publish-subscribe\nvariant of DNS based on the Media-over-QUIC architecture, where we devise a\nstrawman system and protocol proposal to enable pushing RR updates. We provide\na prototype implementation, finding that DNS can benefit from a\npublish-subscribe variant: next to limiting update traffic, it can considerably\nreduce the time it takes for a resolver to receive the latest version of a\nrecord, thereby supporting use cases such as load balancing in content\ndistribution networks. The publish-subscribe architecture also brings new\nchallenges to the DNS, including a higher overhead for endpoints due to\nadditional state management, and increased query latencies on first lookup, due\nto session establishment latencies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The DNS is a key component of the Internet. Originally designed to facilitate\nthe resolution of host names to IP addresses, its scope has continuously\nexpanded over the years, today covering use cases such as load balancing or\nservice discovery. While DNS was initially conceived as a rather static\ndirectory service in which resource records (RR) only change rarely, we have\nseen a number of use cases over the years where a DNS flavor that isn't purely\nbased upon requesting and caching RRs, but rather on an active distribution of\nupdates for all resolvers that showed interest in the respective records in the\npast, would be preferable. In this paper, we thus explore a publish-subscribe\nvariant of DNS based on the Media-over-QUIC architecture, where we devise a\nstrawman system and protocol proposal to enable pushing RR updates. We provide\na prototype implementation, finding that DNS can benefit from a\npublish-subscribe variant: next to limiting update traffic, it can considerably\nreduce the time it takes for a resolver to receive the latest version of a\nrecord, thereby supporting use cases such as load balancing in content\ndistribution networks. The publish-subscribe architecture also brings new\nchallenges to the DNS, including a higher overhead for endpoints due to\nadditional state management, and increased query latencies on first lookup, due\nto session establishment latencies."
                },
                "authors": [
                    {
                        "name": "Mathis Engelbart"
                    },
                    {
                        "name": "Mike Kosek"
                    },
                    {
                        "name": "Lars Eggert"
                    },
                    {
                        "name": "Jörg Ott"
                    }
                ],
                "author_detail": {
                    "name": "Jörg Ott"
                },
                "author": "Jörg Ott",
                "arxiv_doi": "10.1145/3772356.3772416",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3772356.3772416",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.26234v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26234v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "HotNets 2025",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.00090v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.00090v1",
                "updated": "2025-10-30T04:57:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    4,
                    57,
                    26,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T04:57:26Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    4,
                    57,
                    26,
                    3,
                    303,
                    0
                ],
                "title": "LeMiCa: Lexicographic Minimax Path Caching for Efficient Diffusion-Based\n  Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LeMiCa: Lexicographic Minimax Path Caching for Efficient Diffusion-Based\n  Video Generation"
                },
                "summary": "We present LeMiCa, a training-free and efficient acceleration framework for\ndiffusion-based video generation. While existing caching strategies primarily\nfocus on reducing local heuristic errors, they often overlook the accumulation\nof global errors, leading to noticeable content degradation between accelerated\nand original videos. To address this issue, we formulate cache scheduling as a\ndirected graph with error-weighted edges and introduce a Lexicographic Minimax\nPath Optimization strategy that explicitly bounds the worst-case path error.\nThis approach substantially improves the consistency of global content and\nstyle across generated frames. Extensive experiments on multiple text-to-video\nbenchmarks demonstrate that LeMiCa delivers dual improvements in both inference\nspeed and generation quality. Notably, our method achieves a 2.9x speedup on\nthe Latte model and reaches an LPIPS score of 0.05 on Open-Sora, outperforming\nprior caching techniques. Importantly, these gains come with minimal perceptual\nquality degradation, making LeMiCa a robust and generalizable paradigm for\naccelerating diffusion-based video generation. We believe this approach can\nserve as a strong foundation for future research on efficient and reliable\nvideo synthesis. Our code is available at :https://github.com/UnicomAI/LeMiCa",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present LeMiCa, a training-free and efficient acceleration framework for\ndiffusion-based video generation. While existing caching strategies primarily\nfocus on reducing local heuristic errors, they often overlook the accumulation\nof global errors, leading to noticeable content degradation between accelerated\nand original videos. To address this issue, we formulate cache scheduling as a\ndirected graph with error-weighted edges and introduce a Lexicographic Minimax\nPath Optimization strategy that explicitly bounds the worst-case path error.\nThis approach substantially improves the consistency of global content and\nstyle across generated frames. Extensive experiments on multiple text-to-video\nbenchmarks demonstrate that LeMiCa delivers dual improvements in both inference\nspeed and generation quality. Notably, our method achieves a 2.9x speedup on\nthe Latte model and reaches an LPIPS score of 0.05 on Open-Sora, outperforming\nprior caching techniques. Importantly, these gains come with minimal perceptual\nquality degradation, making LeMiCa a robust and generalizable paradigm for\naccelerating diffusion-based video generation. We believe this approach can\nserve as a strong foundation for future research on efficient and reliable\nvideo synthesis. Our code is available at :https://github.com/UnicomAI/LeMiCa"
                },
                "authors": [
                    {
                        "name": "Huanlin Gao"
                    },
                    {
                        "name": "Ping Chen"
                    },
                    {
                        "name": "Fuyuan Shi"
                    },
                    {
                        "name": "Chao Tan"
                    },
                    {
                        "name": "Zhaoxiang Liu"
                    },
                    {
                        "name": "Fang Zhao"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Shiguo Lian"
                    }
                ],
                "author_detail": {
                    "name": "Shiguo Lian"
                },
                "author": "Shiguo Lian",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.00090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.00090v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25600v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25600v2",
                "updated": "2025-10-30T03:43:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    3,
                    43,
                    2,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-29T15:10:17Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    10,
                    17,
                    2,
                    302,
                    0
                ],
                "title": "PureKV: Plug-and-Play KV Cache Optimization with Spatial-Temporal Sparse\n  Attention for Vision-Language Large Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PureKV: Plug-and-Play KV Cache Optimization with Spatial-Temporal Sparse\n  Attention for Vision-Language Large Models"
                },
                "summary": "Vision-Language Large Models (VLLMs) face significant efficiency challenges\nwhen processing high-resolution inputs. The quadratic complexity in attention\nand autoregressive generation, as well as the constantly growing key value (KV)\ncache size, severely hinder the prefilling and decoding stages. Recent efforts\nhave attempted to compress KV cache by identifying and pruning KV cache of less\nimportant tokens, but these methods typically rely on attention scores to\nestimate token importance, making them incompatible with efficient attention\nmechanisms such as FlashAttention and Sparse Attention, which do not explicitly\ncompute attention matrices. Moreover, existing methods overlook how sparse\nattention, while accelerating the prefilling stage, alters the information\nstructure of the KV cache, thereby compromising the effectiveness of downstream\nKV cache compression strategies. To address this issue, we propose PureKV, a\nplug-and-play framework for joint optimization of sparse attention and KV cache\ncompression. We first introduce a KV cache compression strategy that is fully\ncompatible with efficient attention accelerators. Our method utilizes lower\nlayer attention scores to estimate the importance of high layers' KV cache,\nenabling active pruning without compromising accuracy. In addition, we have\ndesigned a Spatial-Temporal Sparse Attention (ST-SpAttn) module specifically\ntailored for video KV cache compression algorithms. This module combines\nspatial and temporal attention sparsity to improve the compression efficiency\nof KV cache optimization algorithms by purifying spatial noise and temporal\nredundancy in KV cache. At the same time, ST-SpAttn also accelerated the\nprefilling stage of VLLMs. Extensive experiments on VLLMs (VideoLLaMA2,\nQwen2.5-VL) have shown that PureKV achieves 5.0 times KV cache compression and\n3.16 times prefill acceleration, with negligible quality degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Large Models (VLLMs) face significant efficiency challenges\nwhen processing high-resolution inputs. The quadratic complexity in attention\nand autoregressive generation, as well as the constantly growing key value (KV)\ncache size, severely hinder the prefilling and decoding stages. Recent efforts\nhave attempted to compress KV cache by identifying and pruning KV cache of less\nimportant tokens, but these methods typically rely on attention scores to\nestimate token importance, making them incompatible with efficient attention\nmechanisms such as FlashAttention and Sparse Attention, which do not explicitly\ncompute attention matrices. Moreover, existing methods overlook how sparse\nattention, while accelerating the prefilling stage, alters the information\nstructure of the KV cache, thereby compromising the effectiveness of downstream\nKV cache compression strategies. To address this issue, we propose PureKV, a\nplug-and-play framework for joint optimization of sparse attention and KV cache\ncompression. We first introduce a KV cache compression strategy that is fully\ncompatible with efficient attention accelerators. Our method utilizes lower\nlayer attention scores to estimate the importance of high layers' KV cache,\nenabling active pruning without compromising accuracy. In addition, we have\ndesigned a Spatial-Temporal Sparse Attention (ST-SpAttn) module specifically\ntailored for video KV cache compression algorithms. This module combines\nspatial and temporal attention sparsity to improve the compression efficiency\nof KV cache optimization algorithms by purifying spatial noise and temporal\nredundancy in KV cache. At the same time, ST-SpAttn also accelerated the\nprefilling stage of VLLMs. Extensive experiments on VLLMs (VideoLLaMA2,\nQwen2.5-VL) have shown that PureKV achieves 5.0 times KV cache compression and\n3.16 times prefill acceleration, with negligible quality degradation."
                },
                "authors": [
                    {
                        "name": "Zhonghua Jiang"
                    },
                    {
                        "name": "Kunxi Li"
                    },
                    {
                        "name": "Yiyun Zhou"
                    },
                    {
                        "name": "Sihao Liu"
                    },
                    {
                        "name": "Zhaode Wang"
                    },
                    {
                        "name": "Chengfei lv"
                    },
                    {
                        "name": "Shengyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shengyu Zhang"
                },
                "author": "Shengyu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25600v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25600v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26104v1",
                "updated": "2025-10-30T03:30:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    3,
                    30,
                    12,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T03:30:12Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    3,
                    30,
                    12,
                    3,
                    303,
                    0
                ],
                "title": "OneTrans: Unified Feature Interaction and Sequence Modeling with One\n  Transformer in Industrial Recommender",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OneTrans: Unified Feature Interaction and Sequence Modeling with One\n  Transformer in Industrial Recommender"
                },
                "summary": "In recommendation systems, scaling up feature-interaction modules (e.g.,\nWukong, RankMixer) or user-behavior sequence modules (e.g., LONGER) has\nachieved notable success. However, these efforts typically proceed on separate\ntracks, which not only hinders bidirectional information exchange but also\nprevents unified optimization and scaling. In this paper, we propose OneTrans,\na unified Transformer backbone that simultaneously performs user-behavior\nsequence modeling and feature interaction. OneTrans employs a unified tokenizer\nto convert both sequential and non-sequential attributes into a single token\nsequence. The stacked OneTrans blocks share parameters across similar\nsequential tokens while assigning token-specific parameters to non-sequential\ntokens. Through causal attention and cross-request KV caching, OneTrans enables\nprecomputation and caching of intermediate representations, significantly\nreducing computational costs during both training and inference. Experimental\nresults on industrial-scale datasets demonstrate that OneTrans scales\nefficiently with increasing parameters, consistently outperforms strong\nbaselines, and yields a 5.68% lift in per-user GMV in online A/B tests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recommendation systems, scaling up feature-interaction modules (e.g.,\nWukong, RankMixer) or user-behavior sequence modules (e.g., LONGER) has\nachieved notable success. However, these efforts typically proceed on separate\ntracks, which not only hinders bidirectional information exchange but also\nprevents unified optimization and scaling. In this paper, we propose OneTrans,\na unified Transformer backbone that simultaneously performs user-behavior\nsequence modeling and feature interaction. OneTrans employs a unified tokenizer\nto convert both sequential and non-sequential attributes into a single token\nsequence. The stacked OneTrans blocks share parameters across similar\nsequential tokens while assigning token-specific parameters to non-sequential\ntokens. Through causal attention and cross-request KV caching, OneTrans enables\nprecomputation and caching of intermediate representations, significantly\nreducing computational costs during both training and inference. Experimental\nresults on industrial-scale datasets demonstrate that OneTrans scales\nefficiently with increasing parameters, consistently outperforms strong\nbaselines, and yields a 5.68% lift in per-user GMV in online A/B tests."
                },
                "authors": [
                    {
                        "name": "Zhaoqi Zhang"
                    },
                    {
                        "name": "Haolei Pei"
                    },
                    {
                        "name": "Jun Guo"
                    },
                    {
                        "name": "Tianyu Wang"
                    },
                    {
                        "name": "Yufei Feng"
                    },
                    {
                        "name": "Hui Sun"
                    },
                    {
                        "name": "Shaowei Liu"
                    },
                    {
                        "name": "Aixin Sun"
                    }
                ],
                "author_detail": {
                    "name": "Aixin Sun"
                },
                "author": "Aixin Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11507v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11507v2",
                "updated": "2025-10-29T21:56:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    21,
                    56,
                    19,
                    2,
                    302,
                    0
                ],
                "published": "2025-07-15T17:23:22Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    17,
                    23,
                    22,
                    1,
                    196,
                    0
                ],
                "title": "Oneiros: KV Cache Optimization through Parameter Remapping for\n  Multi-tenant LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oneiros: KV Cache Optimization through Parameter Remapping for\n  Multi-tenant LLM Serving"
                },
                "summary": "KV cache accelerates LLM inference by avoiding redundant computation, at the\nexpense of memory. To support larger KV caches, prior work extends GPU memory\nwith CPU memory via CPU-offloading. This involves swapping KV cache between GPU\nand CPU memory. However, because the cache updates dynamically, such swapping\nincurs high CPU memory traffic. We make a key observation that model parameters\nremain constant during runtime, unlike the dynamically updated KV cache.\nBuilding on this, we introduce Oneiros, which avoids KV cache swapping by\nremapping, and thereby repurposing, the memory allocated to model parameters\nfor KV cache. This parameter remapping is especially beneficial in multi-tenant\nenvironments, where the memory used for the parameters of the inactive models\ncan be more aggressively reclaimed. Exploiting the high CPU-GPU bandwidth\noffered by the modern hardware, such as the NVIDIA Grace Hopper Superchip, we\nshow that Oneiros significantly outperforms state-of-the-art solutions,\nachieving a reduction of 44.8%-82.5% in tail time-between-token latency,\n20.7%-99.3% in tail time-to-first-token latency, and 6.6%-86.7% higher\nthroughput compared to vLLM. Source code of Oneiros is available at\nhttps://github.com/UT-SysML/Oneiros/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache accelerates LLM inference by avoiding redundant computation, at the\nexpense of memory. To support larger KV caches, prior work extends GPU memory\nwith CPU memory via CPU-offloading. This involves swapping KV cache between GPU\nand CPU memory. However, because the cache updates dynamically, such swapping\nincurs high CPU memory traffic. We make a key observation that model parameters\nremain constant during runtime, unlike the dynamically updated KV cache.\nBuilding on this, we introduce Oneiros, which avoids KV cache swapping by\nremapping, and thereby repurposing, the memory allocated to model parameters\nfor KV cache. This parameter remapping is especially beneficial in multi-tenant\nenvironments, where the memory used for the parameters of the inactive models\ncan be more aggressively reclaimed. Exploiting the high CPU-GPU bandwidth\noffered by the modern hardware, such as the NVIDIA Grace Hopper Superchip, we\nshow that Oneiros significantly outperforms state-of-the-art solutions,\nachieving a reduction of 44.8%-82.5% in tail time-between-token latency,\n20.7%-99.3% in tail time-to-first-token latency, and 6.6%-86.7% higher\nthroughput compared to vLLM. Source code of Oneiros is available at\nhttps://github.com/UT-SysML/Oneiros/."
                },
                "authors": [
                    {
                        "name": "Ruihao Li"
                    },
                    {
                        "name": "Shagnik Pal"
                    },
                    {
                        "name": "Vineeth Narayan Pullu"
                    },
                    {
                        "name": "Prasoon Sinha"
                    },
                    {
                        "name": "Jeeho Ryoo"
                    },
                    {
                        "name": "Lizy K. John"
                    },
                    {
                        "name": "Neeraja J. Yadwadkar"
                    }
                ],
                "author_detail": {
                    "name": "Neeraja J. Yadwadkar"
                },
                "author": "Neeraja J. Yadwadkar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11507v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11507v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26835v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26835v1",
                "updated": "2025-10-29T19:59:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    19,
                    59,
                    45,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T19:59:45Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    19,
                    59,
                    45,
                    2,
                    302,
                    0
                ],
                "title": "Category-Aware Semantic Caching for Heterogeneous LLM Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Category-Aware Semantic Caching for Heterogeneous LLM Workloads"
                },
                "summary": "LLM serving systems process heterogeneous query workloads where different\ncategories exhibit different characteristics. Code queries cluster densely in\nembedding space while conversational queries distribute sparsely. Content\nstaleness varies from minutes (stock data) to months (code patterns). Query\nrepetition patterns range from power-law (code) to uniform (conversation),\nproducing long tail cache hit rate distributions: high-repetition categories\nachieve 40-60% hit rates while low-repetition or volatile categories achieve\n5-15% hit rates. Vector databases must exclude the long tail because remote\nsearch costs (30ms) require 15--20% hit rates to break even, leaving 20-30% of\nproduction traffic uncached. Uniform cache policies compound this problem:\nfixed thresholds cause false positives in dense spaces and miss valid\nparaphrases in sparse spaces; fixed TTLs waste memory or serve stale data. This\npaper presents category-aware semantic caching where similarity thresholds,\nTTLs, and quotas vary by query category. We present a hybrid architecture\nseparating in-memory HNSW search from external document storage, reducing miss\ncost from 30ms to 2ms. This reduction makes low-hit-rate categories\neconomically viable (break-even at 3-5% versus 15-20%), enabling cache coverage\nacross the entire workload distribution. Adaptive load-based policies extend\nthis framework to respond to downstream model load, dynamically adjusting\nthresholds and TTLs to reduce traffic to overloaded models by 9-17% in\ntheoretical projections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM serving systems process heterogeneous query workloads where different\ncategories exhibit different characteristics. Code queries cluster densely in\nembedding space while conversational queries distribute sparsely. Content\nstaleness varies from minutes (stock data) to months (code patterns). Query\nrepetition patterns range from power-law (code) to uniform (conversation),\nproducing long tail cache hit rate distributions: high-repetition categories\nachieve 40-60% hit rates while low-repetition or volatile categories achieve\n5-15% hit rates. Vector databases must exclude the long tail because remote\nsearch costs (30ms) require 15--20% hit rates to break even, leaving 20-30% of\nproduction traffic uncached. Uniform cache policies compound this problem:\nfixed thresholds cause false positives in dense spaces and miss valid\nparaphrases in sparse spaces; fixed TTLs waste memory or serve stale data. This\npaper presents category-aware semantic caching where similarity thresholds,\nTTLs, and quotas vary by query category. We present a hybrid architecture\nseparating in-memory HNSW search from external document storage, reducing miss\ncost from 30ms to 2ms. This reduction makes low-hit-rate categories\neconomically viable (break-even at 3-5% versus 15-20%), enabling cache coverage\nacross the entire workload distribution. Adaptive load-based policies extend\nthis framework to respond to downstream model load, dynamically adjusting\nthresholds and TTLs to reduce traffic to overloaded models by 9-17% in\ntheoretical projections."
                },
                "authors": [
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Xunzhuo Liu"
                    },
                    {
                        "name": "Yue Zhu"
                    },
                    {
                        "name": "Alaa Youssef"
                    },
                    {
                        "name": "Priya Nagpurkar"
                    },
                    {
                        "name": "Huamin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huamin Chen"
                },
                "author": "Huamin Chen",
                "arxiv_comment": "13 pages including reference, position paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26835v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26835v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25695v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25695v1",
                "updated": "2025-10-29T17:00:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    0,
                    16,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T17:00:16Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    0,
                    16,
                    2,
                    302,
                    0
                ],
                "title": "Over 3 kV and Ultra-Low leakage Vertical (011) \\b{eta}-Ga2O3 Power\n  Diodes with Engineered Schottky Contact and High-permittivity Dielectric\n  Field Plate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over 3 kV and Ultra-Low leakage Vertical (011) \\b{eta}-Ga2O3 Power\n  Diodes with Engineered Schottky Contact and High-permittivity Dielectric\n  Field Plate"
                },
                "summary": "We report over 3 kV breakdown voltage and ultra-low leakage (011)\n\\b{eta}-Ga2O3 power devices utilizing Schottky barrier engineering and\nhigh-permittivity (\\k{appa}) dielectric (ZrO2) field plate. The (011)\norientation of \\b{eta}-Ga2O3 enabled low background doping and thick drift\nlayers which are promising to support kV-class vertical \\b{eta}-Ga2O3 power\nswitches. The Schottky barrier engineering was performed with a composite Pt\ncap/PtOx/Pt (1.5 nm) anode contact to take advantage of the enhanced reverse\nblocking capabilities enabled by PtOx while allowing low turn-on voltage by the\ninterfacing thin Pt layer. We also performed a systematic study using a\nco-processed Pt/(011) \\b{eta}-Ga2O3 Schottky barrier diodes (SBDs) on the same\nwafer. The bare SBDs revealed a breakdown voltage of ~1.5 kV, while the\nfield-plate Pt/(011) \\b{eta}-Ga2O3 SBDs achieved an increased breakdown voltage\nof 2.75 kV owing to the edge field management. Further enhancement of the\nbreakdown voltage was achieved by tunneling leakage management using composite\nPt cap/PtOx/Pt (1.5 nm) Schottky contacts that ultimately enabled breakdown\nvoltage of 3.7 kV for the field-plate diodes. Remarkably, the Pt cap/PtOx/Pt\n(1.5 nm) Schottky contacts maintained similar turn-on voltage as the Pt/(011)\n\\b{eta}-Ga2O3 SBDs. The combination of efficient tunneling leakage management\nby composite Pt cap/PtOx/Pt (1.5 nm) contacts with similar turn-on voltage,\nedge field reduction by high-\\k{appa} dielectric ZrO2 field plate, as well as\nthe advantageous material properties offered by (011) \\b{eta}-Ga2O3 demonstrate\na promising strategy for developing ultra-low leakage and multi-kV class\nvertical (011) \\b{eta}-Ga2O3 power devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report over 3 kV breakdown voltage and ultra-low leakage (011)\n\\b{eta}-Ga2O3 power devices utilizing Schottky barrier engineering and\nhigh-permittivity (\\k{appa}) dielectric (ZrO2) field plate. The (011)\norientation of \\b{eta}-Ga2O3 enabled low background doping and thick drift\nlayers which are promising to support kV-class vertical \\b{eta}-Ga2O3 power\nswitches. The Schottky barrier engineering was performed with a composite Pt\ncap/PtOx/Pt (1.5 nm) anode contact to take advantage of the enhanced reverse\nblocking capabilities enabled by PtOx while allowing low turn-on voltage by the\ninterfacing thin Pt layer. We also performed a systematic study using a\nco-processed Pt/(011) \\b{eta}-Ga2O3 Schottky barrier diodes (SBDs) on the same\nwafer. The bare SBDs revealed a breakdown voltage of ~1.5 kV, while the\nfield-plate Pt/(011) \\b{eta}-Ga2O3 SBDs achieved an increased breakdown voltage\nof 2.75 kV owing to the edge field management. Further enhancement of the\nbreakdown voltage was achieved by tunneling leakage management using composite\nPt cap/PtOx/Pt (1.5 nm) Schottky contacts that ultimately enabled breakdown\nvoltage of 3.7 kV for the field-plate diodes. Remarkably, the Pt cap/PtOx/Pt\n(1.5 nm) Schottky contacts maintained similar turn-on voltage as the Pt/(011)\n\\b{eta}-Ga2O3 SBDs. The combination of efficient tunneling leakage management\nby composite Pt cap/PtOx/Pt (1.5 nm) contacts with similar turn-on voltage,\nedge field reduction by high-\\k{appa} dielectric ZrO2 field plate, as well as\nthe advantageous material properties offered by (011) \\b{eta}-Ga2O3 demonstrate\na promising strategy for developing ultra-low leakage and multi-kV class\nvertical (011) \\b{eta}-Ga2O3 power devices."
                },
                "authors": [
                    {
                        "name": "Emerson J. Hollar"
                    },
                    {
                        "name": "Esmat Farzana"
                    }
                ],
                "author_detail": {
                    "name": "Esmat Farzana"
                },
                "author": "Esmat Farzana",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25695v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25695v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25604v1",
                "updated": "2025-10-29T15:12:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    12,
                    35,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T15:12:35Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    12,
                    35,
                    2,
                    302,
                    0
                ],
                "title": "Quickest Change Point Detection with Measurements over a Lossy Link",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quickest Change Point Detection with Measurements over a Lossy Link"
                },
                "summary": "Motivated by Industry 4.0 applications, we consider quickest change detection\n(QCD) of an abrupt change in a process when its measurements are transmitted by\na sensor over a lossy wireless link to a decision maker (DM). The sensor node\nsamples measurements using a Bernoulli sampling process, and places the\nmeasurement samples in the transmit queue of its transmitter. The transmitter\nuses a retransmit-until-success transmission strategy to deliver packets to the\nDM over the lossy link, in which the packet losses are modeled as a Bernoulli\nprocess, with different loss probabilities before and after the change. We pose\nthe QCD problem in the non-Bayesian setting under Lorden's framework, and\npropose a CUSUM algorithm. By defining a suitable Markov process, involving the\nDM measurements and the queue length process, we show that the problem reduces\nto QCD in a Markov process. Characterizing the information measure per\nmeasurement sample at the DM, we establish the asymptotic optimality of our\nalgorithm when the false alarm rate tends to zero. Further, when the DM\nreceives incomplete data due to channel loss, we present asymptotically optimal\nQCD algorithms by suitably modifying the CUSUM algorithm. We then explore the\nlast-come-first-served (LCFS) queuing discipline at the sensor transmit queue\nto lower detection delay in the non-asymptotic case. Next, we consider the case\nof multiple sensors, each with its own wireless transmitter queue, and show\nthat our analysis extends to the case of multiple homogeneous sensors. When the\nsensors are heterogeneous, we present a sensor scheduling algorithm that\nminimizes detection delay by balancing the trade-off between the age of the\nobservations and their information content. Numerical analysis demonstrate\ntrade-offs that can be used to optimize system design parameters in the\nnon-asymptotic regime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated by Industry 4.0 applications, we consider quickest change detection\n(QCD) of an abrupt change in a process when its measurements are transmitted by\na sensor over a lossy wireless link to a decision maker (DM). The sensor node\nsamples measurements using a Bernoulli sampling process, and places the\nmeasurement samples in the transmit queue of its transmitter. The transmitter\nuses a retransmit-until-success transmission strategy to deliver packets to the\nDM over the lossy link, in which the packet losses are modeled as a Bernoulli\nprocess, with different loss probabilities before and after the change. We pose\nthe QCD problem in the non-Bayesian setting under Lorden's framework, and\npropose a CUSUM algorithm. By defining a suitable Markov process, involving the\nDM measurements and the queue length process, we show that the problem reduces\nto QCD in a Markov process. Characterizing the information measure per\nmeasurement sample at the DM, we establish the asymptotic optimality of our\nalgorithm when the false alarm rate tends to zero. Further, when the DM\nreceives incomplete data due to channel loss, we present asymptotically optimal\nQCD algorithms by suitably modifying the CUSUM algorithm. We then explore the\nlast-come-first-served (LCFS) queuing discipline at the sensor transmit queue\nto lower detection delay in the non-asymptotic case. Next, we consider the case\nof multiple sensors, each with its own wireless transmitter queue, and show\nthat our analysis extends to the case of multiple homogeneous sensors. When the\nsensors are heterogeneous, we present a sensor scheduling algorithm that\nminimizes detection delay by balancing the trade-off between the age of the\nobservations and their information content. Numerical analysis demonstrate\ntrade-offs that can be used to optimize system design parameters in the\nnon-asymptotic regime."
                },
                "authors": [
                    {
                        "name": "Krishna Chaythanya KV"
                    },
                    {
                        "name": "Saqib Abbas Baba"
                    },
                    {
                        "name": "Anurag Kumar"
                    },
                    {
                        "name": "Arpan Chattopadhyay"
                    },
                    {
                        "name": "Rajesh Sundaresan"
                    }
                ],
                "author_detail": {
                    "name": "Rajesh Sundaresan"
                },
                "author": "Rajesh Sundaresan",
                "arxiv_comment": "17 pages, 6 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25590v1",
                "updated": "2025-10-29T14:58:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    14,
                    58,
                    37,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T14:58:37Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    14,
                    58,
                    37,
                    2,
                    302,
                    0
                ],
                "title": "RegionE: Adaptive Region-Aware Generation for Efficient Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RegionE: Adaptive Region-Aware Generation for Efficient Image Editing"
                },
                "summary": "Recently, instruction-based image editing (IIE) has received widespread\nattention. In practice, IIE often modifies only specific regions of an image,\nwhile the remaining areas largely remain unchanged. Although these two types of\nregions differ significantly in generation difficulty and computational\nredundancy, existing IIE models do not account for this distinction, instead\napplying a uniform generation process across the entire image. This motivates\nus to propose RegionE, an adaptive, region-aware generation framework that\naccelerates IIE tasks without additional training. Specifically, the RegionE\nframework consists of three main components: 1) Adaptive Region Partition. We\nobserved that the trajectory of unedited regions is straight, allowing for\nmulti-step denoised predictions to be inferred in a single step. Therefore, in\nthe early denoising stages, we partition the image into edited and unedited\nregions based on the difference between the final estimated result and the\nreference image. 2) Region-Aware Generation. After distinguishing the regions,\nwe replace multi-step denoising with one-step prediction for unedited areas.\nFor edited regions, the trajectory is curved, requiring local iterative\ndenoising. To improve the efficiency and quality of local iterative generation,\nwe propose the Region-Instruction KV Cache, which reduces computational cost\nwhile incorporating global information. 3) Adaptive Velocity Decay Cache.\nObserving that adjacent timesteps in edited regions exhibit strong velocity\nsimilarity, we further propose an adaptive velocity decay cache to accelerate\nthe local denoising process. We applied RegionE to state-of-the-art IIE base\nmodels, including Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit. RegionE\nachieved acceleration factors of 2.57, 2.41, and 2.06. Evaluations by GPT-4o\nconfirmed that semantic and perceptual fidelity were well preserved.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, instruction-based image editing (IIE) has received widespread\nattention. In practice, IIE often modifies only specific regions of an image,\nwhile the remaining areas largely remain unchanged. Although these two types of\nregions differ significantly in generation difficulty and computational\nredundancy, existing IIE models do not account for this distinction, instead\napplying a uniform generation process across the entire image. This motivates\nus to propose RegionE, an adaptive, region-aware generation framework that\naccelerates IIE tasks without additional training. Specifically, the RegionE\nframework consists of three main components: 1) Adaptive Region Partition. We\nobserved that the trajectory of unedited regions is straight, allowing for\nmulti-step denoised predictions to be inferred in a single step. Therefore, in\nthe early denoising stages, we partition the image into edited and unedited\nregions based on the difference between the final estimated result and the\nreference image. 2) Region-Aware Generation. After distinguishing the regions,\nwe replace multi-step denoising with one-step prediction for unedited areas.\nFor edited regions, the trajectory is curved, requiring local iterative\ndenoising. To improve the efficiency and quality of local iterative generation,\nwe propose the Region-Instruction KV Cache, which reduces computational cost\nwhile incorporating global information. 3) Adaptive Velocity Decay Cache.\nObserving that adjacent timesteps in edited regions exhibit strong velocity\nsimilarity, we further propose an adaptive velocity decay cache to accelerate\nthe local denoising process. We applied RegionE to state-of-the-art IIE base\nmodels, including Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit. RegionE\nachieved acceleration factors of 2.57, 2.41, and 2.06. Evaluations by GPT-4o\nconfirmed that semantic and perceptual fidelity were well preserved."
                },
                "authors": [
                    {
                        "name": "Pengtao Chen"
                    },
                    {
                        "name": "Xianfang Zeng"
                    },
                    {
                        "name": "Maosen Zhao"
                    },
                    {
                        "name": "Mingzhu Shen"
                    },
                    {
                        "name": "Peng Ye"
                    },
                    {
                        "name": "Bangyin Xiang"
                    },
                    {
                        "name": "Zhibo Wang"
                    },
                    {
                        "name": "Wei Cheng"
                    },
                    {
                        "name": "Gang Yu"
                    },
                    {
                        "name": "Tao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tao Chen"
                },
                "author": "Tao Chen",
                "arxiv_comment": "26 pages, 10 figures, 18 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21710v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21710v2",
                "updated": "2025-10-29T14:46:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    14,
                    46,
                    17,
                    2,
                    302,
                    0
                ],
                "published": "2025-06-26T18:51:04Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    18,
                    51,
                    4,
                    3,
                    177,
                    0
                ],
                "title": "FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual\n  Question Answering"
                },
                "summary": "While Multimodal Large Language Models (MLLMs) offer strong perception and\nreasoning capabilities for image-text input, Visual Question Answering (VQA)\nfocusing on small image details still remains a challenge. Although visual\ncropping techniques seem promising, recent approaches have several limitations:\nthe need for task-specific fine-tuning, low efficiency due to uninformed\nexhaustive search, or incompatibility with efficient attention implementations.\nWe address these shortcomings by proposing a training-free visual cropping\nmethod, dubbed FOCUS, that leverages MLLM-internal representations to guide the\nsearch for the most relevant image region. This is accomplished in four steps:\nfirst, we identify the target object(s) in the VQA prompt; second, we compute\nan object relevance map using the key-value (KV) cache; third, we propose and\nrank relevant image regions based on the map; and finally, we perform the\nfine-grained VQA task using the top-ranked region. As a result of this informed\nsearch strategy, FOCUS achieves strong performance across four fine-grained VQA\ndatasets and three types of MLLMs. It outperforms three popular visual cropping\nmethods in both accuracy and efficiency, and matches the best-performing\nbaseline, ZoomEye, while requiring 3 - 6.5 x less compute.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Multimodal Large Language Models (MLLMs) offer strong perception and\nreasoning capabilities for image-text input, Visual Question Answering (VQA)\nfocusing on small image details still remains a challenge. Although visual\ncropping techniques seem promising, recent approaches have several limitations:\nthe need for task-specific fine-tuning, low efficiency due to uninformed\nexhaustive search, or incompatibility with efficient attention implementations.\nWe address these shortcomings by proposing a training-free visual cropping\nmethod, dubbed FOCUS, that leverages MLLM-internal representations to guide the\nsearch for the most relevant image region. This is accomplished in four steps:\nfirst, we identify the target object(s) in the VQA prompt; second, we compute\nan object relevance map using the key-value (KV) cache; third, we propose and\nrank relevant image regions based on the map; and finally, we perform the\nfine-grained VQA task using the top-ranked region. As a result of this informed\nsearch strategy, FOCUS achieves strong performance across four fine-grained VQA\ndatasets and three types of MLLMs. It outperforms three popular visual cropping\nmethods in both accuracy and efficiency, and matches the best-performing\nbaseline, ZoomEye, while requiring 3 - 6.5 x less compute."
                },
                "authors": [
                    {
                        "name": "Liangyu Zhong"
                    },
                    {
                        "name": "Fabio Rosenthal"
                    },
                    {
                        "name": "Joachim Sicking"
                    },
                    {
                        "name": "Fabian Hüger"
                    },
                    {
                        "name": "Thorsten Bagdonat"
                    },
                    {
                        "name": "Hanno Gottschalk"
                    },
                    {
                        "name": "Leo Schwinn"
                    }
                ],
                "author_detail": {
                    "name": "Leo Schwinn"
                },
                "author": "Leo Schwinn",
                "arxiv_comment": "Accepted by NeurIPS 2025 - main track. Project page:\n  https://focus-mllm-vqa.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21710v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21710v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25412v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25412v1",
                "updated": "2025-10-29T11:29:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    11,
                    29,
                    3,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T11:29:03Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    11,
                    29,
                    3,
                    2,
                    302,
                    0
                ],
                "title": "Serve Programs, Not Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serve Programs, Not Prompts"
                },
                "summary": "Current large language model (LLM) serving systems, primarily designed for\ntext completion, are neither efficient nor adaptable for increasingly complex\nLLM applications due to their inflexible design. We propose a new LLM serving\nsystem architecture that serves programs instead of prompts to address this\nproblem. These programs, called LLM Inference Programs (LIPs), allow users to\ncustomize token prediction and KV cache management at runtime and to offload\nparts of their application logic, such as tool execution, to the server. We\ndescribe an example of this architecture through a system named Symphony, which\nfunctions as an operating system for LIPs. Symphony exposes LLM model\ncomputations via system calls and virtualizes KV cache with a dedicated file\nsystem, while ensuring GPU efficiency with a two-level process scheduling\nscheme. Symphony has the potential to open the door to a more efficient and\nextensible ecosystem for LLM applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current large language model (LLM) serving systems, primarily designed for\ntext completion, are neither efficient nor adaptable for increasingly complex\nLLM applications due to their inflexible design. We propose a new LLM serving\nsystem architecture that serves programs instead of prompts to address this\nproblem. These programs, called LLM Inference Programs (LIPs), allow users to\ncustomize token prediction and KV cache management at runtime and to offload\nparts of their application logic, such as tool execution, to the server. We\ndescribe an example of this architecture through a system named Symphony, which\nfunctions as an operating system for LIPs. Symphony exposes LLM model\ncomputations via system calls and virtualizes KV cache with a dedicated file\nsystem, while ensuring GPU efficiency with a two-level process scheduling\nscheme. Symphony has the potential to open the door to a more efficient and\nextensible ecosystem for LLM applications."
                },
                "authors": [
                    {
                        "name": "In Gim"
                    },
                    {
                        "name": "Lin Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Lin Zhong"
                },
                "author": "Lin Zhong",
                "arxiv_doi": "10.1145/3713082.3730398",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3713082.3730398",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.25412v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25412v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "HotOS 2025. Follow-up implementation work (SOSP 2025) is available at\n  arXiv:2510.24051",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25152v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25152v1",
                "updated": "2025-10-29T04:09:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    4,
                    9,
                    50,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T04:09:50Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    4,
                    9,
                    50,
                    2,
                    302,
                    0
                ],
                "title": "Off-Centered WoS-Type Solvers with Statistical Weighting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Off-Centered WoS-Type Solvers with Statistical Weighting"
                },
                "summary": "Stochastic PDE solvers have emerged as a powerful alternative to traditional\ndiscretization-based methods for solving partial differential equations (PDEs),\nespecially in geometry processing and graphics. While off-centered estimators\nenhance sample reuse in WoS-type Monte Carlo solvers, they introduce\ncorrelation artifacts and bias when Green's functions are approximated. In this\npaper, we propose a statistically weighted off-centered WoS-type estimator that\nleverages local similarity filtering to selectively combine samples across\nneighboring evaluation points. Our method balances bias and variance through a\nprincipled weighting strategy that suppresses unreliable estimators. We\ndemonstrate our approach's effectiveness on various PDEs,including screened\nPoisson equations and boundary conditions, achieving consistent improvements\nover existing solvers such as vanilla Walk on Spheres, mean value caching, and\nboundary value caching. Our method also naturally extends to gradient field\nestimation and mixed boundary problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic PDE solvers have emerged as a powerful alternative to traditional\ndiscretization-based methods for solving partial differential equations (PDEs),\nespecially in geometry processing and graphics. While off-centered estimators\nenhance sample reuse in WoS-type Monte Carlo solvers, they introduce\ncorrelation artifacts and bias when Green's functions are approximated. In this\npaper, we propose a statistically weighted off-centered WoS-type estimator that\nleverages local similarity filtering to selectively combine samples across\nneighboring evaluation points. Our method balances bias and variance through a\nprincipled weighting strategy that suppresses unreliable estimators. We\ndemonstrate our approach's effectiveness on various PDEs,including screened\nPoisson equations and boundary conditions, achieving consistent improvements\nover existing solvers such as vanilla Walk on Spheres, mean value caching, and\nboundary value caching. Our method also naturally extends to gradient field\nestimation and mixed boundary problems."
                },
                "authors": [
                    {
                        "name": "Anchang Bao"
                    },
                    {
                        "name": "Jie Xu"
                    },
                    {
                        "name": "Enya Shen"
                    },
                    {
                        "name": "Jianmin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jianmin Wang"
                },
                "author": "Jianmin Wang",
                "arxiv_comment": "SIGGRAPH Asia 2025 conference paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25152v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25152v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25122v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25122v1",
                "updated": "2025-10-29T03:00:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    3,
                    0,
                    36,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T03:00:36Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    3,
                    0,
                    36,
                    2,
                    302,
                    0
                ],
                "title": "NanoVLA: Routing Decoupled Vision-Language Understanding for Nano-sized\n  Generalist Robotic Policies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NanoVLA: Routing Decoupled Vision-Language Understanding for Nano-sized\n  Generalist Robotic Policies"
                },
                "summary": "Vision-language-action (VLA) models have significantly advanced robotic\nmanipulation by integrating vision-language models (VLMs), and action decoders\ninto a unified architecture. However, their deployment on resource-constrained\nedge devices, such as mobile robots or embedded systems (e.g., Jetson Orin\nNano), remains challenging due to high computational demands, especially in\nreal-world scenarios where power, latency, and computational resources are\ncritical. To close this gap, we introduce Nano-scale Vision-Language Action\n(NanoVLA), a family of lightweight VLA architectures that achieve high\nperformance with minimal resources. Our core innovations include: (1)\nvision-language decoupling that moves conventional early vision and language\ninputs fusion in VLM to late stage, achieving better performance while enabling\ncaching and reduce inference overhead and latency; (2) long-short action\nchunking to ensure smooth, coherent multi-step planning without sacrificing\nreal-time responsiveness; (3) dynamic routing that adaptively assigns\nlightweight or heavy backbones based on task complexity, further optimizing\ninference efficiency. Experimental results on several benchmarks, as well as\nreal-world deployments, demonstrate that NanoVLA achieves up to 52x faster\ninference on edge devices compared to previous state-of-the-art VLA models,\nwith 98% less parameters while maintaining or surpassing their task accuracy\nand generalization. Ablation studies confirm that our decoupling strategy\npreserves cross-task transferability, and the routing module enhances\ncost-performance trade-offs, enabling practical, high-precision robotic\nmanipulation on resource-constrained hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language-action (VLA) models have significantly advanced robotic\nmanipulation by integrating vision-language models (VLMs), and action decoders\ninto a unified architecture. However, their deployment on resource-constrained\nedge devices, such as mobile robots or embedded systems (e.g., Jetson Orin\nNano), remains challenging due to high computational demands, especially in\nreal-world scenarios where power, latency, and computational resources are\ncritical. To close this gap, we introduce Nano-scale Vision-Language Action\n(NanoVLA), a family of lightweight VLA architectures that achieve high\nperformance with minimal resources. Our core innovations include: (1)\nvision-language decoupling that moves conventional early vision and language\ninputs fusion in VLM to late stage, achieving better performance while enabling\ncaching and reduce inference overhead and latency; (2) long-short action\nchunking to ensure smooth, coherent multi-step planning without sacrificing\nreal-time responsiveness; (3) dynamic routing that adaptively assigns\nlightweight or heavy backbones based on task complexity, further optimizing\ninference efficiency. Experimental results on several benchmarks, as well as\nreal-world deployments, demonstrate that NanoVLA achieves up to 52x faster\ninference on edge devices compared to previous state-of-the-art VLA models,\nwith 98% less parameters while maintaining or surpassing their task accuracy\nand generalization. Ablation studies confirm that our decoupling strategy\npreserves cross-task transferability, and the routing module enhances\ncost-performance trade-offs, enabling practical, high-precision robotic\nmanipulation on resource-constrained hardware."
                },
                "authors": [
                    {
                        "name": "Jiahong Chen"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Long Chen"
                    },
                    {
                        "name": "Chuwei Cai"
                    },
                    {
                        "name": "Jinghui Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jinghui Lu"
                },
                "author": "Jinghui Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25122v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25122v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24824v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24824v1",
                "updated": "2025-10-28T15:35:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    15,
                    35,
                    50,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T15:35:50Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    15,
                    35,
                    50,
                    1,
                    301,
                    0
                ],
                "title": "Parallel Loop Transformer for Efficient Test-Time Computation Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel Loop Transformer for Efficient Test-Time Computation Scaling"
                },
                "summary": "Large Language Models (LLMs) are powerful but often too slow and costly for\nreal-world use during inference. Looped transformers save on parameters by\nreusing the same weights for multiple computational steps, or \"loops.\" However,\nthis approach has a major flaw: the loops run one after another, causing\ninference latency and memory requirements to increase with each added loop.\nThis makes them impractical for fast applications. To solve this problem, we\nintroduce the Parallel Loop Transformer (PLT). PLT is a new architecture that\ndelivers the performance benefits of a deep, looped model but with the low\nlatency of a standard, non-looped model. PLT works using two key techniques.\nFirst, Cross-Loop Parallelism (CLP) breaks the sequential dependency by\ncomputing different loops for different tokens at the same time, all within a\nsingle pass. Second, to prevent memory costs from growing, we use an Efficient\nRepresentation Enhancement strategy. This method shares the memory (KV cache)\nfrom the first loop with all other loops. It then uses a Gated Sliding-Window\nAttention (G-SWA) to combine this shared global information with local\ninformation, maintaining high accuracy. Our experiments show that PLT achieves\nthe high accuracy of a traditional looped model but with almost no extra\nlatency or memory cost compared to a standard transformer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are powerful but often too slow and costly for\nreal-world use during inference. Looped transformers save on parameters by\nreusing the same weights for multiple computational steps, or \"loops.\" However,\nthis approach has a major flaw: the loops run one after another, causing\ninference latency and memory requirements to increase with each added loop.\nThis makes them impractical for fast applications. To solve this problem, we\nintroduce the Parallel Loop Transformer (PLT). PLT is a new architecture that\ndelivers the performance benefits of a deep, looped model but with the low\nlatency of a standard, non-looped model. PLT works using two key techniques.\nFirst, Cross-Loop Parallelism (CLP) breaks the sequential dependency by\ncomputing different loops for different tokens at the same time, all within a\nsingle pass. Second, to prevent memory costs from growing, we use an Efficient\nRepresentation Enhancement strategy. This method shares the memory (KV cache)\nfrom the first loop with all other loops. It then uses a Gated Sliding-Window\nAttention (G-SWA) to combine this shared global information with local\ninformation, maintaining high accuracy. Our experiments show that PLT achieves\nthe high accuracy of a traditional looped model but with almost no extra\nlatency or memory cost compared to a standard transformer."
                },
                "authors": [
                    {
                        "name": "Bohong Wu"
                    },
                    {
                        "name": "Mengzhao Chen"
                    },
                    {
                        "name": "Xiang Luo"
                    },
                    {
                        "name": "Shen Yan"
                    },
                    {
                        "name": "Qifan Yu"
                    },
                    {
                        "name": "Fan Xia"
                    },
                    {
                        "name": "Tianqi Zhang"
                    },
                    {
                        "name": "Hongrui Zhan"
                    },
                    {
                        "name": "Zheng Zhong"
                    },
                    {
                        "name": "Xun Zhou"
                    },
                    {
                        "name": "Siyuan Qiao"
                    },
                    {
                        "name": "Xingyan Bin"
                    }
                ],
                "author_detail": {
                    "name": "Xingyan Bin"
                },
                "author": "Xingyan Bin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24824v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24824v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24359v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24359v1",
                "updated": "2025-10-28T12:28:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    12,
                    28,
                    2,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T12:28:02Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    12,
                    28,
                    2,
                    1,
                    301,
                    0
                ],
                "title": "An N-of-1 Artificial Intelligence Ecosystem for Precision Medicine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An N-of-1 Artificial Intelligence Ecosystem for Precision Medicine"
                },
                "summary": "Artificial intelligence in medicine is built to serve the average patient. By\nminimizing error across large datasets, most systems deliver strong aggregate\naccuracy yet falter at the margins: patients with rare variants,\nmultimorbidity, or underrepresented demographics. This average patient fallacy\nerodes both equity and trust. We propose a different design: a multi-agent\necosystem for N-of-1 decision support. In this environment, agents clustered by\norgan systems, patient populations, and analytic modalities draw on a shared\nlibrary of models and evidence synthesis tools. Their results converge in a\ncoordination layer that weighs reliability, uncertainty, and data density\nbefore presenting the clinician with a decision-support packet: risk estimates\nbounded by confidence ranges, outlier flags, and linked evidence. Validation\nshifts from population averages to individual reliability, measured by error in\nlow-density regions, calibration in the small, and risk--coverage trade-offs.\nAnticipated challenges include computational demands, automation bias, and\nregulatory fit, addressed through caching strategies, consensus checks, and\nadaptive trial frameworks. By moving from monolithic models to orchestrated\nintelligence, this approach seeks to align medical AI with the first principle\nof medicine: care that is transparent, equitable, and centered on the\nindividual.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence in medicine is built to serve the average patient. By\nminimizing error across large datasets, most systems deliver strong aggregate\naccuracy yet falter at the margins: patients with rare variants,\nmultimorbidity, or underrepresented demographics. This average patient fallacy\nerodes both equity and trust. We propose a different design: a multi-agent\necosystem for N-of-1 decision support. In this environment, agents clustered by\norgan systems, patient populations, and analytic modalities draw on a shared\nlibrary of models and evidence synthesis tools. Their results converge in a\ncoordination layer that weighs reliability, uncertainty, and data density\nbefore presenting the clinician with a decision-support packet: risk estimates\nbounded by confidence ranges, outlier flags, and linked evidence. Validation\nshifts from population averages to individual reliability, measured by error in\nlow-density regions, calibration in the small, and risk--coverage trade-offs.\nAnticipated challenges include computational demands, automation bias, and\nregulatory fit, addressed through caching strategies, consensus checks, and\nadaptive trial frameworks. By moving from monolithic models to orchestrated\nintelligence, this approach seeks to align medical AI with the first principle\nof medicine: care that is transparent, equitable, and centered on the\nindividual."
                },
                "authors": [
                    {
                        "name": "Pedram Fard"
                    },
                    {
                        "name": "Alaleh Azhir"
                    },
                    {
                        "name": "Neguine Rezaii"
                    },
                    {
                        "name": "Jiazi Tian"
                    },
                    {
                        "name": "Hossein Estiri"
                    }
                ],
                "author_detail": {
                    "name": "Hossein Estiri"
                },
                "author": "Hossein Estiri",
                "arxiv_comment": "This study has been supported by grants from the National Institutes\n  of Health: The National Institute on Aging R01AG074372 and The National\n  Institute of Allergy and Infectious Diseases R01AI165535",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24359v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24359v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24273v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24273v1",
                "updated": "2025-10-28T10:32:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    10,
                    32,
                    52,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T10:32:52Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    10,
                    32,
                    52,
                    1,
                    301,
                    0
                ],
                "title": "SALS: Sparse Attention in Latent Space for KV cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SALS: Sparse Attention in Latent Space for KV cache Compression"
                },
                "summary": "Large Language Models capable of handling extended contexts are in high\ndemand, yet their inference remains challenging due to substantial Key-Value\ncache size and high memory bandwidth requirements. Previous research has\ndemonstrated that KV cache exhibits low-rank characteristics within the hidden\ndimension, suggesting the potential for effective compression. However, due to\nthe widely adopted Rotary Position Embedding mechanism in modern LLMs, naive\nlow-rank compression suffers severe accuracy degradation or creates a new speed\nbottleneck, as the low-rank cache must first be reconstructed in order to apply\nRoPE. In this paper, we introduce two key insights: first, the application of\nRoPE to the key vectors increases their variance, which in turn results in a\nhigher rank; second, after the key vectors are transformed into the latent\nspace, they largely maintain their representation across most layers. Based on\nthese insights, we propose the Sparse Attention in Latent Space framework. SALS\nprojects the KV cache into a compact latent space via low-rank projection, and\nperforms sparse token selection using RoPE-free query-key interactions in this\nspace. By reconstructing only a small subset of important tokens, it avoids the\noverhead of full KV cache reconstruction. We comprehensively evaluate SALS on\nvarious tasks using two large-scale models: LLaMA2-7b-chat and Mistral-7b, and\nadditionally verify its scalability on the RULER-128k benchmark with\nLLaMA3.1-8B-Instruct. Experimental results demonstrate that SALS achieves SOTA\nperformance by maintaining competitive accuracy. Under different settings, SALS\nachieves 6.4-fold KV cache compression and 5.7-fold speed-up in the attention\noperator compared to FlashAttention2 on the 4K sequence. For the end-to-end\nthroughput performance, we achieves 1.4-fold and 4.5-fold improvement compared\nto GPT-fast on 4k and 32K sequences, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models capable of handling extended contexts are in high\ndemand, yet their inference remains challenging due to substantial Key-Value\ncache size and high memory bandwidth requirements. Previous research has\ndemonstrated that KV cache exhibits low-rank characteristics within the hidden\ndimension, suggesting the potential for effective compression. However, due to\nthe widely adopted Rotary Position Embedding mechanism in modern LLMs, naive\nlow-rank compression suffers severe accuracy degradation or creates a new speed\nbottleneck, as the low-rank cache must first be reconstructed in order to apply\nRoPE. In this paper, we introduce two key insights: first, the application of\nRoPE to the key vectors increases their variance, which in turn results in a\nhigher rank; second, after the key vectors are transformed into the latent\nspace, they largely maintain their representation across most layers. Based on\nthese insights, we propose the Sparse Attention in Latent Space framework. SALS\nprojects the KV cache into a compact latent space via low-rank projection, and\nperforms sparse token selection using RoPE-free query-key interactions in this\nspace. By reconstructing only a small subset of important tokens, it avoids the\noverhead of full KV cache reconstruction. We comprehensively evaluate SALS on\nvarious tasks using two large-scale models: LLaMA2-7b-chat and Mistral-7b, and\nadditionally verify its scalability on the RULER-128k benchmark with\nLLaMA3.1-8B-Instruct. Experimental results demonstrate that SALS achieves SOTA\nperformance by maintaining competitive accuracy. Under different settings, SALS\nachieves 6.4-fold KV cache compression and 5.7-fold speed-up in the attention\noperator compared to FlashAttention2 on the 4K sequence. For the end-to-end\nthroughput performance, we achieves 1.4-fold and 4.5-fold improvement compared\nto GPT-fast on 4k and 32K sequences, respectively."
                },
                "authors": [
                    {
                        "name": "Junlin Mu"
                    },
                    {
                        "name": "Hantao Huang"
                    },
                    {
                        "name": "Jihang Zhang"
                    },
                    {
                        "name": "Minghui Yu"
                    },
                    {
                        "name": "Tao Wang"
                    },
                    {
                        "name": "Yidong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yidong Li"
                },
                "author": "Yidong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24273v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24273v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24051v1",
                "updated": "2025-10-28T04:17:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    4,
                    17,
                    55,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T04:17:55Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    4,
                    17,
                    55,
                    1,
                    301,
                    0
                ],
                "title": "Pie: A Programmable Serving System for Emerging LLM Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pie: A Programmable Serving System for Emerging LLM Applications"
                },
                "summary": "Emerging large language model (LLM) applications involve diverse reasoning\nstrategies and agentic workflows, straining the capabilities of existing\nserving systems built on a monolithic token generation loop. This paper\nintroduces Pie, a programmable LLM serving system designed for flexibility and\nefficiency. Pie decomposes the traditional generation loop into fine-grained\nservice handlers exposed via an API and delegates control of the generation\nprocess to user-provided programs, called inferlets. This enables applications\nto implement new KV cache strategies, bespoke generation logic, and seamlessly\nintegrate computation and I/O-entirely within the application, without\nrequiring modifications to the serving system. Pie executes inferlets using\nWebAssembly, benefiting from its lightweight sandboxing. Our evaluation shows\nPie matches state-of-the-art performance on standard tasks (3-12% latency\noverhead) while significantly improving latency and throughput (1.3x-3.4x\nhigher) on agentic workflows by enabling application-specific optimizations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging large language model (LLM) applications involve diverse reasoning\nstrategies and agentic workflows, straining the capabilities of existing\nserving systems built on a monolithic token generation loop. This paper\nintroduces Pie, a programmable LLM serving system designed for flexibility and\nefficiency. Pie decomposes the traditional generation loop into fine-grained\nservice handlers exposed via an API and delegates control of the generation\nprocess to user-provided programs, called inferlets. This enables applications\nto implement new KV cache strategies, bespoke generation logic, and seamlessly\nintegrate computation and I/O-entirely within the application, without\nrequiring modifications to the serving system. Pie executes inferlets using\nWebAssembly, benefiting from its lightweight sandboxing. Our evaluation shows\nPie matches state-of-the-art performance on standard tasks (3-12% latency\noverhead) while significantly improving latency and throughput (1.3x-3.4x\nhigher) on agentic workflows by enabling application-specific optimizations."
                },
                "authors": [
                    {
                        "name": "In Gim"
                    },
                    {
                        "name": "Zhiyao Ma"
                    },
                    {
                        "name": "Seung-seob Lee"
                    },
                    {
                        "name": "Lin Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Lin Zhong"
                },
                "author": "Lin Zhong",
                "arxiv_doi": "10.1145/3731569.3764814",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3731569.3764814",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.24051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "SOSP 2025. Source code available at\n  https://github.com/pie-project/pie",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01068v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01068v4",
                "updated": "2025-10-28T04:00:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    4,
                    0,
                    18,
                    1,
                    301,
                    0
                ],
                "published": "2025-02-03T05:25:09Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    5,
                    25,
                    9,
                    0,
                    34,
                    0
                ],
                "title": "FastKV: KV Cache Compression for Fast Long-Context Processing with\n  Token-Selective Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastKV: KV Cache Compression for Fast Long-Context Processing with\n  Token-Selective Propagation"
                },
                "summary": "While large language models (LLMs) excel at handling long-context sequences,\nthey require substantial prefill computation and key-value (KV) cache, which\ncan heavily burden computational efficiency and memory usage in both prefill\nand decoding stages. Recent works that compress KV caches with prefill\nacceleration reduce this cost but inadvertently tie the prefill compute\nreduction to the decoding KV budget. This coupling arises from overlooking the\nlayer-dependent variation of critical context, often leading to accuracy\ndegradation. To address this issue, we introduce FastKV, a KV cache compression\nframework designed to reduce latency in both prefill and decoding by leveraging\nthe stabilization of token importance in later layers. FastKV performs\nfull-context computation until a Token-Selective Propagation (TSP) layer, which\nforwards only the most informative tokens to subsequent layers. From these\npropagated tokens, FastKV independently selects salient KV entries for caching,\nthereby decoupling KV budget from the prefill compute reduction based on the\nTSP decision. This independent control of the TSP rate and KV retention rate\nenables flexible optimization of efficiency and accuracy. Experimental results\nshow that FastKV achieves speedups of up to 1.82$\\times$ in prefill and\n2.87$\\times$ in decoding compared to the full-context baseline, while matching\nthe accuracy of the baselines that only accelerate the decoding stage. Our code\nis available at https://github.com/dongwonjo/FastKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) excel at handling long-context sequences,\nthey require substantial prefill computation and key-value (KV) cache, which\ncan heavily burden computational efficiency and memory usage in both prefill\nand decoding stages. Recent works that compress KV caches with prefill\nacceleration reduce this cost but inadvertently tie the prefill compute\nreduction to the decoding KV budget. This coupling arises from overlooking the\nlayer-dependent variation of critical context, often leading to accuracy\ndegradation. To address this issue, we introduce FastKV, a KV cache compression\nframework designed to reduce latency in both prefill and decoding by leveraging\nthe stabilization of token importance in later layers. FastKV performs\nfull-context computation until a Token-Selective Propagation (TSP) layer, which\nforwards only the most informative tokens to subsequent layers. From these\npropagated tokens, FastKV independently selects salient KV entries for caching,\nthereby decoupling KV budget from the prefill compute reduction based on the\nTSP decision. This independent control of the TSP rate and KV retention rate\nenables flexible optimization of efficiency and accuracy. Experimental results\nshow that FastKV achieves speedups of up to 1.82$\\times$ in prefill and\n2.87$\\times$ in decoding compared to the full-context baseline, while matching\nthe accuracy of the baselines that only accelerate the decoding stage. Our code\nis available at https://github.com/dongwonjo/FastKV."
                },
                "authors": [
                    {
                        "name": "Dongwon Jo"
                    },
                    {
                        "name": "Jiwon Song"
                    },
                    {
                        "name": "Yulhwa Kim"
                    },
                    {
                        "name": "Jae-Joon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Joon Kim"
                },
                "author": "Jae-Joon Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01068v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01068v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14969v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14969v2",
                "updated": "2025-10-27T21:48:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    27,
                    21,
                    48,
                    48,
                    0,
                    300,
                    0
                ],
                "published": "2025-05-20T23:12:16Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    23,
                    12,
                    16,
                    1,
                    140,
                    0
                ],
                "title": "STree: Speculative Tree Decoding for Hybrid State-Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STree: Speculative Tree Decoding for Hybrid State-Space Models"
                },
                "summary": "Speculative decoding is a technique to leverage hardware concurrency in order\nto enable multiple steps of token generation in a single forward pass, thus\nimproving the efficiency of large-scale autoregressive (AR) Transformer models.\nState-space models (SSMs) are already more efficient than AR Transformers,\nsince their state summarizes all past data with no need to cache or re-process\ntokens in the sliding window context. However, their state can also comprise\nthousands of tokens; so, speculative decoding has recently been extended to\nSSMs. Existing approaches, however, do not leverage the tree-based verification\nmethods, since current SSMs lack the means to compute a token tree efficiently.\nWe propose the first scalable algorithm to perform tree-based speculative\ndecoding in state-space models (SSMs) and hybrid architectures of SSMs and\nTransformer layers. We exploit the structure of accumulated state transition\nmatrices to facilitate tree-based speculative decoding with minimal overhead\nrelative to current SSM implementations. Along with the algorithm, we describe\na hardware-aware implementation that improves naive application of AR\nTransformer tree-based speculative decoding methods to SSMs. Furthermore, we\noutperform vanilla speculative decoding with SSMs even with a baseline drafting\nmodel and tree structure on three different benchmarks, opening up\nopportunities for further speed up with SSM and hybrid model inference. Code\ncan be found at: https://github.com/wyc1997/stree.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is a technique to leverage hardware concurrency in order\nto enable multiple steps of token generation in a single forward pass, thus\nimproving the efficiency of large-scale autoregressive (AR) Transformer models.\nState-space models (SSMs) are already more efficient than AR Transformers,\nsince their state summarizes all past data with no need to cache or re-process\ntokens in the sliding window context. However, their state can also comprise\nthousands of tokens; so, speculative decoding has recently been extended to\nSSMs. Existing approaches, however, do not leverage the tree-based verification\nmethods, since current SSMs lack the means to compute a token tree efficiently.\nWe propose the first scalable algorithm to perform tree-based speculative\ndecoding in state-space models (SSMs) and hybrid architectures of SSMs and\nTransformer layers. We exploit the structure of accumulated state transition\nmatrices to facilitate tree-based speculative decoding with minimal overhead\nrelative to current SSM implementations. Along with the algorithm, we describe\na hardware-aware implementation that improves naive application of AR\nTransformer tree-based speculative decoding methods to SSMs. Furthermore, we\noutperform vanilla speculative decoding with SSMs even with a baseline drafting\nmodel and tree structure on three different benchmarks, opening up\nopportunities for further speed up with SSM and hybrid model inference. Code\ncan be found at: https://github.com/wyc1997/stree."
                },
                "authors": [
                    {
                        "name": "Yangchao Wu"
                    },
                    {
                        "name": "Zongyue Qin"
                    },
                    {
                        "name": "Alex Wong"
                    },
                    {
                        "name": "Stefano Soatto"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Soatto"
                },
                "author": "Stefano Soatto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14969v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14969v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12362v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12362v2",
                "updated": "2025-10-27T17:31:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    27,
                    17,
                    31,
                    15,
                    0,
                    300,
                    0
                ],
                "published": "2024-04-18T17:45:19Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    17,
                    45,
                    19,
                    3,
                    109,
                    0
                ],
                "title": "KV-weights are all you need for skipless transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-weights are all you need for skipless transformers"
                },
                "summary": "He and Hofmann (arXiv:2311.01906) detailed a skipless transformer without the\nV and P (post-attention projection) linear layers, which reduces the total\nnumber of weights. However, this scheme is only applicable to MHA (multi-head\nattention), but not for MQA (multi-query attention) and GQA (grouped-query\nattention). The latter schemes are used by many popular LLMs such as Llama 2,\nMistral, Mixtral, PaLM, and Gemma. Therefore, this micro-paper proposes\nmathematically equivalent versions that are suitable for MQA and GQA. For\nexample, removing Q and P from a skipless version of Mistral-7B would remove\n15% of its weights (and thus reduce its compute and memory complexity). Watch\nour explainer video https://youtu.be/Tx_lMpphd2g and see\nhttps://github.com/OpenMachine-ai/transformer-tricks for code and more\ntransformer tricks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "He and Hofmann (arXiv:2311.01906) detailed a skipless transformer without the\nV and P (post-attention projection) linear layers, which reduces the total\nnumber of weights. However, this scheme is only applicable to MHA (multi-head\nattention), but not for MQA (multi-query attention) and GQA (grouped-query\nattention). The latter schemes are used by many popular LLMs such as Llama 2,\nMistral, Mixtral, PaLM, and Gemma. Therefore, this micro-paper proposes\nmathematically equivalent versions that are suitable for MQA and GQA. For\nexample, removing Q and P from a skipless version of Mistral-7B would remove\n15% of its weights (and thus reduce its compute and memory complexity). Watch\nour explainer video https://youtu.be/Tx_lMpphd2g and see\nhttps://github.com/OpenMachine-ai/transformer-tricks for code and more\ntransformer tricks."
                },
                "authors": [
                    {
                        "name": "Nils Graef"
                    }
                ],
                "author_detail": {
                    "name": "Nils Graef"
                },
                "author": "Nils Graef",
                "arxiv_comment": "6 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12362v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12362v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05530v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05530v3",
                "updated": "2025-10-27T16:20:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    27,
                    16,
                    20,
                    28,
                    0,
                    300,
                    0
                ],
                "published": "2025-03-07T15:54:04Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    54,
                    4,
                    4,
                    66,
                    0
                ],
                "title": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) improves the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, substantially reducing the reliance on expensive vector\ndatabase lookups. To efficiently scale, Proximity employs a locality-sensitive\nhashing (LSH) scheme that enables fast cache lookups while preserving retrieval\naccuracy. We evaluate Proximity using the MMLU and MedRAG question-answering\nbenchmarks. Our experiments demonstrate that Proximity with our LSH scheme and\na realistically-skewed MedRAG workload reduces database calls by 77.2% while\nmaintaining database recall and test accuracy. We experiment with different\nsimilarity tolerances and cache capacities, and show that the time spent within\nthe Proximity cache remains low and constant (4.8 microseconds) even as the\ncache grows substantially in size. Our results demonstrate that approximate\ncaching is a practical and effective strategy for optimizing RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) improves the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, substantially reducing the reliance on expensive vector\ndatabase lookups. To efficiently scale, Proximity employs a locality-sensitive\nhashing (LSH) scheme that enables fast cache lookups while preserving retrieval\naccuracy. We evaluate Proximity using the MMLU and MedRAG question-answering\nbenchmarks. Our experiments demonstrate that Proximity with our LSH scheme and\na realistically-skewed MedRAG workload reduces database calls by 77.2% while\nmaintaining database recall and test accuracy. We experiment with different\nsimilarity tolerances and cache capacities, and show that the time spent within\nthe Proximity cache remains low and constant (4.8 microseconds) even as the\ncache grows substantially in size. Our results demonstrate that approximate\ncaching is a practical and effective strategy for optimizing RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Shai Bergman"
                    },
                    {
                        "name": "Anne-Marie Kermarrec"
                    },
                    {
                        "name": "Diana Petrescu"
                    },
                    {
                        "name": "Rafael Pires"
                    },
                    {
                        "name": "Mathis Randl"
                    },
                    {
                        "name": "Martijn de Vos"
                    },
                    {
                        "name": "Ji Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ji Zhang"
                },
                "author": "Ji Zhang",
                "arxiv_doi": "10.1145/3721462.3770776",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3721462.3770776",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.05530v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05530v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at Middleware '25",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08343v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08343v2",
                "updated": "2025-10-27T14:59:46Z",
                "updated_parsed": [
                    2025,
                    10,
                    27,
                    14,
                    59,
                    46,
                    0,
                    300,
                    0
                ],
                "published": "2025-08-11T10:47:35Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    10,
                    47,
                    35,
                    0,
                    223,
                    0
                ],
                "title": "A Data-driven ML Approach for Maximizing Performance in LLM-Adapter\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Data-driven ML Approach for Maximizing Performance in LLM-Adapter\n  Serving"
                },
                "summary": "With the rapid adoption of Large Language Models (LLMs), LLM-adapters have\nbecome increasingly common, providing lightweight specialization of large-scale\nmodels. Serving hundreds or thousands of these adapters on a single GPU allows\nrequest aggregation, increasing throughput, but may also cause request\nstarvation if GPU memory limits are exceeded. To address this issue, this study\nfocuses on determining the joint configuration of concurrent and parallel\nadapters that maximizes GPU throughput without inducing starvation, given\nheterogeneous adapter and traffic properties. We propose a data-driven ML\napproach leveraging interpretable models to tackle this caching problem and\nintroduce the first Digital Twin capable of reproducing an LLM-adapter serving\nsystem, enabling efficient training data generation. Experiments with the vLLM\nframework and LoRA adapters show that the Digital Twin reproduces throughput\nwithin 5.1% of real results, while the ML approach predicts optimal numbers of\nconcurrent and parallel adapters with an error of at most 7.2% under\nheterogeneous, real-world workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid adoption of Large Language Models (LLMs), LLM-adapters have\nbecome increasingly common, providing lightweight specialization of large-scale\nmodels. Serving hundreds or thousands of these adapters on a single GPU allows\nrequest aggregation, increasing throughput, but may also cause request\nstarvation if GPU memory limits are exceeded. To address this issue, this study\nfocuses on determining the joint configuration of concurrent and parallel\nadapters that maximizes GPU throughput without inducing starvation, given\nheterogeneous adapter and traffic properties. We propose a data-driven ML\napproach leveraging interpretable models to tackle this caching problem and\nintroduce the first Digital Twin capable of reproducing an LLM-adapter serving\nsystem, enabling efficient training data generation. Experiments with the vLLM\nframework and LoRA adapters show that the Digital Twin reproduces throughput\nwithin 5.1% of real results, while the ML approach predicts optimal numbers of\nconcurrent and parallel adapters with an error of at most 7.2% under\nheterogeneous, real-world workloads."
                },
                "authors": [
                    {
                        "name": "Ferran Agullo"
                    },
                    {
                        "name": "Joan Oliveras"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Alberto Gutierrez-Torre"
                    },
                    {
                        "name": "Olivier Tardieu"
                    },
                    {
                        "name": "Alaa Youssef"
                    },
                    {
                        "name": "Jordi Torres"
                    },
                    {
                        "name": "Josep Ll. Berral"
                    }
                ],
                "author_detail": {
                    "name": "Josep Ll. Berral"
                },
                "author": "Josep Ll. Berral",
                "arxiv_comment": "Accepted in a computer science workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08343v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08343v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01488v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01488v2",
                "updated": "2025-10-27T11:55:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    27,
                    11,
                    55,
                    7,
                    0,
                    300,
                    0
                ],
                "published": "2025-08-02T21:00:55Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    21,
                    0,
                    55,
                    5,
                    214,
                    0
                ],
                "title": "PESTO: Real-Time Pitch Estimation with Self-supervised\n  Transposition-equivariant Objective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PESTO: Real-Time Pitch Estimation with Self-supervised\n  Transposition-equivariant Objective"
                },
                "summary": "In this paper, we introduce PESTO, a self-supervised learning approach for\nsingle-pitch estimation using a Siamese architecture. Our model processes\nindividual frames of a Variable-$Q$ Transform (VQT) and predicts pitch\ndistributions. The neural network is designed to be equivariant to\ntranslations, notably thanks to a Toeplitz fully-connected layer. In addition,\nwe construct pitch-shifted pairs by translating and cropping the VQT frames and\ntrain our model with a novel class-based transposition-equivariant objective,\neliminating the need for annotated data. Thanks to this architecture and\ntraining objective, our model achieves remarkable performances while being very\nlightweight ($130$k parameters). Evaluations on music and speech datasets\n(MIR-1K, MDB-stem-synth, and PTDB) demonstrate that PESTO not only outperforms\nself-supervised baselines but also competes with supervised methods, exhibiting\nsuperior cross-dataset generalization. Finally, we enhance PESTO's practical\nutility by developing a streamable VQT implementation using cached\nconvolutions. Combined with our model's low latency (less than 10 ms) and\nminimal parameter count, this makes PESTO particularly suitable for real-time\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce PESTO, a self-supervised learning approach for\nsingle-pitch estimation using a Siamese architecture. Our model processes\nindividual frames of a Variable-$Q$ Transform (VQT) and predicts pitch\ndistributions. The neural network is designed to be equivariant to\ntranslations, notably thanks to a Toeplitz fully-connected layer. In addition,\nwe construct pitch-shifted pairs by translating and cropping the VQT frames and\ntrain our model with a novel class-based transposition-equivariant objective,\neliminating the need for annotated data. Thanks to this architecture and\ntraining objective, our model achieves remarkable performances while being very\nlightweight ($130$k parameters). Evaluations on music and speech datasets\n(MIR-1K, MDB-stem-synth, and PTDB) demonstrate that PESTO not only outperforms\nself-supervised baselines but also competes with supervised methods, exhibiting\nsuperior cross-dataset generalization. Finally, we enhance PESTO's practical\nutility by developing a streamable VQT implementation using cached\nconvolutions. Combined with our model's low latency (less than 10 ms) and\nminimal parameter count, this makes PESTO particularly suitable for real-time\napplications."
                },
                "authors": [
                    {
                        "name": "Alain Riou"
                    },
                    {
                        "name": "Bernardo Torres"
                    },
                    {
                        "name": "Ben Hayes"
                    },
                    {
                        "name": "Stefan Lattner"
                    },
                    {
                        "name": "Gaëtan Hadjeres"
                    },
                    {
                        "name": "Gaël Richard"
                    },
                    {
                        "name": "Geoffroy Peeters"
                    }
                ],
                "author_detail": {
                    "name": "Geoffroy Peeters"
                },
                "author": "Geoffroy Peeters",
                "arxiv_doi": "10.5334/TISMIR.251",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.5334/TISMIR.251",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.01488v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01488v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Transactions of the International Society for Music Information\n  Retrieval, 8(1): 334-352 (2025)",
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22876v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22876v1",
                "updated": "2025-10-26T23:59:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    26,
                    23,
                    59,
                    23,
                    6,
                    299,
                    0
                ],
                "published": "2025-10-26T23:59:23Z",
                "published_parsed": [
                    2025,
                    10,
                    26,
                    23,
                    59,
                    23,
                    6,
                    299,
                    0
                ],
                "title": "Batch Speculative Decoding Done Right",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch Speculative Decoding Done Right"
                },
                "summary": "Speculative decoding speeds up LLM inference by using a small draft model to\npropose multiple tokens that a target model verifies in parallel. Extending\nthis idea to batches is essential for production serving, but it introduces the\nragged tensor problem: sequences in the same batch accept different numbers of\ndraft tokens, breaking right-alignment and corrupting position IDs, attention\nmasks, and KV-cache state. We show that several existing batch implementations\nviolate output equivalence-the fundamental requirement that speculative\ndecoding must produce identical token sequences to standard autoregressive\ngeneration. These violations occur precisely due to improper handling of the\nragged tensor problem. In response, we (1) characterize the synchronization\nrequirements that guarantee correctness, (2) present a correctness-first batch\nspeculative decoding EQSPEC that exposes realignment as consuming 40% of\noverhead, and (3) introduce EXSPEC, which maintains a sliding pool of sequences\nand dynamically forms same-length groups, to reduce the realignment overhead\nwhile preserving per-sequence speculative speedups. On the SpecBench dataset,\nacross Vicuna-7B/68M, Qwen3-8B/0.6B, and GLM-4-9B/0.6B target/draft pairs, our\napproach achieves up to 3$\\times$ throughput improvement at batch size 8\ncompared to batch size 1, with efficient scaling through batch size 8, while\nmaintaining 95% output equivalence. Our method requires no custom kernels and\nintegrates cleanly with existing inference stacks. Our code is available at\nhttps://github.com/eBay/spec_dec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding speeds up LLM inference by using a small draft model to\npropose multiple tokens that a target model verifies in parallel. Extending\nthis idea to batches is essential for production serving, but it introduces the\nragged tensor problem: sequences in the same batch accept different numbers of\ndraft tokens, breaking right-alignment and corrupting position IDs, attention\nmasks, and KV-cache state. We show that several existing batch implementations\nviolate output equivalence-the fundamental requirement that speculative\ndecoding must produce identical token sequences to standard autoregressive\ngeneration. These violations occur precisely due to improper handling of the\nragged tensor problem. In response, we (1) characterize the synchronization\nrequirements that guarantee correctness, (2) present a correctness-first batch\nspeculative decoding EQSPEC that exposes realignment as consuming 40% of\noverhead, and (3) introduce EXSPEC, which maintains a sliding pool of sequences\nand dynamically forms same-length groups, to reduce the realignment overhead\nwhile preserving per-sequence speculative speedups. On the SpecBench dataset,\nacross Vicuna-7B/68M, Qwen3-8B/0.6B, and GLM-4-9B/0.6B target/draft pairs, our\napproach achieves up to 3$\\times$ throughput improvement at batch size 8\ncompared to batch size 1, with efficient scaling through batch size 8, while\nmaintaining 95% output equivalence. Our method requires no custom kernels and\nintegrates cleanly with existing inference stacks. Our code is available at\nhttps://github.com/eBay/spec_dec."
                },
                "authors": [
                    {
                        "name": "Ranran Haoran Zhang"
                    },
                    {
                        "name": "Soumik Dey"
                    },
                    {
                        "name": "Ashirbad Mishra"
                    },
                    {
                        "name": "Hansi Wu"
                    },
                    {
                        "name": "Binbin Li"
                    },
                    {
                        "name": "Rui Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Zhang"
                },
                "author": "Rui Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22876v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22876v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10367v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10367v3",
                "updated": "2025-10-26T13:31:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    26,
                    13,
                    31,
                    41,
                    6,
                    299,
                    0
                ],
                "published": "2025-07-14T15:09:01Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    9,
                    1,
                    0,
                    195,
                    0
                ],
                "title": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline"
                },
                "summary": "Client-side metadata caching has long been considered an effective method for\naccelerating metadata operations in distributed file systems (DFSs). However,\nwe have found that client-side state (e.g., caching) is not only ineffective\nbut also consumes valuable memory resources in the deep learning pipelines. We\nthus propose FalconFS, a DFS optimized for deep learning pipelines with the\nstateless-client architecture. Specifically, instead of performing client-side\npath resolution and caching, FalconFS efficiently resolves paths on the server\nside using hybrid metadata indexing and lazy namespace replication. FalconFS\nalso boosts server concurrency with concurrent request merging and provides\neasy deployment with VFS shortcut. Evaluations against CephFS and Lustre show\nthat FalconFS achieves up to 5.72$\\times$ throughput for small file read/write\nand up to 12.81$\\times$ throughput for deep learning model training. FalconFS\nhas been running in Huawei autonomous driving system's production environment\nwith 10,000 NPUs for one year and has been open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Client-side metadata caching has long been considered an effective method for\naccelerating metadata operations in distributed file systems (DFSs). However,\nwe have found that client-side state (e.g., caching) is not only ineffective\nbut also consumes valuable memory resources in the deep learning pipelines. We\nthus propose FalconFS, a DFS optimized for deep learning pipelines with the\nstateless-client architecture. Specifically, instead of performing client-side\npath resolution and caching, FalconFS efficiently resolves paths on the server\nside using hybrid metadata indexing and lazy namespace replication. FalconFS\nalso boosts server concurrency with concurrent request merging and provides\neasy deployment with VFS shortcut. Evaluations against CephFS and Lustre show\nthat FalconFS achieves up to 5.72$\\times$ throughput for small file read/write\nand up to 12.81$\\times$ throughput for deep learning model training. FalconFS\nhas been running in Huawei autonomous driving system's production environment\nwith 10,000 NPUs for one year and has been open-sourced."
                },
                "authors": [
                    {
                        "name": "Jingwei Xu"
                    },
                    {
                        "name": "Junbin Kang"
                    },
                    {
                        "name": "Mingkai Dong"
                    },
                    {
                        "name": "Mingyu Liu"
                    },
                    {
                        "name": "Lu Zhang"
                    },
                    {
                        "name": "Shaohong Guo"
                    },
                    {
                        "name": "Ziyan Qiu"
                    },
                    {
                        "name": "Mingzhen You"
                    },
                    {
                        "name": "Ziyi Tian"
                    },
                    {
                        "name": "Anqi Yu"
                    },
                    {
                        "name": "Tianhong Ding"
                    },
                    {
                        "name": "Xinwei Hu"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "Accepted by NSDI'26",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10367v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10367v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22556v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22556v1",
                "updated": "2025-10-26T07:17:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    26,
                    7,
                    17,
                    10,
                    6,
                    299,
                    0
                ],
                "published": "2025-10-26T07:17:10Z",
                "published_parsed": [
                    2025,
                    10,
                    26,
                    7,
                    17,
                    10,
                    6,
                    299,
                    0
                ],
                "title": "SABlock: Semantic-Aware KV Cache Eviction with Adaptive Compression\n  Block Size",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SABlock: Semantic-Aware KV Cache Eviction with Adaptive Compression\n  Block Size"
                },
                "summary": "The growing memory footprint of the Key-Value (KV) cache poses a severe\nscalability bottleneck for long-context Large Language Model (LLM) inference.\nWhile KV cache eviction has emerged as an effective solution by discarding less\ncritical tokens, existing token-, block-, and sentence-level compression\nmethods struggle to balance semantic coherence and memory efficiency. To this\nend, we introduce SABlock, a \\underline{s}emantic-aware KV cache eviction\nframework with \\underline{a}daptive \\underline{block} sizes. Specifically,\nSABlock first performs semantic segmentation to align compression boundaries\nwith linguistic structures, then applies segment-guided token scoring to refine\ntoken importance estimation. Finally, for each segment, a budget-driven search\nstrategy adaptively determines the optimal block size that preserves semantic\nintegrity while improving compression efficiency under a given cache budget.\nExtensive experiments on long-context benchmarks demonstrate that SABlock\nconsistently outperforms state-of-the-art baselines under the same memory\nbudgets. For instance, on Needle-in-a-Haystack (NIAH), SABlock achieves 99.9%\nretrieval accuracy with only 96 KV entries, nearly matching the performance of\nthe full-cache baseline that retains up to 8K entries. Under a fixed cache\nbudget of 1,024, SABlock further reduces peak memory usage by 46.28% and\nachieves up to 9.5x faster decoding on a 128K context length.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing memory footprint of the Key-Value (KV) cache poses a severe\nscalability bottleneck for long-context Large Language Model (LLM) inference.\nWhile KV cache eviction has emerged as an effective solution by discarding less\ncritical tokens, existing token-, block-, and sentence-level compression\nmethods struggle to balance semantic coherence and memory efficiency. To this\nend, we introduce SABlock, a \\underline{s}emantic-aware KV cache eviction\nframework with \\underline{a}daptive \\underline{block} sizes. Specifically,\nSABlock first performs semantic segmentation to align compression boundaries\nwith linguistic structures, then applies segment-guided token scoring to refine\ntoken importance estimation. Finally, for each segment, a budget-driven search\nstrategy adaptively determines the optimal block size that preserves semantic\nintegrity while improving compression efficiency under a given cache budget.\nExtensive experiments on long-context benchmarks demonstrate that SABlock\nconsistently outperforms state-of-the-art baselines under the same memory\nbudgets. For instance, on Needle-in-a-Haystack (NIAH), SABlock achieves 99.9%\nretrieval accuracy with only 96 KV entries, nearly matching the performance of\nthe full-cache baseline that retains up to 8K entries. Under a fixed cache\nbudget of 1,024, SABlock further reduces peak memory usage by 46.28% and\nachieves up to 9.5x faster decoding on a 128K context length."
                },
                "authors": [
                    {
                        "name": "Jinhan Chen"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Xianjun Gao"
                    },
                    {
                        "name": "Shilong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Shilong Wang"
                },
                "author": "Shilong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22556v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22556v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04077v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04077v3",
                "updated": "2025-10-26T04:25:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    26,
                    4,
                    25,
                    10,
                    6,
                    299,
                    0
                ],
                "published": "2025-02-06T13:41:46Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    41,
                    46,
                    3,
                    37,
                    0
                ],
                "title": "AttentionPredictor: Temporal Patterns Matter for KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttentionPredictor: Temporal Patterns Matter for KV Cache Compression"
                },
                "summary": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through static modeling of attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the temporal patterns in attention scores, resulting in\na noticeable degradation in LLM performance. To address this challenge, we\npropose AttentionPredictor, which is the first learning-based method to\ndirectly predict attention patterns for KV cache compression and critical token\nidentification. Specifically, AttentionPredictor learns a lightweight, unified\nconvolution model to dynamically capture spatiotemporal patterns and predict\nthe next-token attention scores. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score and shares the unified\nprediction model, which consumes negligible memory, among all transformer\nlayers. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n13$\\times$ KV cache compression and 5.6$\\times$ speedup in a cache offloading\nscenario with comparable LLM performance, significantly outperforming the\nstate-of-the-arts. The code is available at\nhttps://github.com/MIRALab-USTC/LLM-AttentionPredictor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through static modeling of attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the temporal patterns in attention scores, resulting in\na noticeable degradation in LLM performance. To address this challenge, we\npropose AttentionPredictor, which is the first learning-based method to\ndirectly predict attention patterns for KV cache compression and critical token\nidentification. Specifically, AttentionPredictor learns a lightweight, unified\nconvolution model to dynamically capture spatiotemporal patterns and predict\nthe next-token attention scores. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score and shares the unified\nprediction model, which consumes negligible memory, among all transformer\nlayers. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n13$\\times$ KV cache compression and 5.6$\\times$ speedup in a cache offloading\nscenario with comparable LLM performance, significantly outperforming the\nstate-of-the-arts. The code is available at\nhttps://github.com/MIRALab-USTC/LLM-AttentionPredictor."
                },
                "authors": [
                    {
                        "name": "Qingyue Yang"
                    },
                    {
                        "name": "Jie Wang"
                    },
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zhihai Wang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Jianye Hao"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    },
                    {
                        "name": "Bin Li"
                    }
                ],
                "author_detail": {
                    "name": "Bin Li"
                },
                "author": "Bin Li",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04077v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04077v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.23657v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.23657v1",
                "updated": "2025-10-26T01:25:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    26,
                    1,
                    25,
                    24,
                    6,
                    299,
                    0
                ],
                "published": "2025-10-26T01:25:24Z",
                "published_parsed": [
                    2025,
                    10,
                    26,
                    1,
                    25,
                    24,
                    6,
                    299,
                    0
                ],
                "title": "A machine learning framework integrating seed traits and plasma\n  parameters for predicting germination uplift in crops",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A machine learning framework integrating seed traits and plasma\n  parameters for predicting germination uplift in crops"
                },
                "summary": "Cold plasma (CP) is an eco-friendly method to enhance seed germination, yet\noutcomes remain difficult to predict due to complex seed--plasma--environment\ninteractions. This study introduces the first machine learning framework to\nforecast germination uplift in soybean, barley, sunflower, radish, and tomato\nunder dielectric barrier discharge (DBD) plasma. Among the models tested (GB,\nXGB, ET, and hybrids), Extra Trees (ET) performed best (R\\textsuperscript{2} =\n0.919; RMSE = 3.21; MAE = 2.62), improving to R\\textsuperscript{2} = 0.925\nafter feature reduction. Engineering analysis revealed a hormetic response:\nnegligible effects at $<$7 kV or $<$200 s, maximum germination at 7--15 kV for\n200--500 s, and reduced germination beyond 20 kV or prolonged exposures.\nDischarge power was also a dominant factor, with germination rate maximizing at\n$\\geq$100 W with low exposure time. Species and cultivar-level predictions\nshowed radish (MAE = 1.46) and soybean (MAE = 2.05) were modeled with high\nconsistency, while sunflower remained slightly higher variable (MAE = 3.80).\nAmong cultivars, Williams (MAE = 1.23) and Sari (1.33) were well predicted,\nwhile Arian (2.86) and Ny\\'{\\i}rs\\'{e}gi fekete (3.74) were comparatively\npoorly captured. This framework was also embedded into MLflow, providing a\ndecision-support tool for optimizing CP seed germination in precision\nagriculture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cold plasma (CP) is an eco-friendly method to enhance seed germination, yet\noutcomes remain difficult to predict due to complex seed--plasma--environment\ninteractions. This study introduces the first machine learning framework to\nforecast germination uplift in soybean, barley, sunflower, radish, and tomato\nunder dielectric barrier discharge (DBD) plasma. Among the models tested (GB,\nXGB, ET, and hybrids), Extra Trees (ET) performed best (R\\textsuperscript{2} =\n0.919; RMSE = 3.21; MAE = 2.62), improving to R\\textsuperscript{2} = 0.925\nafter feature reduction. Engineering analysis revealed a hormetic response:\nnegligible effects at $<$7 kV or $<$200 s, maximum germination at 7--15 kV for\n200--500 s, and reduced germination beyond 20 kV or prolonged exposures.\nDischarge power was also a dominant factor, with germination rate maximizing at\n$\\geq$100 W with low exposure time. Species and cultivar-level predictions\nshowed radish (MAE = 1.46) and soybean (MAE = 2.05) were modeled with high\nconsistency, while sunflower remained slightly higher variable (MAE = 3.80).\nAmong cultivars, Williams (MAE = 1.23) and Sari (1.33) were well predicted,\nwhile Arian (2.86) and Ny\\'{\\i}rs\\'{e}gi fekete (3.74) were comparatively\npoorly captured. This framework was also embedded into MLflow, providing a\ndecision-support tool for optimizing CP seed germination in precision\nagriculture."
                },
                "authors": [
                    {
                        "name": "Saklain Niam"
                    },
                    {
                        "name": "Tashfiqur Rahman"
                    },
                    {
                        "name": "Md. Amjad Patwary"
                    },
                    {
                        "name": "Mukarram Hossain"
                    }
                ],
                "author_detail": {
                    "name": "Mukarram Hossain"
                },
                "author": "Mukarram Hossain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.23657v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.23657v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22467v1",
                "updated": "2025-10-26T00:50:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    26,
                    0,
                    50,
                    12,
                    6,
                    299,
                    0
                ],
                "published": "2025-10-26T00:50:12Z",
                "published_parsed": [
                    2025,
                    10,
                    26,
                    0,
                    50,
                    12,
                    6,
                    299,
                    0
                ],
                "title": "Backward-Friendly Optimization: Training Large Language Models with\n  Approximate Gradients under Memory Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backward-Friendly Optimization: Training Large Language Models with\n  Approximate Gradients under Memory Constraints"
                },
                "summary": "Full fine-tuning of Large Language Models (LLMs) is notoriously\nmemory-intensive, primarily because conventional optimizers such as SGD or Adam\nassume access to exact gradients derived from cached activations. Existing\nsolutions either alter the model architecture (e.g., reversible networks) or\ntrade memory for computation (e.g., activation checkpointing), but the\noptimizer itself remains untouched. In this work, we introduce GradLite, a\nbackward-friendly optimizer that relaxes the requirement of exact gradients,\nenabling efficient training even when intermediate activations are aggressively\ndiscarded or approximated. GradLite leverages two key techniques: (i) low-rank\nJacobian approximation, which reduces the dimensionality of backpropagated\nerror signals, and (ii) error-feedback correction, which accumulates and\ncompensates approximation errors across iterations to preserve convergence\nguarantees. We provide a theoretical analysis showing that GradLite maintains\nunbiased gradient estimates with bounded variance, ensuring convergence rates\ncomparable to Adam. Empirically, GradLite reduces optimizer-state and\nactivation memory consumption by up to 50\\% without architectural changes, and\nachieves on-par or superior downstream performance on reasoning (MMLU, GSM8K),\nmultilingual, and dialogue benchmarks compared to checkpointing and\noptimizer-centric baselines (LoMo, GaLore).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full fine-tuning of Large Language Models (LLMs) is notoriously\nmemory-intensive, primarily because conventional optimizers such as SGD or Adam\nassume access to exact gradients derived from cached activations. Existing\nsolutions either alter the model architecture (e.g., reversible networks) or\ntrade memory for computation (e.g., activation checkpointing), but the\noptimizer itself remains untouched. In this work, we introduce GradLite, a\nbackward-friendly optimizer that relaxes the requirement of exact gradients,\nenabling efficient training even when intermediate activations are aggressively\ndiscarded or approximated. GradLite leverages two key techniques: (i) low-rank\nJacobian approximation, which reduces the dimensionality of backpropagated\nerror signals, and (ii) error-feedback correction, which accumulates and\ncompensates approximation errors across iterations to preserve convergence\nguarantees. We provide a theoretical analysis showing that GradLite maintains\nunbiased gradient estimates with bounded variance, ensuring convergence rates\ncomparable to Adam. Empirically, GradLite reduces optimizer-state and\nactivation memory consumption by up to 50\\% without architectural changes, and\nachieves on-par or superior downstream performance on reasoning (MMLU, GSM8K),\nmultilingual, and dialogue benchmarks compared to checkpointing and\noptimizer-centric baselines (LoMo, GaLore)."
                },
                "authors": [
                    {
                        "name": "Jing Yang"
                    },
                    {
                        "name": "Kaitong Cai"
                    },
                    {
                        "name": "Yijia Fan"
                    },
                    {
                        "name": "Yufeng Yang"
                    },
                    {
                        "name": "Keze Wang"
                    }
                ],
                "author_detail": {
                    "name": "Keze Wang"
                },
                "author": "Keze Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10524v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10524v3",
                "updated": "2025-10-25T14:12:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    25,
                    14,
                    12,
                    56,
                    5,
                    298,
                    0
                ],
                "published": "2025-07-14T17:49:00Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    49,
                    0,
                    0,
                    195,
                    0
                ],
                "title": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive\n  Token-Level Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive\n  Token-Level Computation"
                },
                "summary": "Scaling language models unlocks impressive capabilities, but the accompanying\ncomputational and memory demands make both training and deployment expensive.\nExisting efficiency efforts typically target either parameter sharing or\nadaptive computation, leaving open the question of how to attain both\nsimultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework\nthat combines the two axes of efficiency inside a single Recursive Transformer.\nMoR reuses a shared stack of layers across recursion steps to achieve parameter\nefficiency, while lightweight routers enable adaptive token-level thinking by\ndynamically assigning different recursion depths to individual tokens. This\nallows MoR to focus quadratic attention computation only among tokens still\nactive at a given recursion depth, further improving memory access efficiency\nby selectively caching only their key-value pairs. Beyond these core\nmechanisms, we also propose a KV sharing variant that reuses KV pairs from the\nfirst recursion, specifically designed to further decrease memory footprint.\nAcross model scales ranging from 135M to 1.7B parameters, MoR forms a new\nPareto frontier: at equal training FLOPs and smaller model sizes, it\nsignificantly lowers validation perplexity and improves few-shot accuracy,\nwhile delivering higher throughput compared with vanilla and existing recursive\nbaselines. These gains demonstrate that MoR is an effective path towards\nlarge-model quality without incurring large-model cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models unlocks impressive capabilities, but the accompanying\ncomputational and memory demands make both training and deployment expensive.\nExisting efficiency efforts typically target either parameter sharing or\nadaptive computation, leaving open the question of how to attain both\nsimultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework\nthat combines the two axes of efficiency inside a single Recursive Transformer.\nMoR reuses a shared stack of layers across recursion steps to achieve parameter\nefficiency, while lightweight routers enable adaptive token-level thinking by\ndynamically assigning different recursion depths to individual tokens. This\nallows MoR to focus quadratic attention computation only among tokens still\nactive at a given recursion depth, further improving memory access efficiency\nby selectively caching only their key-value pairs. Beyond these core\nmechanisms, we also propose a KV sharing variant that reuses KV pairs from the\nfirst recursion, specifically designed to further decrease memory footprint.\nAcross model scales ranging from 135M to 1.7B parameters, MoR forms a new\nPareto frontier: at equal training FLOPs and smaller model sizes, it\nsignificantly lowers validation perplexity and improves few-shot accuracy,\nwhile delivering higher throughput compared with vanilla and existing recursive\nbaselines. These gains demonstrate that MoR is an effective path towards\nlarge-model quality without incurring large-model cost."
                },
                "authors": [
                    {
                        "name": "Sangmin Bae"
                    },
                    {
                        "name": "Yujin Kim"
                    },
                    {
                        "name": "Reza Bayat"
                    },
                    {
                        "name": "Sungnyun Kim"
                    },
                    {
                        "name": "Jiyoun Ha"
                    },
                    {
                        "name": "Tal Schuster"
                    },
                    {
                        "name": "Adam Fisch"
                    },
                    {
                        "name": "Hrayr Harutyunyan"
                    },
                    {
                        "name": "Ziwei Ji"
                    },
                    {
                        "name": "Aaron Courville"
                    },
                    {
                        "name": "Se-Young Yun"
                    }
                ],
                "author_detail": {
                    "name": "Se-Young Yun"
                },
                "author": "Se-Young Yun",
                "arxiv_comment": "38 pages, 9 figures, 17 tables, codes at\n  https://github.com/raymin0223/mixture_of_recursions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10524v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10524v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.23649v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.23649v1",
                "updated": "2025-10-25T11:43:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    25,
                    11,
                    43,
                    27,
                    5,
                    298,
                    0
                ],
                "published": "2025-10-25T11:43:27Z",
                "published_parsed": [
                    2025,
                    10,
                    25,
                    11,
                    43,
                    27,
                    5,
                    298,
                    0
                ],
                "title": "Efficient Low Rank Attention for Long-Context Inference in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Low Rank Attention for Long-Context Inference in Large\n  Language Models"
                },
                "summary": "As the length of input text grows, the key-value (KV) cache in LLMs imposes\nprohibitive GPU memory costs and limits long-context inference on resource\nconstrained devices. Existing approaches, such as KV quantization and pruning,\nreduce memory usage but suffer from numerical precision loss or suboptimal\nretention of key-value pairs. We introduce Low Rank Query and Key attention\n(LRQK), a two-stage framework that jointly decomposes the full-precision query\nand key matrices into compact rank-\\(r\\) factors during the prefill stage, and\nthen uses these low-dimensional projections to compute proxy attention scores\nin \\(\\mathcal{O}(lr)\\) time at each decode step. By selecting only the\ntop-\\(k\\) tokens and a small fixed set of recent tokens, LRQK employs a mixed\nGPU-CPU cache with a hit-and-miss mechanism that transfers only missing\nfull-precision KV pairs, thereby preserving exact attention outputs while\nreducing CPU-GPU data movement. Extensive experiments on the RULER and\nLongBench benchmarks with LLaMA-3-8B and Qwen2.5-7B demonstrate that LRQK\nmatches or surpasses leading sparse-attention methods in long context settings,\nwhile delivering significant memory savings with minimal loss in accuracy. Our\ncode is available at https://github.com/tenghuilee/LRQK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the length of input text grows, the key-value (KV) cache in LLMs imposes\nprohibitive GPU memory costs and limits long-context inference on resource\nconstrained devices. Existing approaches, such as KV quantization and pruning,\nreduce memory usage but suffer from numerical precision loss or suboptimal\nretention of key-value pairs. We introduce Low Rank Query and Key attention\n(LRQK), a two-stage framework that jointly decomposes the full-precision query\nand key matrices into compact rank-\\(r\\) factors during the prefill stage, and\nthen uses these low-dimensional projections to compute proxy attention scores\nin \\(\\mathcal{O}(lr)\\) time at each decode step. By selecting only the\ntop-\\(k\\) tokens and a small fixed set of recent tokens, LRQK employs a mixed\nGPU-CPU cache with a hit-and-miss mechanism that transfers only missing\nfull-precision KV pairs, thereby preserving exact attention outputs while\nreducing CPU-GPU data movement. Extensive experiments on the RULER and\nLongBench benchmarks with LLaMA-3-8B and Qwen2.5-7B demonstrate that LRQK\nmatches or surpasses leading sparse-attention methods in long context settings,\nwhile delivering significant memory savings with minimal loss in accuracy. Our\ncode is available at https://github.com/tenghuilee/LRQK."
                },
                "authors": [
                    {
                        "name": "Tenghui Li"
                    },
                    {
                        "name": "Guoxu Zhou"
                    },
                    {
                        "name": "Xuyang Zhao"
                    },
                    {
                        "name": "Yuning Qiu"
                    },
                    {
                        "name": "Qibin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Qibin Zhao"
                },
                "author": "Qibin Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.23649v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.23649v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22145v1",
                "updated": "2025-10-25T03:34:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    25,
                    3,
                    34,
                    34,
                    5,
                    298,
                    0
                ],
                "published": "2025-10-25T03:34:34Z",
                "published_parsed": [
                    2025,
                    10,
                    25,
                    3,
                    34,
                    34,
                    5,
                    298,
                    0
                ],
                "title": "Fundamental Limits of Coded Caching with Fixed Subpacketization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fundamental Limits of Coded Caching with Fixed Subpacketization"
                },
                "summary": "Coded caching is a promising technique to create coded multicast\nopportunities for cache-aided networks. By splitting each file into $F$ equal\npackets (i.e., the subpacketization level $F$) and letting each user cache a\nset of packets, the transmission load can be significantly reduced via coded\nmulticasting. It has been shown that a higher subpacketization level could\npotentially lead to a lower transmission load, as more packets can be combined\nfor efficient transmission. On the other hand, a larger $F$ indicates a higher\ncoding complexity and is problematic from a practical perspective when $F$ is\nextremely large. Despite many works attempting to design coded caching schemes\nwith low subpacketization levels, a fundamental problem remains open: What is\nthe minimum transmission load given any fixed subpacketization level? In this\npaper, we consider the classical cache-aided networks with identically uncoded\nplacement and one-shot delivery strategy, and investigate the fundamental\ntrade-off between the transmission load and the subpacketization level. We\npropose a \\emph{general} lower bound on the transmission load for any fixed\nsubpacketization by reformulating the centralized coded caching schemes via the\ncombinatorial structure of the corresponding placement delivery array. The\nlower bound also recovers existing optimality results for the bipartite graph\nscheme (including the well-known Maddah-Ali and Niesen (MN) scheme and the\nconjugate MN scheme) as well as the grouping bipartite graph scheme.\nFurthermore, by carefully exploiting the combinatorial structure and computing\nthe union size of sorted sets, we establish a new optimality result, i.e., the\npartition scheme can achieve the optimal rate-subpacketization trade-off.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching is a promising technique to create coded multicast\nopportunities for cache-aided networks. By splitting each file into $F$ equal\npackets (i.e., the subpacketization level $F$) and letting each user cache a\nset of packets, the transmission load can be significantly reduced via coded\nmulticasting. It has been shown that a higher subpacketization level could\npotentially lead to a lower transmission load, as more packets can be combined\nfor efficient transmission. On the other hand, a larger $F$ indicates a higher\ncoding complexity and is problematic from a practical perspective when $F$ is\nextremely large. Despite many works attempting to design coded caching schemes\nwith low subpacketization levels, a fundamental problem remains open: What is\nthe minimum transmission load given any fixed subpacketization level? In this\npaper, we consider the classical cache-aided networks with identically uncoded\nplacement and one-shot delivery strategy, and investigate the fundamental\ntrade-off between the transmission load and the subpacketization level. We\npropose a \\emph{general} lower bound on the transmission load for any fixed\nsubpacketization by reformulating the centralized coded caching schemes via the\ncombinatorial structure of the corresponding placement delivery array. The\nlower bound also recovers existing optimality results for the bipartite graph\nscheme (including the well-known Maddah-Ali and Niesen (MN) scheme and the\nconjugate MN scheme) as well as the grouping bipartite graph scheme.\nFurthermore, by carefully exploiting the combinatorial structure and computing\nthe union size of sorted sets, we establish a new optimality result, i.e., the\npartition scheme can achieve the optimal rate-subpacketization trade-off."
                },
                "authors": [
                    {
                        "name": "Minquan Cheng"
                    },
                    {
                        "name": "Yifei Huang"
                    },
                    {
                        "name": "Youlong Wu"
                    },
                    {
                        "name": "Jinyan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jinyan Wang"
                },
                "author": "Jinyan Wang",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10270v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10270v3",
                "updated": "2025-10-25T02:29:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    25,
                    2,
                    29,
                    47,
                    5,
                    298,
                    0
                ],
                "published": "2025-03-13T11:26:45Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    11,
                    26,
                    45,
                    3,
                    72,
                    0
                ],
                "title": "EEdit: Rethinking the Spatial and Temporal Redundancy for Efficient\n  Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EEdit: Rethinking the Spatial and Temporal Redundancy for Efficient\n  Image Editing"
                },
                "summary": "Inversion-based image editing is rapidly gaining momentum while suffering\nfrom significant computation overhead, hindering its application in real-time\ninteractive scenarios. In this paper, we rethink that the redundancy in\ninversion-based image editing exists in both the spatial and temporal\ndimensions, such as the unnecessary computation in unedited regions and the\nredundancy in the inversion progress. To tackle these challenges, we propose a\npractical framework, named EEdit, to achieve efficient image editing.\nSpecifically, we introduce three techniques to solve them one by one. For\nspatial redundancy, spatial locality caching is introduced to compute the\nedited region and its neighboring regions while skipping the unedited regions,\nand token indexing preprocessing is designed to further accelerate the caching.\nFor temporal redundancy, inversion step skipping is proposed to reuse the\nlatent for efficient editing. Our experiments demonstrate an average of 2.46\n$\\times$ acceleration without performance drop in a wide range of editing tasks\nincluding prompt-guided image editing, dragging and image composition. Our\ncodes are available at https://github.com/yuriYanZeXuan/EEdit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inversion-based image editing is rapidly gaining momentum while suffering\nfrom significant computation overhead, hindering its application in real-time\ninteractive scenarios. In this paper, we rethink that the redundancy in\ninversion-based image editing exists in both the spatial and temporal\ndimensions, such as the unnecessary computation in unedited regions and the\nredundancy in the inversion progress. To tackle these challenges, we propose a\npractical framework, named EEdit, to achieve efficient image editing.\nSpecifically, we introduce three techniques to solve them one by one. For\nspatial redundancy, spatial locality caching is introduced to compute the\nedited region and its neighboring regions while skipping the unedited regions,\nand token indexing preprocessing is designed to further accelerate the caching.\nFor temporal redundancy, inversion step skipping is proposed to reuse the\nlatent for efficient editing. Our experiments demonstrate an average of 2.46\n$\\times$ acceleration without performance drop in a wide range of editing tasks\nincluding prompt-guided image editing, dragging and image composition. Our\ncodes are available at https://github.com/yuriYanZeXuan/EEdit"
                },
                "authors": [
                    {
                        "name": "Zexuan Yan"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Wenteng Chen"
                    },
                    {
                        "name": "Qifeng Chen"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "accepted by ICCV2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10270v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10270v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22049v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22049v1",
                "updated": "2025-10-24T22:17:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    22,
                    17,
                    49,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T22:17:49Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    22,
                    17,
                    49,
                    4,
                    297,
                    0
                ],
                "title": "Massive Memorization with Hundreds of Trillions of Parameters for\n  Sequential Transducer Generative Recommenders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Massive Memorization with Hundreds of Trillions of Parameters for\n  Sequential Transducer Generative Recommenders"
                },
                "summary": "Modern large-scale recommendation systems rely heavily on user interaction\nhistory sequences to enhance the model performance. The advent of large\nlanguage models and sequential modeling techniques, particularly\ntransformer-like architectures, has led to significant advancements recently\n(e.g., HSTU, SIM, and TWIN models). While scaling to ultra-long user histories\n(10k to 100k items) generally improves model performance, it also creates\nsignificant challenges on latency, queries per second (QPS) and GPU cost in\nindustry-scale recommendation systems. Existing models do not adequately\naddress these industrial scalability issues. In this paper, we propose a novel\ntwo-stage modeling framework, namely VIrtual Sequential Target Attention\n(VISTA), which decomposes traditional target attention from a candidate item to\nuser history items into two distinct stages: (1) user history summarization\ninto a few hundred tokens; followed by (2) candidate item attention to those\ntokens. These summarization token embeddings are then cached in storage system\nand then utilized as sequence features for downstream model training and\ninference. This novel design for scalability enables VISTA to scale to lifelong\nuser histories (up to one million items) while keeping downstream training and\ninference costs fixed, which is essential in industry. Our approach achieves\nsignificant improvements in offline and online metrics and has been\nsuccessfully deployed on an industry leading recommendation platform serving\nbillions of users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large-scale recommendation systems rely heavily on user interaction\nhistory sequences to enhance the model performance. The advent of large\nlanguage models and sequential modeling techniques, particularly\ntransformer-like architectures, has led to significant advancements recently\n(e.g., HSTU, SIM, and TWIN models). While scaling to ultra-long user histories\n(10k to 100k items) generally improves model performance, it also creates\nsignificant challenges on latency, queries per second (QPS) and GPU cost in\nindustry-scale recommendation systems. Existing models do not adequately\naddress these industrial scalability issues. In this paper, we propose a novel\ntwo-stage modeling framework, namely VIrtual Sequential Target Attention\n(VISTA), which decomposes traditional target attention from a candidate item to\nuser history items into two distinct stages: (1) user history summarization\ninto a few hundred tokens; followed by (2) candidate item attention to those\ntokens. These summarization token embeddings are then cached in storage system\nand then utilized as sequence features for downstream model training and\ninference. This novel design for scalability enables VISTA to scale to lifelong\nuser histories (up to one million items) while keeping downstream training and\ninference costs fixed, which is essential in industry. Our approach achieves\nsignificant improvements in offline and online metrics and has been\nsuccessfully deployed on an industry leading recommendation platform serving\nbillions of users."
                },
                "authors": [
                    {
                        "name": "Zhimin Chen"
                    },
                    {
                        "name": "Chenyu Zhao"
                    },
                    {
                        "name": "Ka Chun Mo"
                    },
                    {
                        "name": "Yunjiang Jiang"
                    },
                    {
                        "name": "Jane H. Lee"
                    },
                    {
                        "name": "Shouwei Chen"
                    },
                    {
                        "name": "Khushhall Chandra Mahajan"
                    },
                    {
                        "name": "Ning Jiang"
                    },
                    {
                        "name": "Kai Ren"
                    },
                    {
                        "name": "Jinhui Li"
                    },
                    {
                        "name": "Wen-Yun Yang"
                    }
                ],
                "author_detail": {
                    "name": "Wen-Yun Yang"
                },
                "author": "Wen-Yun Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22049v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22049v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21696v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21696v1",
                "updated": "2025-10-24T17:56:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    17,
                    56,
                    37,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T17:56:37Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    17,
                    56,
                    37,
                    4,
                    297,
                    0
                ],
                "title": "BachVid: Training-Free Video Generation with Consistent Background and\n  Character",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BachVid: Training-Free Video Generation with Consistent Background and\n  Character"
                },
                "summary": "Diffusion Transformers (DiTs) have recently driven significant progress in\ntext-to-video (T2V) generation. However, generating multiple videos with\nconsistent characters and backgrounds remains a significant challenge. Existing\nmethods typically rely on reference images or extensive training, and often\nonly address character consistency, leaving background consistency to\nimage-to-video models. We introduce BachVid, the first training-free method\nthat achieves consistent video generation without needing any reference images.\nOur approach is based on a systematic analysis of DiT's attention mechanism and\nintermediate features, revealing its ability to extract foreground masks and\nidentify matching points during the denoising process. Our method leverages\nthis finding by first generating an identity video and caching the intermediate\nvariables, and then inject these cached variables into corresponding positions\nin newly generated videos, ensuring both foreground and background consistency\nacross multiple videos. Experimental results demonstrate that BachVid achieves\nrobust consistency in generated videos without requiring additional training,\noffering a novel and efficient solution for consistent video generation without\nrelying on reference images or additional training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have recently driven significant progress in\ntext-to-video (T2V) generation. However, generating multiple videos with\nconsistent characters and backgrounds remains a significant challenge. Existing\nmethods typically rely on reference images or extensive training, and often\nonly address character consistency, leaving background consistency to\nimage-to-video models. We introduce BachVid, the first training-free method\nthat achieves consistent video generation without needing any reference images.\nOur approach is based on a systematic analysis of DiT's attention mechanism and\nintermediate features, revealing its ability to extract foreground masks and\nidentify matching points during the denoising process. Our method leverages\nthis finding by first generating an identity video and caching the intermediate\nvariables, and then inject these cached variables into corresponding positions\nin newly generated videos, ensuring both foreground and background consistency\nacross multiple videos. Experimental results demonstrate that BachVid achieves\nrobust consistency in generated videos without requiring additional training,\noffering a novel and efficient solution for consistent video generation without\nrelying on reference images or additional training."
                },
                "authors": [
                    {
                        "name": "Han Yan"
                    },
                    {
                        "name": "Xibin Song"
                    },
                    {
                        "name": "Yifu Wang"
                    },
                    {
                        "name": "Hongdong Li"
                    },
                    {
                        "name": "Pan Ji"
                    },
                    {
                        "name": "Chao Ma"
                    }
                ],
                "author_detail": {
                    "name": "Chao Ma"
                },
                "author": "Chao Ma",
                "arxiv_comment": "Project page: https://wolfball.github.io/bachvid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21696v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21696v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.20787v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.20787v2",
                "updated": "2025-10-24T16:56:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    16,
                    56,
                    22,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-23T17:53:03Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    17,
                    53,
                    3,
                    3,
                    296,
                    0
                ],
                "title": "Alleviating Forgetfulness of Linear Attention by Hybrid Sparse Attention\n  and Contextualized Learnable Token Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alleviating Forgetfulness of Linear Attention by Hybrid Sparse Attention\n  and Contextualized Learnable Token Eviction"
                },
                "summary": "Linear-attention models that compress the entire input sequence into a\nfixed-size recurrent state offer an efficient alternative to Transformers, but\ntheir finite memory induces forgetfulness that harms retrieval-intensive tasks.\nTo mitigate the issue, we explore a series of hybrid models that restore direct\naccess to past tokens. We interleave token mixers with intermediate time and\nspace complexity between linear and full attention, including sparse attention\nwith token eviction, and the query-aware native sparse attention. Particularly,\nwe propose a novel learnable token eviction approach. Combined with\nsliding-window attention, an end-to-end trainable lightweight CNN aggregates\ninformation from both past and future adjacent tokens to adaptively retain a\nlimited set of critical KV-pairs per head, maintaining linear attention's\nconstant time and space complexity. Efficient Triton kernels for the sparse\nattention mechanisms are provided. Empirical evaluations on retrieval-intensive\nbenchmarks support the effectiveness of our approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear-attention models that compress the entire input sequence into a\nfixed-size recurrent state offer an efficient alternative to Transformers, but\ntheir finite memory induces forgetfulness that harms retrieval-intensive tasks.\nTo mitigate the issue, we explore a series of hybrid models that restore direct\naccess to past tokens. We interleave token mixers with intermediate time and\nspace complexity between linear and full attention, including sparse attention\nwith token eviction, and the query-aware native sparse attention. Particularly,\nwe propose a novel learnable token eviction approach. Combined with\nsliding-window attention, an end-to-end trainable lightweight CNN aggregates\ninformation from both past and future adjacent tokens to adaptively retain a\nlimited set of critical KV-pairs per head, maintaining linear attention's\nconstant time and space complexity. Efficient Triton kernels for the sparse\nattention mechanisms are provided. Empirical evaluations on retrieval-intensive\nbenchmarks support the effectiveness of our approaches."
                },
                "authors": [
                    {
                        "name": "Mutian He"
                    },
                    {
                        "name": "Philip N. Garner"
                    }
                ],
                "author_detail": {
                    "name": "Philip N. Garner"
                },
                "author": "Philip N. Garner",
                "arxiv_comment": "19 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.20787v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.20787v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17388v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17388v2",
                "updated": "2025-10-24T14:55:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    14,
                    55,
                    42,
                    4,
                    297,
                    0
                ],
                "published": "2025-09-22T06:52:35Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    6,
                    52,
                    35,
                    0,
                    265,
                    0
                ],
                "title": "Prefetching in Deep Memory Hierarchies with NVRAM as Main Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prefetching in Deep Memory Hierarchies with NVRAM as Main Memory"
                },
                "summary": "Emerging applications, such as big data analytics and machine learning,\nrequire increasingly large amounts of main memory, often exceeding the capacity\nof current commodity processors built on DRAM technology. To address this,\nrecent research has focused on off-chip memory controllers that facilitate\naccess to diverse memory media, each with unique density and latency\ncharacteristics. While these solutions improve memory system performance, they\nalso exacerbate the already significant memory latency. As a result,\nmulti-level prefetching techniques are essential to mitigate these extended\nlatencies.\n  This paper investigates the advantages of prefetching across both sides of\nthe memory system: the off-chip memory and the on-chip cache hierarchy. Our\nprimary objective is to assess the impact of a multi-level prefetching engine\non overall system performance. Additionally, we analyze the individual\ncontribution of each prefetching level to system efficiency. To achieve this,\nthe study evaluates two key prefetching approaches: HMC (Hybrid Memory\nController) and HMC+L1, both of which employ prefetching mechanisms commonly\nused by processor vendors. The HMC approach integrates a prefetcher within the\noff-chip hybrid memory controller, while the HMC+L1 approach combines this with\nadditional L1 on-chip prefetchers.\n  Experimental results on an out-of-order execution processor show that on-chip\ncache prefetchers are crucial for maximizing the benefits of off-chip\nprefetching, which in turn further enhances performance. Specifically, the\noff-chip HMC prefetcher achieves coverage and accuracy rates exceeding 60% and\nup to 80%, while the combined HMC+L1 approach boosts off-chip prefetcher\ncoverage to as much as 92%. Consequently, overall performance increases from 9%\nwith the HMC approach to 12% when L1 prefetching is also employed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging applications, such as big data analytics and machine learning,\nrequire increasingly large amounts of main memory, often exceeding the capacity\nof current commodity processors built on DRAM technology. To address this,\nrecent research has focused on off-chip memory controllers that facilitate\naccess to diverse memory media, each with unique density and latency\ncharacteristics. While these solutions improve memory system performance, they\nalso exacerbate the already significant memory latency. As a result,\nmulti-level prefetching techniques are essential to mitigate these extended\nlatencies.\n  This paper investigates the advantages of prefetching across both sides of\nthe memory system: the off-chip memory and the on-chip cache hierarchy. Our\nprimary objective is to assess the impact of a multi-level prefetching engine\non overall system performance. Additionally, we analyze the individual\ncontribution of each prefetching level to system efficiency. To achieve this,\nthe study evaluates two key prefetching approaches: HMC (Hybrid Memory\nController) and HMC+L1, both of which employ prefetching mechanisms commonly\nused by processor vendors. The HMC approach integrates a prefetcher within the\noff-chip hybrid memory controller, while the HMC+L1 approach combines this with\nadditional L1 on-chip prefetchers.\n  Experimental results on an out-of-order execution processor show that on-chip\ncache prefetchers are crucial for maximizing the benefits of off-chip\nprefetching, which in turn further enhances performance. Specifically, the\noff-chip HMC prefetcher achieves coverage and accuracy rates exceeding 60% and\nup to 80%, while the combined HMC+L1 approach boosts off-chip prefetcher\ncoverage to as much as 92%. Consequently, overall performance increases from 9%\nwith the HMC approach to 12% when L1 prefetching is also employed."
                },
                "authors": [
                    {
                        "name": "Manel Lurbe"
                    },
                    {
                        "name": "Miguel Avargues"
                    },
                    {
                        "name": "Salvador Petit"
                    },
                    {
                        "name": "Maria E. Gomez"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Guanhao Wang"
                    },
                    {
                        "name": "Julio Sahuquillo"
                    }
                ],
                "author_detail": {
                    "name": "Julio Sahuquillo"
                },
                "author": "Julio Sahuquillo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17388v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17388v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22922v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22922v2",
                "updated": "2025-10-24T11:53:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    11,
                    53,
                    34,
                    4,
                    297,
                    0
                ],
                "published": "2025-06-28T15:15:31Z",
                "published_parsed": [
                    2025,
                    6,
                    28,
                    15,
                    15,
                    31,
                    5,
                    179,
                    0
                ],
                "title": "Global Predecessor Indexing: Avoiding Binary Search in Weighted Job\n  Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Global Predecessor Indexing: Avoiding Binary Search in Weighted Job\n  Scheduling"
                },
                "summary": "We present an improved solution to the Weighted Job Scheduling (WJS) problem.\nWhile the classical dynamic programming (DP) solution for $n$ jobs runs in $O(n\n\\log(n))$ time due to comparison-based sorting and per-job binary search, we\neliminate the binary search bottleneck. In its place, we introduce a novel\nmulti-phase preprocessing technique called \\emph{Global Predecessor Indexing\n(GPI)}, which computes the latest non-overlapping job (i.e., the predecessor)\nfor all jobs via a two-pointer linear-time pass after sorting. This yields a\ntime complexity of $O(S(n) + n)$ where $S(n)$ is the time to sort all jobs. GPI\nenables direct use in the classical DP recurrence. When combined with\nlinear-time sorting, GPI yields a complete $O(n)$ solution. Even with\ncomparison-based sorting, GPI significantly outperforms the classical solution\nin practice by avoiding repeated binary searches in favor of the more\ncache-efficient extra sort and two-pointer pass.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an improved solution to the Weighted Job Scheduling (WJS) problem.\nWhile the classical dynamic programming (DP) solution for $n$ jobs runs in $O(n\n\\log(n))$ time due to comparison-based sorting and per-job binary search, we\neliminate the binary search bottleneck. In its place, we introduce a novel\nmulti-phase preprocessing technique called \\emph{Global Predecessor Indexing\n(GPI)}, which computes the latest non-overlapping job (i.e., the predecessor)\nfor all jobs via a two-pointer linear-time pass after sorting. This yields a\ntime complexity of $O(S(n) + n)$ where $S(n)$ is the time to sort all jobs. GPI\nenables direct use in the classical DP recurrence. When combined with\nlinear-time sorting, GPI yields a complete $O(n)$ solution. Even with\ncomparison-based sorting, GPI significantly outperforms the classical solution\nin practice by avoiding repeated binary searches in favor of the more\ncache-efficient extra sort and two-pointer pass."
                },
                "authors": [
                    {
                        "name": "Amit Joshi"
                    }
                ],
                "author_detail": {
                    "name": "Amit Joshi"
                },
                "author": "Amit Joshi",
                "arxiv_comment": "6 pages, 9 figures including tables. Short theoretical and practical\n  paper on improved dynamic programming for weighted job scheduling with\n  linear-time preprocessing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22922v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22922v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21361v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21361v1",
                "updated": "2025-10-24T11:42:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    11,
                    42,
                    38,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T11:42:38Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    11,
                    42,
                    38,
                    4,
                    297,
                    0
                ],
                "title": "Compositional Monte Carlo Tree Diffusion for Extendable Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compositional Monte Carlo Tree Diffusion for Extendable Planning"
                },
                "summary": "Monte Carlo Tree Diffusion (MCTD) integrates diffusion models with structured\ntree search to enable effective trajectory exploration through stepwise\nreasoning. However, MCTD remains fundamentally limited by training trajectory\nlengths. While periodic replanning allows plan concatenation for longer plan\ngeneration, the planning process remains locally confined, as MCTD searches\nwithin individual trajectories without access to global context. We propose\nCompositional Monte Carlo Tree Diffusion (C-MCTD), a framework that elevates\nplanning from individual trajectory optimization to reasoning over complete\nplan compositions. C-MCTD introduces three complementary components: (1) Online\nComposer, which performs globally-aware planning by searching across entire\nplan compositions; (2) Distributed Composer, which reduces search complexity\nthrough parallel exploration from multiple starting points; and (3) Preplan\nComposer, which accelerates inference by leveraging cached plan graphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monte Carlo Tree Diffusion (MCTD) integrates diffusion models with structured\ntree search to enable effective trajectory exploration through stepwise\nreasoning. However, MCTD remains fundamentally limited by training trajectory\nlengths. While periodic replanning allows plan concatenation for longer plan\ngeneration, the planning process remains locally confined, as MCTD searches\nwithin individual trajectories without access to global context. We propose\nCompositional Monte Carlo Tree Diffusion (C-MCTD), a framework that elevates\nplanning from individual trajectory optimization to reasoning over complete\nplan compositions. C-MCTD introduces three complementary components: (1) Online\nComposer, which performs globally-aware planning by searching across entire\nplan compositions; (2) Distributed Composer, which reduces search complexity\nthrough parallel exploration from multiple starting points; and (3) Preplan\nComposer, which accelerates inference by leveraging cached plan graphs."
                },
                "authors": [
                    {
                        "name": "Jaesik Yoon"
                    },
                    {
                        "name": "Hyeonseo Cho"
                    },
                    {
                        "name": "Sungjin Ahn"
                    }
                ],
                "author_detail": {
                    "name": "Sungjin Ahn"
                },
                "author": "Sungjin Ahn",
                "arxiv_comment": "24 pages, 4 figures, NeurIPS 25 Spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21361v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19240v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19240v2",
                "updated": "2025-10-24T08:35:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    8,
                    35,
                    21,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-22T04:48:41Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    4,
                    48,
                    41,
                    2,
                    295,
                    0
                ],
                "title": "A General Solution for the Implementation of CI/CD in Embedded Linux\n  Development",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A General Solution for the Implementation of CI/CD in Embedded Linux\n  Development"
                },
                "summary": "With the growing use of embedded systems in various industries, the need for\nautomated platforms for the development and deployment of customized\nLinux-based operating systems has become more important. This research was\nconducted with the aim of designing and implementing an integrated and\nreproducible infrastructure for the development, building, and testing of a\nLinux-based operating system using the Yocto Project. The proposed structure\nwas implemented based on a three-layer architecture consisting of the main\nYocto repositories, a custom layer (meta-custom), and a coordinating manifest\nlayer to ensure version synchronization, scalability, and reproducibility.\nThree sample projects, including libhelloworld, helloworld, and the kernel\nmodule hello mod, were developed and integrated into the build process.\nContinuous Integration and Continuous Deployment pipelines were implemented\nwith GitLab CI and combined with an isolated Docker environment to automate and\nstreamline the build and testing workflows. Using a local cache server\ncontaining hashserv, downloads and sstate cache significantly reduced the build\ntime. The functionality and stability of the system were verified through six\nboot test scenarios in the QEMU simulator. The results show that the proposed\ndesign not only ensures reproducibility but also can be extended to advanced\napplications such as continuous deployment of real-time Linux versions. Future\nrecommendations include expanding automated tests, implementing system\nmonitoring with Prometheus and Grafana, using distributed builds, optimizing\nwith Docker multi-stage builds, and enabling continuous deployment of real-time\nLinux changes to provide a stable and scalable model for industrial and\nresearch projects in embedded systems with a rapid and reliable development\ncycle.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing use of embedded systems in various industries, the need for\nautomated platforms for the development and deployment of customized\nLinux-based operating systems has become more important. This research was\nconducted with the aim of designing and implementing an integrated and\nreproducible infrastructure for the development, building, and testing of a\nLinux-based operating system using the Yocto Project. The proposed structure\nwas implemented based on a three-layer architecture consisting of the main\nYocto repositories, a custom layer (meta-custom), and a coordinating manifest\nlayer to ensure version synchronization, scalability, and reproducibility.\nThree sample projects, including libhelloworld, helloworld, and the kernel\nmodule hello mod, were developed and integrated into the build process.\nContinuous Integration and Continuous Deployment pipelines were implemented\nwith GitLab CI and combined with an isolated Docker environment to automate and\nstreamline the build and testing workflows. Using a local cache server\ncontaining hashserv, downloads and sstate cache significantly reduced the build\ntime. The functionality and stability of the system were verified through six\nboot test scenarios in the QEMU simulator. The results show that the proposed\ndesign not only ensures reproducibility but also can be extended to advanced\napplications such as continuous deployment of real-time Linux versions. Future\nrecommendations include expanding automated tests, implementing system\nmonitoring with Prometheus and Grafana, using distributed builds, optimizing\nwith Docker multi-stage builds, and enabling continuous deployment of real-time\nLinux changes to provide a stable and scalable model for industrial and\nresearch projects in embedded systems with a rapid and reliable development\ncycle."
                },
                "authors": [
                    {
                        "name": "Behnam Agahi"
                    },
                    {
                        "name": "Hamed Farbeh"
                    }
                ],
                "author_detail": {
                    "name": "Hamed Farbeh"
                },
                "author": "Hamed Farbeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19240v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19240v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15745v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15745v2",
                "updated": "2025-10-24T05:39:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    5,
                    39,
                    3,
                    4,
                    297,
                    0
                ],
                "published": "2025-06-18T02:22:14Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    2,
                    22,
                    14,
                    2,
                    169,
                    0
                ],
                "title": "InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video\n  Understanding"
                },
                "summary": "Modern multimodal large language models (MLLMs) can reason over hour-long\nvideo, yet their key-value (KV) cache grows linearly with time-quickly\nexceeding the fixed memory of phones, AR glasses, and edge robots. Prior\ncompression schemes either assume the whole video and user query are available\noffline or must first build the full cache, so memory still scales with stream\nlength. InfiniPot-V is the first training-free, query-agnostic framework that\nenforces a hard, length-independent memory cap for streaming video\nunderstanding. During video encoding it monitors the cache and, once a user-set\nthreshold is reached, runs a lightweight compression pass that (i) removes\ntemporally redundant tokens via Temporal-axis Redundancy (TaR) metric and (ii)\nkeeps semantically significant tokens via Value-Norm (VaN) ranking. Across four\nopen-source MLLMs and four long-video and streaming-video benchmarks,\nInfiniPot-V cuts peak GPU memory by up to 94%, sustains real-time generation,\nand matches or surpasses full-cache accuracy-even in multi-turn dialogues. By\ndissolving the KV cache bottleneck without retraining or query knowledge,\nInfiniPot-V closes the gap for on-device streaming video assistants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern multimodal large language models (MLLMs) can reason over hour-long\nvideo, yet their key-value (KV) cache grows linearly with time-quickly\nexceeding the fixed memory of phones, AR glasses, and edge robots. Prior\ncompression schemes either assume the whole video and user query are available\noffline or must first build the full cache, so memory still scales with stream\nlength. InfiniPot-V is the first training-free, query-agnostic framework that\nenforces a hard, length-independent memory cap for streaming video\nunderstanding. During video encoding it monitors the cache and, once a user-set\nthreshold is reached, runs a lightweight compression pass that (i) removes\ntemporally redundant tokens via Temporal-axis Redundancy (TaR) metric and (ii)\nkeeps semantically significant tokens via Value-Norm (VaN) ranking. Across four\nopen-source MLLMs and four long-video and streaming-video benchmarks,\nInfiniPot-V cuts peak GPU memory by up to 94%, sustains real-time generation,\nand matches or surpasses full-cache accuracy-even in multi-turn dialogues. By\ndissolving the KV cache bottleneck without retraining or query knowledge,\nInfiniPot-V closes the gap for on-device streaming video assistants."
                },
                "authors": [
                    {
                        "name": "Minsoo Kim"
                    },
                    {
                        "name": "Kyuhong Shim"
                    },
                    {
                        "name": "Jungwook Choi"
                    },
                    {
                        "name": "Simyung Chang"
                    }
                ],
                "author_detail": {
                    "name": "Simyung Chang"
                },
                "author": "Simyung Chang",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15745v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15745v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13866v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13866v2",
                "updated": "2025-10-24T04:48:06Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    4,
                    48,
                    6,
                    4,
                    297,
                    0
                ],
                "published": "2025-05-20T03:21:52Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    3,
                    21,
                    52,
                    1,
                    140,
                    0
                ],
                "title": "Reasoning Path Compression: Compressing Generation Trajectories for\n  Efficient LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Path Compression: Compressing Generation Trajectories for\n  Efficient LLM Reasoning"
                },
                "summary": "Recent reasoning-focused language models achieve high accuracy by generating\nlengthy intermediate reasoning paths before producing final answers. While this\napproach is effective in solving problems that require logical thinking, long\nreasoning paths significantly increase memory usage and reduce throughput of\ntoken generation, limiting the practical deployment of such models. We propose\nReasoning Path Compression (RPC), a training-free method that accelerates\ninference by leveraging the semantic sparsity of reasoning paths. RPC\nperiodically compresses the KV cache by retaining cache entries that receive\nhigh importance score, which are computed using a selector window composed of\nrecently generated queries. Experiments show that RPC improves generation\nthroughput of QwQ-32B by up to 1.60$\\times$ compared to the inference with full\nKV cache, with an accuracy drop of 1.2\\% on the AIME 2024 benchmark. Our\nfindings demonstrate that semantic sparsity in reasoning traces can be\neffectively exploited for compression, offering a practical path toward\nefficient deployment of reasoning LLMs. Our code is available at\nhttps://github.com/jiwonsong-dev/ReasoningPathCompression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent reasoning-focused language models achieve high accuracy by generating\nlengthy intermediate reasoning paths before producing final answers. While this\napproach is effective in solving problems that require logical thinking, long\nreasoning paths significantly increase memory usage and reduce throughput of\ntoken generation, limiting the practical deployment of such models. We propose\nReasoning Path Compression (RPC), a training-free method that accelerates\ninference by leveraging the semantic sparsity of reasoning paths. RPC\nperiodically compresses the KV cache by retaining cache entries that receive\nhigh importance score, which are computed using a selector window composed of\nrecently generated queries. Experiments show that RPC improves generation\nthroughput of QwQ-32B by up to 1.60$\\times$ compared to the inference with full\nKV cache, with an accuracy drop of 1.2\\% on the AIME 2024 benchmark. Our\nfindings demonstrate that semantic sparsity in reasoning traces can be\neffectively exploited for compression, offering a practical path toward\nefficient deployment of reasoning LLMs. Our code is available at\nhttps://github.com/jiwonsong-dev/ReasoningPathCompression."
                },
                "authors": [
                    {
                        "name": "Jiwon Song"
                    },
                    {
                        "name": "Dongwon Jo"
                    },
                    {
                        "name": "Yulhwa Kim"
                    },
                    {
                        "name": "Jae-Joon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Joon Kim"
                },
                "author": "Jae-Joon Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13866v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13866v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06425v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06425v5",
                "updated": "2025-10-23T23:35:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    23,
                    35,
                    32,
                    3,
                    296,
                    0
                ],
                "published": "2025-01-11T03:37:10Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    37,
                    10,
                    5,
                    11,
                    0
                ],
                "title": "Tensor Product Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor Product Attention Is All You Need"
                },
                "summary": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, substantially shrinking the KV cache size\nat inference time. By factorizing these representations into contextual\nlow-rank components and seamlessly integrating with Rotary Position Embedding\n(RoPE), TPA achieves improved model quality alongside memory efficiency. Based\non TPA, we introduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation on\nlanguage modeling tasks, we demonstrate that T6 surpasses or matches the\nperformance of standard Transformer baselines including Multi-Head Attention\n(MHA), Multi-Query Attention (MQA), Grouped-Query Attention (GQA), and\nMulti-Head Latent Attention (MLA) across various metrics, including perplexity\nand a range of established evaluation benchmarks. Notably, TPA's memory\nefficiency and computational efficiency at decoding stage enables processing\nlonger sequences under fixed resource constraints, addressing a critical\nscalability challenge in modern language models. Project Page:\nhttps://github.com/tensorgi/TPA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, substantially shrinking the KV cache size\nat inference time. By factorizing these representations into contextual\nlow-rank components and seamlessly integrating with Rotary Position Embedding\n(RoPE), TPA achieves improved model quality alongside memory efficiency. Based\non TPA, we introduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation on\nlanguage modeling tasks, we demonstrate that T6 surpasses or matches the\nperformance of standard Transformer baselines including Multi-Head Attention\n(MHA), Multi-Query Attention (MQA), Grouped-Query Attention (GQA), and\nMulti-Head Latent Attention (MLA) across various metrics, including perplexity\nand a range of established evaluation benchmarks. Notably, TPA's memory\nefficiency and computational efficiency at decoding stage enables processing\nlonger sequences under fixed resource constraints, addressing a critical\nscalability challenge in modern language models. Project Page:\nhttps://github.com/tensorgi/TPA."
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Yifeng Liu"
                    },
                    {
                        "name": "Huizhuo Yuan"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Yang Yuan"
                    },
                    {
                        "name": "Quanquan Gu"
                    },
                    {
                        "name": "Andrew Chi-Chih Yao"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Chi-Chih Yao"
                },
                "author": "Andrew Chi-Chih Yao",
                "arxiv_comment": "Published in NeurIPS 2025 (Spotlight); Project Page:\n  https://github.com/tensorgi/TPA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06425v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06425v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16986v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16986v2",
                "updated": "2025-10-23T21:31:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    21,
                    31,
                    35,
                    3,
                    296,
                    0
                ],
                "published": "2025-05-22T17:54:32Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    54,
                    32,
                    3,
                    142,
                    0
                ],
                "title": "T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic\n  Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic\n  Planning"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities as\nintelligent agents capable of solving complex problems. However, effective\nplanning in scenarios involving dependencies between API or tool\ncalls-particularly in multi-turn conversations-remains a significant challenge.\nTo address this, we introduce T1, a tool-augmented, multi-domain, multi-turn\nconversational dataset specifically designed to capture and manage inter-tool\ndependencies across diverse domains. T1 enables rigorous evaluation of agents'\nability to coordinate tool use across nine distinct domains (4 single domain\nand 5 multi-domain) with the help of an integrated caching mechanism for both\nshort- and long-term memory, while supporting dynamic replanning-such as\ndeciding whether to recompute or reuse cached results. Beyond facilitating\nresearch on tool use and planning, T1 also serves as a benchmark for evaluating\nthe performance of open-weight and proprietary large language models. We\npresent results powered by T1-Agent, highlighting their ability to plan and\nreason in complex, tool-dependent scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities as\nintelligent agents capable of solving complex problems. However, effective\nplanning in scenarios involving dependencies between API or tool\ncalls-particularly in multi-turn conversations-remains a significant challenge.\nTo address this, we introduce T1, a tool-augmented, multi-domain, multi-turn\nconversational dataset specifically designed to capture and manage inter-tool\ndependencies across diverse domains. T1 enables rigorous evaluation of agents'\nability to coordinate tool use across nine distinct domains (4 single domain\nand 5 multi-domain) with the help of an integrated caching mechanism for both\nshort- and long-term memory, while supporting dynamic replanning-such as\ndeciding whether to recompute or reuse cached results. Beyond facilitating\nresearch on tool use and planning, T1 also serves as a benchmark for evaluating\nthe performance of open-weight and proprietary large language models. We\npresent results powered by T1-Agent, highlighting their ability to plan and\nreason in complex, tool-dependent scenarios."
                },
                "authors": [
                    {
                        "name": "Amartya Chakraborty"
                    },
                    {
                        "name": "Paresh Dashore"
                    },
                    {
                        "name": "Nadia Bathaee"
                    },
                    {
                        "name": "Anmol Jain"
                    },
                    {
                        "name": "Anirban Das"
                    },
                    {
                        "name": "Shi-Xiong Zhang"
                    },
                    {
                        "name": "Sambit Sahu"
                    },
                    {
                        "name": "Milind Naphade"
                    },
                    {
                        "name": "Genta Indra Winata"
                    }
                ],
                "author_detail": {
                    "name": "Genta Indra Winata"
                },
                "author": "Genta Indra Winata",
                "arxiv_comment": "Accepted by NeurIPS 2025 Datasets and Benchmarks Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16986v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16986v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00566v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00566v3",
                "updated": "2025-10-23T19:45:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    19,
                    45,
                    39,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-01T06:38:45Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    6,
                    38,
                    45,
                    2,
                    274,
                    0
                ],
                "title": "Panorama: Fast-Track Nearest Neighbors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Panorama: Fast-Track Nearest Neighbors"
                },
                "summary": "Approximate Nearest-Neighbor Search (ANNS) efficiently finds data items whose\nembeddings are close to that of a given query in a high-dimensional space,\naiming to balance accuracy with speed. Used in recommendation systems, image\nand video retrieval, natural language processing, and retrieval-augmented\ngeneration (RAG), ANNS algorithms such as IVFPQ, HNSW graphs, Annoy, and MRPT\nutilize graph, tree, clustering, and quantization techniques to navigate large\nvector spaces. Despite this progress, ANNS systems spend up to 99% of query\ntime to compute distances in their final refinement phase. In this paper, we\npresent PANORAMA, a machine learning-driven approach that tackles the ANNS\nverification bottleneck through data-adaptive learned orthogonal transforms\nthat facilitate the accretive refinement of distance bounds. Such transforms\ncompact over 90% of signal energy into the first half of dimensions, enabling\nearly candidate pruning with partial distance computations. We integrate\nPANORAMA into state-of-the-art ANNS methods, namely IVFPQ/Flat, HNSW, MRPT, and\nAnnoy, without index modification, using level-major memory layouts,\nSIMD-vectorized partial distance computations, and cache-aware access patterns.\nExperiments across diverse datasets -- from image-based CIFAR-10 and GIST to\nmodern embedding spaces including OpenAI's Ada 2 and Large 3 -- demonstrate\nthat PANORAMA affords a 2--30$\\times$ end-to-end speedup with no recall loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate Nearest-Neighbor Search (ANNS) efficiently finds data items whose\nembeddings are close to that of a given query in a high-dimensional space,\naiming to balance accuracy with speed. Used in recommendation systems, image\nand video retrieval, natural language processing, and retrieval-augmented\ngeneration (RAG), ANNS algorithms such as IVFPQ, HNSW graphs, Annoy, and MRPT\nutilize graph, tree, clustering, and quantization techniques to navigate large\nvector spaces. Despite this progress, ANNS systems spend up to 99% of query\ntime to compute distances in their final refinement phase. In this paper, we\npresent PANORAMA, a machine learning-driven approach that tackles the ANNS\nverification bottleneck through data-adaptive learned orthogonal transforms\nthat facilitate the accretive refinement of distance bounds. Such transforms\ncompact over 90% of signal energy into the first half of dimensions, enabling\nearly candidate pruning with partial distance computations. We integrate\nPANORAMA into state-of-the-art ANNS methods, namely IVFPQ/Flat, HNSW, MRPT, and\nAnnoy, without index modification, using level-major memory layouts,\nSIMD-vectorized partial distance computations, and cache-aware access patterns.\nExperiments across diverse datasets -- from image-based CIFAR-10 and GIST to\nmodern embedding spaces including OpenAI's Ada 2 and Large 3 -- demonstrate\nthat PANORAMA affords a 2--30$\\times$ end-to-end speedup with no recall loss."
                },
                "authors": [
                    {
                        "name": "Vansh Ramani"
                    },
                    {
                        "name": "Alexis Schlomer"
                    },
                    {
                        "name": "Akash Nayar"
                    },
                    {
                        "name": "Sayan Ranu"
                    },
                    {
                        "name": "Jignesh M. Patel"
                    },
                    {
                        "name": "Panagiotis Karras"
                    }
                ],
                "author_detail": {
                    "name": "Panagiotis Karras"
                },
                "author": "Panagiotis Karras",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00566v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00566v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07254v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07254v3",
                "updated": "2025-10-23T18:52:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    18,
                    52,
                    25,
                    3,
                    296,
                    0
                ],
                "published": "2025-06-08T18:43:31Z",
                "published_parsed": [
                    2025,
                    6,
                    8,
                    18,
                    43,
                    31,
                    6,
                    159,
                    0
                ],
                "title": "A Stable Whitening Optimizer for Efficient Neural Network Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Stable Whitening Optimizer for Efficient Neural Network Training"
                },
                "summary": "In this work, we take an experimentally grounded look at neural network\noptimization. Building on the Shampoo family of algorithms, we identify and\nalleviate three key issues, resulting in the proposed SPlus method. First, we\nfind that naive Shampoo is prone to divergence when matrix-inverses are cached\nfor long periods. We introduce an alternate bounded update combining a\nhistorical eigenbasis with instantaneous normalization, resulting in\nacross-the-board stability and significantly lower computational requirements.\nSecond, we adapt a shape-aware scaling to enable learning rate transfer across\nnetwork width. Third, we find that high learning rates result in large\nparameter noise, and propose a simple iterate-averaging scheme which unblocks\nfaster learning. To properly confirm these findings, we introduce a pointed\nTransformer training benchmark, considering three objectives (language\nmodelling, image classification, and diffusion modelling) across different\nstages of training. On average, SPlus is able to reach the validation\nperformance of Adam within 44-58% of the gradient steps and 62-83% of the\nwallclock time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we take an experimentally grounded look at neural network\noptimization. Building on the Shampoo family of algorithms, we identify and\nalleviate three key issues, resulting in the proposed SPlus method. First, we\nfind that naive Shampoo is prone to divergence when matrix-inverses are cached\nfor long periods. We introduce an alternate bounded update combining a\nhistorical eigenbasis with instantaneous normalization, resulting in\nacross-the-board stability and significantly lower computational requirements.\nSecond, we adapt a shape-aware scaling to enable learning rate transfer across\nnetwork width. Third, we find that high learning rates result in large\nparameter noise, and propose a simple iterate-averaging scheme which unblocks\nfaster learning. To properly confirm these findings, we introduce a pointed\nTransformer training benchmark, considering three objectives (language\nmodelling, image classification, and diffusion modelling) across different\nstages of training. On average, SPlus is able to reach the validation\nperformance of Adam within 44-58% of the gradient steps and 62-83% of the\nwallclock time."
                },
                "authors": [
                    {
                        "name": "Kevin Frans"
                    },
                    {
                        "name": "Sergey Levine"
                    },
                    {
                        "name": "Pieter Abbeel"
                    }
                ],
                "author_detail": {
                    "name": "Pieter Abbeel"
                },
                "author": "Pieter Abbeel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07254v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07254v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.20707v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.20707v1",
                "updated": "2025-10-23T16:17:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    16,
                    17,
                    47,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-23T16:17:47Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    16,
                    17,
                    47,
                    3,
                    296,
                    0
                ],
                "title": "Mixing Importance with Diversity: Joint Optimization for KV Cache\n  Compression in Large Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixing Importance with Diversity: Joint Optimization for KV Cache\n  Compression in Large Vision-Language Models"
                },
                "summary": "Recent large vision-language models (LVLMs) demonstrate remarkable\ncapabilities in processing extended multi-modal sequences, yet the resulting\nkey-value (KV) cache expansion creates a critical memory bottleneck that\nfundamentally limits deployment scalability. While existing KV cache\ncompression methods focus on retaining high-importance KV pairs to minimize\nstorage, they often overlook the modality-specific semantic redundancy patterns\nthat emerge distinctively in multi-modal KV caches. In this work, we first\nanalyze how, beyond simple importance, the KV cache in LVLMs exhibits varying\nlevels of redundancy across attention heads. We show that relying solely on\nimportance can only cover a subset of the full KV cache information\ndistribution, leading to potential loss of semantic coverage. To address this,\nwe propose \\texttt{MixKV}, a novel method that mixes importance with diversity\nfor optimized KV cache compression in LVLMs. \\texttt{MixKV} adapts to head-wise\nsemantic redundancy, selectively balancing diversity and importance when\ncompressing KV pairs. Extensive experiments demonstrate that \\texttt{MixKV}\nconsistently enhances existing methods across multiple LVLMs. Under extreme\ncompression (budget=64), \\texttt{MixKV} improves baseline methods by an average\nof \\textbf{5.1\\%} across five multi-modal understanding benchmarks and achieves\nremarkable gains of \\textbf{8.0\\%} and \\textbf{9.0\\%} for SnapKV and AdaKV on\nGUI grounding tasks, all while maintaining comparable inference efficiency.\nFurthermore, \\texttt{MixKV} extends seamlessly to LLMs with comparable\nperformance gains. Our code is available at\n\\href{https://github.com/xuyang-liu16/MixKV}{\\textcolor{citeblue}{https://github.com/xuyang-liu16/MixKV}}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large vision-language models (LVLMs) demonstrate remarkable\ncapabilities in processing extended multi-modal sequences, yet the resulting\nkey-value (KV) cache expansion creates a critical memory bottleneck that\nfundamentally limits deployment scalability. While existing KV cache\ncompression methods focus on retaining high-importance KV pairs to minimize\nstorage, they often overlook the modality-specific semantic redundancy patterns\nthat emerge distinctively in multi-modal KV caches. In this work, we first\nanalyze how, beyond simple importance, the KV cache in LVLMs exhibits varying\nlevels of redundancy across attention heads. We show that relying solely on\nimportance can only cover a subset of the full KV cache information\ndistribution, leading to potential loss of semantic coverage. To address this,\nwe propose \\texttt{MixKV}, a novel method that mixes importance with diversity\nfor optimized KV cache compression in LVLMs. \\texttt{MixKV} adapts to head-wise\nsemantic redundancy, selectively balancing diversity and importance when\ncompressing KV pairs. Extensive experiments demonstrate that \\texttt{MixKV}\nconsistently enhances existing methods across multiple LVLMs. Under extreme\ncompression (budget=64), \\texttt{MixKV} improves baseline methods by an average\nof \\textbf{5.1\\%} across five multi-modal understanding benchmarks and achieves\nremarkable gains of \\textbf{8.0\\%} and \\textbf{9.0\\%} for SnapKV and AdaKV on\nGUI grounding tasks, all while maintaining comparable inference efficiency.\nFurthermore, \\texttt{MixKV} extends seamlessly to LLMs with comparable\nperformance gains. Our code is available at\n\\href{https://github.com/xuyang-liu16/MixKV}{\\textcolor{citeblue}{https://github.com/xuyang-liu16/MixKV}}."
                },
                "authors": [
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Xiyan Gui"
                    },
                    {
                        "name": "Yuchao Zhang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "Our code is available at https://github.com/xuyang-liu16/MixKV",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.20707v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.20707v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16407v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16407v2",
                "updated": "2025-10-23T15:26:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    15,
                    26,
                    38,
                    3,
                    296,
                    0
                ],
                "published": "2025-09-19T20:31:38Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    20,
                    31,
                    38,
                    4,
                    262,
                    0
                ],
                "title": "WarpSpeed: A High-Performance Library for Concurrent GPU Hash Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WarpSpeed: A High-Performance Library for Concurrent GPU Hash Tables"
                },
                "summary": "GPU hash tables are increasingly used to accelerate data processing, but\ntheir limited functionality restricts adoption in large-scale data processing\napplications. Current limitations include incomplete concurrency support and\nmissing compound operations such as upserts.\n  This paper presents WarpSpeed, a library of high-performance concurrent GPU\nhash tables with a unified benchmarking framework for performance analysis.\nWarpSpeed implements eight state-of-the-art Nvidia GPU hash table designs and\nprovides a rich API designed for modern GPU applications. Our evaluation uses\ndiverse benchmarks to assess both correctness and scalability, and we\ndemonstrate real-world impact by integrating these hash tables into three\ndownstream applications.\n  We propose several optimization techniques to reduce concurrency overhead,\nincluding fingerprint-based metadata to minimize cache line probes and\nspecialized Nvidia GPU instructions for lock-free queries. Our findings provide\nnew insights into concurrent GPU hash table design and offer practical guidance\nfor developing efficient, scalable data structures on modern GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU hash tables are increasingly used to accelerate data processing, but\ntheir limited functionality restricts adoption in large-scale data processing\napplications. Current limitations include incomplete concurrency support and\nmissing compound operations such as upserts.\n  This paper presents WarpSpeed, a library of high-performance concurrent GPU\nhash tables with a unified benchmarking framework for performance analysis.\nWarpSpeed implements eight state-of-the-art Nvidia GPU hash table designs and\nprovides a rich API designed for modern GPU applications. Our evaluation uses\ndiverse benchmarks to assess both correctness and scalability, and we\ndemonstrate real-world impact by integrating these hash tables into three\ndownstream applications.\n  We propose several optimization techniques to reduce concurrency overhead,\nincluding fingerprint-based metadata to minimize cache line probes and\nspecialized Nvidia GPU instructions for lock-free queries. Our findings provide\nnew insights into concurrent GPU hash table design and offer practical guidance\nfor developing efficient, scalable data structures on modern GPUs."
                },
                "authors": [
                    {
                        "name": "Hunter McCoy"
                    },
                    {
                        "name": "Prashant Pandey"
                    }
                ],
                "author_detail": {
                    "name": "Prashant Pandey"
                },
                "author": "Prashant Pandey",
                "arxiv_comment": "Accepted to ALENEX`26",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16407v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16407v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13251v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13251v4",
                "updated": "2025-10-23T14:23:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    14,
                    23,
                    24,
                    3,
                    296,
                    0
                ],
                "published": "2025-02-18T19:22:44Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    19,
                    22,
                    44,
                    1,
                    49,
                    0
                ],
                "title": "Neural Attention Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Attention Search"
                },
                "summary": "We present Neural Attention Search (NAtS), a framework that automatically\nevaluates the importance of each token within a sequence and determines if the\ncorresponding token can be dropped after several steps. This approach can\nefficiently reduce the KV cache sizes required by transformer-based models\nduring inference and thus reduce inference costs. In this paper, we design a\nsearch space that contains three token types: (i) Global Tokens will be\npreserved and queried by all the following tokens. (ii) Local Tokens survive\nuntil the next global token appears. (iii) Sliding Window Tokens have an impact\non the inference of a fixed size of the next following tokens. Similar to the\nOne-Shot Neural Architecture Search approach, this token-type information can\nbe learned jointly with the architecture weights via a learnable attention\nmask. Experiments on both training a new transformer from scratch and\nfine-tuning existing large language models show that NAtS can efficiently\nreduce the KV cache size required for the models while maintaining the models'\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Neural Attention Search (NAtS), a framework that automatically\nevaluates the importance of each token within a sequence and determines if the\ncorresponding token can be dropped after several steps. This approach can\nefficiently reduce the KV cache sizes required by transformer-based models\nduring inference and thus reduce inference costs. In this paper, we design a\nsearch space that contains three token types: (i) Global Tokens will be\npreserved and queried by all the following tokens. (ii) Local Tokens survive\nuntil the next global token appears. (iii) Sliding Window Tokens have an impact\non the inference of a fixed size of the next following tokens. Similar to the\nOne-Shot Neural Architecture Search approach, this token-type information can\nbe learned jointly with the architecture weights via a learnable attention\nmask. Experiments on both training a new transformer from scratch and\nfine-tuning existing large language models show that NAtS can efficiently\nreduce the KV cache size required for the models while maintaining the models'\nperformance."
                },
                "authors": [
                    {
                        "name": "Difan Deng"
                    },
                    {
                        "name": "Marius Lindauer"
                    }
                ],
                "author_detail": {
                    "name": "Marius Lindauer"
                },
                "author": "Marius Lindauer",
                "arxiv_comment": "35 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13251v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13251v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.20878v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.20878v1",
                "updated": "2025-10-23T12:28:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    12,
                    28,
                    58,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-23T12:28:58Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    12,
                    28,
                    58,
                    3,
                    296,
                    0
                ],
                "title": "HA-RAG: Hotness-Aware RAG Acceleration via Mixed Precision and Data\n  Placement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HA-RAG: Hotness-Aware RAG Acceleration via Mixed Precision and Data\n  Placement"
                },
                "summary": "Retrieval-Augmented Generation (RAG) improves model output accuracy by\nleveraging external knowledge bases, serving as an effective solution to\naddress hallucination issues and knowledge-update delays in Large Language\nModels (LLMs). However, the introduction of external knowledge bases presents\nRAG with challenges in long-context processing, significantly increasing memory\nconsumption and inference latency. Existing research accelerates inference by\nprecomputing Key and Value (KV) of the knowledge base and loading them\non-demand during inference. Based on the access frequency of different KV\nchunks within the external knowledge base, this paper proposes a hotness-aware\nRAG (HA-RAG) inference optimization system. First, leveraging the numerical\ndistribution of KV chunks, we introduce a hotness-aware mixed-precision\ncompressing and loading method to reduce disk I/O and memory access overhead.\nSecond, we design a hotness-aware data placement strategy that prioritizes\nstoring frequently accessed KV chunks in high-speed memory to improve data\naccess efficiency. Experimental results demonstrate that, compared with\nTurboRAG, the proposed HA-RAG achieves an average speedup of 2.10x and maximum\nspeedup of 10.49x in Time-To-First-Token (TTFT) with negligible accuracy loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) improves model output accuracy by\nleveraging external knowledge bases, serving as an effective solution to\naddress hallucination issues and knowledge-update delays in Large Language\nModels (LLMs). However, the introduction of external knowledge bases presents\nRAG with challenges in long-context processing, significantly increasing memory\nconsumption and inference latency. Existing research accelerates inference by\nprecomputing Key and Value (KV) of the knowledge base and loading them\non-demand during inference. Based on the access frequency of different KV\nchunks within the external knowledge base, this paper proposes a hotness-aware\nRAG (HA-RAG) inference optimization system. First, leveraging the numerical\ndistribution of KV chunks, we introduce a hotness-aware mixed-precision\ncompressing and loading method to reduce disk I/O and memory access overhead.\nSecond, we design a hotness-aware data placement strategy that prioritizes\nstoring frequently accessed KV chunks in high-speed memory to improve data\naccess efficiency. Experimental results demonstrate that, compared with\nTurboRAG, the proposed HA-RAG achieves an average speedup of 2.10x and maximum\nspeedup of 10.49x in Time-To-First-Token (TTFT) with negligible accuracy loss."
                },
                "authors": [
                    {
                        "name": "Danying Ge"
                    },
                    {
                        "name": "Jianhua Gao"
                    },
                    {
                        "name": "Yixue Yang"
                    },
                    {
                        "name": "Weixing Ji"
                    }
                ],
                "author_detail": {
                    "name": "Weixing Ji"
                },
                "author": "Weixing Ji",
                "arxiv_comment": "13 pages,16 figures,2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.20878v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.20878v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.4; E.4; I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21865v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21865v1",
                "updated": "2025-10-23T10:35:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    10,
                    35,
                    35,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-23T10:35:35Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    10,
                    35,
                    35,
                    3,
                    296,
                    0
                ],
                "title": "Prefetching Cache Optimization Using Graph Neural Networks: A Modular\n  Framework and Conceptual Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prefetching Cache Optimization Using Graph Neural Networks: A Modular\n  Framework and Conceptual Analysis"
                },
                "summary": "Caching and prefetching techniques are fundamental to modern computing,\nserving to bridge the growing performance gap between processors and memory.\nTraditional prefetching strategies are often limited by their reliance on\npredefined heuristics or simplified statistical models, which fail to capture\nthe complex, non-linear dependencies in modern data access patterns. This paper\nintroduces a modular framework leveraging Graph Neural Networks (GNNs) to model\nand predict access patterns within graph-structured data, focusing on web\nnavigation and hierarchical file systems. The toolchain consists of: a route\nmapper for extracting structural information, a graph constructor for creating\ngraph representations, a walk session generator for simulating user behaviors,\nand a gnn prefetch module for training and inference. We provide a detailed\nconceptual analysis showing how GNN-based approaches can outperform\nconventional methods by learning intricate dependencies. This work offers both\ntheoretical foundations and a practical, replicable pipeline for future\nresearch in graph-driven systems optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching and prefetching techniques are fundamental to modern computing,\nserving to bridge the growing performance gap between processors and memory.\nTraditional prefetching strategies are often limited by their reliance on\npredefined heuristics or simplified statistical models, which fail to capture\nthe complex, non-linear dependencies in modern data access patterns. This paper\nintroduces a modular framework leveraging Graph Neural Networks (GNNs) to model\nand predict access patterns within graph-structured data, focusing on web\nnavigation and hierarchical file systems. The toolchain consists of: a route\nmapper for extracting structural information, a graph constructor for creating\ngraph representations, a walk session generator for simulating user behaviors,\nand a gnn prefetch module for training and inference. We provide a detailed\nconceptual analysis showing how GNN-based approaches can outperform\nconventional methods by learning intricate dependencies. This work offers both\ntheoretical foundations and a practical, replicable pipeline for future\nresearch in graph-driven systems optimization."
                },
                "authors": [
                    {
                        "name": "F. I. Qowy"
                    }
                ],
                "author_detail": {
                    "name": "F. I. Qowy"
                },
                "author": "F. I. Qowy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21865v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21865v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.20400v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.20400v1",
                "updated": "2025-10-23T10:06:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    10,
                    6,
                    48,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-23T10:06:48Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    10,
                    6,
                    48,
                    3,
                    296,
                    0
                ],
                "title": "Squire: A General-Purpose Accelerator to Exploit Fine-Grain Parallelism\n  on Dependency-Bound Kernels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Squire: A General-Purpose Accelerator to Exploit Fine-Grain Parallelism\n  on Dependency-Bound Kernels"
                },
                "summary": "Multiple HPC applications are often bottlenecked by compute-intensive kernels\nimplementing complex dependency patterns (data-dependency bound). Traditional\ngeneral-purpose accelerators struggle to effectively exploit fine-grain\nparallelism due to limitations in implementing convoluted data-dependency\npatterns (like SIMD) and overheads due to synchronization and data transfers\n(like GPGPUs). In contrast, custom FPGA and ASIC designs offer improved\nperformance and energy efficiency at a high cost in hardware design and\nprogramming complexity and often lack the flexibility to process different\nworkloads. We propose Squire, a general-purpose accelerator designed to exploit\nfine-grain parallelism effectively on dependency-bound kernels. Each Squire\naccelerator has a set of general-purpose low-power in-order cores that can\nrapidly communicate among themselves and directly access data from the L2\ncache. Our proposal integrates one Squire accelerator per core in a typical\nmulticore system, allowing the acceleration of dependency-bound kernels within\nparallel tasks with minimal software changes. As a case study, we evaluate\nSquire's effectiveness by accelerating five kernels that implement complex\ndependency patterns. We use three of these kernels to build an end-to-end\nread-mapping tool that will be used to evaluate Squire. Squire obtains speedups\nup to 7.64$\\times$ in dynamic programming kernels. Overall, Squire provides an\nacceleration for an end-to-end application of 3.66$\\times$. In addition, Squire\nreduces energy consumption by up to 56% with a minimal area overhead of 10.5%\ncompared to a Neoverse-N1 baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiple HPC applications are often bottlenecked by compute-intensive kernels\nimplementing complex dependency patterns (data-dependency bound). Traditional\ngeneral-purpose accelerators struggle to effectively exploit fine-grain\nparallelism due to limitations in implementing convoluted data-dependency\npatterns (like SIMD) and overheads due to synchronization and data transfers\n(like GPGPUs). In contrast, custom FPGA and ASIC designs offer improved\nperformance and energy efficiency at a high cost in hardware design and\nprogramming complexity and often lack the flexibility to process different\nworkloads. We propose Squire, a general-purpose accelerator designed to exploit\nfine-grain parallelism effectively on dependency-bound kernels. Each Squire\naccelerator has a set of general-purpose low-power in-order cores that can\nrapidly communicate among themselves and directly access data from the L2\ncache. Our proposal integrates one Squire accelerator per core in a typical\nmulticore system, allowing the acceleration of dependency-bound kernels within\nparallel tasks with minimal software changes. As a case study, we evaluate\nSquire's effectiveness by accelerating five kernels that implement complex\ndependency patterns. We use three of these kernels to build an end-to-end\nread-mapping tool that will be used to evaluate Squire. Squire obtains speedups\nup to 7.64$\\times$ in dynamic programming kernels. Overall, Squire provides an\nacceleration for an end-to-end application of 3.66$\\times$. In addition, Squire\nreduces energy consumption by up to 56% with a minimal area overhead of 10.5%\ncompared to a Neoverse-N1 baseline."
                },
                "authors": [
                    {
                        "name": "Rubén Langarita"
                    },
                    {
                        "name": "Jesús Alastruey-Benedé"
                    },
                    {
                        "name": "Pablo Ibáñez-Marín"
                    },
                    {
                        "name": "Santiago Marco-Sola"
                    },
                    {
                        "name": "Miquel Moretó"
                    },
                    {
                        "name": "Adrià Armejach"
                    }
                ],
                "author_detail": {
                    "name": "Adrià Armejach"
                },
                "author": "Adrià Armejach",
                "arxiv_comment": "11 pages, 10 figures, 5 tables, 4 algorithms, accepted on PACT25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.20400v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.20400v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11843v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11843v2",
                "updated": "2025-10-23T09:55:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    9,
                    55,
                    50,
                    3,
                    296,
                    0
                ],
                "published": "2024-11-18T18:59:15Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    59,
                    15,
                    0,
                    323,
                    0
                ],
                "title": "Bi-Mamba: Towards Accurate 1-Bit State Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bi-Mamba: Towards Accurate 1-Bit State Space Models"
                },
                "summary": "The typical Selective State-Space Model (SSM) used in Mamba addresses several\nlimitations of Transformers, such as the quadratic computational complexity\nwith respect to sequence length and the significant memory requirements during\ninference due to the key-value (KV) cache. However, the increasing size of\nMamba models continues to pose challenges for training and deployment,\nparticularly due to their substantial computational demands during both\ntraining and inference. In this work, we introduce $\\texttt{Bi-Mamba}$, a\nscalable and powerful 1-bit Mamba architecture designed to enable more\nefficient large language models (LLMs), with model sizes of 780M, 1.3B, and\n2.7B parameters. $\\texttt{Bi-Mamba}$ models are trained from scratch on a\nstandard LLM-scale dataset using an autoregressive distillation loss. Extensive\nexperiments on language modeling benchmarks demonstrate that\n$\\texttt{Bi-Mamba}$ achieves performance comparable to its full-precision (FP16\nor BF16) counterparts, while outperforming post-training binarization (PTB)\nMamba and binarization-aware training (BAT) Transformer baselines. Moreover,\n$\\texttt{Bi-Mamba}$ drastically reduces memory usage and computational cost\ncompared to the original Mamba. Our work pioneers a new line of\nlinear-complexity LLMs under low-bit representation and provides the way for\nthe design of specialized hardware optimized for efficient 1-bit Mamba-based\nmodels. Code and the pre-trained weights are available at\nhttps://github.com/Tangshengku/Bi-Mamba.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The typical Selective State-Space Model (SSM) used in Mamba addresses several\nlimitations of Transformers, such as the quadratic computational complexity\nwith respect to sequence length and the significant memory requirements during\ninference due to the key-value (KV) cache. However, the increasing size of\nMamba models continues to pose challenges for training and deployment,\nparticularly due to their substantial computational demands during both\ntraining and inference. In this work, we introduce $\\texttt{Bi-Mamba}$, a\nscalable and powerful 1-bit Mamba architecture designed to enable more\nefficient large language models (LLMs), with model sizes of 780M, 1.3B, and\n2.7B parameters. $\\texttt{Bi-Mamba}$ models are trained from scratch on a\nstandard LLM-scale dataset using an autoregressive distillation loss. Extensive\nexperiments on language modeling benchmarks demonstrate that\n$\\texttt{Bi-Mamba}$ achieves performance comparable to its full-precision (FP16\nor BF16) counterparts, while outperforming post-training binarization (PTB)\nMamba and binarization-aware training (BAT) Transformer baselines. Moreover,\n$\\texttt{Bi-Mamba}$ drastically reduces memory usage and computational cost\ncompared to the original Mamba. Our work pioneers a new line of\nlinear-complexity LLMs under low-bit representation and provides the way for\nthe design of specialized hardware optimized for efficient 1-bit Mamba-based\nmodels. Code and the pre-trained weights are available at\nhttps://github.com/Tangshengku/Bi-Mamba."
                },
                "authors": [
                    {
                        "name": "Shengkun Tang"
                    },
                    {
                        "name": "Liqun Ma"
                    },
                    {
                        "name": "Haonan Li"
                    },
                    {
                        "name": "Mingjie Sun"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Shen"
                },
                "author": "Zhiqiang Shen",
                "arxiv_comment": "Accepted in TMLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11843v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11843v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.16807v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.16807v2",
                "updated": "2025-10-23T08:29:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    8,
                    29,
                    11,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-19T12:17:42Z",
                "published_parsed": [
                    2025,
                    10,
                    19,
                    12,
                    17,
                    42,
                    6,
                    292,
                    0
                ],
                "title": "Improving Model Representation and Reducing KV Cache via Skip\n  Connections with First Value Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Model Representation and Reducing KV Cache via Skip\n  Connections with First Value Heads"
                },
                "summary": "Transformer models have driven breakthroughs across various language tasks by\ntheir strong capability to learn rich contextual representations. Scaling them\nto improve representation, however, often demands substantial memory and\ncompute costs, such as the Key-Value (KV) cache used during auto-regressive\ndecoding. Skip connections offer a promising way to improve representation\nwithout bloating resource usage, yet most prior works either improve\nexpressivity while leaving KV costs unchanged, or reduce memory at the cost of\nweaker representation. In this work, we propose SkipV1Former, a Transformer\nvariant that uses skip connections from the first layer's Value heads to\nstrengthen model representation and reduce KV cache. Specifically, from the\nsecond block onward, each layer reuses half of its Value heads from the very\nfirst layer, while computing the other half as usual-cutting Value projections\nand V cache by nearly 50 \\%. Theoretically, we show that routing uncompressed\nfirst-layer Values into deeper layers restores information lost to compression\nand accelerates the model's implicit mesa-optimization-a key pattern of\nTransformer in auto-regressive tasks. Empirically, across different model\nscales, SkipV1Former delivers consistent reductions of approximately 25 \\% in\nKV cache while improving perplexity relative to standard Multi-Head Attention\n(MHA) Transformers and some advanced variants. Moreover, we propose a recipe\nfor uptraining existing MHA Transformer checkpoints to SkipV1Former with only\n10-15\\% additional compute. Finally, SkipV1Former can seamlessly combine\nadvanced methods like Group-Query Attention and Multi-Latent Attention to\nachieve further KV cache savings and performance improvement. When combined\nwith YOCO, it cuts KV cache size by nearly 50 \\% while still improving\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer models have driven breakthroughs across various language tasks by\ntheir strong capability to learn rich contextual representations. Scaling them\nto improve representation, however, often demands substantial memory and\ncompute costs, such as the Key-Value (KV) cache used during auto-regressive\ndecoding. Skip connections offer a promising way to improve representation\nwithout bloating resource usage, yet most prior works either improve\nexpressivity while leaving KV costs unchanged, or reduce memory at the cost of\nweaker representation. In this work, we propose SkipV1Former, a Transformer\nvariant that uses skip connections from the first layer's Value heads to\nstrengthen model representation and reduce KV cache. Specifically, from the\nsecond block onward, each layer reuses half of its Value heads from the very\nfirst layer, while computing the other half as usual-cutting Value projections\nand V cache by nearly 50 \\%. Theoretically, we show that routing uncompressed\nfirst-layer Values into deeper layers restores information lost to compression\nand accelerates the model's implicit mesa-optimization-a key pattern of\nTransformer in auto-regressive tasks. Empirically, across different model\nscales, SkipV1Former delivers consistent reductions of approximately 25 \\% in\nKV cache while improving perplexity relative to standard Multi-Head Attention\n(MHA) Transformers and some advanced variants. Moreover, we propose a recipe\nfor uptraining existing MHA Transformer checkpoints to SkipV1Former with only\n10-15\\% additional compute. Finally, SkipV1Former can seamlessly combine\nadvanced methods like Group-Query Attention and Multi-Latent Attention to\nachieve further KV cache savings and performance improvement. When combined\nwith YOCO, it cuts KV cache size by nearly 50 \\% while still improving\nperformance."
                },
                "authors": [
                    {
                        "name": "Zhoutong Wu"
                    },
                    {
                        "name": "Yuan Zhang"
                    },
                    {
                        "name": "Yiming Dong"
                    },
                    {
                        "name": "Chenheng Zhang"
                    },
                    {
                        "name": "Cong Fang"
                    },
                    {
                        "name": "Kun Yuan"
                    },
                    {
                        "name": "Zhouchen Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zhouchen Lin"
                },
                "author": "Zhouchen Lin",
                "arxiv_comment": "The code is available at:\n  \\url{https://github.com/Zhoutong-Wu/SkipV1Former}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.16807v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.16807v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.20230v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.20230v1",
                "updated": "2025-10-23T05:22:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    5,
                    22,
                    9,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-23T05:22:09Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    5,
                    22,
                    9,
                    3,
                    296,
                    0
                ],
                "title": "Soft Phonon Charge-Density Wave Formation in the Kagome Metal\n  KV$_3$Sb$_5$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soft Phonon Charge-Density Wave Formation in the Kagome Metal\n  KV$_3$Sb$_5$"
                },
                "summary": "A range of of unusual emergent behaviors have been reported in the\ncharge-density wave (CDW) state of the $A$V$_3$Sb$_5$ ($A=~$K, Rb, Cs) kagome\nmetals, including a CDW formation process without soft phonons, which points to\nan unconventional CDW mechanism. Here, we use inelastic x-ray scattering to\nshow that the CDW in KV$_3$Sb$_5$ forms via phonons that soften to zero energy\nat the CDW ordering vector ($L$-point) around $T_{\\rm CDW}=78$~K. These soft\nphonons exhibit a remarkable in-plane anisotropy, extending over a much larger\nmomentum range along $L$-$A$ relative to $L$-$H$, which leads to diffuse\nscattering common among $A$V$_3$Sb$_5$. Using first-principles calculations, we\nfind that the momentum-dependent electron-phonon coupling (EPC) is peaked at\n$L$ and exhibits the same in-plane anisotropy as the phonon softening.\nConversely, the electronic susceptibility is not peaked at $L$ and shows the\nopposite in-plane anisotropy. Our findings favor momentum-dependent EPC as the\ndriving mechanism of the CDW in KV$_3$Sb$_5$, with a CDW formation process\nsimilar to that of transition metal dichalcogenides.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A range of of unusual emergent behaviors have been reported in the\ncharge-density wave (CDW) state of the $A$V$_3$Sb$_5$ ($A=~$K, Rb, Cs) kagome\nmetals, including a CDW formation process without soft phonons, which points to\nan unconventional CDW mechanism. Here, we use inelastic x-ray scattering to\nshow that the CDW in KV$_3$Sb$_5$ forms via phonons that soften to zero energy\nat the CDW ordering vector ($L$-point) around $T_{\\rm CDW}=78$~K. These soft\nphonons exhibit a remarkable in-plane anisotropy, extending over a much larger\nmomentum range along $L$-$A$ relative to $L$-$H$, which leads to diffuse\nscattering common among $A$V$_3$Sb$_5$. Using first-principles calculations, we\nfind that the momentum-dependent electron-phonon coupling (EPC) is peaked at\n$L$ and exhibits the same in-plane anisotropy as the phonon softening.\nConversely, the electronic susceptibility is not peaked at $L$ and shows the\nopposite in-plane anisotropy. Our findings favor momentum-dependent EPC as the\ndriving mechanism of the CDW in KV$_3$Sb$_5$, with a CDW formation process\nsimilar to that of transition metal dichalcogenides."
                },
                "authors": [
                    {
                        "name": "Yifan Wang"
                    },
                    {
                        "name": "Chenchao Xu"
                    },
                    {
                        "name": "Zhimian Wu"
                    },
                    {
                        "name": "Huachen Rao"
                    },
                    {
                        "name": "Zhaoyang Shan"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Guanghan Cao"
                    },
                    {
                        "name": "Michael Smidman"
                    },
                    {
                        "name": "Ming Shi"
                    },
                    {
                        "name": "Huiqiu Yuan"
                    },
                    {
                        "name": "Tao Wu"
                    },
                    {
                        "name": "Xianhui Chen"
                    },
                    {
                        "name": "Chao Cao"
                    },
                    {
                        "name": "Yu Song"
                    }
                ],
                "author_detail": {
                    "name": "Yu Song"
                },
                "author": "Yu Song",
                "arxiv_comment": "submitted to journal in July 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.20230v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.20230v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19258v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19258v4",
                "updated": "2025-10-23T00:47:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    0,
                    47,
                    24,
                    3,
                    296,
                    0
                ],
                "published": "2024-10-25T02:22:00Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    2,
                    22,
                    0,
                    4,
                    299,
                    0
                ],
                "title": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning"
                },
                "summary": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark. Codes are\navailable at https://github.com/FYYFU/HeadKV",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark. Codes are\navailable at https://github.com/FYYFU/HeadKV"
                },
                "authors": [
                    {
                        "name": "Yu Fu"
                    },
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Abedelkadir Asi"
                    },
                    {
                        "name": "Wayne Xiong"
                    },
                    {
                        "name": "Yue Dong"
                    },
                    {
                        "name": "Wen Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Xiao"
                },
                "author": "Wen Xiao",
                "arxiv_comment": "Accepted to ICLR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19258v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19258v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00744v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00744v2",
                "updated": "2025-10-23T00:40:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    0,
                    40,
                    38,
                    3,
                    296,
                    0
                ],
                "published": "2025-05-31T23:16:53Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    23,
                    16,
                    53,
                    5,
                    151,
                    0
                ],
                "title": "Blending Complementary Memory Systems in Hybrid Quadratic-Linear\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Blending Complementary Memory Systems in Hybrid Quadratic-Linear\n  Transformers"
                },
                "summary": "We develop hybrid memory architectures for general-purpose sequence\nprocessing neural networks, that combine key-value memory using softmax\nattention (KV-memory) with fast weight memory through dynamic synaptic\nmodulation (FW-memory) -- the core principles of quadratic and linear\ntransformers, respectively. These two memory systems have complementary but\nindividually limited properties: KV-memory offers precise retrieval but is\nconstrained by quadratic complexity in sequence length, while FW-memory\nsupports arbitrarily long sequences and enables more expressive computation but\nsacrifices precise recall. We propose and compare three methods to blend these\ntwo systems into a single memory system, differing in how and when input\ninformation is delivered to each system, to leverage the strengths of both. We\nconduct experiments on general language modeling and retrieval tasks by\ntraining 340M- and 1.3B-parameter models from scratch, as well as on synthetic\nalgorithmic tasks designed to precisely illustrate the benefits of certain\nhybrid methods over others. We also evaluate our hybrid memory systems on\nreinforcement learning in partially observable environments. Overall, we\ndemonstrate how a well-designed hybrid can overcome the limitations of its\nindividual components, offering new insights into the design principle of\nneural memory systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop hybrid memory architectures for general-purpose sequence\nprocessing neural networks, that combine key-value memory using softmax\nattention (KV-memory) with fast weight memory through dynamic synaptic\nmodulation (FW-memory) -- the core principles of quadratic and linear\ntransformers, respectively. These two memory systems have complementary but\nindividually limited properties: KV-memory offers precise retrieval but is\nconstrained by quadratic complexity in sequence length, while FW-memory\nsupports arbitrarily long sequences and enables more expressive computation but\nsacrifices precise recall. We propose and compare three methods to blend these\ntwo systems into a single memory system, differing in how and when input\ninformation is delivered to each system, to leverage the strengths of both. We\nconduct experiments on general language modeling and retrieval tasks by\ntraining 340M- and 1.3B-parameter models from scratch, as well as on synthetic\nalgorithmic tasks designed to precisely illustrate the benefits of certain\nhybrid methods over others. We also evaluate our hybrid memory systems on\nreinforcement learning in partially observable environments. Overall, we\ndemonstrate how a well-designed hybrid can overcome the limitations of its\nindividual components, offering new insights into the design principle of\nneural memory systems."
                },
                "authors": [
                    {
                        "name": "Kazuki Irie"
                    },
                    {
                        "name": "Morris Yau"
                    },
                    {
                        "name": "Samuel J. Gershman"
                    }
                ],
                "author_detail": {
                    "name": "Samuel J. Gershman"
                },
                "author": "Samuel J. Gershman",
                "arxiv_comment": "Accepted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00744v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00744v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03712v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03712v2",
                "updated": "2025-10-22T23:56:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    23,
                    56,
                    45,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-04T07:22:39Z",
                "published_parsed": [
                    2025,
                    10,
                    4,
                    7,
                    22,
                    39,
                    5,
                    277,
                    0
                ],
                "title": "Detecting and Preventing Latent Risk Accumulation in High-Performance\n  Software Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting and Preventing Latent Risk Accumulation in High-Performance\n  Software Systems"
                },
                "summary": "Modern distributed systems employ aggressive optimization strategies that\ncreate latent risks - hidden vulnerabilities where exceptional performance\nmasks catastrophic fragility when optimizations fail. Cache layers achieving\n99% hit rates can obscure database bottlenecks until cache failures trigger\n100x load amplification and cascading collapse. Current reliability engineering\nfocuses on reactive incident response rather than proactive detection of\noptimization-induced vulnerabilities. This paper presents the first\ncomprehensive framework for systematic latent risk detection, prevention, and\noptimization through integrated mathematical modeling, intelligent perturbation\ntesting, and risk-aware performance optimization. We introduce the Latent Risk\nIndex (LRI) that correlates strongly with incident severity (r=0.863, p<0.001),\nenabling predictive risk assessment. Our framework integrates three systems:\nHYDRA employing six optimization-aware perturbation strategies achieving 89.7%\nrisk discovery rates, RAVEN providing continuous production monitoring with\n92.9% precision and 93.8% recall across 1,748 scenarios, and APEX enabling\nrisk-aware optimization maintaining 96.6% baseline performance while reducing\nlatent risks by 59.2%. Evaluation across three testbed environments\ndemonstrates strong statistical validation with large effect sizes (Cohen\nd>2.0) and exceptional reproducibility (r>0.92). Production deployment over 24\nweeks shows 69.1% mean time to recovery reduction, 78.6% incident severity\nreduction, and 81 prevented incidents generating 1.44M USD average annual\nbenefits with 3.2-month ROI. Our approach transforms reliability engineering\nfrom reactive incident management to proactive risk-aware optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern distributed systems employ aggressive optimization strategies that\ncreate latent risks - hidden vulnerabilities where exceptional performance\nmasks catastrophic fragility when optimizations fail. Cache layers achieving\n99% hit rates can obscure database bottlenecks until cache failures trigger\n100x load amplification and cascading collapse. Current reliability engineering\nfocuses on reactive incident response rather than proactive detection of\noptimization-induced vulnerabilities. This paper presents the first\ncomprehensive framework for systematic latent risk detection, prevention, and\noptimization through integrated mathematical modeling, intelligent perturbation\ntesting, and risk-aware performance optimization. We introduce the Latent Risk\nIndex (LRI) that correlates strongly with incident severity (r=0.863, p<0.001),\nenabling predictive risk assessment. Our framework integrates three systems:\nHYDRA employing six optimization-aware perturbation strategies achieving 89.7%\nrisk discovery rates, RAVEN providing continuous production monitoring with\n92.9% precision and 93.8% recall across 1,748 scenarios, and APEX enabling\nrisk-aware optimization maintaining 96.6% baseline performance while reducing\nlatent risks by 59.2%. Evaluation across three testbed environments\ndemonstrates strong statistical validation with large effect sizes (Cohen\nd>2.0) and exceptional reproducibility (r>0.92). Production deployment over 24\nweeks shows 69.1% mean time to recovery reduction, 78.6% incident severity\nreduction, and 81 prevented incidents generating 1.44M USD average annual\nbenefits with 3.2-month ROI. Our approach transforms reliability engineering\nfrom reactive incident management to proactive risk-aware optimization."
                },
                "authors": [
                    {
                        "name": "Jahidul Arafat"
                    },
                    {
                        "name": "Kh. M. Moniruzzaman"
                    },
                    {
                        "name": "Shamim Hossain"
                    },
                    {
                        "name": "Fariha Tasmin"
                    }
                ],
                "author_detail": {
                    "name": "Fariha Tasmin"
                },
                "author": "Fariha Tasmin",
                "arxiv_comment": "26 pages, 12 tables, 4 figures. Academic-industry collaboration.\n  Framework (HYDRA, RAVEN, APEX) for optimization-induced vulnerabilities.\n  Evaluated: 2,160 configs, 12.7TB data, 1,748 scenarios",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03712v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03712v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M15, 90B25, 68T05, 90C29",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.4; C.2.4; D.2.5; D.4.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19670v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19670v1",
                "updated": "2025-10-22T15:16:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    16,
                    56,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T15:16:56Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    16,
                    56,
                    2,
                    295,
                    0
                ],
                "title": "CoSense-LLM: Semantics at the Edge with Cost- and Uncertainty-Aware\n  Cloud-Edge Cooperation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoSense-LLM: Semantics at the Edge with Cost- and Uncertainty-Aware\n  Cloud-Edge Cooperation"
                },
                "summary": "We present CoSense-LLM, an edge-first framework that turns continuous\nmultimodal sensor streams (for example Wi-Fi CSI, IMU, audio, RFID, and\nlightweight vision) into compact, verifiable semantic tokens and coordinates\nwith large language models under explicit latency, energy, bandwidth, and\nprivacy constraints. CoSense-LLM has four parts: (i) SenseFusion, a lightweight\nencoder that aligns sensor embeddings with language and compresses them into\nshort discrete code sequences; (ii) Edge-RAG, a local hybrid retrieval layer\nthat grounds generation in site specific policies and notes; (iii)\nPromptRouter, a cost and uncertainty aware policy that selects edge only\ngeneration, edge plus retrieval, or compact cloud escalation; and (iv) Secure\nExecution, an auditable redaction path that enforces data minimization so raw\nwaveforms never leave the device. The system works with modern serving\noptimizations, including paged or streaming KV caches, FlashAttention style\nkernels, speculative decoding, and quantized LoRA adapters, and supports on\ndevice personalization and federated updates under non IID drift. Across home,\noffice, and clinic deployments, CoSense-LLM delivers grounded explanations\nwhile meeting tight service level objectives: it sustains sub second (p95) end\nto end latency on edge dominant paths, reduces inter tier token and bandwidth\ncosts by preferring local retrieval grounded responses, and preserves privacy\nby transmitting only discrete codes and redacted metadata. Ablations show that\nEdge-RAG improves factual consistency and reduces contradictions, calibrated\nuncertainty enables selective abstention and controlled escalations, and KV\nplus decoding accelerators lower energy per decision. The results support an\nedge first design that treats semantics, privacy, and predictable latency as co\nequal goals for large model deployments in interference prone environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present CoSense-LLM, an edge-first framework that turns continuous\nmultimodal sensor streams (for example Wi-Fi CSI, IMU, audio, RFID, and\nlightweight vision) into compact, verifiable semantic tokens and coordinates\nwith large language models under explicit latency, energy, bandwidth, and\nprivacy constraints. CoSense-LLM has four parts: (i) SenseFusion, a lightweight\nencoder that aligns sensor embeddings with language and compresses them into\nshort discrete code sequences; (ii) Edge-RAG, a local hybrid retrieval layer\nthat grounds generation in site specific policies and notes; (iii)\nPromptRouter, a cost and uncertainty aware policy that selects edge only\ngeneration, edge plus retrieval, or compact cloud escalation; and (iv) Secure\nExecution, an auditable redaction path that enforces data minimization so raw\nwaveforms never leave the device. The system works with modern serving\noptimizations, including paged or streaming KV caches, FlashAttention style\nkernels, speculative decoding, and quantized LoRA adapters, and supports on\ndevice personalization and federated updates under non IID drift. Across home,\noffice, and clinic deployments, CoSense-LLM delivers grounded explanations\nwhile meeting tight service level objectives: it sustains sub second (p95) end\nto end latency on edge dominant paths, reduces inter tier token and bandwidth\ncosts by preferring local retrieval grounded responses, and preserves privacy\nby transmitting only discrete codes and redacted metadata. Ablations show that\nEdge-RAG improves factual consistency and reduces contradictions, calibrated\nuncertainty enables selective abstention and controlled escalations, and KV\nplus decoding accelerators lower energy per decision. The results support an\nedge first design that treats semantics, privacy, and predictable latency as co\nequal goals for large model deployments in interference prone environments."
                },
                "authors": [
                    {
                        "name": "Hasan Akgul"
                    },
                    {
                        "name": "Mari Eplik"
                    },
                    {
                        "name": "Javier Rojas"
                    },
                    {
                        "name": "Aina Binti Abdullah"
                    },
                    {
                        "name": "Pieter van der Merwe"
                    }
                ],
                "author_detail": {
                    "name": "Pieter van der Merwe"
                },
                "author": "Pieter van der Merwe",
                "arxiv_comment": "19 pages,8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19670v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19670v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; C.2.4; C.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08666v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08666v3",
                "updated": "2025-10-22T14:33:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    14,
                    33,
                    49,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-09T16:19:42Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    16,
                    19,
                    42,
                    3,
                    282,
                    0
                ],
                "title": "dInfer: An Efficient Inference Framework for Diffusion Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "dInfer: An Efficient Inference Framework for Diffusion Language Models"
                },
                "summary": "Diffusion-based large language models (dLLMs) have emerged as a promising\nalternative to autoregressive (AR) LLMs, leveraging denoising-based generation\nto enable inherent parallelism. Even more and more open-sourced dLLM models\nemerge, yet their widespread adoption remains constrained by the lack of a\nstandardized and efficient inference framework. We present dInfer, an efficient\nand extensible framework for dLLM inference. dInfer decomposes the inference\npipeline into four modular components--model, diffusion iteration manager,\ndecoding strategy, and KV-cache manager--and integrates novel algorithms for\neach component alongside system-level optimizations. Through this combination\nof algorithmic innovations and system enhancements, dInfer achieves substantial\nefficiency gains without compromising output quality on LLaDA-MoE. At batch\nsize 1, it surpasses 1,100 tokens per second on HumanEval and averages over 800\ntokens per second across six benchmarks on $8\\times$ H800 GPUs. Compared to\nprior systems, dInfer delivers a $10\\times$ speedup over Fast-dLLM while\nmaintaining similar model performance. Even compared to the AR model (with a\ncomparable number of activation parameters and performance) QWen2.5-3B, which\nis highly optimized with the latest vLLM inference engine, dInfer still\ndelivers a $2$-$3\\times$ speedup. The implementation of dInfer is open-sourced\nat https://github.com/inclusionAI/dInfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based large language models (dLLMs) have emerged as a promising\nalternative to autoregressive (AR) LLMs, leveraging denoising-based generation\nto enable inherent parallelism. Even more and more open-sourced dLLM models\nemerge, yet their widespread adoption remains constrained by the lack of a\nstandardized and efficient inference framework. We present dInfer, an efficient\nand extensible framework for dLLM inference. dInfer decomposes the inference\npipeline into four modular components--model, diffusion iteration manager,\ndecoding strategy, and KV-cache manager--and integrates novel algorithms for\neach component alongside system-level optimizations. Through this combination\nof algorithmic innovations and system enhancements, dInfer achieves substantial\nefficiency gains without compromising output quality on LLaDA-MoE. At batch\nsize 1, it surpasses 1,100 tokens per second on HumanEval and averages over 800\ntokens per second across six benchmarks on $8\\times$ H800 GPUs. Compared to\nprior systems, dInfer delivers a $10\\times$ speedup over Fast-dLLM while\nmaintaining similar model performance. Even compared to the AR model (with a\ncomparable number of activation parameters and performance) QWen2.5-3B, which\nis highly optimized with the latest vLLM inference engine, dInfer still\ndelivers a $2$-$3\\times$ speedup. The implementation of dInfer is open-sourced\nat https://github.com/inclusionAI/dInfer."
                },
                "authors": [
                    {
                        "name": "Yuxin Ma"
                    },
                    {
                        "name": "Lun Du"
                    },
                    {
                        "name": "Lanning Wei"
                    },
                    {
                        "name": "Kun Chen"
                    },
                    {
                        "name": "Qian Xu"
                    },
                    {
                        "name": "Kangyu Wang"
                    },
                    {
                        "name": "Guofeng Feng"
                    },
                    {
                        "name": "Guoshan Lu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Xiaojing Qi"
                    },
                    {
                        "name": "Xinyuan Zhang"
                    },
                    {
                        "name": "Zhen Tao"
                    },
                    {
                        "name": "Haibo Feng"
                    },
                    {
                        "name": "Ziyun Jiang"
                    },
                    {
                        "name": "Ying Xu"
                    },
                    {
                        "name": "Zenan Huang"
                    },
                    {
                        "name": "Yihong Zhuang"
                    },
                    {
                        "name": "Haokai Xu"
                    },
                    {
                        "name": "Jiaqi Hu"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    },
                    {
                        "name": "Junbo Zhao"
                    },
                    {
                        "name": "Jianguo Li"
                    },
                    {
                        "name": "Da Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Da Zheng"
                },
                "author": "Da Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08666v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08666v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19875v1",
                "updated": "2025-10-22T09:42:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    9,
                    42,
                    29,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T09:42:29Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    9,
                    42,
                    29,
                    2,
                    295,
                    0
                ],
                "title": "Stream: Scaling up Mechanistic Interpretability to Long Context in LLMs\n  via Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stream: Scaling up Mechanistic Interpretability to Long Context in LLMs\n  via Sparse Attention"
                },
                "summary": "As Large Language Models (LLMs) scale to million-token contexts, traditional\nMechanistic Interpretability techniques for analyzing attention scale\nquadratically with context length, demanding terabytes of memory beyond 100,000\ntokens. We introduce Sparse Tracing, a novel technique that leverages dynamic\nsparse attention to efficiently analyze long context attention patterns. We\npresent Stream, a compilable hierarchical pruning algorithm that estimates\nper-head sparse attention masks in near-linear time $O(T \\log T)$ and linear\nspace $O(T)$, enabling one-pass interpretability at scale. Stream performs a\nbinary-search-style refinement to retain only the top-$k$ key blocks per query\nwhile preserving the model's next-token behavior. We apply Stream to long\nchain-of-thought reasoning traces and identify thought anchors while pruning\n97-99\\% of token interactions. On the RULER benchmark, Stream preserves\ncritical retrieval paths while discarding 90-96\\% of interactions and exposes\nlayer-wise routes from the needle to output. Our method offers a practical\ndrop-in tool for analyzing attention patterns and tracing information flow\nwithout terabytes of caches. By making long context interpretability feasible\non consumer GPUs, Sparse Tracing helps democratize chain-of-thought monitoring.\nCode is available at https://anonymous.4open.science/r/stream-03B8/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) scale to million-token contexts, traditional\nMechanistic Interpretability techniques for analyzing attention scale\nquadratically with context length, demanding terabytes of memory beyond 100,000\ntokens. We introduce Sparse Tracing, a novel technique that leverages dynamic\nsparse attention to efficiently analyze long context attention patterns. We\npresent Stream, a compilable hierarchical pruning algorithm that estimates\nper-head sparse attention masks in near-linear time $O(T \\log T)$ and linear\nspace $O(T)$, enabling one-pass interpretability at scale. Stream performs a\nbinary-search-style refinement to retain only the top-$k$ key blocks per query\nwhile preserving the model's next-token behavior. We apply Stream to long\nchain-of-thought reasoning traces and identify thought anchors while pruning\n97-99\\% of token interactions. On the RULER benchmark, Stream preserves\ncritical retrieval paths while discarding 90-96\\% of interactions and exposes\nlayer-wise routes from the needle to output. Our method offers a practical\ndrop-in tool for analyzing attention patterns and tracing information flow\nwithout terabytes of caches. By making long context interpretability feasible\non consumer GPUs, Sparse Tracing helps democratize chain-of-thought monitoring.\nCode is available at https://anonymous.4open.science/r/stream-03B8/."
                },
                "authors": [
                    {
                        "name": "J Rosser"
                    },
                    {
                        "name": "José Luis Redondo García"
                    },
                    {
                        "name": "Gustavo Penha"
                    },
                    {
                        "name": "Konstantina Palla"
                    },
                    {
                        "name": "Hugues Bouchard"
                    }
                ],
                "author_detail": {
                    "name": "Hugues Bouchard"
                },
                "author": "Hugues Bouchard",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T40",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2511.02828v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02828v1",
                "updated": "2025-11-04T18:56:27Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    18,
                    56,
                    27,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T18:56:27Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    18,
                    56,
                    27,
                    1,
                    308,
                    0
                ],
                "title": "Searching Within Galaxies for the Earliest Signs of Quenching With\n  Spatially Resolved Star Formation Histories in UVCANDELS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Searching Within Galaxies for the Earliest Signs of Quenching With\n  Spatially Resolved Star Formation Histories in UVCANDELS"
                },
                "summary": "Understanding the complicated processes that regulate star formation and\ncause a galaxy to become quiescent is key to our comprehension of galaxy\nevolution. We used nine well resolved star-forming z<1 galaxies from the\nUVCANDELS survey, where a total of 10 HST bands including UV follow up in\nUVIS/F275W allow us to reconstruct the star formation histories (SFHs) of\nregions across each galaxy. This approach provides a powerful tool to explore\nthe spatio-temporal connection between star formation and galaxy evolution. The\nspatial and temporal profiles of stellar mass and star formation rate surface\ndensity were obtained from the SFHs of these regions. We measure scaling\nrelations and projected radial profiles of regions within each galaxy at the\ntime of observation and at 1 Gyr lookback time, noting possible trends in the\nevolution. By comparing the change in star formation over time we can infer the\ntiming and location of star formation and see early signs of star formation\nshut off before quenching occurs. We compared the star formation rate density\n-- stellar mass density scaling relations for individual galaxies as they\nevolve from 1 Gyr lookback time. The correlation lines pivot around a\nlog-stellar mass surface density of 7.25 [$M_\\odot$ $kpc^{-2}$] may be evidence\nof a self-regulating process on these scales. Radial profiles of galaxy Log\nsSFR show an overall decrease over 1 Gyr, but five galaxies show a greater\nchange in Log sSFR at the outskirts than the center indicating a possible early\nonset of quenching in these galaxies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the complicated processes that regulate star formation and\ncause a galaxy to become quiescent is key to our comprehension of galaxy\nevolution. We used nine well resolved star-forming z<1 galaxies from the\nUVCANDELS survey, where a total of 10 HST bands including UV follow up in\nUVIS/F275W allow us to reconstruct the star formation histories (SFHs) of\nregions across each galaxy. This approach provides a powerful tool to explore\nthe spatio-temporal connection between star formation and galaxy evolution. The\nspatial and temporal profiles of stellar mass and star formation rate surface\ndensity were obtained from the SFHs of these regions. We measure scaling\nrelations and projected radial profiles of regions within each galaxy at the\ntime of observation and at 1 Gyr lookback time, noting possible trends in the\nevolution. By comparing the change in star formation over time we can infer the\ntiming and location of star formation and see early signs of star formation\nshut off before quenching occurs. We compared the star formation rate density\n-- stellar mass density scaling relations for individual galaxies as they\nevolve from 1 Gyr lookback time. The correlation lines pivot around a\nlog-stellar mass surface density of 7.25 [$M_\\odot$ $kpc^{-2}$] may be evidence\nof a self-regulating process on these scales. Radial profiles of galaxy Log\nsSFR show an overall decrease over 1 Gyr, but five galaxies show a greater\nchange in Log sSFR at the outskirts than the center indicating a possible early\nonset of quenching in these galaxies."
                },
                "authors": [
                    {
                        "name": "Charlotte Olsen"
                    },
                    {
                        "name": "Eric Gawiser"
                    },
                    {
                        "name": "Charlotte Welker"
                    },
                    {
                        "name": "Harry Teplitz"
                    },
                    {
                        "name": "Kartheik Iyer"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Marc Rafelski"
                    },
                    {
                        "name": "Rogier A. Windhorst"
                    },
                    {
                        "name": "Anton Koekemoer"
                    },
                    {
                        "name": "Anahita Alavi"
                    },
                    {
                        "name": "Ben Sunnquist"
                    },
                    {
                        "name": "Norman Grogin"
                    },
                    {
                        "name": "Yicheng Guo"
                    },
                    {
                        "name": "Christopher J. Conselice"
                    },
                    {
                        "name": "L. Y. Aaron Yung"
                    },
                    {
                        "name": "Kalina Nedkova"
                    },
                    {
                        "name": "Bahram Mobasher"
                    },
                    {
                        "name": "Ray A. Lucas"
                    },
                    {
                        "name": "Vihang Mehta"
                    },
                    {
                        "name": "Y. Sophia Dai"
                    },
                    {
                        "name": "Jonathan P. Gardner"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan P. Gardner"
                },
                "author": "Jonathan P. Gardner",
                "arxiv_comment": "31 pages, 28 figures. Submitted to ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02828v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02828v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10124v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10124v2",
                "updated": "2025-11-04T18:50:46Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    18,
                    50,
                    46,
                    1,
                    308,
                    0
                ],
                "published": "2025-01-17T11:27:58Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    11,
                    27,
                    58,
                    4,
                    17,
                    0
                ],
                "title": "Gene Regulatory Network Inference in the Presence of Selection Bias and\n  Latent Confounders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gene Regulatory Network Inference in the Presence of Selection Bias and\n  Latent Confounders"
                },
                "summary": "Gene regulatory network inference (GRNI) aims to discover how genes causally\nregulate each other from gene expression data. It is well-known that\nstatistical dependencies in observed data do not necessarily imply causation,\nas spurious dependencies may arise from latent confounders, such as non-coding\nRNAs. Numerous GRNI methods have thus been proposed to address this confounding\nissue. However, dependencies may also result from selection--only cells\nsatisfying certain survival or inclusion criteria are observed--while these\nselection-induced spurious dependencies are frequently overlooked in gene\nexpression data analyses. In this work, we show that such selection is\nubiquitous and, when ignored or conflated with true regulations, can lead to\nflawed causal interpretation and misguided intervention recommendations. To\naddress this challenge, a fundamental question arises: can we distinguish\ndependencies due to regulation, confounding, and crucially, selection? We show\nthat gene perturbations offer a simple yet effective answer: selection-induced\ndependencies are symmetric under perturbation, while those from regulation or\nconfounding are not. Building on this motivation, we propose GISL (Gene\nregulatory network Inference in the presence of Selection bias and Latent\nconfounders), a principled algorithm that leverages perturbation data to\nuncover both true gene regulatory relations and non-regulatory mechanisms of\nselection and confounding up to the equivalence class. Experiments on synthetic\nand real-world gene expression data demonstrate the effectiveness of our\nmethod.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gene regulatory network inference (GRNI) aims to discover how genes causally\nregulate each other from gene expression data. It is well-known that\nstatistical dependencies in observed data do not necessarily imply causation,\nas spurious dependencies may arise from latent confounders, such as non-coding\nRNAs. Numerous GRNI methods have thus been proposed to address this confounding\nissue. However, dependencies may also result from selection--only cells\nsatisfying certain survival or inclusion criteria are observed--while these\nselection-induced spurious dependencies are frequently overlooked in gene\nexpression data analyses. In this work, we show that such selection is\nubiquitous and, when ignored or conflated with true regulations, can lead to\nflawed causal interpretation and misguided intervention recommendations. To\naddress this challenge, a fundamental question arises: can we distinguish\ndependencies due to regulation, confounding, and crucially, selection? We show\nthat gene perturbations offer a simple yet effective answer: selection-induced\ndependencies are symmetric under perturbation, while those from regulation or\nconfounding are not. Building on this motivation, we propose GISL (Gene\nregulatory network Inference in the presence of Selection bias and Latent\nconfounders), a principled algorithm that leverages perturbation data to\nuncover both true gene regulatory relations and non-regulatory mechanisms of\nselection and confounding up to the equivalence class. Experiments on synthetic\nand real-world gene expression data demonstrate the effectiveness of our\nmethod."
                },
                "authors": [
                    {
                        "name": "Gongxu Luo"
                    },
                    {
                        "name": "Haoyue Dai"
                    },
                    {
                        "name": "Loka Li"
                    },
                    {
                        "name": "Chengqian Gao"
                    },
                    {
                        "name": "Boyang Sun"
                    },
                    {
                        "name": "Kun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Kun Zhang"
                },
                "author": "Kun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10124v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10124v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09586v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09586v3",
                "updated": "2025-11-04T18:44:17Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    18,
                    44,
                    17,
                    1,
                    308,
                    0
                ],
                "published": "2024-09-15T02:13:03Z",
                "published_parsed": [
                    2024,
                    9,
                    15,
                    2,
                    13,
                    3,
                    6,
                    259,
                    0
                ],
                "title": "ValueCompass: A Framework for Measuring Contextual Value Alignment\n  Between Human and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ValueCompass: A Framework for Measuring Contextual Value Alignment\n  Between Human and LLMs"
                },
                "summary": "As AI systems become more advanced, ensuring their alignment with a diverse\nrange of individuals and societal values becomes increasingly critical. But how\ncan we capture fundamental human values and assess the degree to which AI\nsystems align with them? We introduce ValueCompass, a framework of fundamental\nvalues, grounded in psychological theory and a systematic review, to identify\nand evaluate human-AI alignment. We apply ValueCompass to measure the value\nalignment of humans and large language models (LLMs) across four real-world\nscenarios: collaborative writing, education, public sectors, and healthcare.\nOur findings reveal concerning misalignments between humans and LLMs, such as\nhumans frequently endorse values like \"National Security\" which were largely\nrejected by LLMs. We also observe that values differ across scenarios,\nhighlighting the need for context-aware AI alignment strategies. This work\nprovides valuable insights into the design space of human-AI alignment, laying\nthe foundations for developing AI systems that responsibly reflect societal\nvalues and ethics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI systems become more advanced, ensuring their alignment with a diverse\nrange of individuals and societal values becomes increasingly critical. But how\ncan we capture fundamental human values and assess the degree to which AI\nsystems align with them? We introduce ValueCompass, a framework of fundamental\nvalues, grounded in psychological theory and a systematic review, to identify\nand evaluate human-AI alignment. We apply ValueCompass to measure the value\nalignment of humans and large language models (LLMs) across four real-world\nscenarios: collaborative writing, education, public sectors, and healthcare.\nOur findings reveal concerning misalignments between humans and LLMs, such as\nhumans frequently endorse values like \"National Security\" which were largely\nrejected by LLMs. We also observe that values differ across scenarios,\nhighlighting the need for context-aware AI alignment strategies. This work\nprovides valuable insights into the design space of human-AI alignment, laying\nthe foundations for developing AI systems that responsibly reflect societal\nvalues and ethics."
                },
                "authors": [
                    {
                        "name": "Hua Shen"
                    },
                    {
                        "name": "Tiffany Knearem"
                    },
                    {
                        "name": "Reshmi Ghosh"
                    },
                    {
                        "name": "Yu-Ju Yang"
                    },
                    {
                        "name": "Nicholas Clark"
                    },
                    {
                        "name": "Tanushree Mitra"
                    },
                    {
                        "name": "Yun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Yun Huang"
                },
                "author": "Yun Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09586v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09586v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00032v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00032v2",
                "updated": "2025-11-04T18:36:26Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    18,
                    36,
                    26,
                    1,
                    308,
                    0
                ],
                "published": "2025-07-30T08:49:13Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    8,
                    49,
                    13,
                    2,
                    211,
                    0
                ],
                "title": "Strategic Communication and Language Bias in Multi-Agent LLM\n  Coordination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strategic Communication and Language Bias in Multi-Agent LLM\n  Coordination"
                },
                "summary": "Large Language Model (LLM)-based agents are increasingly deployed in\nmulti-agent scenarios where coordination is crucial but not always assured.\nResearch shows that the way strategic scenarios are framed linguistically can\naffect cooperation. This paper explores whether allowing agents to communicate\namplifies these language-driven effects. Leveraging FAIRGAME, we simulate\none-shot and repeated games across different languages and models, both with\nand without communication. Our experiments, conducted with two advanced\nLLMs-GPT-4o and Llama 4 Maverick-reveal that communication significantly\ninfluences agent behavior, though its impact varies by language, personality,\nand game structure. These findings underscore the dual role of communication in\nfostering coordination and reinforcing biases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based agents are increasingly deployed in\nmulti-agent scenarios where coordination is crucial but not always assured.\nResearch shows that the way strategic scenarios are framed linguistically can\naffect cooperation. This paper explores whether allowing agents to communicate\namplifies these language-driven effects. Leveraging FAIRGAME, we simulate\none-shot and repeated games across different languages and models, both with\nand without communication. Our experiments, conducted with two advanced\nLLMs-GPT-4o and Llama 4 Maverick-reveal that communication significantly\ninfluences agent behavior, though its impact varies by language, personality,\nand game structure. These findings underscore the dual role of communication in\nfostering coordination and reinforcing biases."
                },
                "authors": [
                    {
                        "name": "Alessio Buscemi"
                    },
                    {
                        "name": "Daniele Proverbio"
                    },
                    {
                        "name": "Alessandro Di Stefano"
                    },
                    {
                        "name": "The Anh Han"
                    },
                    {
                        "name": "German Castignani"
                    },
                    {
                        "name": "Pietro Liò"
                    }
                ],
                "author_detail": {
                    "name": "Pietro Liò"
                },
                "author": "Pietro Liò",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00032v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00032v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02808v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02808v1",
                "updated": "2025-11-04T18:30:09Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    18,
                    30,
                    9,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T18:30:09Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    18,
                    30,
                    9,
                    1,
                    308,
                    0
                ],
                "title": "Reliable Parameter Inference for the Epoch of Reionization using\n  Balanced Neural Ratio Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable Parameter Inference for the Epoch of Reionization using\n  Balanced Neural Ratio Estimation"
                },
                "summary": "We present an application of the Balanced Neural Ratio Estimation (BNRE)\nalgorithm to improve the statistical validity of parameter estimates used to\ncharacterize the Epoch of Reionization, where the common assumption of a\nmultivariate Gaussian likelihood leads to overconfident and biased posterior\ndistributions. Using a two-parameter model of the Ly$\\alpha$ forest\nautocorrelation function, we show that BNRE yields posterior distributions that\nare significantly better calibrated than those obtained under the Gaussian\nlikelihood assumption, as verified through the Test of Accuracy with Random\nPoints (TARP) and Simulation-Based Calibration (SBC) diagnostics. These results\ndemonstrate the potential of Simulation-Based Inference (SBI) methods, and in\nparticular BNRE, to provide statistically robust parameter constraints within\nexisting astrophysical modeling frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an application of the Balanced Neural Ratio Estimation (BNRE)\nalgorithm to improve the statistical validity of parameter estimates used to\ncharacterize the Epoch of Reionization, where the common assumption of a\nmultivariate Gaussian likelihood leads to overconfident and biased posterior\ndistributions. Using a two-parameter model of the Ly$\\alpha$ forest\nautocorrelation function, we show that BNRE yields posterior distributions that\nare significantly better calibrated than those obtained under the Gaussian\nlikelihood assumption, as verified through the Test of Accuracy with Random\nPoints (TARP) and Simulation-Based Calibration (SBC) diagnostics. These results\ndemonstrate the potential of Simulation-Based Inference (SBI) methods, and in\nparticular BNRE, to provide statistically robust parameter constraints within\nexisting astrophysical modeling frameworks."
                },
                "authors": [
                    {
                        "name": "Diego González-Hernández"
                    },
                    {
                        "name": "Molly Wolfson"
                    },
                    {
                        "name": "Joseph F. Hennawi"
                    }
                ],
                "author_detail": {
                    "name": "Joseph F. Hennawi"
                },
                "author": "Joseph F. Hennawi",
                "arxiv_comment": "Machine Learning and the Physical Sciences Workshop, NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02808v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02808v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02805v1",
                "updated": "2025-11-04T18:27:39Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    18,
                    27,
                    39,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T18:27:39Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    18,
                    27,
                    39,
                    1,
                    308,
                    0
                ],
                "title": "MemSearcher: Training LLMs to Reason, Search and Manage Memory via\n  End-to-End Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemSearcher: Training LLMs to Reason, Search and Manage Memory via\n  End-to-End Reinforcement Learning"
                },
                "summary": "Typical search agents concatenate the entire interaction history into the LLM\ncontext, preserving information integrity but producing long, noisy contexts,\nresulting in high computation and memory costs. In contrast, using only the\ncurrent turn avoids this overhead but discards essential information. This\ntrade-off limits the scalability of search agents. To address this challenge,\nwe propose MemSearcher, an agent workflow that iteratively maintains a compact\nmemory and combines the current turn with it. At each turn, MemSearcher fuses\nthe user's question with the memory to generate reasoning traces, perform\nsearch actions, and update memory to retain only information essential for\nsolving the task. This design stabilizes context length across multi-turn\ninteractions, improving efficiency without sacrificing accuracy. To optimize\nthis workflow, we introduce multi-context GRPO, an end-to-end RL framework that\njointly optimize reasoning, search strategies, and memory management of\nMemSearcher Agents. Specifically, multi-context GRPO samples groups of\ntrajectories under different contexts and propagates trajectory-level\nadvantages across all conversations within them. Trained on the same dataset as\nSearch-R1, MemSearcher achieves significant improvements over strong baselines\non seven public benchmarks: +11% on Qwen2.5-3B-Instruct and +12% on\nQwen2.5-7B-Instruct relative average gains. Notably, the 3B-based MemSearcher\neven outperforms 7B-based baselines, demonstrating that striking a balance\nbetween information integrity and efficiency yields both higher accuracy and\nlower computational overhead. The code and models will be publicly available at\nhttps://github.com/icip-cas/MemSearcher",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Typical search agents concatenate the entire interaction history into the LLM\ncontext, preserving information integrity but producing long, noisy contexts,\nresulting in high computation and memory costs. In contrast, using only the\ncurrent turn avoids this overhead but discards essential information. This\ntrade-off limits the scalability of search agents. To address this challenge,\nwe propose MemSearcher, an agent workflow that iteratively maintains a compact\nmemory and combines the current turn with it. At each turn, MemSearcher fuses\nthe user's question with the memory to generate reasoning traces, perform\nsearch actions, and update memory to retain only information essential for\nsolving the task. This design stabilizes context length across multi-turn\ninteractions, improving efficiency without sacrificing accuracy. To optimize\nthis workflow, we introduce multi-context GRPO, an end-to-end RL framework that\njointly optimize reasoning, search strategies, and memory management of\nMemSearcher Agents. Specifically, multi-context GRPO samples groups of\ntrajectories under different contexts and propagates trajectory-level\nadvantages across all conversations within them. Trained on the same dataset as\nSearch-R1, MemSearcher achieves significant improvements over strong baselines\non seven public benchmarks: +11% on Qwen2.5-3B-Instruct and +12% on\nQwen2.5-7B-Instruct relative average gains. Notably, the 3B-based MemSearcher\neven outperforms 7B-based baselines, demonstrating that striking a balance\nbetween information integrity and efficiency yields both higher accuracy and\nlower computational overhead. The code and models will be publicly available at\nhttps://github.com/icip-cas/MemSearcher"
                },
                "authors": [
                    {
                        "name": "Qianhao Yuan"
                    },
                    {
                        "name": "Jie Lou"
                    },
                    {
                        "name": "Zichao Li"
                    },
                    {
                        "name": "Jiawei Chen"
                    },
                    {
                        "name": "Yaojie Lu"
                    },
                    {
                        "name": "Hongyu Lin"
                    },
                    {
                        "name": "Le Sun"
                    },
                    {
                        "name": "Debing Zhang"
                    },
                    {
                        "name": "Xianpei Han"
                    }
                ],
                "author_detail": {
                    "name": "Xianpei Han"
                },
                "author": "Xianpei Han",
                "arxiv_comment": "Project page: https://github.com/icip-cas/MemSearcher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02802v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02802v1",
                "updated": "2025-11-04T18:25:17Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    18,
                    25,
                    17,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T18:25:17Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    18,
                    25,
                    17,
                    1,
                    308,
                    0
                ],
                "title": "TabTune: A Unified Library for Inference and Fine-Tuning Tabular\n  Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TabTune: A Unified Library for Inference and Fine-Tuning Tabular\n  Foundation Models"
                },
                "summary": "Tabular foundation models represent a growing paradigm in structured data\nlearning, extending the benefits of large-scale pretraining to tabular domains.\nHowever, their adoption remains limited due to heterogeneous preprocessing\npipelines, fragmented APIs, inconsistent fine-tuning procedures, and the\nabsence of standardized evaluation for deployment-oriented metrics such as\ncalibration and fairness. We present TabTune, a unified library that\nstandardizes the complete workflow for tabular foundation models through a\nsingle interface. TabTune provides consistent access to seven state-of-the-art\nmodels supporting multiple adaptation strategies, including zero-shot\ninference, meta-learning, supervised fine-tuning (SFT), and parameter-efficient\nfine-tuning (PEFT). The framework automates model-aware preprocessing, manages\narchitectural heterogeneity internally, and integrates evaluation modules for\nperformance, calibration, and fairness. Designed for extensibility and\nreproducibility, TabTune enables consistent benchmarking of adaptation\nstrategies of tabular foundation models. The library is open source and\navailable at https://github.com/Lexsi-Labs/TabTune .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tabular foundation models represent a growing paradigm in structured data\nlearning, extending the benefits of large-scale pretraining to tabular domains.\nHowever, their adoption remains limited due to heterogeneous preprocessing\npipelines, fragmented APIs, inconsistent fine-tuning procedures, and the\nabsence of standardized evaluation for deployment-oriented metrics such as\ncalibration and fairness. We present TabTune, a unified library that\nstandardizes the complete workflow for tabular foundation models through a\nsingle interface. TabTune provides consistent access to seven state-of-the-art\nmodels supporting multiple adaptation strategies, including zero-shot\ninference, meta-learning, supervised fine-tuning (SFT), and parameter-efficient\nfine-tuning (PEFT). The framework automates model-aware preprocessing, manages\narchitectural heterogeneity internally, and integrates evaluation modules for\nperformance, calibration, and fairness. Designed for extensibility and\nreproducibility, TabTune enables consistent benchmarking of adaptation\nstrategies of tabular foundation models. The library is open source and\navailable at https://github.com/Lexsi-Labs/TabTune ."
                },
                "authors": [
                    {
                        "name": "Aditya Tanna"
                    },
                    {
                        "name": "Pratinav Seth"
                    },
                    {
                        "name": "Mohamed Bouadi"
                    },
                    {
                        "name": "Utsav Avaiya"
                    },
                    {
                        "name": "Vinay Kumar Sankarapu"
                    }
                ],
                "author_detail": {
                    "name": "Vinay Kumar Sankarapu"
                },
                "author": "Vinay Kumar Sankarapu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02802v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02802v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02795v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02795v1",
                "updated": "2025-11-04T18:20:17Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    18,
                    20,
                    17,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T18:20:17Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    18,
                    20,
                    17,
                    1,
                    308,
                    0
                ],
                "title": "Can LLMs subtract numbers?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs subtract numbers?"
                },
                "summary": "We present a systematic study of subtraction in large language models (LLMs).\nWhile prior benchmarks emphasize addition and multiplication, subtraction has\nreceived comparatively little attention despite being structurally distinct as\na non-commutative operation. We evaluate eight pretrained LLMs spanning four\nfamilies on addition and subtraction problems. Our experiments reveal that\nsubtraction accuracy lags behind addition by a wide margin. We find that the\nerrors for ($a-b$) are concentrated in cases where ($a<b$). In such cases, LLMs\nfrequently produce the correct magnitude but omit the negative sign. Probing\nanalyses show that LLMs internally encode whether results should be negative,\nyet this information is often not reflected in generated outputs. We further\ntest well-known techniques such as few-shot learning and instruction-tuning to\nsee if they can improve the LLMs' performance. Our results suggest that while\nfew-shot prompting yields modest gains, the instruction-tuned models achieve\nnear-perfect accuracies in generating the negative sign. Together, these\nfindings provide a clearer characterization of the limitations and\nrecoverability of LLMs' arithmetic capabilities in subtraction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a systematic study of subtraction in large language models (LLMs).\nWhile prior benchmarks emphasize addition and multiplication, subtraction has\nreceived comparatively little attention despite being structurally distinct as\na non-commutative operation. We evaluate eight pretrained LLMs spanning four\nfamilies on addition and subtraction problems. Our experiments reveal that\nsubtraction accuracy lags behind addition by a wide margin. We find that the\nerrors for ($a-b$) are concentrated in cases where ($a<b$). In such cases, LLMs\nfrequently produce the correct magnitude but omit the negative sign. Probing\nanalyses show that LLMs internally encode whether results should be negative,\nyet this information is often not reflected in generated outputs. We further\ntest well-known techniques such as few-shot learning and instruction-tuning to\nsee if they can improve the LLMs' performance. Our results suggest that while\nfew-shot prompting yields modest gains, the instruction-tuned models achieve\nnear-perfect accuracies in generating the negative sign. Together, these\nfindings provide a clearer characterization of the limitations and\nrecoverability of LLMs' arithmetic capabilities in subtraction."
                },
                "authors": [
                    {
                        "name": "Mayank Jobanputra"
                    },
                    {
                        "name": "Nils Philipp Walter"
                    },
                    {
                        "name": "Maitrey Mehta"
                    },
                    {
                        "name": "Blerta Veseli"
                    },
                    {
                        "name": "Evan Parker Kelly Chapple"
                    },
                    {
                        "name": "Yifan Wang"
                    },
                    {
                        "name": "Sneha Chetani"
                    },
                    {
                        "name": "Ellie Pavlick"
                    },
                    {
                        "name": "Antonio Vergari"
                    },
                    {
                        "name": "Vera Demberg"
                    }
                ],
                "author_detail": {
                    "name": "Vera Demberg"
                },
                "author": "Vera Demberg",
                "arxiv_comment": "Work-in-progress; MathNLP non-archival presentation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02795v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02795v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07129v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07129v2",
                "updated": "2025-11-04T18:17:08Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    18,
                    17,
                    8,
                    1,
                    308,
                    0
                ],
                "published": "2025-07-08T20:01:15Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    20,
                    1,
                    15,
                    1,
                    189,
                    0
                ],
                "title": "Growing Transformers: Modular Composition and Layer-wise Expansion on a\n  Frozen Substrate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Growing Transformers: Modular Composition and Layer-wise Expansion on a\n  Frozen Substrate"
                },
                "summary": "The prevailing paradigm for scaling large language models (LLMs) involves\nmonolithic, end-to-end training, a resource-intensive process that lacks\nflexibility. This paper explores an alternative, constructive scaling paradigm,\nenabled by the principle of emergent semantics in Transformers with frozen,\nnon-semantic input embeddings. We posit that because high-level meaning is a\ncompositional property of a Transformer's deep layers, not its input vectors,\nthe embedding layer and trained lower layers can serve as a fixed foundation.\nThis liberates backpropagation to focus solely on newly added components,\nmaking incremental growth viable. We operationalize this with a layer-wise\nconstructive methodology that combines strict layer freezing in early stages\nwith efficient, holistic fine-tuning of the entire model stack via low-rank\nadaptation (LoRA) as complexity increases. This method not only demonstrates\nstable convergence but also reveals a direct correlation between model depth\nand the emergence of complex reasoning abilities, such as those required for\nSQuAD, which are absent in shallower models. In a controlled study, our\nconstructively grown model rivals the performance of a monolithically trained\nbaseline of the same size, validating the efficiency and efficacy of the\napproach. Our findings suggest a path towards a paradigm shift from monolithic\noptimization towards a more biological or constructive model of AI development.\nThis opens a path for more resource-efficient scaling, continual learning, and\na more modular approach to building powerful AI systems. We release all code\nand models to facilitate further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The prevailing paradigm for scaling large language models (LLMs) involves\nmonolithic, end-to-end training, a resource-intensive process that lacks\nflexibility. This paper explores an alternative, constructive scaling paradigm,\nenabled by the principle of emergent semantics in Transformers with frozen,\nnon-semantic input embeddings. We posit that because high-level meaning is a\ncompositional property of a Transformer's deep layers, not its input vectors,\nthe embedding layer and trained lower layers can serve as a fixed foundation.\nThis liberates backpropagation to focus solely on newly added components,\nmaking incremental growth viable. We operationalize this with a layer-wise\nconstructive methodology that combines strict layer freezing in early stages\nwith efficient, holistic fine-tuning of the entire model stack via low-rank\nadaptation (LoRA) as complexity increases. This method not only demonstrates\nstable convergence but also reveals a direct correlation between model depth\nand the emergence of complex reasoning abilities, such as those required for\nSQuAD, which are absent in shallower models. In a controlled study, our\nconstructively grown model rivals the performance of a monolithically trained\nbaseline of the same size, validating the efficiency and efficacy of the\napproach. Our findings suggest a path towards a paradigm shift from monolithic\noptimization towards a more biological or constructive model of AI development.\nThis opens a path for more resource-efficient scaling, continual learning, and\na more modular approach to building powerful AI systems. We release all code\nand models to facilitate further research."
                },
                "authors": [
                    {
                        "name": "A. Bochkov"
                    }
                ],
                "author_detail": {
                    "name": "A. Bochkov"
                },
                "author": "A. Bochkov",
                "arxiv_comment": "Controlled Comparative Study added",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07129v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07129v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02785v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02785v1",
                "updated": "2025-11-04T18:06:30Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    18,
                    6,
                    30,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T18:06:30Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    18,
                    6,
                    30,
                    1,
                    308,
                    0
                ],
                "title": "Enhancing Federated Learning Privacy with QUBO",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Federated Learning Privacy with QUBO"
                },
                "summary": "Federated learning (FL) is a widely used method for training machine learning\n(ML) models in a scalable way while preserving privacy (i.e., without\ncentralizing raw data). Prior research shows that the risk of exposing\nsensitive data increases cumulatively as the number of iterations where a\nclient's updates are included in the aggregated model increase. Attackers can\nlaunch membership inference attacks (MIA; deciding whether a sample or client\nparticipated), property inference attacks (PIA; inferring attributes of a\nclient's data), and model inversion attacks (MI; reconstructing inputs),\nthereby inferring client-specific attributes and, in some cases, reconstructing\ninputs. In this paper, we mitigate risk by substantially reducing per client\nexposure using a quantum computing-inspired quadratic unconstrained binary\noptimization (QUBO) formulation that selects a small subset of client updates\nmost relevant for each training round. In this work, we focus on two threat\nvectors: (i) information leakage by clients during training and (ii)\nadversaries who can query or obtain the global model. We assume a trusted\ncentral server and do not model server compromise. This method also assumes\nthat the server has access to a validation/test set with global data\ndistribution. Experiments on the MNIST dataset with 300 clients in 20 rounds\nshowed a 95.2% per-round and 49% cumulative privacy exposure reduction, with\n147 clients' updates never being used during training while maintaining in\ngeneral the full-aggregation accuracy or even better. The method proved to be\nefficient at lower scale and more complex model as well. A CINIC-10\ndataset-based experiment with 30 clients resulted in 82% per-round privacy\nimprovement and 33% cumulative privacy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated learning (FL) is a widely used method for training machine learning\n(ML) models in a scalable way while preserving privacy (i.e., without\ncentralizing raw data). Prior research shows that the risk of exposing\nsensitive data increases cumulatively as the number of iterations where a\nclient's updates are included in the aggregated model increase. Attackers can\nlaunch membership inference attacks (MIA; deciding whether a sample or client\nparticipated), property inference attacks (PIA; inferring attributes of a\nclient's data), and model inversion attacks (MI; reconstructing inputs),\nthereby inferring client-specific attributes and, in some cases, reconstructing\ninputs. In this paper, we mitigate risk by substantially reducing per client\nexposure using a quantum computing-inspired quadratic unconstrained binary\noptimization (QUBO) formulation that selects a small subset of client updates\nmost relevant for each training round. In this work, we focus on two threat\nvectors: (i) information leakage by clients during training and (ii)\nadversaries who can query or obtain the global model. We assume a trusted\ncentral server and do not model server compromise. This method also assumes\nthat the server has access to a validation/test set with global data\ndistribution. Experiments on the MNIST dataset with 300 clients in 20 rounds\nshowed a 95.2% per-round and 49% cumulative privacy exposure reduction, with\n147 clients' updates never being used during training while maintaining in\ngeneral the full-aggregation accuracy or even better. The method proved to be\nefficient at lower scale and more complex model as well. A CINIC-10\ndataset-based experiment with 30 clients resulted in 82% per-round privacy\nimprovement and 33% cumulative privacy."
                },
                "authors": [
                    {
                        "name": "Andras Ferenczi"
                    },
                    {
                        "name": "Sutapa Samanta"
                    },
                    {
                        "name": "Dagen Wang"
                    },
                    {
                        "name": "Todd Hodges"
                    }
                ],
                "author_detail": {
                    "name": "Todd Hodges"
                },
                "author": "Todd Hodges",
                "arxiv_comment": "8 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02785v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02785v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08503v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08503v2",
                "updated": "2025-11-04T18:03:08Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    18,
                    3,
                    8,
                    1,
                    308,
                    0
                ],
                "published": "2024-08-16T03:01:35Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    3,
                    1,
                    35,
                    4,
                    229,
                    0
                ],
                "title": "Computational strategies for cross-species knowledge transfer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational strategies for cross-species knowledge transfer"
                },
                "summary": "Research organisms provide invaluable insights into human biology and\ndiseases, serving as essential tools for functional experiments, disease\nmodeling, and drug testing. However, evolutionary divergence between humans and\nresearch organisms hinders effective knowledge transfer across species. Here,\nwe review state-of-the-art methods for computationally transferring knowledge\nacross species, primarily focusing on methods that utilize transcriptome data\nand/or molecular networks. Our review addresses four key areas: (1)\ntransferring disease and gene annotation knowledge across species, (2)\nidentifying functionally equivalent molecular components, (3) inferring\nequivalent perturbed genes or gene sets, and (4) identifying equivalent cell\ntypes. We conclude with an outlook on future directions and several key\nchallenges that remain in cross-species knowledge transfer, including\nintroducing the concept of \"agnology\" to describe functional equivalence of\nbiological entities, regardless of their evolutionary origins. This concept is\nbecoming pervasive in integrative data-driven models where evolutionary origins\nof functions can remain unresolved.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research organisms provide invaluable insights into human biology and\ndiseases, serving as essential tools for functional experiments, disease\nmodeling, and drug testing. However, evolutionary divergence between humans and\nresearch organisms hinders effective knowledge transfer across species. Here,\nwe review state-of-the-art methods for computationally transferring knowledge\nacross species, primarily focusing on methods that utilize transcriptome data\nand/or molecular networks. Our review addresses four key areas: (1)\ntransferring disease and gene annotation knowledge across species, (2)\nidentifying functionally equivalent molecular components, (3) inferring\nequivalent perturbed genes or gene sets, and (4) identifying equivalent cell\ntypes. We conclude with an outlook on future directions and several key\nchallenges that remain in cross-species knowledge transfer, including\nintroducing the concept of \"agnology\" to describe functional equivalence of\nbiological entities, regardless of their evolutionary origins. This concept is\nbecoming pervasive in integrative data-driven models where evolutionary origins\nof functions can remain unresolved."
                },
                "authors": [
                    {
                        "name": "Hao Yuan"
                    },
                    {
                        "name": "Christopher A. Mancuso"
                    },
                    {
                        "name": "Kayla Johnson"
                    },
                    {
                        "name": "Ingo Braasch"
                    },
                    {
                        "name": "Arjun Krishnan"
                    }
                ],
                "author_detail": {
                    "name": "Arjun Krishnan"
                },
                "author": "Arjun Krishnan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08503v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08503v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.MN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.11070v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.11070v2",
                "updated": "2025-11-04T17:58:02Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    17,
                    58,
                    2,
                    1,
                    308,
                    0
                ],
                "published": "2024-02-16T20:55:22Z",
                "published_parsed": [
                    2024,
                    2,
                    16,
                    20,
                    55,
                    22,
                    4,
                    47,
                    0
                ],
                "title": "Scalable Analysis of Bipartite Experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Analysis of Bipartite Experiments"
                },
                "summary": "Bipartite Experiments are randomized experiments where the treatment is\napplied to a set of units (randomization units) that is different from the\nunits of analysis, and randomization units and analysis units are connected\nthrough a bipartite graph. The scale of experimentation at large online\nplatforms necessitates both accurate inference in the presence of a large\nbipartite interference graph, as well as a highly scalable implementation. In\nthis paper, we describe new methods for inference that enable practical,\nscalable analysis of bipartite experiments: (1) We propose CA-ERL, a\ncovariate-adjusted variant of the exposure-reweighted-linear (ERL) estimator\n[9], which empirically yields 60-90% variance reduction. (2) We introduce a\nrandomization-based method for inference and prove asymptotic validity of a\nWald-type confidence interval under graph sparsity assumptions. (3) We present\na linear-time algorithm for randomization inference of the CA-ERL estimator,\nwhich can be easily implemented in query engines like Presto or Spark. We\nevaluate our methods both on a real experiment at Meta that randomized\ntreatment on Facebook Groups and analyzed user-level metrics, as well as\nsimulations on synthetic data. The real-world data shows that our CA-ERL\nestimator reduces the confidence interval (CI) width by 60-90% (compared to\nERL) in a practical setting. The simulations using synthetic data show that our\nrandomization inference procedure achieves correct coverage across instances,\nwhile the ERL estimator has incorrectly small CI widths for instances with\nlarge true effect sizes and is overly conservative when the bipartite graph is\ndense.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bipartite Experiments are randomized experiments where the treatment is\napplied to a set of units (randomization units) that is different from the\nunits of analysis, and randomization units and analysis units are connected\nthrough a bipartite graph. The scale of experimentation at large online\nplatforms necessitates both accurate inference in the presence of a large\nbipartite interference graph, as well as a highly scalable implementation. In\nthis paper, we describe new methods for inference that enable practical,\nscalable analysis of bipartite experiments: (1) We propose CA-ERL, a\ncovariate-adjusted variant of the exposure-reweighted-linear (ERL) estimator\n[9], which empirically yields 60-90% variance reduction. (2) We introduce a\nrandomization-based method for inference and prove asymptotic validity of a\nWald-type confidence interval under graph sparsity assumptions. (3) We present\na linear-time algorithm for randomization inference of the CA-ERL estimator,\nwhich can be easily implemented in query engines like Presto or Spark. We\nevaluate our methods both on a real experiment at Meta that randomized\ntreatment on Facebook Groups and analyzed user-level metrics, as well as\nsimulations on synthetic data. The real-world data shows that our CA-ERL\nestimator reduces the confidence interval (CI) width by 60-90% (compared to\nERL) in a practical setting. The simulations using synthetic data show that our\nrandomization inference procedure achieves correct coverage across instances,\nwhile the ERL estimator has incorrectly small CI widths for instances with\nlarge true effect sizes and is overly conservative when the bipartite graph is\ndense."
                },
                "authors": [
                    {
                        "name": "Liang Shi"
                    },
                    {
                        "name": "Edvard Bakhitov"
                    },
                    {
                        "name": "Kenneth Hung"
                    },
                    {
                        "name": "Brian Karrer"
                    },
                    {
                        "name": "Charlie Walker"
                    },
                    {
                        "name": "Monica Bhole"
                    },
                    {
                        "name": "Okke Schrijvers"
                    }
                ],
                "author_detail": {
                    "name": "Okke Schrijvers"
                },
                "author": "Okke Schrijvers",
                "arxiv_comment": "Proof of validity of the variance estimator is wrong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.11070v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.11070v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02769v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02769v1",
                "updated": "2025-11-04T17:56:00Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    17,
                    56,
                    0,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T17:56:00Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    17,
                    56,
                    0,
                    1,
                    308,
                    0
                ],
                "title": "STAR-VAE: Latent Variable Transformers for Scalable and Controllable\n  Molecular Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STAR-VAE: Latent Variable Transformers for Scalable and Controllable\n  Molecular Generation"
                },
                "summary": "The chemical space of drug-like molecules is vast, motivating the development\nof generative models that must learn broad chemical distributions, enable\nconditional generation by capturing structure-property representations, and\nprovide fast molecular generation. Meeting the objectives depends on modeling\nchoices, including the probabilistic modeling approach, the conditional\ngenerative formulation, the architecture, and the molecular input\nrepresentation. To address the challenges, we present STAR-VAE\n(Selfies-encoded, Transformer-based, AutoRegressive Variational Auto Encoder),\na scalable latent-variable framework with a Transformer encoder and an\nautoregressive Transformer decoder. It is trained on 79 million drug-like\nmolecules from PubChem, using SELFIES to guarantee syntactic validity. The\nlatent-variable formulation enables conditional generation: a property\npredictor supplies a conditioning signal that is applied consistently to the\nlatent prior, the inference network, and the decoder. Our contributions are:\n(i) a Transformer-based latent-variable encoder-decoder model trained on\nSELFIES representations; (ii) a principled conditional latent-variable\nformulation for property-guided generation; and (iii) efficient finetuning with\nlow-rank adapters (LoRA) in both encoder and decoder, enabling fast adaptation\nwith limited property and activity data. On the GuacaMol and MOSES benchmarks,\nour approach matches or exceeds baselines, and latent-space analyses reveal\nsmooth, semantically structured representations that support both unconditional\nexploration and property-aware generation. On the Tartarus benchmarks, the\nconditional model shifts docking-score distributions toward stronger predicted\nbinding. These results suggest that a modernized, scale-appropriate VAE remains\ncompetitive for molecular generation when paired with principled conditioning\nand parameter-efficient finetuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The chemical space of drug-like molecules is vast, motivating the development\nof generative models that must learn broad chemical distributions, enable\nconditional generation by capturing structure-property representations, and\nprovide fast molecular generation. Meeting the objectives depends on modeling\nchoices, including the probabilistic modeling approach, the conditional\ngenerative formulation, the architecture, and the molecular input\nrepresentation. To address the challenges, we present STAR-VAE\n(Selfies-encoded, Transformer-based, AutoRegressive Variational Auto Encoder),\na scalable latent-variable framework with a Transformer encoder and an\nautoregressive Transformer decoder. It is trained on 79 million drug-like\nmolecules from PubChem, using SELFIES to guarantee syntactic validity. The\nlatent-variable formulation enables conditional generation: a property\npredictor supplies a conditioning signal that is applied consistently to the\nlatent prior, the inference network, and the decoder. Our contributions are:\n(i) a Transformer-based latent-variable encoder-decoder model trained on\nSELFIES representations; (ii) a principled conditional latent-variable\nformulation for property-guided generation; and (iii) efficient finetuning with\nlow-rank adapters (LoRA) in both encoder and decoder, enabling fast adaptation\nwith limited property and activity data. On the GuacaMol and MOSES benchmarks,\nour approach matches or exceeds baselines, and latent-space analyses reveal\nsmooth, semantically structured representations that support both unconditional\nexploration and property-aware generation. On the Tartarus benchmarks, the\nconditional model shifts docking-score distributions toward stronger predicted\nbinding. These results suggest that a modernized, scale-appropriate VAE remains\ncompetitive for molecular generation when paired with principled conditioning\nand parameter-efficient finetuning."
                },
                "authors": [
                    {
                        "name": "Bum Chul Kwon"
                    },
                    {
                        "name": "Ben Shapira"
                    },
                    {
                        "name": "Moshiko Raboh"
                    },
                    {
                        "name": "Shreyans Sethi"
                    },
                    {
                        "name": "Shruti Murarka"
                    },
                    {
                        "name": "Joseph A Morrone"
                    },
                    {
                        "name": "Jianying Hu"
                    },
                    {
                        "name": "Parthasarathy Suryanarayanan"
                    }
                ],
                "author_detail": {
                    "name": "Parthasarathy Suryanarayanan"
                },
                "author": "Parthasarathy Suryanarayanan",
                "arxiv_comment": "16 pages, 3 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02769v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02769v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.00130v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.00130v2",
                "updated": "2025-11-04T17:53:35Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    17,
                    53,
                    35,
                    1,
                    308,
                    0
                ],
                "published": "2025-10-31T10:25:48Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    10,
                    25,
                    48,
                    4,
                    304,
                    0
                ],
                "title": "A Comparative Analysis of LLM Adaptation: SFT, LoRA, and ICL in\n  Data-Scarce Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comparative Analysis of LLM Adaptation: SFT, LoRA, and ICL in\n  Data-Scarce Scenarios"
                },
                "summary": "The remarkable capabilities of Large Language Models (LLMs) often need to be\ntailored for specific applications, requiring the integration of new knowledge\nor the acquisition of new skills. While full fine-tuning is a powerful\nadaptation method, it is computationally expensive and can lead to a\ndegradation of general reasoning abilities, a phenomenon known as catastrophic\nforgetting. A range of alternative techniques exists, each with its own\ntrade-offs. In-Context Learning (ICL) is fast but limited by context length,\nwhile Parameter-Efficient Fine-Tuning (PEFT) methods like Low-Rank Adaptation\n(LoRA) offer a middle ground by minimizing parameter changes. However, the\nchallenge of catastrophic forgetting persists, raising questions about the best\nadaptation strategy for a given task. This paper presents a comparative\nanalysis of Supervised Finetuning (SFT), LoRA, and ICL in data-scarce\nscenarios. We find that LoRA provides the most effective balance, successfully\ninstilling new skills with minimal impact on the base model's general\nknowledge. In contrast, while SFT excels at skill acquisition, it is highly\nsusceptible to catastrophic forgetting. ICL is effective for incorporating\nfactual knowledge but struggles with complex skills. Our findings offer a\npractical framework for selecting an LLM adaptation strategy. We highlight the\ncritical distinction between skill acquisition and knowledge integration,\nclarify the trade-offs between task-specific performance and the preservation\nof general capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The remarkable capabilities of Large Language Models (LLMs) often need to be\ntailored for specific applications, requiring the integration of new knowledge\nor the acquisition of new skills. While full fine-tuning is a powerful\nadaptation method, it is computationally expensive and can lead to a\ndegradation of general reasoning abilities, a phenomenon known as catastrophic\nforgetting. A range of alternative techniques exists, each with its own\ntrade-offs. In-Context Learning (ICL) is fast but limited by context length,\nwhile Parameter-Efficient Fine-Tuning (PEFT) methods like Low-Rank Adaptation\n(LoRA) offer a middle ground by minimizing parameter changes. However, the\nchallenge of catastrophic forgetting persists, raising questions about the best\nadaptation strategy for a given task. This paper presents a comparative\nanalysis of Supervised Finetuning (SFT), LoRA, and ICL in data-scarce\nscenarios. We find that LoRA provides the most effective balance, successfully\ninstilling new skills with minimal impact on the base model's general\nknowledge. In contrast, while SFT excels at skill acquisition, it is highly\nsusceptible to catastrophic forgetting. ICL is effective for incorporating\nfactual knowledge but struggles with complex skills. Our findings offer a\npractical framework for selecting an LLM adaptation strategy. We highlight the\ncritical distinction between skill acquisition and knowledge integration,\nclarify the trade-offs between task-specific performance and the preservation\nof general capabilities."
                },
                "authors": [
                    {
                        "name": "Bernd Bohnet"
                    },
                    {
                        "name": "Rumen Dangovski"
                    },
                    {
                        "name": "Kevin Swersky"
                    },
                    {
                        "name": "Sherry Moore"
                    },
                    {
                        "name": "Arslan Chaudhry"
                    },
                    {
                        "name": "Kathleen Kenealy"
                    },
                    {
                        "name": "Noah Fiedel"
                    }
                ],
                "author_detail": {
                    "name": "Noah Fiedel"
                },
                "author": "Noah Fiedel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.00130v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.00130v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02764v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02764v1",
                "updated": "2025-11-04T17:45:19Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    17,
                    45,
                    19,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T17:45:19Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    17,
                    45,
                    19,
                    1,
                    308,
                    0
                ],
                "title": "Peer effect analysis with latent processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Peer effect analysis with latent processes"
                },
                "summary": "I study peer effects that arise from irreversible decisions in the absence of\na standard social equilibrium. I model a latent sequence of decisions in\ncontinuous time and obtain a closed-form expression for the likelihood, which\nallows to estimate proposed causal estimands. The method avoids regression on\nconditional expectations or linear-in-means regression -- and thus\nreflection-type problems (Manski, 1993) or simultaneity issues -- by modeling\nthe (unobserved) realized direction of causality, whose probability is\nidentified. Under a parsimonious parametric specification, I introduce a peer\neffect parameter meant to capture the causal influence of first-movers on their\npeers. Various forms of peer effect heterogeneity can be accommodated.\nParameters are shown to be consistently estimated by maximum likelihood methods\nand lend themselves to standard inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I study peer effects that arise from irreversible decisions in the absence of\na standard social equilibrium. I model a latent sequence of decisions in\ncontinuous time and obtain a closed-form expression for the likelihood, which\nallows to estimate proposed causal estimands. The method avoids regression on\nconditional expectations or linear-in-means regression -- and thus\nreflection-type problems (Manski, 1993) or simultaneity issues -- by modeling\nthe (unobserved) realized direction of causality, whose probability is\nidentified. Under a parsimonious parametric specification, I introduce a peer\neffect parameter meant to capture the causal influence of first-movers on their\npeers. Various forms of peer effect heterogeneity can be accommodated.\nParameters are shown to be consistently estimated by maximum likelihood methods\nand lend themselves to standard inference."
                },
                "authors": [
                    {
                        "name": "Vincent Starck"
                    }
                ],
                "author_detail": {
                    "name": "Vincent Starck"
                },
                "author": "Vincent Starck",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02764v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02764v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02759v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02759v1",
                "updated": "2025-11-04T17:36:57Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    17,
                    36,
                    57,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T17:36:57Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    17,
                    36,
                    57,
                    1,
                    308,
                    0
                ],
                "title": "LLM-Supported Formal Knowledge Representation for Enhancing Control\n  Engineering Content with an Interactive Semantic Layer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Supported Formal Knowledge Representation for Enhancing Control\n  Engineering Content with an Interactive Semantic Layer"
                },
                "summary": "The rapid growth of research output in control engineering calls for new\napproaches to structure and formalize domain knowledge. This paper briefly\ndescribes an LLM-supported method for semi-automated generation of formal\nknowledge representations that combine human readability with machine\ninterpretability and increased expressiveness. Based on the Imperative\nRepresentation of Knowledge (PyIRK) framework, we demonstrate how language\nmodels can assist in transforming natural-language descriptions and\nmathematical definitions (available as LaTeX source code) into a formalized\nknowledge graph. As a first application we present the generation of an\n``interactive semantic layer'' to enhance the source documents in order to\nfacilitate knowledge transfer. From our perspective this contributes to the\nvision of easily accessible, collaborative, and verifiable knowledge bases for\nthe control engineering domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of research output in control engineering calls for new\napproaches to structure and formalize domain knowledge. This paper briefly\ndescribes an LLM-supported method for semi-automated generation of formal\nknowledge representations that combine human readability with machine\ninterpretability and increased expressiveness. Based on the Imperative\nRepresentation of Knowledge (PyIRK) framework, we demonstrate how language\nmodels can assist in transforming natural-language descriptions and\nmathematical definitions (available as LaTeX source code) into a formalized\nknowledge graph. As a first application we present the generation of an\n``interactive semantic layer'' to enhance the source documents in order to\nfacilitate knowledge transfer. From our perspective this contributes to the\nvision of easily accessible, collaborative, and verifiable knowledge bases for\nthe control engineering domain."
                },
                "authors": [
                    {
                        "name": "Julius Fiedler"
                    },
                    {
                        "name": "Carsten Knoll"
                    },
                    {
                        "name": "Klaus Röbenack"
                    }
                ],
                "author_detail": {
                    "name": "Klaus Röbenack"
                },
                "arxiv_affiliation": "Institute of Control Theory at TU Dresden",
                "author": "Klaus Röbenack",
                "arxiv_comment": "4 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02759v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02759v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02757v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02757v1",
                "updated": "2025-11-04T17:35:52Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    17,
                    35,
                    52,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T17:35:52Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    17,
                    35,
                    52,
                    1,
                    308,
                    0
                ],
                "title": "ConMeZO: Adaptive Descent-Direction Sampling for Gradient-Free\n  Finetuning of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConMeZO: Adaptive Descent-Direction Sampling for Gradient-Free\n  Finetuning of Large Language Models"
                },
                "summary": "Zeroth-order or derivative-free optimization (MeZO) is an attractive strategy\nfor finetuning large language models (LLMs) because it eliminates the memory\noverhead of backpropagation. However, it converges slowly due to the inherent\ncurse of dimensionality when searching for descent directions in the\nhigh-dimensional parameter space of billion-scale LLMs. We propose ConMeZO, a\nnovel zeroth-order optimizer that accelerates convergence by adaptive\ndirectional sampling. Instead of drawing the direction uniformly at random,\nConMeZO restricts the sampling to a cone centered around a momentum estimate.\nThis concentrates the search in directions where the true gradient is more\nlikely to lie and thus reduces the effect of high dimensions. We prove that\nConMeZO achieves the same worst-case convergence rate as MeZO. Empirically,\nwhen finetuning LLMs on natural language tasks, ConMeZO is up to 2X faster than\nMeZO while retaining the low-memory footprint of zeroth-order methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zeroth-order or derivative-free optimization (MeZO) is an attractive strategy\nfor finetuning large language models (LLMs) because it eliminates the memory\noverhead of backpropagation. However, it converges slowly due to the inherent\ncurse of dimensionality when searching for descent directions in the\nhigh-dimensional parameter space of billion-scale LLMs. We propose ConMeZO, a\nnovel zeroth-order optimizer that accelerates convergence by adaptive\ndirectional sampling. Instead of drawing the direction uniformly at random,\nConMeZO restricts the sampling to a cone centered around a momentum estimate.\nThis concentrates the search in directions where the true gradient is more\nlikely to lie and thus reduces the effect of high dimensions. We prove that\nConMeZO achieves the same worst-case convergence rate as MeZO. Empirically,\nwhen finetuning LLMs on natural language tasks, ConMeZO is up to 2X faster than\nMeZO while retaining the low-memory footprint of zeroth-order methods."
                },
                "authors": [
                    {
                        "name": "Lejs Deen Behric"
                    },
                    {
                        "name": "Liang Zhang"
                    },
                    {
                        "name": "Bingcong Li"
                    },
                    {
                        "name": "Kiran Koshy Thekumparampil"
                    }
                ],
                "author_detail": {
                    "name": "Kiran Koshy Thekumparampil"
                },
                "author": "Kiran Koshy Thekumparampil",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02757v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02757v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02755v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02755v1",
                "updated": "2025-11-04T17:35:17Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    17,
                    35,
                    17,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T17:35:17Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    17,
                    35,
                    17,
                    1,
                    308,
                    0
                ],
                "title": "Controlling Performance and Budget of a Centralized Multi-agent LLM\n  System with Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controlling Performance and Budget of a Centralized Multi-agent LLM\n  System with Reinforcement Learning"
                },
                "summary": "Large language models (LLMs) exhibit complementary strengths across domains\nand come with varying inference costs, motivating the design of multi-agent LLM\nsystems where specialized models collaborate efficiently. Existing approaches\npredominantly rely on decentralized frameworks, which invoke multiple LLMs for\nevery input and thus lead to substantial and uncontrolled inference costs. In\nthis work, we introduce a centralized multi-LLM framework, where a controller\nLLM selectively coordinates a pool of expert models in a cost-efficient and\ncost-controllable manner. We formulate this coordination problem as\nreinforcement learning with dual objectives: maximizing task performance while\nminimizing the overall inference cost. In addition, we expect the multi-agent\nsystem to have adapted behavior with different budget conditions during\ninference. To this end, we propose CoRL, a reinforcement learning framework\nthat optimizes the performance cost trade-off in a controllable multi-budget\nsetting. Experiments on four diverse benchmarks demonstrate that CoRL enables a\nsingle system to surpass the best expert LLM under high-budget settings, while\nmaintaining strong performance in more economical low-budget modes,\nhighlighting the effectiveness of centralized coordination for scalable and\ncost-efficient multi-agent LLM systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit complementary strengths across domains\nand come with varying inference costs, motivating the design of multi-agent LLM\nsystems where specialized models collaborate efficiently. Existing approaches\npredominantly rely on decentralized frameworks, which invoke multiple LLMs for\nevery input and thus lead to substantial and uncontrolled inference costs. In\nthis work, we introduce a centralized multi-LLM framework, where a controller\nLLM selectively coordinates a pool of expert models in a cost-efficient and\ncost-controllable manner. We formulate this coordination problem as\nreinforcement learning with dual objectives: maximizing task performance while\nminimizing the overall inference cost. In addition, we expect the multi-agent\nsystem to have adapted behavior with different budget conditions during\ninference. To this end, we propose CoRL, a reinforcement learning framework\nthat optimizes the performance cost trade-off in a controllable multi-budget\nsetting. Experiments on four diverse benchmarks demonstrate that CoRL enables a\nsingle system to surpass the best expert LLM under high-budget settings, while\nmaintaining strong performance in more economical low-budget modes,\nhighlighting the effectiveness of centralized coordination for scalable and\ncost-efficient multi-agent LLM systems."
                },
                "authors": [
                    {
                        "name": "Bowen Jin"
                    },
                    {
                        "name": "TJ Collins"
                    },
                    {
                        "name": "Donghan Yu"
                    },
                    {
                        "name": "Mert Cemri"
                    },
                    {
                        "name": "Shenao Zhang"
                    },
                    {
                        "name": "Mengyu Li"
                    },
                    {
                        "name": "Jay Tang"
                    },
                    {
                        "name": "Tian Qin"
                    },
                    {
                        "name": "Zhiyang Xu"
                    },
                    {
                        "name": "Jiarui Lu"
                    },
                    {
                        "name": "Guoli Yin"
                    },
                    {
                        "name": "Jiawei Han"
                    },
                    {
                        "name": "Zirui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zirui Wang"
                },
                "author": "Zirui Wang",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02755v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02755v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02754v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02754v1",
                "updated": "2025-11-04T17:35:12Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    17,
                    35,
                    12,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T17:35:12Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    17,
                    35,
                    12,
                    1,
                    308,
                    0
                ],
                "title": "DANIEL: A Distributed and Scalable Approach for Global Representation\n  Learning with EHR Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DANIEL: A Distributed and Scalable Approach for Global Representation\n  Learning with EHR Applications"
                },
                "summary": "Classical probabilistic graphical models face fundamental challenges in\nmodern data environments, which are characterized by high dimensionality,\nsource heterogeneity, and stringent data-sharing constraints. In this work, we\nrevisit the Ising model, a well-established member of the Markov Random Field\n(MRF) family, and develop a distributed framework that enables scalable and\nprivacy-preserving representation learning from large-scale binary data with\ninherent low-rank structure. Our approach optimizes a non-convex surrogate loss\nfunction via bi-factored gradient descent, offering substantial computational\nand communication advantages over conventional convex approaches. We evaluate\nour algorithm on multi-institutional electronic health record (EHR) datasets\nfrom 58,248 patients across the University of Pittsburgh Medical Center (UPMC)\nand Mass General Brigham (MGB), demonstrating superior performance in global\nrepresentation learning and downstream clinical tasks, including relationship\ndetection, patient phenotyping, and patient clustering. These results highlight\na broader potential for statistical inference in federated, high-dimensional\nsettings while addressing the practical challenges of data complexity and\nmulti-institutional integration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Classical probabilistic graphical models face fundamental challenges in\nmodern data environments, which are characterized by high dimensionality,\nsource heterogeneity, and stringent data-sharing constraints. In this work, we\nrevisit the Ising model, a well-established member of the Markov Random Field\n(MRF) family, and develop a distributed framework that enables scalable and\nprivacy-preserving representation learning from large-scale binary data with\ninherent low-rank structure. Our approach optimizes a non-convex surrogate loss\nfunction via bi-factored gradient descent, offering substantial computational\nand communication advantages over conventional convex approaches. We evaluate\nour algorithm on multi-institutional electronic health record (EHR) datasets\nfrom 58,248 patients across the University of Pittsburgh Medical Center (UPMC)\nand Mass General Brigham (MGB), demonstrating superior performance in global\nrepresentation learning and downstream clinical tasks, including relationship\ndetection, patient phenotyping, and patient clustering. These results highlight\na broader potential for statistical inference in federated, high-dimensional\nsettings while addressing the practical challenges of data complexity and\nmulti-institutional integration."
                },
                "authors": [
                    {
                        "name": "Zebin Wang"
                    },
                    {
                        "name": "Ziming Gan"
                    },
                    {
                        "name": "Weijing Tang"
                    },
                    {
                        "name": "Zongqi Xia"
                    },
                    {
                        "name": "Tianrun Cai"
                    },
                    {
                        "name": "Tianxi Cai"
                    },
                    {
                        "name": "Junwei Lu"
                    }
                ],
                "author_detail": {
                    "name": "Junwei Lu"
                },
                "author": "Junwei Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02754v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02754v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02752v1",
                "updated": "2025-11-04T17:31:39Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    17,
                    31,
                    39,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T17:31:39Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    17,
                    31,
                    39,
                    1,
                    308,
                    0
                ],
                "title": "AI Diffusion in Low Resource Language Countries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Diffusion in Low Resource Language Countries"
                },
                "summary": "Artificial intelligence (AI) is diffusing globally at unprecedented speed,\nbut adoption remains uneven. Frontier Large Language Models (LLMs) are known to\nperform poorly on low-resource languages due to data scarcity. We hypothesize\nthat this performance deficit reduces the utility of AI, thereby slowing\nadoption in Low-Resource Language Countries (LRLCs). To test this, we use a\nweighted regression model to isolate the language effect from socioeconomic and\ndemographic factors, finding that LRLCs have a share of AI users that is\napproximately 20% lower relative to their baseline. These results indicate that\nlinguistic accessibility is a significant, independent barrier to equitable AI\ndiffusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence (AI) is diffusing globally at unprecedented speed,\nbut adoption remains uneven. Frontier Large Language Models (LLMs) are known to\nperform poorly on low-resource languages due to data scarcity. We hypothesize\nthat this performance deficit reduces the utility of AI, thereby slowing\nadoption in Low-Resource Language Countries (LRLCs). To test this, we use a\nweighted regression model to isolate the language effect from socioeconomic and\ndemographic factors, finding that LRLCs have a share of AI users that is\napproximately 20% lower relative to their baseline. These results indicate that\nlinguistic accessibility is a significant, independent barrier to equitable AI\ndiffusion."
                },
                "authors": [
                    {
                        "name": "Amit Misra"
                    },
                    {
                        "name": "Syed Waqas Zamir"
                    },
                    {
                        "name": "Wassim Hamidouche"
                    },
                    {
                        "name": "Inbal Becker-Reshef"
                    },
                    {
                        "name": "Juan Lavista Ferres"
                    }
                ],
                "author_detail": {
                    "name": "Juan Lavista Ferres"
                },
                "author": "Juan Lavista Ferres",
                "arxiv_comment": "9 pages, 4 tables. Also available at\n  https://aka.ms/AI_Diffusion_Low_Resource_Language_Countries",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.00985v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.00985v2",
                "updated": "2025-11-04T17:28:21Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    17,
                    28,
                    21,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-02T15:57:18Z",
                "published_parsed": [
                    2025,
                    11,
                    2,
                    15,
                    57,
                    18,
                    6,
                    306,
                    0
                ],
                "title": "ORANGE: An Online Reflection ANd GEneration framework with Domain\n  Knowledge for Text-to-SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ORANGE: An Online Reflection ANd GEneration framework with Domain\n  Knowledge for Text-to-SQL"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable progress in\ntranslating natural language to SQL, but a significant semantic gap persists\nbetween their general knowledge and domain-specific semantics of databases.\nHistorical translation logs constitute a rich source of this missing in-domain\nknowledge, where SQL queries inherently encapsulate real-world usage patterns\nof database schema. Existing methods primarily enhance the reasoning process\nfor individual translations but fail to accumulate in-domain knowledge from\npast translations. We introduce ORANGE, an online self-evolutionary framework\nthat constructs database-specific knowledge bases by parsing SQL queries from\ntranslation logs. By accumulating in-domain knowledge that contains schema and\ndata semantics, ORANGE progressively reduces the semantic gap and enhances the\naccuracy of subsequent SQL translations. To ensure reliability, we propose a\nnovel nested Chain-of-Thought SQL-to-Text strategy with tuple-semantic\ntracking, which reduces semantic errors during knowledge generation.\nExperiments on multiple benchmarks confirm the practicality of ORANGE,\ndemonstrating its effectiveness for real-world Text-to-SQL deployment,\nparticularly in handling complex and domain-specific queries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable progress in\ntranslating natural language to SQL, but a significant semantic gap persists\nbetween their general knowledge and domain-specific semantics of databases.\nHistorical translation logs constitute a rich source of this missing in-domain\nknowledge, where SQL queries inherently encapsulate real-world usage patterns\nof database schema. Existing methods primarily enhance the reasoning process\nfor individual translations but fail to accumulate in-domain knowledge from\npast translations. We introduce ORANGE, an online self-evolutionary framework\nthat constructs database-specific knowledge bases by parsing SQL queries from\ntranslation logs. By accumulating in-domain knowledge that contains schema and\ndata semantics, ORANGE progressively reduces the semantic gap and enhances the\naccuracy of subsequent SQL translations. To ensure reliability, we propose a\nnovel nested Chain-of-Thought SQL-to-Text strategy with tuple-semantic\ntracking, which reduces semantic errors during knowledge generation.\nExperiments on multiple benchmarks confirm the practicality of ORANGE,\ndemonstrating its effectiveness for real-world Text-to-SQL deployment,\nparticularly in handling complex and domain-specific queries."
                },
                "authors": [
                    {
                        "name": "Yiwen Jiao"
                    },
                    {
                        "name": "Tonghui Ren"
                    },
                    {
                        "name": "Yuche Gao"
                    },
                    {
                        "name": "Zhenying He"
                    },
                    {
                        "name": "Yinan Jing"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "X. Sean Wang"
                    }
                ],
                "author_detail": {
                    "name": "X. Sean Wang"
                },
                "author": "X. Sean Wang",
                "arxiv_comment": "16 pages, 4 figures, preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.00985v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.00985v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02749v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02749v1",
                "updated": "2025-11-04T17:22:49Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    17,
                    22,
                    49,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T17:22:49Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    17,
                    22,
                    49,
                    1,
                    308,
                    0
                ],
                "title": "Using Span Queries to Optimize for Cache and Attention Locality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Span Queries to Optimize for Cache and Attention Locality"
                },
                "summary": "Clients are evolving beyond chat completion, and now include a variety of\ninnovative inference-time scaling and deep reasoning techniques. At the same\ntime, inference servers remain heavily optimized for chat completion. Prior\nwork has shown that large improvements to KV cache hit rate are possible if\ninference servers evolve towards these non-chat use cases. However, they offer\nsolutions that are also optimized for a single use case, RAG. In this paper, we\nintroduce the span query to generalize the interface to the inference server.\nWe demonstrate that chat, RAG, inference-time scaling, and agentic workloads\ncan all be expressed as span queries. We show how the critical distinction that\nhad been assumed by prior work lies in whether the order of the inputs matter\n-- do they commute? In chat, they do not. In RAG, they often do. This paper\nintroduces span queries, which are expression trees of inference calls, linked\ntogether with commutativity constraints. We describe span query syntax and\nsemantics. We show how they can be automatically optimized to improve KV cache\nlocality. We show how a small change to vLLM (affecting only 492 lines) can\nenable high-performance execution of span queries. Using this stack, we\ndemonstrate that span queries can achieve 10-20x reductions in TTFT for two\ndistinct non-chat use cases. Finally, we show that span queries can also be\noptimized to improve attention locality, so as to avoid the so-called\nlost-in-the-middle problem. We demonstrate that an attention-optimized span\nquery on a 2b parameter model vastly outperforms the accuracy of a stock\ninference server using an 8b model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clients are evolving beyond chat completion, and now include a variety of\ninnovative inference-time scaling and deep reasoning techniques. At the same\ntime, inference servers remain heavily optimized for chat completion. Prior\nwork has shown that large improvements to KV cache hit rate are possible if\ninference servers evolve towards these non-chat use cases. However, they offer\nsolutions that are also optimized for a single use case, RAG. In this paper, we\nintroduce the span query to generalize the interface to the inference server.\nWe demonstrate that chat, RAG, inference-time scaling, and agentic workloads\ncan all be expressed as span queries. We show how the critical distinction that\nhad been assumed by prior work lies in whether the order of the inputs matter\n-- do they commute? In chat, they do not. In RAG, they often do. This paper\nintroduces span queries, which are expression trees of inference calls, linked\ntogether with commutativity constraints. We describe span query syntax and\nsemantics. We show how they can be automatically optimized to improve KV cache\nlocality. We show how a small change to vLLM (affecting only 492 lines) can\nenable high-performance execution of span queries. Using this stack, we\ndemonstrate that span queries can achieve 10-20x reductions in TTFT for two\ndistinct non-chat use cases. Finally, we show that span queries can also be\noptimized to improve attention locality, so as to avoid the so-called\nlost-in-the-middle problem. We demonstrate that an attention-optimized span\nquery on a 2b parameter model vastly outperforms the accuracy of a stock\ninference server using an 8b model."
                },
                "authors": [
                    {
                        "name": "Paul Castro"
                    },
                    {
                        "name": "Nick Mitchell"
                    },
                    {
                        "name": "Nathan Ordonez"
                    },
                    {
                        "name": "Thomas Parnell"
                    },
                    {
                        "name": "Mudhakar Srivatsa"
                    },
                    {
                        "name": "Antoni Viros i Martin"
                    }
                ],
                "author_detail": {
                    "name": "Antoni Viros i Martin"
                },
                "author": "Antoni Viros i Martin",
                "arxiv_comment": "12 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02749v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02749v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02748v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02748v1",
                "updated": "2025-11-04T17:22:22Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    17,
                    22,
                    22,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T17:22:22Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    17,
                    22,
                    22,
                    1,
                    308,
                    0
                ],
                "title": "Agentic World Modeling for 6G: Near-Real-Time Generative State-Space\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic World Modeling for 6G: Near-Real-Time Generative State-Space\n  Reasoning"
                },
                "summary": "We argue that sixth-generation (6G) intelligence is not fluent token\nprediction but the capacity to imagine and choose -- to simulate future\nscenarios, weigh trade-offs, and act with calibrated uncertainty. We reframe\nopen radio access network (O-RAN) near-real-time (Near-RT) control via\ncounterfactual dynamics and a world modeling (WM) paradigm that learns an\naction-conditioned generative state space. This enables quantitative \"what-if\"\nforecasting beyond large language models (LLMs) as the primary modeling\nprimitive. Actions such as physical resource blocks (PRBs) are treated as\nfirst-class control inputs in a causal world model, and both aleatoric and\nepistemic uncertainty are modeled for prediction and what-if analysis. An\nagentic, model predictive control (MPC)-based cross-entropy method (CEM)\nplanner operates over short horizons, using prior-mean rollouts within\ndata-driven PRB bounds to maximize a deterministic reward. The model couples\nmulti-scale structured state-space mixtures (MS3M) with a compact stochastic\nlatent to form WM-MS3M, summarizing key performance indicators (KPIs) histories\nand predicting next-step KPIs under hypothetical PRB sequences. On realistic\nO-RAN traces, WM-MS3M cuts mean absolute error (MAE) by 1.69% versus MS3M with\n32% fewer parameters and similar latency, and achieves 35-80% lower root mean\nsquared error (RMSE) than attention/hybrid baselines with 2.3-4.1x faster\ninference, enabling rare-event simulation and offline policy screening.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We argue that sixth-generation (6G) intelligence is not fluent token\nprediction but the capacity to imagine and choose -- to simulate future\nscenarios, weigh trade-offs, and act with calibrated uncertainty. We reframe\nopen radio access network (O-RAN) near-real-time (Near-RT) control via\ncounterfactual dynamics and a world modeling (WM) paradigm that learns an\naction-conditioned generative state space. This enables quantitative \"what-if\"\nforecasting beyond large language models (LLMs) as the primary modeling\nprimitive. Actions such as physical resource blocks (PRBs) are treated as\nfirst-class control inputs in a causal world model, and both aleatoric and\nepistemic uncertainty are modeled for prediction and what-if analysis. An\nagentic, model predictive control (MPC)-based cross-entropy method (CEM)\nplanner operates over short horizons, using prior-mean rollouts within\ndata-driven PRB bounds to maximize a deterministic reward. The model couples\nmulti-scale structured state-space mixtures (MS3M) with a compact stochastic\nlatent to form WM-MS3M, summarizing key performance indicators (KPIs) histories\nand predicting next-step KPIs under hypothetical PRB sequences. On realistic\nO-RAN traces, WM-MS3M cuts mean absolute error (MAE) by 1.69% versus MS3M with\n32% fewer parameters and similar latency, and achieves 35-80% lower root mean\nsquared error (RMSE) than attention/hybrid baselines with 2.3-4.1x faster\ninference, enabling rare-event simulation and offline policy screening."
                },
                "authors": [
                    {
                        "name": "Farhad Rezazadeh"
                    },
                    {
                        "name": "Hatim Chergui"
                    },
                    {
                        "name": "Merouane Debbah"
                    },
                    {
                        "name": "Houbing Song"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Lingjia Liu"
                    }
                ],
                "author_detail": {
                    "name": "Lingjia Liu"
                },
                "author": "Lingjia Liu",
                "arxiv_comment": "13 Pages, 3 Figures, 4 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02748v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02748v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17211v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17211v2",
                "updated": "2025-11-04T17:20:16Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    17,
                    20,
                    16,
                    1,
                    308,
                    0
                ],
                "published": "2025-09-21T19:38:58Z",
                "published_parsed": [
                    2025,
                    9,
                    21,
                    19,
                    38,
                    58,
                    6,
                    264,
                    0
                ],
                "title": "The changing look of the neutrino-emitter blazar candidate 5BZQ\n  J1243+4043",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The changing look of the neutrino-emitter blazar candidate 5BZQ\n  J1243+4043"
                },
                "summary": "In recent years, changing-look blazars have called the traditional view of BL\nLacs-flat spectrum radio quasars into question within the empirical\nclassification of blazars. Based on the intensity of optical lines, they appear\nto transition between the two classes over time. We focus on the blazar 5BZQ\nJ1243+4043, recently proposed as a promising candidate for the emission of\nhigh-energy neutrinos observed by the IceCube Neutrino Observatory and reported\nas a changing-look blazar in the literature. We study the spectral properties\nof this blazar, inferring its radiation field and accretion regime across\ndifferent epochs. This study presents new optical spectroscopy observations of\n5BZQ J1243+4043 taken with Gran Telescopio Canarias. We used this new dataset\nand two optical spectra available from the literature to investigate the\ncontinuum and line emissions and pinpoint the physical properties of the\nsource. In particular, we used the emission lines to probe the accretion\nregime. The newly collected data for 5BZQ J1243+4043 shows broad emission\nlines, consistent with the spectrum of the first epoch and the redshift z =\n1.5181$\\pm$0.0002 known from the literature. For the second epoch, the spectrum\nappears featureless and so, we placed limits on the emission lines and related\nproperties. We observed spectral variability for both the continuum and line\nemissions among the three spectra. Nonetheless, the accretion properties of the\nblazar generally remain unvaried, indicating that the intrinsic physics stays\nthe same across the three epochs. In the broader multi-messenger context, this\nsuggests that, despite the changing look in the optical band, the candidate\nneutrino-emitter blazar 5BZQ J1243+4043 is still characterized by the presence\nof intense external radiation fields and radiatively efficient accretion,\ntypical of high-excitation radio galaxies, which may foster neutrino\nproduction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, changing-look blazars have called the traditional view of BL\nLacs-flat spectrum radio quasars into question within the empirical\nclassification of blazars. Based on the intensity of optical lines, they appear\nto transition between the two classes over time. We focus on the blazar 5BZQ\nJ1243+4043, recently proposed as a promising candidate for the emission of\nhigh-energy neutrinos observed by the IceCube Neutrino Observatory and reported\nas a changing-look blazar in the literature. We study the spectral properties\nof this blazar, inferring its radiation field and accretion regime across\ndifferent epochs. This study presents new optical spectroscopy observations of\n5BZQ J1243+4043 taken with Gran Telescopio Canarias. We used this new dataset\nand two optical spectra available from the literature to investigate the\ncontinuum and line emissions and pinpoint the physical properties of the\nsource. In particular, we used the emission lines to probe the accretion\nregime. The newly collected data for 5BZQ J1243+4043 shows broad emission\nlines, consistent with the spectrum of the first epoch and the redshift z =\n1.5181$\\pm$0.0002 known from the literature. For the second epoch, the spectrum\nappears featureless and so, we placed limits on the emission lines and related\nproperties. We observed spectral variability for both the continuum and line\nemissions among the three spectra. Nonetheless, the accretion properties of the\nblazar generally remain unvaried, indicating that the intrinsic physics stays\nthe same across the three epochs. In the broader multi-messenger context, this\nsuggests that, despite the changing look in the optical band, the candidate\nneutrino-emitter blazar 5BZQ J1243+4043 is still characterized by the presence\nof intense external radiation fields and radiatively efficient accretion,\ntypical of high-excitation radio galaxies, which may foster neutrino\nproduction."
                },
                "authors": [
                    {
                        "name": "Alessandra Azzollini"
                    },
                    {
                        "name": "Sara Buson"
                    },
                    {
                        "name": "Alexis Coleiro"
                    }
                ],
                "author_detail": {
                    "name": "Alexis Coleiro"
                },
                "author": "Alexis Coleiro",
                "arxiv_doi": "10.1051/0004-6361/202557347",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1051/0004-6361/202557347",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.17211v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17211v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "5 pages, 6 figures, 2 tables. Accepted for publication in A&A",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02734v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02734v1",
                "updated": "2025-11-04T16:58:29Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    16,
                    58,
                    29,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T16:58:29Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    16,
                    58,
                    29,
                    1,
                    308,
                    0
                ],
                "title": "CostBench: Evaluating Multi-Turn Cost-Optimal Planning and Adaptation in\n  Dynamic Environments for LLM Tool-Use Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CostBench: Evaluating Multi-Turn Cost-Optimal Planning and Adaptation in\n  Dynamic Environments for LLM Tool-Use Agents"
                },
                "summary": "Current evaluations of Large Language Model (LLM) agents primarily emphasize\ntask completion, often overlooking resource efficiency and adaptability. This\nneglects a crucial capability: agents' ability to devise and adjust\ncost-optimal plans in response to changing environments. To bridge this gap, we\nintroduce CostBench, a scalable, cost-centric benchmark designed to evaluate\nagents' economic reasoning and replanning abilities. Situated in the\ntravel-planning domain, CostBench comprises tasks solvable via multiple\nsequences of atomic and composite tools with diverse, customizable costs. It\nalso supports four types of dynamic blocking events, such as tool failures and\ncost changes, to simulate real-world unpredictability and necessitate agents to\nadapt in real time. Evaluating leading open-sourced and proprietary models on\nCostBench reveals a substantial gap in cost-aware planning: agents frequently\nfail to identify cost-optimal solutions in static settings, with even GPT-5\nachieving less than 75% exact match rate on the hardest tasks, and performance\nfurther dropping by around 40% under dynamic conditions. By diagnosing these\nweaknesses, CostBench lays the groundwork for developing future agents that are\nboth economically rational and robust.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current evaluations of Large Language Model (LLM) agents primarily emphasize\ntask completion, often overlooking resource efficiency and adaptability. This\nneglects a crucial capability: agents' ability to devise and adjust\ncost-optimal plans in response to changing environments. To bridge this gap, we\nintroduce CostBench, a scalable, cost-centric benchmark designed to evaluate\nagents' economic reasoning and replanning abilities. Situated in the\ntravel-planning domain, CostBench comprises tasks solvable via multiple\nsequences of atomic and composite tools with diverse, customizable costs. It\nalso supports four types of dynamic blocking events, such as tool failures and\ncost changes, to simulate real-world unpredictability and necessitate agents to\nadapt in real time. Evaluating leading open-sourced and proprietary models on\nCostBench reveals a substantial gap in cost-aware planning: agents frequently\nfail to identify cost-optimal solutions in static settings, with even GPT-5\nachieving less than 75% exact match rate on the hardest tasks, and performance\nfurther dropping by around 40% under dynamic conditions. By diagnosing these\nweaknesses, CostBench lays the groundwork for developing future agents that are\nboth economically rational and robust."
                },
                "authors": [
                    {
                        "name": "Jiayu Liu"
                    },
                    {
                        "name": "Cheng Qian"
                    },
                    {
                        "name": "Zhaochen Su"
                    },
                    {
                        "name": "Qing Zong"
                    },
                    {
                        "name": "Shijue Huang"
                    },
                    {
                        "name": "Bingxiang He"
                    },
                    {
                        "name": "Yi R. Fung"
                    }
                ],
                "author_detail": {
                    "name": "Yi R. Fung"
                },
                "author": "Yi R. Fung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02734v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02734v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04263v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04263v2",
                "updated": "2025-11-04T16:47:39Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    16,
                    47,
                    39,
                    1,
                    308,
                    0
                ],
                "published": "2025-10-05T16:09:31Z",
                "published_parsed": [
                    2025,
                    10,
                    5,
                    16,
                    9,
                    31,
                    6,
                    278,
                    0
                ],
                "title": "Efficient Latent Variable Causal Discovery: Combining Score Search and\n  Targeted Testing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Latent Variable Causal Discovery: Combining Score Search and\n  Targeted Testing"
                },
                "summary": "Learning causal structure from observational data is especially challenging\nwhen latent variables or selection bias are present. The Fast Causal Inference\n(FCI) algorithm addresses this setting but often performs exhaustive\nconditional independence tests across many subsets, leading to spurious\nindependence claims, extra or missing edges, and unreliable orientations. We\npresent a family of score-guided mixed-strategy causal search algorithms that\nbuild on this tradition. First, we introduce BOSS-FCI and GRaSP-FCI,\nstraightforward variants of GFCI that substitute BOSS or GRaSP for FGES,\nthereby retaining correctness while incurring different scalability tradeoffs.\nSecond, we develop FCI Targeted-testing (FCIT), a novel mixed-strategy method\nthat improves upon these variants by replacing exhaustive all-subsets testing\nwith targeted tests guided by BOSS, yielding well-formed PAGs with higher\nprecision and efficiency. Finally, we propose a simple heuristic, LV-Dumb (also\nknown as BOSS-POD), which bypasses latent-variable-specific reasoning and\ndirectly returns the PAG of the BOSS DAG. Although not strictly correct in the\nFCI sense, it scales better and often achieves superior accuracy in practice.\nSimulations and real-data analyses demonstrate that BOSS-FCI and GRaSP-FCI\nprovide sound baselines, FCIT improves both efficiency and reliability, and\nLV-Dumb offers a practical heuristic with strong empirical performance.\nTogether, these method highlight the value of score-guided and targeted\nstrategies for scalable latent-variable causal discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning causal structure from observational data is especially challenging\nwhen latent variables or selection bias are present. The Fast Causal Inference\n(FCI) algorithm addresses this setting but often performs exhaustive\nconditional independence tests across many subsets, leading to spurious\nindependence claims, extra or missing edges, and unreliable orientations. We\npresent a family of score-guided mixed-strategy causal search algorithms that\nbuild on this tradition. First, we introduce BOSS-FCI and GRaSP-FCI,\nstraightforward variants of GFCI that substitute BOSS or GRaSP for FGES,\nthereby retaining correctness while incurring different scalability tradeoffs.\nSecond, we develop FCI Targeted-testing (FCIT), a novel mixed-strategy method\nthat improves upon these variants by replacing exhaustive all-subsets testing\nwith targeted tests guided by BOSS, yielding well-formed PAGs with higher\nprecision and efficiency. Finally, we propose a simple heuristic, LV-Dumb (also\nknown as BOSS-POD), which bypasses latent-variable-specific reasoning and\ndirectly returns the PAG of the BOSS DAG. Although not strictly correct in the\nFCI sense, it scales better and often achieves superior accuracy in practice.\nSimulations and real-data analyses demonstrate that BOSS-FCI and GRaSP-FCI\nprovide sound baselines, FCIT improves both efficiency and reliability, and\nLV-Dumb offers a practical heuristic with strong empirical performance.\nTogether, these method highlight the value of score-guided and targeted\nstrategies for scalable latent-variable causal discovery."
                },
                "authors": [
                    {
                        "name": "Joseph Ramsey"
                    },
                    {
                        "name": "Bryan Andrews"
                    },
                    {
                        "name": "Peter Spirtes"
                    }
                ],
                "author_detail": {
                    "name": "Peter Spirtes"
                },
                "author": "Peter Spirtes",
                "arxiv_comment": "30 pages, 44 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04263v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04263v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10924v8",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10924v8",
                "updated": "2025-11-04T16:46:04Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    16,
                    46,
                    4,
                    1,
                    308,
                    0
                ],
                "published": "2024-12-14T18:18:52Z",
                "published_parsed": [
                    2024,
                    12,
                    14,
                    18,
                    18,
                    52,
                    5,
                    349,
                    0
                ],
                "title": "Tokens, the oft-overlooked appetizer: Large language models, the\n  distributional hypothesis, and meaning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokens, the oft-overlooked appetizer: Large language models, the\n  distributional hypothesis, and meaning"
                },
                "summary": "Tokenization is a necessary component within the current architecture of many\nlanguage mod-els, including the transformer-based large language models (LLMs)\nof Generative AI, yet its impact on the model's cognition is often overlooked.\nWe argue that LLMs demonstrate that the Distributional Hypothesis (DH) is\nsufficient for reasonably human-like language performance (particularly with\nrespect to inferential lexical competence), and that the emergence of\nhuman-meaningful linguistic units among tokens and current structural\nconstraints motivate changes to existing, linguistically-agnostic tokenization\ntechniques, particularly with respect to their roles as (1) vehicles for\nconveying salient distributional patterns from human language to the model and\nas (2) semantic primitives. We explore tokenizations from a BPE tokenizer;\nextant model vocabularies obtained from Hugging Face and tiktoken; and the\ninformation in exemplar token vectors as they move through the layers of a\nRoBERTa (large) model. Besides creating suboptimal semantic building blocks and\nobscuring the model's access to the necessary distributional patterns, we\ndescribe how tokens and pretraining can act as a backdoor for bias and other\nunwanted content, which current alignment practices may not remediate.\nAdditionally, we relay evidence that the tokenization algorithm's objective\nfunction impacts the LLM's cognition, despite being arguably meaningfully\ninsulated from the main system intelligence. Finally, we discuss implications\nfor architectural choices, meaning construction, the primacy of language for\nthought, and LLM cognition. [First uploaded to arXiv in December, 2024.]",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokenization is a necessary component within the current architecture of many\nlanguage mod-els, including the transformer-based large language models (LLMs)\nof Generative AI, yet its impact on the model's cognition is often overlooked.\nWe argue that LLMs demonstrate that the Distributional Hypothesis (DH) is\nsufficient for reasonably human-like language performance (particularly with\nrespect to inferential lexical competence), and that the emergence of\nhuman-meaningful linguistic units among tokens and current structural\nconstraints motivate changes to existing, linguistically-agnostic tokenization\ntechniques, particularly with respect to their roles as (1) vehicles for\nconveying salient distributional patterns from human language to the model and\nas (2) semantic primitives. We explore tokenizations from a BPE tokenizer;\nextant model vocabularies obtained from Hugging Face and tiktoken; and the\ninformation in exemplar token vectors as they move through the layers of a\nRoBERTa (large) model. Besides creating suboptimal semantic building blocks and\nobscuring the model's access to the necessary distributional patterns, we\ndescribe how tokens and pretraining can act as a backdoor for bias and other\nunwanted content, which current alignment practices may not remediate.\nAdditionally, we relay evidence that the tokenization algorithm's objective\nfunction impacts the LLM's cognition, despite being arguably meaningfully\ninsulated from the main system intelligence. Finally, we discuss implications\nfor architectural choices, meaning construction, the primacy of language for\nthought, and LLM cognition. [First uploaded to arXiv in December, 2024.]"
                },
                "authors": [
                    {
                        "name": "Julia Witte Zimmerman"
                    },
                    {
                        "name": "Denis Hudon"
                    },
                    {
                        "name": "Kathryn Cramer"
                    },
                    {
                        "name": "Alejandro J. Ruiz"
                    },
                    {
                        "name": "Calla Beauregard"
                    },
                    {
                        "name": "Ashley Fehr"
                    },
                    {
                        "name": "Mikaela Irene Fudolig"
                    },
                    {
                        "name": "Bradford Demarest"
                    },
                    {
                        "name": "Yoshi Meke Bird"
                    },
                    {
                        "name": "Milo Z. Trujillo"
                    },
                    {
                        "name": "Christopher M. Danforth"
                    },
                    {
                        "name": "Peter Sheridan Dodds"
                    }
                ],
                "author_detail": {
                    "name": "Peter Sheridan Dodds"
                },
                "author": "Peter Sheridan Dodds",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10924v8",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10924v8",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19999v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19999v2",
                "updated": "2025-11-04T16:44:29Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    16,
                    44,
                    29,
                    1,
                    308,
                    0
                ],
                "published": "2025-08-27T15:59:47Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    15,
                    59,
                    47,
                    2,
                    239,
                    0
                ],
                "title": "Linear-Time Demonstration Selection for In-Context Learning via Gradient\n  Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear-Time Demonstration Selection for In-Context Learning via Gradient\n  Estimation"
                },
                "summary": "This paper introduces an algorithm to select demonstration examples for\nin-context learning of a query set. Given a set of $n$ examples, how can we\nquickly select $k$ out of $n$ to best serve as the conditioning for downstream\ninference? This problem has broad applications in prompt tuning and\nchain-of-thought reasoning. Since model weights remain fixed during in-context\nlearning, previous work has sought to design methods based on the similarity of\ntoken embeddings. This work proposes a new approach based on gradients of the\noutput taken in the input embedding space. Our approach estimates model outputs\nthrough a first-order approximation using the gradients. Then, we apply this\nestimation to multiple randomly sampled subsets. Finally, we aggregate the\nsampled subset outcomes to form an influence score for each demonstration, and\nselect $k$ most relevant examples. This procedure only requires pre-computing\nmodel outputs and gradients once, resulting in a linear-time algorithm relative\nto model and training set sizes. Extensive experiments across various models\nand datasets validate the efficiency of our approach. We show that the gradient\nestimation procedure yields approximations of full inference with less than\n${1}\\%$ error across six datasets. This allows us to scale up subset selection\nthat would otherwise run full inference by up to ${37.7}\\times$ on models with\nup to $34$ billion parameters, and outperform existing selection methods based\non input embeddings by ${11}\\%$ on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces an algorithm to select demonstration examples for\nin-context learning of a query set. Given a set of $n$ examples, how can we\nquickly select $k$ out of $n$ to best serve as the conditioning for downstream\ninference? This problem has broad applications in prompt tuning and\nchain-of-thought reasoning. Since model weights remain fixed during in-context\nlearning, previous work has sought to design methods based on the similarity of\ntoken embeddings. This work proposes a new approach based on gradients of the\noutput taken in the input embedding space. Our approach estimates model outputs\nthrough a first-order approximation using the gradients. Then, we apply this\nestimation to multiple randomly sampled subsets. Finally, we aggregate the\nsampled subset outcomes to form an influence score for each demonstration, and\nselect $k$ most relevant examples. This procedure only requires pre-computing\nmodel outputs and gradients once, resulting in a linear-time algorithm relative\nto model and training set sizes. Extensive experiments across various models\nand datasets validate the efficiency of our approach. We show that the gradient\nestimation procedure yields approximations of full inference with less than\n${1}\\%$ error across six datasets. This allows us to scale up subset selection\nthat would otherwise run full inference by up to ${37.7}\\times$ on models with\nup to $34$ billion parameters, and outperform existing selection methods based\non input embeddings by ${11}\\%$ on average."
                },
                "authors": [
                    {
                        "name": "Ziniu Zhang"
                    },
                    {
                        "name": "Zhenshuo Zhang"
                    },
                    {
                        "name": "Dongyue Li"
                    },
                    {
                        "name": "Lu Wang"
                    },
                    {
                        "name": "Jennifer Dy"
                    },
                    {
                        "name": "Hongyang R. Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hongyang R. Zhang"
                },
                "author": "Hongyang R. Zhang",
                "arxiv_comment": "19 pages. EMNLP'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19999v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19999v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.01304v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.01304v2",
                "updated": "2025-11-04T16:36:28Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    16,
                    36,
                    28,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-03T07:38:57Z",
                "published_parsed": [
                    2025,
                    11,
                    3,
                    7,
                    38,
                    57,
                    0,
                    307,
                    0
                ],
                "title": "Positive Semi-definite Latent Factor Grouping-Boosted Cluster-reasoning\n  Instance Disentangled Learning for WSI Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Positive Semi-definite Latent Factor Grouping-Boosted Cluster-reasoning\n  Instance Disentangled Learning for WSI Representation"
                },
                "summary": "Multiple instance learning (MIL) has been widely used for representing\nwhole-slide pathology images. However, spatial, semantic, and decision\nentanglements among instances limit its representation and interpretability. To\naddress these challenges, we propose a latent factor grouping-boosted\ncluster-reasoning instance disentangled learning framework for whole-slide\nimage (WSI) interpretable representation in three phases. First, we introduce a\nnovel positive semi-definite latent factor grouping that maps instances into a\nlatent subspace, effectively mitigating spatial entanglement in MIL. To\nalleviate semantic entanglement, we employs instance probability counterfactual\ninference and optimization via cluster-reasoning instance disentangling.\nFinally, we employ a generalized linear weighted decision via instance effect\nre-weighting to address decision entanglement. Extensive experiments on\nmulticentre datasets demonstrate that our model outperforms all\nstate-of-the-art models. Moreover, it attains pathologist-aligned\ninterpretability through disentangled representations and a transparent\ndecision-making process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiple instance learning (MIL) has been widely used for representing\nwhole-slide pathology images. However, spatial, semantic, and decision\nentanglements among instances limit its representation and interpretability. To\naddress these challenges, we propose a latent factor grouping-boosted\ncluster-reasoning instance disentangled learning framework for whole-slide\nimage (WSI) interpretable representation in three phases. First, we introduce a\nnovel positive semi-definite latent factor grouping that maps instances into a\nlatent subspace, effectively mitigating spatial entanglement in MIL. To\nalleviate semantic entanglement, we employs instance probability counterfactual\ninference and optimization via cluster-reasoning instance disentangling.\nFinally, we employ a generalized linear weighted decision via instance effect\nre-weighting to address decision entanglement. Extensive experiments on\nmulticentre datasets demonstrate that our model outperforms all\nstate-of-the-art models. Moreover, it attains pathologist-aligned\ninterpretability through disentangled representations and a transparent\ndecision-making process."
                },
                "authors": [
                    {
                        "name": "Chentao Li"
                    },
                    {
                        "name": "Behzad Bozorgtabar"
                    },
                    {
                        "name": "Yifang Ping"
                    },
                    {
                        "name": "Pan Huang"
                    },
                    {
                        "name": "Jing Qin"
                    }
                ],
                "author_detail": {
                    "name": "Jing Qin"
                },
                "author": "Jing Qin",
                "arxiv_comment": "Our code is available at https://github.com/Prince-Lee-PathAI/PG-CIDL",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.01304v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.01304v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02713v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02713v1",
                "updated": "2025-11-04T16:31:44Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    16,
                    31,
                    44,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T16:31:44Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    16,
                    31,
                    44,
                    1,
                    308,
                    0
                ],
                "title": "ReleaseEval: A Benchmark for Evaluating Language Models in Automated\n  Release Note Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReleaseEval: A Benchmark for Evaluating Language Models in Automated\n  Release Note Generation"
                },
                "summary": "Automated release note generation addresses the challenge of documenting\nfrequent software updates, where manual efforts are time-consuming and prone to\nhuman error. Although recent advances in language models further enhance this\nprocess, progress remains hindered by dataset limitations, including the lack\nof explicit licensing and limited reproducibility, and incomplete task design\nthat relies mainly on commit messages for summarization while overlooking\nfine-grained contexts such as commit hierarchies and code changes. To fill this\ngap, we introduce ReleaseEval, a reproducible and openly licensed benchmark\ndesigned to systematically evaluate language models for automated release note\ngeneration. ReleaseEval comprises 94,987 release notes from 3,369 repositories\nacross 6 programming languages, and supports three task settings with three\nlevels of input granularity: (1) commit2sum, which generates release notes from\ncommit messages; (2) tree2sum, which incorporates commit tree structures; and\n(3) diff2sum, which leverages fine-grained code diffs. Both automated and human\nevaluations show that large language models consistently outperform traditional\nbaselines across all tasks, achieving substantial gains on tree2sum, while\nstill struggling on diff2sum. These findings highlight LLMs' proficiency in\nleveraging structured information while revealing challenges in abstracting\nfrom long code diffs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated release note generation addresses the challenge of documenting\nfrequent software updates, where manual efforts are time-consuming and prone to\nhuman error. Although recent advances in language models further enhance this\nprocess, progress remains hindered by dataset limitations, including the lack\nof explicit licensing and limited reproducibility, and incomplete task design\nthat relies mainly on commit messages for summarization while overlooking\nfine-grained contexts such as commit hierarchies and code changes. To fill this\ngap, we introduce ReleaseEval, a reproducible and openly licensed benchmark\ndesigned to systematically evaluate language models for automated release note\ngeneration. ReleaseEval comprises 94,987 release notes from 3,369 repositories\nacross 6 programming languages, and supports three task settings with three\nlevels of input granularity: (1) commit2sum, which generates release notes from\ncommit messages; (2) tree2sum, which incorporates commit tree structures; and\n(3) diff2sum, which leverages fine-grained code diffs. Both automated and human\nevaluations show that large language models consistently outperform traditional\nbaselines across all tasks, achieving substantial gains on tree2sum, while\nstill struggling on diff2sum. These findings highlight LLMs' proficiency in\nleveraging structured information while revealing challenges in abstracting\nfrom long code diffs."
                },
                "authors": [
                    {
                        "name": "Qianru Meng"
                    },
                    {
                        "name": "Zhaochun Ren"
                    },
                    {
                        "name": "Joost Visser"
                    }
                ],
                "author_detail": {
                    "name": "Joost Visser"
                },
                "author": "Joost Visser",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02713v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02713v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02711v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02711v1",
                "updated": "2025-11-04T16:30:55Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    16,
                    30,
                    55,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T16:30:55Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    16,
                    30,
                    55,
                    1,
                    308,
                    0
                ],
                "title": "Relational Deep Dive: Error-Aware Queries Over Unstructured Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relational Deep Dive: Error-Aware Queries Over Unstructured Data"
                },
                "summary": "Unstructured data is pervasive, but analytical queries demand structured\nrepresentations, creating a significant extraction challenge. Existing methods\nlike RAG lack schema awareness and struggle with cross-document alignment,\nleading to high error rates. We propose ReDD (Relational Deep Dive), a\nframework that dynamically discovers query-specific schemas, populates\nrelational tables, and ensures error-aware extraction with provable guarantees.\nReDD features a two-stage pipeline: (1) Iterative Schema Discovery (ISD)\nidentifies minimal, joinable schemas tailored to each query, and (2) Tabular\nData Population (TDP) extracts and corrects data using lightweight classifiers\ntrained on LLM hidden states. A main contribution of ReDD is SCAPE, a\nstatistically calibrated method for error detection with coverage guarantees,\nand SCAPE-HYB, a hybrid approach that optimizes the trade-off between accuracy\nand human correction costs. Experiments across diverse datasets demonstrate\nReDD's effectiveness, reducing data extraction errors from up to 30% to below\n1% while maintaining high schema completeness (100% recall) and precision.\nReDD's modular design enables fine-grained control over accuracy-cost\ntrade-offs, making it a robust solution for high-stakes analytical queries over\nunstructured corpora.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unstructured data is pervasive, but analytical queries demand structured\nrepresentations, creating a significant extraction challenge. Existing methods\nlike RAG lack schema awareness and struggle with cross-document alignment,\nleading to high error rates. We propose ReDD (Relational Deep Dive), a\nframework that dynamically discovers query-specific schemas, populates\nrelational tables, and ensures error-aware extraction with provable guarantees.\nReDD features a two-stage pipeline: (1) Iterative Schema Discovery (ISD)\nidentifies minimal, joinable schemas tailored to each query, and (2) Tabular\nData Population (TDP) extracts and corrects data using lightweight classifiers\ntrained on LLM hidden states. A main contribution of ReDD is SCAPE, a\nstatistically calibrated method for error detection with coverage guarantees,\nand SCAPE-HYB, a hybrid approach that optimizes the trade-off between accuracy\nand human correction costs. Experiments across diverse datasets demonstrate\nReDD's effectiveness, reducing data extraction errors from up to 30% to below\n1% while maintaining high schema completeness (100% recall) and precision.\nReDD's modular design enables fine-grained control over accuracy-cost\ntrade-offs, making it a robust solution for high-stakes analytical queries over\nunstructured corpora."
                },
                "authors": [
                    {
                        "name": "Daren Chao"
                    },
                    {
                        "name": "Kaiwen Chen"
                    },
                    {
                        "name": "Naiqing Guan"
                    },
                    {
                        "name": "Nick Koudas"
                    }
                ],
                "author_detail": {
                    "name": "Nick Koudas"
                },
                "author": "Nick Koudas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02711v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02711v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.00970v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.00970v2",
                "updated": "2025-11-04T16:30:29Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    16,
                    30,
                    29,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-02T15:24:41Z",
                "published_parsed": [
                    2025,
                    11,
                    2,
                    15,
                    24,
                    41,
                    6,
                    306,
                    0
                ],
                "title": "Thermodynamic Length in Stochastic Thermodynamics of\n  Far-From-Equilibrium Systems: Unification of Fluctuation Relation and\n  Thermodynamic Uncertainty Relation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thermodynamic Length in Stochastic Thermodynamics of\n  Far-From-Equilibrium Systems: Unification of Fluctuation Relation and\n  Thermodynamic Uncertainty Relation"
                },
                "summary": "The Boltzmann distribution for an equilibrium system constrains the\nstatistics of the system by the energetics. Despite the non-equilibrium\ngeneralization of the Boltzmann distribution being studied extensively, a\nunified framework valid for far-from-equilibrium discrete state systems is\nlacking. Here, we derive an exact path-integral representation for discrete\nstate processes and represent it using the exponential of the action for\nstochastic transition dynamics. Solving the variational problem, the effective\naction is shown to be equal to the inferred entropy production rate (a\nthermodynamic quantity) and a non-quadratic dissipation function of the\nthermodynamic length (TL) defined for microscopic stochastic currents (a\ndynamic quantity). This formulates a far-from-equilibrium analog of the\nBoltzmann distribution, namely, the minimum action principle. The non-quadratic\ndissipation function is physically attributed to incorporating non-Gaussian\nfluctuations or far-from-equilibrium non-conservative driving. Further, an\nexact large deviation dynamical rate functional is derived. The equivalence of\nthe variational formulation with the information geometric formulation is\nproved. The non-quadratic TL recovers the non-quadratic thermodynamic-kinetic\nuncertainty relation (TKUR) and the speed limits, which are tighter than the\nclose-to-equilibrium quadratic formulations. Moreover, if the transition\naffinities are known, the non-quadratic TL recovers the fluctuation relation\n(FR). The minimum action principle manifests the non-quadratic TKUR and FR as\ntwo faces corresponding to the thermodynamic inference and partial control\ndescriptions, respectively. In addition, the validity of these results is\nextended to coarse-grained observable currents, strengthening the\nexperimental/numerical applicability of them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Boltzmann distribution for an equilibrium system constrains the\nstatistics of the system by the energetics. Despite the non-equilibrium\ngeneralization of the Boltzmann distribution being studied extensively, a\nunified framework valid for far-from-equilibrium discrete state systems is\nlacking. Here, we derive an exact path-integral representation for discrete\nstate processes and represent it using the exponential of the action for\nstochastic transition dynamics. Solving the variational problem, the effective\naction is shown to be equal to the inferred entropy production rate (a\nthermodynamic quantity) and a non-quadratic dissipation function of the\nthermodynamic length (TL) defined for microscopic stochastic currents (a\ndynamic quantity). This formulates a far-from-equilibrium analog of the\nBoltzmann distribution, namely, the minimum action principle. The non-quadratic\ndissipation function is physically attributed to incorporating non-Gaussian\nfluctuations or far-from-equilibrium non-conservative driving. Further, an\nexact large deviation dynamical rate functional is derived. The equivalence of\nthe variational formulation with the information geometric formulation is\nproved. The non-quadratic TL recovers the non-quadratic thermodynamic-kinetic\nuncertainty relation (TKUR) and the speed limits, which are tighter than the\nclose-to-equilibrium quadratic formulations. Moreover, if the transition\naffinities are known, the non-quadratic TL recovers the fluctuation relation\n(FR). The minimum action principle manifests the non-quadratic TKUR and FR as\ntwo faces corresponding to the thermodynamic inference and partial control\ndescriptions, respectively. In addition, the validity of these results is\nextended to coarse-grained observable currents, strengthening the\nexperimental/numerical applicability of them."
                },
                "authors": [
                    {
                        "name": "Atul Tanaji Mohite"
                    },
                    {
                        "name": "Heiko Rieger"
                    }
                ],
                "author_detail": {
                    "name": "Heiko Rieger"
                },
                "author": "Heiko Rieger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.00970v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.00970v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22211v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22211v2",
                "updated": "2025-11-04T16:26:40Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    16,
                    26,
                    40,
                    1,
                    308,
                    0
                ],
                "published": "2024-10-29T16:39:28Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    16,
                    39,
                    28,
                    1,
                    303,
                    0
                ],
                "title": "ProMQA: Question Answering Dataset for Multimodal Procedural Activity\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProMQA: Question Answering Dataset for Multimodal Procedural Activity\n  Understanding"
                },
                "summary": "Multimodal systems have great potential to assist humans in procedural\nactivities, where people follow instructions to achieve their goals. Despite\ndiverse application scenarios, systems are typically evaluated on traditional\nclassification tasks, e.g., action recognition or temporal action segmentation.\nIn this paper, we present a novel evaluation dataset, ProMQA, to measure system\nadvancements in application-oriented scenarios. ProMQA consists of 401\nmultimodal procedural QA pairs on user recording of procedural activities,\ni.e., cooking, coupled with their corresponding instructions/recipes. For QA\nannotation, we take a cost-effective human-LLM collaborative approach, where\nthe existing annotation is augmented with LLM-generated QA pairs that are later\nverified by humans. We then provide the benchmark results to set the baseline\nperformance on ProMQA. Our experiment reveals a significant gap between human\nperformance and that of current systems, including competitive proprietary\nmultimodal models. We hope our dataset sheds light on new aspects of models'\nmultimodal understanding capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal systems have great potential to assist humans in procedural\nactivities, where people follow instructions to achieve their goals. Despite\ndiverse application scenarios, systems are typically evaluated on traditional\nclassification tasks, e.g., action recognition or temporal action segmentation.\nIn this paper, we present a novel evaluation dataset, ProMQA, to measure system\nadvancements in application-oriented scenarios. ProMQA consists of 401\nmultimodal procedural QA pairs on user recording of procedural activities,\ni.e., cooking, coupled with their corresponding instructions/recipes. For QA\nannotation, we take a cost-effective human-LLM collaborative approach, where\nthe existing annotation is augmented with LLM-generated QA pairs that are later\nverified by humans. We then provide the benchmark results to set the baseline\nperformance on ProMQA. Our experiment reveals a significant gap between human\nperformance and that of current systems, including competitive proprietary\nmultimodal models. We hope our dataset sheds light on new aspects of models'\nmultimodal understanding capabilities."
                },
                "authors": [
                    {
                        "name": "Kimihiro Hasegawa"
                    },
                    {
                        "name": "Wiradee Imrattanatrai"
                    },
                    {
                        "name": "Zhi-Qi Cheng"
                    },
                    {
                        "name": "Masaki Asada"
                    },
                    {
                        "name": "Susan Holm"
                    },
                    {
                        "name": "Yuran Wang"
                    },
                    {
                        "name": "Ken Fukuda"
                    },
                    {
                        "name": "Teruko Mitamura"
                    }
                ],
                "author_detail": {
                    "name": "Teruko Mitamura"
                },
                "author": "Teruko Mitamura",
                "arxiv_comment": "NAACL2025, Code and Data: https://github.com/kimihiroh/promqa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22211v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22211v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01100v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01100v2",
                "updated": "2025-11-04T16:26:26Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    16,
                    26,
                    26,
                    1,
                    308,
                    0
                ],
                "published": "2025-04-01T18:16:11Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    18,
                    16,
                    11,
                    1,
                    91,
                    0
                ],
                "title": "Repetitions are not all alike: distinct mechanisms sustain repetition in\n  language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Repetitions are not all alike: distinct mechanisms sustain repetition in\n  language models"
                },
                "summary": "Large Language Models (LLMs) can sometimes degrade into repetitive loops,\npersistently generating identical word sequences. Because repetition is rare in\nnatural human language, its frequent occurrence across diverse tasks and\ncontexts in LLMs remains puzzling. Here we investigate whether behaviorally\nsimilar repetition patterns arise from distinct underlying mechanisms and how\nthese mechanisms develop during model training. We contrast two conditions:\nrepetitions elicited by natural text prompts with those induced by in-context\nlearning (ICL) setups that explicitly require copying behavior. Our analyses\nreveal that ICL-induced repetition relies on a dedicated network of attention\nheads that progressively specialize over training, whereas naturally occurring\nrepetition emerges early and lacks a defined circuitry. Attention inspection\nfurther shows that natural repetition focuses disproportionately on\nlow-information tokens, suggesting a fallback behavior when relevant context\ncannot be retrieved. These results indicate that superficially similar\nrepetition behaviors originate from qualitatively different internal processes,\nreflecting distinct modes of failure and adaptation in language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can sometimes degrade into repetitive loops,\npersistently generating identical word sequences. Because repetition is rare in\nnatural human language, its frequent occurrence across diverse tasks and\ncontexts in LLMs remains puzzling. Here we investigate whether behaviorally\nsimilar repetition patterns arise from distinct underlying mechanisms and how\nthese mechanisms develop during model training. We contrast two conditions:\nrepetitions elicited by natural text prompts with those induced by in-context\nlearning (ICL) setups that explicitly require copying behavior. Our analyses\nreveal that ICL-induced repetition relies on a dedicated network of attention\nheads that progressively specialize over training, whereas naturally occurring\nrepetition emerges early and lacks a defined circuitry. Attention inspection\nfurther shows that natural repetition focuses disproportionately on\nlow-information tokens, suggesting a fallback behavior when relevant context\ncannot be retrieved. These results indicate that superficially similar\nrepetition behaviors originate from qualitatively different internal processes,\nreflecting distinct modes of failure and adaptation in language models."
                },
                "authors": [
                    {
                        "name": "Matéo Mahaut"
                    },
                    {
                        "name": "Francesca Franzon"
                    }
                ],
                "author_detail": {
                    "name": "Francesca Franzon"
                },
                "author": "Francesca Franzon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01100v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01100v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16116v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16116v3",
                "updated": "2025-11-04T16:26:20Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    16,
                    26,
                    20,
                    1,
                    308,
                    0
                ],
                "published": "2025-04-18T16:40:39Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    16,
                    40,
                    39,
                    4,
                    108,
                    0
                ],
                "title": "DMind Benchmark: Toward a Holistic Assessment of LLM Capabilities across\n  the Web3 Domain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DMind Benchmark: Toward a Holistic Assessment of LLM Capabilities across\n  the Web3 Domain"
                },
                "summary": "Large Language Models (LLMs) have achieved impressive performance in diverse\nnatural language processing tasks, but specialized domains such as Web3 present\nnew challenges and require more tailored evaluation. Despite the significant\nuser base and capital flows in Web3, encompassing smart contracts,\ndecentralized finance (DeFi), non-fungible tokens (NFTs), decentralized\nautonomous organizations (DAOs), on-chain governance, and novel\ntoken-economics, no comprehensive benchmark has systematically assessed LLM\nperformance in this domain. To address this gap, we introduce the DMind\nBenchmark, a holistic Web3-oriented evaluation suite covering nine critical\nsubfields: fundamental blockchain concepts, blockchain infrastructure, smart\ncontract, DeFi mechanisms, DAOs, NFTs, token economics, meme concept, and\nsecurity vulnerabilities. Beyond multiple-choice questions, DMind Benchmark\nfeatures domain-specific tasks such as contract debugging and on-chain numeric\nreasoning, mirroring real-world scenarios. We evaluated 26 models, including\nChatGPT, Claude, DeepSeek, Gemini, Grok, and Qwen, uncovering notable\nperformance gaps in specialized areas like token economics and\nsecurity-critical contract analysis. While some models excel in blockchain\ninfrastructure tasks, advanced subfields remain challenging. Our benchmark\ndataset and evaluation pipeline are open-sourced on\nhttps://huggingface.co/datasets/DMindAI/DMind_Benchmark, reaching number one in\nHugging Face's trending dataset charts within a week of release.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved impressive performance in diverse\nnatural language processing tasks, but specialized domains such as Web3 present\nnew challenges and require more tailored evaluation. Despite the significant\nuser base and capital flows in Web3, encompassing smart contracts,\ndecentralized finance (DeFi), non-fungible tokens (NFTs), decentralized\nautonomous organizations (DAOs), on-chain governance, and novel\ntoken-economics, no comprehensive benchmark has systematically assessed LLM\nperformance in this domain. To address this gap, we introduce the DMind\nBenchmark, a holistic Web3-oriented evaluation suite covering nine critical\nsubfields: fundamental blockchain concepts, blockchain infrastructure, smart\ncontract, DeFi mechanisms, DAOs, NFTs, token economics, meme concept, and\nsecurity vulnerabilities. Beyond multiple-choice questions, DMind Benchmark\nfeatures domain-specific tasks such as contract debugging and on-chain numeric\nreasoning, mirroring real-world scenarios. We evaluated 26 models, including\nChatGPT, Claude, DeepSeek, Gemini, Grok, and Qwen, uncovering notable\nperformance gaps in specialized areas like token economics and\nsecurity-critical contract analysis. While some models excel in blockchain\ninfrastructure tasks, advanced subfields remain challenging. Our benchmark\ndataset and evaluation pipeline are open-sourced on\nhttps://huggingface.co/datasets/DMindAI/DMind_Benchmark, reaching number one in\nHugging Face's trending dataset charts within a week of release."
                },
                "authors": [
                    {
                        "name": "Enhao Huang"
                    },
                    {
                        "name": "Pengyu Sun"
                    },
                    {
                        "name": "Zixin Lin"
                    },
                    {
                        "name": "Alex Chen"
                    },
                    {
                        "name": "Joey Ouyang"
                    },
                    {
                        "name": "Haobo Wang"
                    },
                    {
                        "name": "Kaichun Hu"
                    },
                    {
                        "name": "James Yi"
                    },
                    {
                        "name": "Frank Li"
                    },
                    {
                        "name": "Zhiyu Zhang"
                    },
                    {
                        "name": "Tianxiang Xu"
                    },
                    {
                        "name": "Gang Zhao"
                    },
                    {
                        "name": "Ziang Ling"
                    },
                    {
                        "name": "Lowes Yang"
                    }
                ],
                "author_detail": {
                    "name": "Lowes Yang"
                },
                "author": "Lowes Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16116v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16116v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02704v1",
                "updated": "2025-11-04T16:24:48Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    16,
                    24,
                    48,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T16:24:48Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    16,
                    24,
                    48,
                    1,
                    308,
                    0
                ],
                "title": "Policy Gradient Methods for Information-Theoretic Opacity in Markov\n  Decision Processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Policy Gradient Methods for Information-Theoretic Opacity in Markov\n  Decision Processes"
                },
                "summary": "Opacity, or non-interference, is a property ensuring that an external\nobserver cannot infer confidential information (the \"secret\") from system\nobservations. We introduce an information-theoretic measure of opacity, which\nquantifies information leakage using the conditional entropy of the secret\ngiven the observer's partial observations in a system modeled as a Markov\ndecision process (MDP). Our objective is to find a control policy that\nmaximizes opacity while satisfying task performance constraints, assuming that\nan informed observer is aware of the control policy and system dynamics.\nSpecifically, we consider a class of opacity called state-based opacity, where\nthe secret is a propositional formula about the past or current state of the\nsystem, and a special case of state-based opacity called language-based\nopacity, where the secret is defined by a temporal logic formula (LTL) or a\nregular language recognized by a finite-state automaton. First, we prove that\nfinite-memory policies can outperform Markov policies in optimizing\ninformation-theoretic opacity. Second, we develop an algorithm to compute a\nmaximally opaque Markov policy using a primal-dual gradient-based algorithm,\nand prove its convergence. Since opacity cannot be expressed as a cumulative\ncost, we develop a novel method to compute the gradient of conditional entropy\nwith respect to policy parameters using observable operators in hidden Markov\nmodels. The experimental results validate the effectiveness and optimality of\nour proposed methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Opacity, or non-interference, is a property ensuring that an external\nobserver cannot infer confidential information (the \"secret\") from system\nobservations. We introduce an information-theoretic measure of opacity, which\nquantifies information leakage using the conditional entropy of the secret\ngiven the observer's partial observations in a system modeled as a Markov\ndecision process (MDP). Our objective is to find a control policy that\nmaximizes opacity while satisfying task performance constraints, assuming that\nan informed observer is aware of the control policy and system dynamics.\nSpecifically, we consider a class of opacity called state-based opacity, where\nthe secret is a propositional formula about the past or current state of the\nsystem, and a special case of state-based opacity called language-based\nopacity, where the secret is defined by a temporal logic formula (LTL) or a\nregular language recognized by a finite-state automaton. First, we prove that\nfinite-memory policies can outperform Markov policies in optimizing\ninformation-theoretic opacity. Second, we develop an algorithm to compute a\nmaximally opaque Markov policy using a primal-dual gradient-based algorithm,\nand prove its convergence. Since opacity cannot be expressed as a cumulative\ncost, we develop a novel method to compute the gradient of conditional entropy\nwith respect to policy parameters using observable operators in hidden Markov\nmodels. The experimental results validate the effectiveness and optimality of\nour proposed methods."
                },
                "authors": [
                    {
                        "name": "Chongyang Shi"
                    },
                    {
                        "name": "Sumukha Udupa"
                    },
                    {
                        "name": "Michael R. Dorothy"
                    },
                    {
                        "name": "Shuo Han"
                    },
                    {
                        "name": "Jie Fu"
                    }
                ],
                "author_detail": {
                    "name": "Jie Fu"
                },
                "author": "Jie Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.01854v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.01854v2",
                "updated": "2025-11-04T16:24:47Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    16,
                    24,
                    47,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-03T18:58:28Z",
                "published_parsed": [
                    2025,
                    11,
                    3,
                    18,
                    58,
                    28,
                    0,
                    307,
                    0
                ],
                "title": "Tool-to-Agent Retrieval: Bridging Tools and Agents for Scalable LLM\n  Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool-to-Agent Retrieval: Bridging Tools and Agents for Scalable LLM\n  Multi-Agent Systems"
                },
                "summary": "Recent advances in LLM Multi-Agent Systems enable scalable orchestration of\nsub-agents, each coordinating hundreds or thousands of tools or Model Context\nProtocol (MCP) servers. However, existing retrieval methods typically match\nqueries against coarse agent-level descriptions before routing, which obscures\nfine-grained tool functionality and often results in suboptimal agent\nselection. We introduce Tool-to-Agent Retrieval, a unified framework that\nembeds both tools and their parent agents in a shared vector space and connects\nthem through metadata relationships. By explicitly representing tool\ncapabilities and traversing metadata to the agent level, Tool-to-Agent\nRetrieval enables granular tool-level or agent-level retrieval, ensuring that\nagents and their underlying tools or MCP servers are equally represented\nwithout the context dilution that arises from chunking many tools together.\nEvaluating Tool-to-Agent Retrieval across eight embedding models, our approach\nachieves consistent improvements of 19.4% in Recall@5 and 17.7% in nDCG@5 over\nprevious state-of-the-art agent retrievers on the LiveMCPBench benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in LLM Multi-Agent Systems enable scalable orchestration of\nsub-agents, each coordinating hundreds or thousands of tools or Model Context\nProtocol (MCP) servers. However, existing retrieval methods typically match\nqueries against coarse agent-level descriptions before routing, which obscures\nfine-grained tool functionality and often results in suboptimal agent\nselection. We introduce Tool-to-Agent Retrieval, a unified framework that\nembeds both tools and their parent agents in a shared vector space and connects\nthem through metadata relationships. By explicitly representing tool\ncapabilities and traversing metadata to the agent level, Tool-to-Agent\nRetrieval enables granular tool-level or agent-level retrieval, ensuring that\nagents and their underlying tools or MCP servers are equally represented\nwithout the context dilution that arises from chunking many tools together.\nEvaluating Tool-to-Agent Retrieval across eight embedding models, our approach\nachieves consistent improvements of 19.4% in Recall@5 and 17.7% in nDCG@5 over\nprevious state-of-the-art agent retrievers on the LiveMCPBench benchmark."
                },
                "authors": [
                    {
                        "name": "Elias Lumer"
                    },
                    {
                        "name": "Faheem Nizar"
                    },
                    {
                        "name": "Anmol Gulati"
                    },
                    {
                        "name": "Pradeep Honaganahalli Basavaraju"
                    },
                    {
                        "name": "Vamse Kumar Subbiah"
                    }
                ],
                "author_detail": {
                    "name": "Vamse Kumar Subbiah"
                },
                "author": "Vamse Kumar Subbiah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.01854v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.01854v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08903v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08903v4",
                "updated": "2025-11-04T16:18:24Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    16,
                    18,
                    24,
                    1,
                    308,
                    0
                ],
                "published": "2025-05-13T18:45:10Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    18,
                    45,
                    10,
                    1,
                    133,
                    0
                ],
                "title": "Assessing and Advancing Benchmarks for Evaluating Large Language Models\n  in Software Engineering Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing and Advancing Benchmarks for Evaluating Large Language Models\n  in Software Engineering Tasks"
                },
                "summary": "Large language models (LLMs) are gaining increasing popularity in software\nengineering (SE) due to their unprecedented performance across various\napplications. These models are increasingly being utilized for a range of SE\ntasks, including requirements engineering and design, code analysis and\ngeneration, software maintenance, and quality assurance. As LLMs become more\nintegral to SE, evaluating their effectiveness is crucial for understanding\ntheir potential in this field. In recent years, substantial efforts have been\nmade to assess LLM performance in various SE tasks, resulting in the creation\nof several benchmarks tailored to this purpose. This paper offers a thorough\nreview of 291 benchmarks, addressing three main aspects: what benchmarks are\navailable, how benchmarks are constructed, and the future outlook for these\nbenchmarks. We begin by examining SE tasks such as requirements engineering and\ndesign, coding assistant, software testing, AIOPs, software maintenance, and\nquality management. We then analyze the benchmarks and their development\nprocesses, highlighting the limitations of existing benchmarks. Additionally,\nwe discuss the successes and failures of LLMs in different software tasks and\nexplore future opportunities and challenges for SE-related benchmarks. We aim\nto provide a comprehensive overview of benchmark research in SE and offer\ninsights to support the creation of more effective evaluation tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are gaining increasing popularity in software\nengineering (SE) due to their unprecedented performance across various\napplications. These models are increasingly being utilized for a range of SE\ntasks, including requirements engineering and design, code analysis and\ngeneration, software maintenance, and quality assurance. As LLMs become more\nintegral to SE, evaluating their effectiveness is crucial for understanding\ntheir potential in this field. In recent years, substantial efforts have been\nmade to assess LLM performance in various SE tasks, resulting in the creation\nof several benchmarks tailored to this purpose. This paper offers a thorough\nreview of 291 benchmarks, addressing three main aspects: what benchmarks are\navailable, how benchmarks are constructed, and the future outlook for these\nbenchmarks. We begin by examining SE tasks such as requirements engineering and\ndesign, coding assistant, software testing, AIOPs, software maintenance, and\nquality management. We then analyze the benchmarks and their development\nprocesses, highlighting the limitations of existing benchmarks. Additionally,\nwe discuss the successes and failures of LLMs in different software tasks and\nexplore future opportunities and challenges for SE-related benchmarks. We aim\nto provide a comprehensive overview of benchmark research in SE and offer\ninsights to support the creation of more effective evaluation tools."
                },
                "authors": [
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Feifei Niu"
                    },
                    {
                        "name": "Junkai Chen"
                    },
                    {
                        "name": "Xin Zhou"
                    },
                    {
                        "name": "Junwei Zhang"
                    },
                    {
                        "name": "Junda He"
                    },
                    {
                        "name": "Xin Xia"
                    },
                    {
                        "name": "David Lo"
                    }
                ],
                "author_detail": {
                    "name": "David Lo"
                },
                "author": "David Lo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08903v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08903v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02690v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02690v1",
                "updated": "2025-11-04T16:14:56Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    16,
                    14,
                    56,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T16:14:56Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    16,
                    14,
                    56,
                    1,
                    308,
                    0
                ],
                "title": "Curriculum Design for Trajectory-Constrained Agent: Compressing\n  Chain-of-Thought Tokens in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Curriculum Design for Trajectory-Constrained Agent: Compressing\n  Chain-of-Thought Tokens in LLMs"
                },
                "summary": "Training agents to operate under strict constraints during deployment, such\nas limited resource budgets or stringent safety requirements, presents\nsignificant challenges, especially when these constraints render the task\ncomplex. In this work, we propose a curriculum learning strategy that gradually\ntightens constraints during training, enabling the agent to incrementally\nmaster the deployment requirements. Inspired by self-paced learning techniques\nin unconstrained reinforcement learning (RL), our approach facilitates a\nsmoother transition to challenging environments by initially training on\nsimplified versions of the constraints and progressively introducing the full\ndeployment conditions. We provide a theoretical analysis using an RL agent in a\nbinary-tree Markov Decision Process (MDP) to demonstrate that our curriculum\nstrategy can accelerate training relative to a baseline approach that imposes\nthe trajectory constraints from the outset. Moreover, we empirically validate\nthe effectiveness and generality of our method across both RL and large\nlanguage model (LLM) agents in diverse settings, including a binary-tree MDP, a\nmulti-task navigation domain, and a math reasoning task with two benchmarks.\nThese results highlight the potential of curriculum design in enhancing the\nefficiency and performance of agents operating under complex trajectory\nconstraints during deployment. Moreover, when applied to LLMs, our strategy\nenables compression of output chain-of-thought tokens, achieving a substantial\ninference speedup on consumer hardware, demonstrating its effectiveness for\nresource-constrained deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training agents to operate under strict constraints during deployment, such\nas limited resource budgets or stringent safety requirements, presents\nsignificant challenges, especially when these constraints render the task\ncomplex. In this work, we propose a curriculum learning strategy that gradually\ntightens constraints during training, enabling the agent to incrementally\nmaster the deployment requirements. Inspired by self-paced learning techniques\nin unconstrained reinforcement learning (RL), our approach facilitates a\nsmoother transition to challenging environments by initially training on\nsimplified versions of the constraints and progressively introducing the full\ndeployment conditions. We provide a theoretical analysis using an RL agent in a\nbinary-tree Markov Decision Process (MDP) to demonstrate that our curriculum\nstrategy can accelerate training relative to a baseline approach that imposes\nthe trajectory constraints from the outset. Moreover, we empirically validate\nthe effectiveness and generality of our method across both RL and large\nlanguage model (LLM) agents in diverse settings, including a binary-tree MDP, a\nmulti-task navigation domain, and a math reasoning task with two benchmarks.\nThese results highlight the potential of curriculum design in enhancing the\nefficiency and performance of agents operating under complex trajectory\nconstraints during deployment. Moreover, when applied to LLMs, our strategy\nenables compression of output chain-of-thought tokens, achieving a substantial\ninference speedup on consumer hardware, demonstrating its effectiveness for\nresource-constrained deployment."
                },
                "authors": [
                    {
                        "name": "Georgios Tzannetos"
                    },
                    {
                        "name": "Parameswaran Kamalaruban"
                    },
                    {
                        "name": "Adish Singla"
                    }
                ],
                "author_detail": {
                    "name": "Adish Singla"
                },
                "author": "Adish Singla",
                "arxiv_comment": "NeurIPS'25 paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02690v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02690v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02687v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02687v1",
                "updated": "2025-11-04T16:10:57Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    16,
                    10,
                    57,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T16:10:57Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    16,
                    10,
                    57,
                    1,
                    308,
                    0
                ],
                "title": "The Collaboration Gap",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Collaboration Gap"
                },
                "summary": "The trajectory of AI development suggests that we will increasingly rely on\nagent-based systems composed of independently developed agents with different\ninformation, privileges, and tools. The success of these systems will\ncritically depend on effective collaboration among these heterogeneous agents,\neven under partial observability. Despite intense interest, few empirical\nstudies have evaluated such agent-agent collaboration at scale. We propose a\ncollaborative maze-solving benchmark that (i) isolates collaborative\ncapabilities, (ii) modulates problem complexity, (iii) enables scalable\nautomated grading, and (iv) imposes no output-format constraints, preserving\necological plausibility. Using this framework, we evaluate 32 leading open- and\nclosed-source models in solo, homogeneous, and heterogeneous pairings. Our\nresults reveal a \"collaboration gap\": models that perform well solo often\ndegrade substantially when required to collaborate. Collaboration can break\ndown dramatically; for instance, small distilled models that solve mazes well\nalone may fail almost completely in certain pairings. We find that starting\nwith the stronger agent often improves outcomes, motivating a \"relay inference\"\napproach where the stronger agent leads before handing off to the weaker one,\nclosing much of the gap. Our findings argue for (1) collaboration-aware\nevaluation, (2) training strategies developed to enhance collaborative\ncapabilities, and (3) interaction design that reliably elicits agents' latent\nskills, guidance that applies to AI-AI and human-AI collaboration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The trajectory of AI development suggests that we will increasingly rely on\nagent-based systems composed of independently developed agents with different\ninformation, privileges, and tools. The success of these systems will\ncritically depend on effective collaboration among these heterogeneous agents,\neven under partial observability. Despite intense interest, few empirical\nstudies have evaluated such agent-agent collaboration at scale. We propose a\ncollaborative maze-solving benchmark that (i) isolates collaborative\ncapabilities, (ii) modulates problem complexity, (iii) enables scalable\nautomated grading, and (iv) imposes no output-format constraints, preserving\necological plausibility. Using this framework, we evaluate 32 leading open- and\nclosed-source models in solo, homogeneous, and heterogeneous pairings. Our\nresults reveal a \"collaboration gap\": models that perform well solo often\ndegrade substantially when required to collaborate. Collaboration can break\ndown dramatically; for instance, small distilled models that solve mazes well\nalone may fail almost completely in certain pairings. We find that starting\nwith the stronger agent often improves outcomes, motivating a \"relay inference\"\napproach where the stronger agent leads before handing off to the weaker one,\nclosing much of the gap. Our findings argue for (1) collaboration-aware\nevaluation, (2) training strategies developed to enhance collaborative\ncapabilities, and (3) interaction design that reliably elicits agents' latent\nskills, guidance that applies to AI-AI and human-AI collaboration."
                },
                "authors": [
                    {
                        "name": "Tim R. Davidson"
                    },
                    {
                        "name": "Adam Fourney"
                    },
                    {
                        "name": "Saleema Amershi"
                    },
                    {
                        "name": "Robert West"
                    },
                    {
                        "name": "Eric Horvitz"
                    },
                    {
                        "name": "Ece Kamar"
                    }
                ],
                "author_detail": {
                    "name": "Ece Kamar"
                },
                "author": "Ece Kamar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02687v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02687v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02685v1",
                "updated": "2025-11-04T16:09:28Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    16,
                    9,
                    28,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T16:09:28Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    16,
                    9,
                    28,
                    1,
                    308,
                    0
                ],
                "title": "Modality-Transition Representation Learning for Visible-Infrared Person\n  Re-Identification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modality-Transition Representation Learning for Visible-Infrared Person\n  Re-Identification"
                },
                "summary": "Visible-infrared person re-identification (VI-ReID) technique could associate\nthe pedestrian images across visible and infrared modalities in the practical\nscenarios of background illumination changes. However, a substantial gap\ninherently exists between these two modalities. Besides, existing methods\nprimarily rely on intermediate representations to align cross-modal features of\nthe same person. The intermediate feature representations are usually create by\ngenerating intermediate images (kind of data enhancement), or fusing\nintermediate features (more parameters, lack of interpretability), and they do\nnot make good use of the intermediate features. Thus, we propose a novel\nVI-ReID framework via Modality-Transition Representation Learning (MTRL) with a\nmiddle generated image as a transmitter from visible to infrared modals, which\nare fully aligned with the original visible images and similar to the infrared\nmodality. After that, using a modality-transition contrastive loss and a\nmodality-query regularization loss for training, which could align the\ncross-modal features more effectively. Notably, our proposed framework does not\nneed any additional parameters, which achieves the same inference speed to the\nbackbone while improving its performance on VI-ReID task. Extensive\nexperimental results illustrate that our model significantly and consistently\noutperforms existing SOTAs on three typical VI-ReID datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visible-infrared person re-identification (VI-ReID) technique could associate\nthe pedestrian images across visible and infrared modalities in the practical\nscenarios of background illumination changes. However, a substantial gap\ninherently exists between these two modalities. Besides, existing methods\nprimarily rely on intermediate representations to align cross-modal features of\nthe same person. The intermediate feature representations are usually create by\ngenerating intermediate images (kind of data enhancement), or fusing\nintermediate features (more parameters, lack of interpretability), and they do\nnot make good use of the intermediate features. Thus, we propose a novel\nVI-ReID framework via Modality-Transition Representation Learning (MTRL) with a\nmiddle generated image as a transmitter from visible to infrared modals, which\nare fully aligned with the original visible images and similar to the infrared\nmodality. After that, using a modality-transition contrastive loss and a\nmodality-query regularization loss for training, which could align the\ncross-modal features more effectively. Notably, our proposed framework does not\nneed any additional parameters, which achieves the same inference speed to the\nbackbone while improving its performance on VI-ReID task. Extensive\nexperimental results illustrate that our model significantly and consistently\noutperforms existing SOTAs on three typical VI-ReID datasets."
                },
                "authors": [
                    {
                        "name": "Chao Yuan"
                    },
                    {
                        "name": "Zanwu Liu"
                    },
                    {
                        "name": "Guiwei Zhang"
                    },
                    {
                        "name": "Haoxuan Xu"
                    },
                    {
                        "name": "Yujian Zhao"
                    },
                    {
                        "name": "Guanglin Niu"
                    },
                    {
                        "name": "Bo Li"
                    }
                ],
                "author_detail": {
                    "name": "Bo Li"
                },
                "author": "Bo Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02681v1",
                "updated": "2025-11-04T16:05:25Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    16,
                    5,
                    25,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T16:05:25Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    16,
                    5,
                    25,
                    1,
                    308,
                    0
                ],
                "title": "Optimal Singular Damage: Efficient LLM Inference in Low Storage Regimes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Singular Damage: Efficient LLM Inference in Low Storage Regimes"
                },
                "summary": "Large language models (LLMs) are increasingly prevalent across diverse\napplications. However, their enormous size limits storage and processing\ncapabilities to a few well-resourced stakeholders. As a result, most\napplications rely on pre-trained LLMs, fine-tuned for specific tasks. However,\neven storing the fine-tuned versions of these models remains a significant\nchallenge due to the wide range of tasks they address. Recently, studies show\nthat fine-tuning these models primarily affects a small fraction of parameters,\nhighlighting the need for more efficient storage of fine-tuned models. This\npaper focuses on efficient storage of parameter updates in pre-trained models\nafter fine-tuning. To address this challenge, we leverage the observation that\nfine-tuning updates are both low-rank and sparse, which can be utilized for\nstorage efficiency. However, using only low-rank approximation or\nsparsification may discard critical singular components that enhance model\nexpressivity. We first observe that given the same memory budget, sparsified\nlow-rank approximations with larger ranks outperform standard low-rank\napproximations with smaller ranks. Building on this, we propose our method,\noptimal singular damage, that selectively sparsifies low-rank approximated\nupdates by leveraging the interleaved importance of singular vectors, ensuring\nthat the most impactful components are retained. We demonstrate through\nextensive experiments that our proposed methods lead to significant storage\nefficiency and superior accuracy within the same memory budget compared to\nemploying the low-rank approximation or sparsification individually.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly prevalent across diverse\napplications. However, their enormous size limits storage and processing\ncapabilities to a few well-resourced stakeholders. As a result, most\napplications rely on pre-trained LLMs, fine-tuned for specific tasks. However,\neven storing the fine-tuned versions of these models remains a significant\nchallenge due to the wide range of tasks they address. Recently, studies show\nthat fine-tuning these models primarily affects a small fraction of parameters,\nhighlighting the need for more efficient storage of fine-tuned models. This\npaper focuses on efficient storage of parameter updates in pre-trained models\nafter fine-tuning. To address this challenge, we leverage the observation that\nfine-tuning updates are both low-rank and sparse, which can be utilized for\nstorage efficiency. However, using only low-rank approximation or\nsparsification may discard critical singular components that enhance model\nexpressivity. We first observe that given the same memory budget, sparsified\nlow-rank approximations with larger ranks outperform standard low-rank\napproximations with smaller ranks. Building on this, we propose our method,\noptimal singular damage, that selectively sparsifies low-rank approximated\nupdates by leveraging the interleaved importance of singular vectors, ensuring\nthat the most impactful components are retained. We demonstrate through\nextensive experiments that our proposed methods lead to significant storage\nefficiency and superior accuracy within the same memory budget compared to\nemploying the low-rank approximation or sparsification individually."
                },
                "authors": [
                    {
                        "name": "Mohammadsajad Alipour"
                    },
                    {
                        "name": "Mohammad Mohammadi Amiri"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Mohammadi Amiri"
                },
                "author": "Mohammad Mohammadi Amiri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21925v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21925v2",
                "updated": "2025-11-04T15:56:14Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    15,
                    56,
                    14,
                    1,
                    308,
                    0
                ],
                "published": "2024-10-29T10:35:13Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    10,
                    35,
                    13,
                    1,
                    303,
                    0
                ],
                "title": "Redshift-Space Distortion constraints on neutrino mass and models to\n  alleviate the Hubble tension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Redshift-Space Distortion constraints on neutrino mass and models to\n  alleviate the Hubble tension"
                },
                "summary": "We discuss the neutrino mass and Hubble tension solutions and examine their\neffects on the Redshift-Space Distortion (RSD) observations. An analysis with\nRSD data indicates smaller amplitude of perturbation. Including RSD data\nresults in a slightly weaker upper limit on the neutrino mass than that derived\nfor data without RSD, which is common in other extended models too. We have\nevaluated the impacts of RSD observations on some extended models, including\nthe varying electron mass model, a time-dependent dark energy model with two\nparameter equations of state (EOS), and a model where the number of neutrino\nspecies is free. When we estimate the cosmological parameters for data\nincluding RSD, we found that the EOS parameter for dark energy is larger than\nthat of the cosmological constant, and the effective number of neutrino species\nis smaller than the standard value, which infers a smaller present Hubble\nparameter $H_0$. From the viewpoint of cosmological tensions, the varying\nelectron mass model with non-zero neutrino mass option looks promising to relax\nthe Hubble tension and the $S_8$ tension simultaneously.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We discuss the neutrino mass and Hubble tension solutions and examine their\neffects on the Redshift-Space Distortion (RSD) observations. An analysis with\nRSD data indicates smaller amplitude of perturbation. Including RSD data\nresults in a slightly weaker upper limit on the neutrino mass than that derived\nfor data without RSD, which is common in other extended models too. We have\nevaluated the impacts of RSD observations on some extended models, including\nthe varying electron mass model, a time-dependent dark energy model with two\nparameter equations of state (EOS), and a model where the number of neutrino\nspecies is free. When we estimate the cosmological parameters for data\nincluding RSD, we found that the EOS parameter for dark energy is larger than\nthat of the cosmological constant, and the effective number of neutrino species\nis smaller than the standard value, which infers a smaller present Hubble\nparameter $H_0$. From the viewpoint of cosmological tensions, the varying\nelectron mass model with non-zero neutrino mass option looks promising to relax\nthe Hubble tension and the $S_8$ tension simultaneously."
                },
                "authors": [
                    {
                        "name": "Yo Toda"
                    },
                    {
                        "name": "Osamu Seto"
                    }
                ],
                "author_detail": {
                    "name": "Osamu Seto"
                },
                "author": "Osamu Seto",
                "arxiv_doi": "10.1103/PhysRevD.111.083551",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevD.111.083551",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.21925v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21925v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "22 pages, 6 figures",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07109v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07109v3",
                "updated": "2025-11-04T15:55:56Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    15,
                    55,
                    56,
                    1,
                    308,
                    0
                ],
                "published": "2024-10-09T17:45:47Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    45,
                    47,
                    2,
                    283,
                    0
                ],
                "title": "I Want to Break Free! Persuasion and Anti-Social Behavior of LLMs in\n  Multi-Agent Settings with Social Hierarchy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I Want to Break Free! Persuasion and Anti-Social Behavior of LLMs in\n  Multi-Agent Settings with Social Hierarchy"
                },
                "summary": "As LLM-based agents become increasingly autonomous and will more freely\ninteract with each other, studying the interplay among them becomes crucial to\nanticipate emergent phenomena and potential risks. In this work, we provide an\nin-depth analysis of the interactions among agents within a simulated\nhierarchical social environment, drawing inspiration from the Stanford Prison\nExperiment. Leveraging 2,400 conversations across six LLMs (i.e., LLama3,\nOrca2, Command-r, Mixtral, Mistral2, and gpt4.1) and 240 experimental\nscenarios, we analyze persuasion and anti-social behavior between a guard and a\nprisoner agent with differing objectives. We first document model-specific\nconversational failures in this multi-agent power dynamic context, thereby\nnarrowing our analytic sample to 1,600 conversations. Among models\ndemonstrating successful interaction, we find that goal setting significantly\ninfluences persuasiveness but not anti-social behavior. Moreover, agent\npersonas, especially the guard's, substantially impact both successful\npersuasion by the prisoner and the manifestation of anti-social actions.\nNotably, we observe the emergence of anti-social conduct even in absence of\nexplicit negative personality prompts. These results have important\nimplications for the development of interactive LLM agents and the ongoing\ndiscussion of their societal impact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As LLM-based agents become increasingly autonomous and will more freely\ninteract with each other, studying the interplay among them becomes crucial to\nanticipate emergent phenomena and potential risks. In this work, we provide an\nin-depth analysis of the interactions among agents within a simulated\nhierarchical social environment, drawing inspiration from the Stanford Prison\nExperiment. Leveraging 2,400 conversations across six LLMs (i.e., LLama3,\nOrca2, Command-r, Mixtral, Mistral2, and gpt4.1) and 240 experimental\nscenarios, we analyze persuasion and anti-social behavior between a guard and a\nprisoner agent with differing objectives. We first document model-specific\nconversational failures in this multi-agent power dynamic context, thereby\nnarrowing our analytic sample to 1,600 conversations. Among models\ndemonstrating successful interaction, we find that goal setting significantly\ninfluences persuasiveness but not anti-social behavior. Moreover, agent\npersonas, especially the guard's, substantially impact both successful\npersuasion by the prisoner and the manifestation of anti-social actions.\nNotably, we observe the emergence of anti-social conduct even in absence of\nexplicit negative personality prompts. These results have important\nimplications for the development of interactive LLM agents and the ongoing\ndiscussion of their societal impact."
                },
                "authors": [
                    {
                        "name": "Gian Maria Campedelli"
                    },
                    {
                        "name": "Nicolò Penzo"
                    },
                    {
                        "name": "Massimo Stefan"
                    },
                    {
                        "name": "Roberto Dessì"
                    },
                    {
                        "name": "Marco Guerini"
                    },
                    {
                        "name": "Bruno Lepri"
                    },
                    {
                        "name": "Jacopo Staiano"
                    }
                ],
                "author_detail": {
                    "name": "Jacopo Staiano"
                },
                "author": "Jacopo Staiano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07109v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07109v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02674v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02674v1",
                "updated": "2025-11-04T15:54:33Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    15,
                    54,
                    33,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T15:54:33Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    15,
                    54,
                    33,
                    1,
                    308,
                    0
                ],
                "title": "EasyTUS: A Comprehensive Framework for Fast and Accurate Table Union\n  Search across Data Lakes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EasyTUS: A Comprehensive Framework for Fast and Accurate Table Union\n  Search across Data Lakes"
                },
                "summary": "Data lakes enable easy maintenance of heterogeneous data in its native form.\nWhile this flexibility can accelerate data ingestion, it shifts the complexity\nof data preparation and query processing to data discovery tasks. One such task\nis Table Union Search (TUS), which identifies tables that can be unioned with a\ngiven input table. In this work, we present EasyTUS, a comprehensive framework\nthat leverages Large Language Models (LLMs) to perform efficient and scalable\nTable Union Search across data lakes. EasyTUS implements the search pipeline as\nthree modular steps: Table Serialization for consistent formatting and\nsampling, Table Representation that utilizes LLMs to generate embeddings, and\nVector Search that leverages approximate nearest neighbor indexing for semantic\nmatching. To enable reproducible and systematic evaluation, in this paper, we\nalso introduce TUSBench, a novel standardized benchmarking environment within\nthe EasyTUS framework. TUSBench supports unified comparisons across approaches\nand data lakes, promoting transparency and progress in the field. Our\nexperiments using TUSBench show that EasyTUS consistently outperforms most of\nthe state-of the-art approaches, achieving improvements in average of up to\n34.3% in Mean Average Precision (MAP), up to 79.2x speedup in data preparation,\nand up to 7.7x faster query processing performance. Furthermore, EasyTUS\nmaintains strong performance even in metadata-absent settings, highlighting its\nrobustness and adaptability across data lakes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data lakes enable easy maintenance of heterogeneous data in its native form.\nWhile this flexibility can accelerate data ingestion, it shifts the complexity\nof data preparation and query processing to data discovery tasks. One such task\nis Table Union Search (TUS), which identifies tables that can be unioned with a\ngiven input table. In this work, we present EasyTUS, a comprehensive framework\nthat leverages Large Language Models (LLMs) to perform efficient and scalable\nTable Union Search across data lakes. EasyTUS implements the search pipeline as\nthree modular steps: Table Serialization for consistent formatting and\nsampling, Table Representation that utilizes LLMs to generate embeddings, and\nVector Search that leverages approximate nearest neighbor indexing for semantic\nmatching. To enable reproducible and systematic evaluation, in this paper, we\nalso introduce TUSBench, a novel standardized benchmarking environment within\nthe EasyTUS framework. TUSBench supports unified comparisons across approaches\nand data lakes, promoting transparency and progress in the field. Our\nexperiments using TUSBench show that EasyTUS consistently outperforms most of\nthe state-of the-art approaches, achieving improvements in average of up to\n34.3% in Mean Average Precision (MAP), up to 79.2x speedup in data preparation,\nand up to 7.7x faster query processing performance. Furthermore, EasyTUS\nmaintains strong performance even in metadata-absent settings, highlighting its\nrobustness and adaptability across data lakes."
                },
                "authors": [
                    {
                        "name": "Tim Otto"
                    }
                ],
                "author_detail": {
                    "name": "Tim Otto"
                },
                "author": "Tim Otto",
                "arxiv_comment": "Copyright 2025 IEEE. This is the author's version of the work that\n  has been accepted for publication in Proceedings of the IEEE International\n  Conference on Big Data (IEEE BigData 2025). The final version of record is\n  available at: tba",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02674v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02674v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24303v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24303v2",
                "updated": "2025-11-04T15:46:42Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    15,
                    46,
                    42,
                    1,
                    308,
                    0
                ],
                "published": "2025-10-28T11:12:43Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    11,
                    12,
                    43,
                    1,
                    301,
                    0
                ],
                "title": "Retrieval and Argumentation Enhanced Multi-Agent LLMs for Judgmental\n  Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval and Argumentation Enhanced Multi-Agent LLMs for Judgmental\n  Forecasting"
                },
                "summary": "Judgmental forecasting is the task of making predictions about future events\nbased on human judgment. This task can be seen as a form of claim verification,\nwhere the claim corresponds to a future event and the task is to assess the\nplausibility of that event. In this paper, we propose a novel multi-agent\nframework for claim verification, whereby different agents may disagree on\nclaim veracity and bring specific evidence for and against the claims,\nrepresented as quantitative bipolar argumentation frameworks (QBAFs). We then\ninstantiate the framework for supporting claim verification, with a variety of\nagents realised with Large Language Models (LLMs): (1) ArgLLM agents, an\nexisting approach for claim verification that generates and evaluates QBAFs;\n(2) RbAM agents, whereby LLM-empowered Relation-based Argument Mining (RbAM)\nfrom external sources is used to generate QBAFs; (3) RAG-ArgLLM agents,\nextending ArgLLM agents with a form of Retrieval-Augmented Generation (RAG) of\narguments from external sources. Finally, we conduct experiments with two\nstandard judgmental forecasting datasets, with instances of our framework with\ntwo or three agents, empowered by six different base LLMs. We observe that\ncombining evidence from agents can improve forecasting accuracy, especially in\nthe case of three agents, while providing an explainable combination of\nevidence for claim verification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Judgmental forecasting is the task of making predictions about future events\nbased on human judgment. This task can be seen as a form of claim verification,\nwhere the claim corresponds to a future event and the task is to assess the\nplausibility of that event. In this paper, we propose a novel multi-agent\nframework for claim verification, whereby different agents may disagree on\nclaim veracity and bring specific evidence for and against the claims,\nrepresented as quantitative bipolar argumentation frameworks (QBAFs). We then\ninstantiate the framework for supporting claim verification, with a variety of\nagents realised with Large Language Models (LLMs): (1) ArgLLM agents, an\nexisting approach for claim verification that generates and evaluates QBAFs;\n(2) RbAM agents, whereby LLM-empowered Relation-based Argument Mining (RbAM)\nfrom external sources is used to generate QBAFs; (3) RAG-ArgLLM agents,\nextending ArgLLM agents with a form of Retrieval-Augmented Generation (RAG) of\narguments from external sources. Finally, we conduct experiments with two\nstandard judgmental forecasting datasets, with instances of our framework with\ntwo or three agents, empowered by six different base LLMs. We observe that\ncombining evidence from agents can improve forecasting accuracy, especially in\nthe case of three agents, while providing an explainable combination of\nevidence for claim verification."
                },
                "authors": [
                    {
                        "name": "Deniz Gorur"
                    },
                    {
                        "name": "Antonio Rago"
                    },
                    {
                        "name": "Francesca Toni"
                    }
                ],
                "author_detail": {
                    "name": "Francesca Toni"
                },
                "author": "Francesca Toni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24303v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24303v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.04426v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.04426v2",
                "updated": "2025-11-04T15:42:13Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    15,
                    42,
                    13,
                    1,
                    308,
                    0
                ],
                "published": "2025-03-06T13:35:59Z",
                "published_parsed": [
                    2025,
                    3,
                    6,
                    13,
                    35,
                    59,
                    3,
                    65,
                    0
                ],
                "title": "FORTALESA: Fault-Tolerant Reconfigurable Systolic Array for DNN\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FORTALESA: Fault-Tolerant Reconfigurable Systolic Array for DNN\n  Inference"
                },
                "summary": "The emergence of Deep Neural Networks (DNNs) in mission- and safety-critical\napplications brings their reliability to the front. High performance demands of\nDNNs require the use of specialized hardware accelerators. Systolic array\narchitecture is widely used in DNN accelerators due to its parallelism and\nregular structure. This work presents a run-time reconfigurable systolic array\narchitecture with three execution modes and four implementation options. All\nfour implementations are evaluated in terms of resource utilization,\nthroughput, and fault tolerance improvement. The proposed architecture is used\nfor reliability enhancement of DNN inference on systolic array through\nheterogeneous mapping of different network layers to different execution modes.\nThe approach is supported by a novel reliability assessment method based on\nfault propagation analysis. It is used for the exploration of the appropriate\nexecution mode--layer mapping for DNN inference. The proposed architecture\nefficiently protects registers and MAC units of systolic array PEs from\ntransient and permanent faults. The reconfigurability feature enables a speedup\nof up to $3\\times$, depending on layer vulnerability. Furthermore, it requires\n$6\\times$ fewer resources compared to static redundancy and $2.5\\times$ fewer\nresources compared to the previously proposed solution for transient faults.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of Deep Neural Networks (DNNs) in mission- and safety-critical\napplications brings their reliability to the front. High performance demands of\nDNNs require the use of specialized hardware accelerators. Systolic array\narchitecture is widely used in DNN accelerators due to its parallelism and\nregular structure. This work presents a run-time reconfigurable systolic array\narchitecture with three execution modes and four implementation options. All\nfour implementations are evaluated in terms of resource utilization,\nthroughput, and fault tolerance improvement. The proposed architecture is used\nfor reliability enhancement of DNN inference on systolic array through\nheterogeneous mapping of different network layers to different execution modes.\nThe approach is supported by a novel reliability assessment method based on\nfault propagation analysis. It is used for the exploration of the appropriate\nexecution mode--layer mapping for DNN inference. The proposed architecture\nefficiently protects registers and MAC units of systolic array PEs from\ntransient and permanent faults. The reconfigurability feature enables a speedup\nof up to $3\\times$, depending on layer vulnerability. Furthermore, it requires\n$6\\times$ fewer resources compared to static redundancy and $2.5\\times$ fewer\nresources compared to the previously proposed solution for transient faults."
                },
                "authors": [
                    {
                        "name": "Natalia Cherezova"
                    },
                    {
                        "name": "Artur Jutman"
                    },
                    {
                        "name": "Maksim Jenihhin"
                    }
                ],
                "author_detail": {
                    "name": "Maksim Jenihhin"
                },
                "author": "Maksim Jenihhin",
                "arxiv_doi": "10.1016/j.micpro.2025.105222",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.micpro.2025.105222",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.04426v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.04426v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "13 pages, 15 figures",
                "arxiv_journal_ref": "Cherezova, N., Jutman, A., & Jenihhin, M. (2025). FORTALESA:\n  Fault-tolerant reconfigurable systolic array for DNN inference.\n  Microprocessors and Microsystems, 119, 105222",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25223v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25223v2",
                "updated": "2025-11-04T15:40:16Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    15,
                    40,
                    16,
                    1,
                    308,
                    0
                ],
                "published": "2025-10-29T06:57:32Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    6,
                    57,
                    32,
                    2,
                    302,
                    0
                ],
                "title": "FELA: A Multi-Agent Evolutionary System for Feature Engineering of\n  Industrial Event Log Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FELA: A Multi-Agent Evolutionary System for Feature Engineering of\n  Industrial Event Log Data"
                },
                "summary": "Event log data, recording fine-grained user actions and system events,\nrepresent one of the most valuable assets for modern digital services. However,\nthe complexity and heterogeneity of industrial event logs--characterized by\nlarge scale, high dimensionality, diverse data types, and intricate temporal or\nrelational structures--make feature engineering extremely challenging. Existing\nautomatic feature engineering approaches, such as AutoML or genetic methods,\noften suffer from limited explainability, rigid predefined operations, and poor\nadaptability to complicated heterogeneous data. In this paper, we propose FELA\n(Feature Engineering LLM Agents), a multi-agent evolutionary system that\nautonomously extracts meaningful and high-performing features from complex\nindustrial event log data. FELA integrates the reasoning and coding\ncapabilities of large language models (LLMs) with an insight-guided\nself-evolution paradigm. Specifically, FELA employs specialized agents--Idea\nAgents, Code Agents, and Critic Agents--to collaboratively generate, validate,\nand implement novel feature ideas. An Evaluation Agent summarizes feedback and\nupdates a hierarchical knowledge base and dual-memory system to enable\ncontinual improvement. Moreover, FELA introduces an agentic evolution\nalgorithm, combining reinforcement learning and genetic algorithm principles to\nbalance exploration and exploitation across the idea space. Extensive\nexperiments on real industrial datasets demonstrate that FELA can generate\nexplainable, domain-relevant features that significantly improve model\nperformance while reducing manual effort. Our results highlight the potential\nof LLM-based multi-agent systems as a general framework for automated,\ninterpretable, and adaptive feature engineering in complex real-world\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Event log data, recording fine-grained user actions and system events,\nrepresent one of the most valuable assets for modern digital services. However,\nthe complexity and heterogeneity of industrial event logs--characterized by\nlarge scale, high dimensionality, diverse data types, and intricate temporal or\nrelational structures--make feature engineering extremely challenging. Existing\nautomatic feature engineering approaches, such as AutoML or genetic methods,\noften suffer from limited explainability, rigid predefined operations, and poor\nadaptability to complicated heterogeneous data. In this paper, we propose FELA\n(Feature Engineering LLM Agents), a multi-agent evolutionary system that\nautonomously extracts meaningful and high-performing features from complex\nindustrial event log data. FELA integrates the reasoning and coding\ncapabilities of large language models (LLMs) with an insight-guided\nself-evolution paradigm. Specifically, FELA employs specialized agents--Idea\nAgents, Code Agents, and Critic Agents--to collaboratively generate, validate,\nand implement novel feature ideas. An Evaluation Agent summarizes feedback and\nupdates a hierarchical knowledge base and dual-memory system to enable\ncontinual improvement. Moreover, FELA introduces an agentic evolution\nalgorithm, combining reinforcement learning and genetic algorithm principles to\nbalance exploration and exploitation across the idea space. Extensive\nexperiments on real industrial datasets demonstrate that FELA can generate\nexplainable, domain-relevant features that significantly improve model\nperformance while reducing manual effort. Our results highlight the potential\nof LLM-based multi-agent systems as a general framework for automated,\ninterpretable, and adaptive feature engineering in complex real-world\nenvironments."
                },
                "authors": [
                    {
                        "name": "Kun Ouyang"
                    },
                    {
                        "name": "Haoyu Wang"
                    },
                    {
                        "name": "Dong Fang"
                    }
                ],
                "author_detail": {
                    "name": "Dong Fang"
                },
                "author": "Dong Fang",
                "arxiv_comment": "14 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25223v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25223v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.00689v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.00689v2",
                "updated": "2025-11-04T15:19:44Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    15,
                    19,
                    44,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-01T20:12:19Z",
                "published_parsed": [
                    2025,
                    11,
                    1,
                    20,
                    12,
                    19,
                    5,
                    305,
                    0
                ],
                "title": "Do Methods to Jailbreak and Defend LLMs Generalize Across Languages?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Methods to Jailbreak and Defend LLMs Generalize Across Languages?"
                },
                "summary": "Large language models (LLMs) undergo safety alignment after training and\ntuning, yet recent work shows that safety can be bypassed through jailbreak\nattacks. While many jailbreaks and defenses exist, their cross-lingual\ngeneralization remains underexplored. This paper presents the first systematic\nmultilingual evaluation of jailbreaks and defenses across ten languages --\nspanning high-, medium-, and low-resource languages -- using six LLMs on\nHarmBench and AdvBench. We assess two jailbreak types: logical-expression-based\nand adversarial-prompt-based. For both types, attack success and defense\nrobustness vary across languages: high-resource languages are safer under\nstandard queries but more vulnerable to adversarial ones. Simple defenses can\nbe effective, but are language- and model-dependent. These findings call for\nlanguage-aware and cross-lingual safety benchmarks for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) undergo safety alignment after training and\ntuning, yet recent work shows that safety can be bypassed through jailbreak\nattacks. While many jailbreaks and defenses exist, their cross-lingual\ngeneralization remains underexplored. This paper presents the first systematic\nmultilingual evaluation of jailbreaks and defenses across ten languages --\nspanning high-, medium-, and low-resource languages -- using six LLMs on\nHarmBench and AdvBench. We assess two jailbreak types: logical-expression-based\nand adversarial-prompt-based. For both types, attack success and defense\nrobustness vary across languages: high-resource languages are safer under\nstandard queries but more vulnerable to adversarial ones. Simple defenses can\nbe effective, but are language- and model-dependent. These findings call for\nlanguage-aware and cross-lingual safety benchmarks for LLMs."
                },
                "authors": [
                    {
                        "name": "Berk Atil"
                    },
                    {
                        "name": "Rebecca J. Passonneau"
                    },
                    {
                        "name": "Fred Morstatter"
                    }
                ],
                "author_detail": {
                    "name": "Fred Morstatter"
                },
                "author": "Fred Morstatter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.00689v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.00689v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02653v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02653v1",
                "updated": "2025-11-04T15:18:37Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    15,
                    18,
                    37,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T15:18:37Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    15,
                    18,
                    37,
                    1,
                    308,
                    0
                ],
                "title": "A Bayesian Inference of Hybrid Stars with Large Quark Cores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Bayesian Inference of Hybrid Stars with Large Quark Cores"
                },
                "summary": "Neutron stars (NSs) are interesting objects capable of reaching densities\nunattainable on Earth. The properties of matter under these conditions remain a\nmystery. Exotic matter, including quark matter, may be present in the NS core.\nIn this work, we explore the possible compositions of NS cores, in particular,\nthe possible existence of large quark cores. We use the Relativistic Mean Field\n(RMF) model with nonlinear terms for the hadron phase and the\nNambu-Jona-Lasinio (NJL) model and Mean Field Theory of Quantum Chromodynamics\n(MFTQCD) for the quark phase. Through Bayesian inference, we obtain different\nsets of equations: four sets with hybrid equations (three using the NJL model\nand the other using the MFTQCD model), and one set with only the hadron phase.\nWe impose constraints regarding the properties of nuclear matter, X-ray\nobservational data from NICER, perturbative QCD (pQCD) calculations, and\ncausality on all sets. One set of hybrid NJL equations of state was also\nconstrained by adding the GW170817 detection. All sets can describe\nobservational data and theoretical restrictions. The MFTQCD allows for a phase\ntransition to quark matter at lower densities compared to the NJL models. The\nMFTQCD model indicates that NSs with 1.4 solar mass have quark matter in their\ninner core. However, NJL models suggest that it is more probable that 1.4 solar\nmass NSs do not contain quark matter. Both the MFTQCD and NJL models agree that\nthere is quark matter in 2 solar mass NSs. It is discussed that hybrid stars\nwith a stiff quark equation of state could explain a larger radius of more\nmassive stars, such as two solar mass stars, with respect to the canonical NS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neutron stars (NSs) are interesting objects capable of reaching densities\nunattainable on Earth. The properties of matter under these conditions remain a\nmystery. Exotic matter, including quark matter, may be present in the NS core.\nIn this work, we explore the possible compositions of NS cores, in particular,\nthe possible existence of large quark cores. We use the Relativistic Mean Field\n(RMF) model with nonlinear terms for the hadron phase and the\nNambu-Jona-Lasinio (NJL) model and Mean Field Theory of Quantum Chromodynamics\n(MFTQCD) for the quark phase. Through Bayesian inference, we obtain different\nsets of equations: four sets with hybrid equations (three using the NJL model\nand the other using the MFTQCD model), and one set with only the hadron phase.\nWe impose constraints regarding the properties of nuclear matter, X-ray\nobservational data from NICER, perturbative QCD (pQCD) calculations, and\ncausality on all sets. One set of hybrid NJL equations of state was also\nconstrained by adding the GW170817 detection. All sets can describe\nobservational data and theoretical restrictions. The MFTQCD allows for a phase\ntransition to quark matter at lower densities compared to the NJL models. The\nMFTQCD model indicates that NSs with 1.4 solar mass have quark matter in their\ninner core. However, NJL models suggest that it is more probable that 1.4 solar\nmass NSs do not contain quark matter. Both the MFTQCD and NJL models agree that\nthere is quark matter in 2 solar mass NSs. It is discussed that hybrid stars\nwith a stiff quark equation of state could explain a larger radius of more\nmassive stars, such as two solar mass stars, with respect to the canonical NS."
                },
                "authors": [
                    {
                        "name": "Milena Albino"
                    },
                    {
                        "name": "Tuhin Malik"
                    },
                    {
                        "name": "Márcio Ferreira"
                    },
                    {
                        "name": "Constança Providência"
                    }
                ],
                "author_detail": {
                    "name": "Constança Providência"
                },
                "author": "Constança Providência",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02653v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02653v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "nucl-th",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02651v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02651v1",
                "updated": "2025-11-04T15:17:43Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    15,
                    17,
                    43,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T15:17:43Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    15,
                    17,
                    43,
                    1,
                    308,
                    0
                ],
                "title": "Apriel-H1: Towards Efficient Enterprise Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Apriel-H1: Towards Efficient Enterprise Reasoning Models"
                },
                "summary": "Large Language Models (LLMs) achieve remarkable reasoning capabilities\nthrough transformer architectures with attention mechanisms. However,\ntransformers suffer from quadratic time and memory complexity in the attention\nmodule (MHA) and require caching key-value states during inference, which\nseverely limits throughput and scalability. High inference throughput is\ncritical for agentic tasks, long-context reasoning, efficient deployment under\nhigh request loads, and more efficient test-time compute scaling.\n  State Space Models (SSMs) such as Mamba offer a promising alternative with\nlinear inference complexity and a constant memory footprint via recurrent\ncomputation with fixed-size hidden states. In this technical report we\nintroduce the Apriel-H1 family of hybrid LLMs that combine transformer\nattention and SSM sequence mixers for efficient reasoning at 15B model size.\nThese models are obtained through incremental distillation from a pretrained\nreasoning transformer, Apriel-Nemotron-15B-Thinker, progressively replacing\nless critical attention layers with linear Mamba blocks.\n  We release multiple post-distillation variants of Apriel-H1-15B-Thinker with\ndifferent SSM-to-MHA ratios and analyse how reasoning performance degrades as\nmore Mamba layers replace MHA. Additionally, we release a 30/50 hybrid variant\nof Apriel-H1, further fine-tuned on a supervised dataset of reasoning traces,\nachieving over 2x higher inference throughput when deployed in the\nproduction-ready vLLM environment, with minimal degradation in reasoning\nperformance. This shows that distilled hybrid SSM-Transformer architectures can\ndeliver substantial efficiency gains over the pretrained transformer equivalent\nwithout substantially compromising the reasoning quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) achieve remarkable reasoning capabilities\nthrough transformer architectures with attention mechanisms. However,\ntransformers suffer from quadratic time and memory complexity in the attention\nmodule (MHA) and require caching key-value states during inference, which\nseverely limits throughput and scalability. High inference throughput is\ncritical for agentic tasks, long-context reasoning, efficient deployment under\nhigh request loads, and more efficient test-time compute scaling.\n  State Space Models (SSMs) such as Mamba offer a promising alternative with\nlinear inference complexity and a constant memory footprint via recurrent\ncomputation with fixed-size hidden states. In this technical report we\nintroduce the Apriel-H1 family of hybrid LLMs that combine transformer\nattention and SSM sequence mixers for efficient reasoning at 15B model size.\nThese models are obtained through incremental distillation from a pretrained\nreasoning transformer, Apriel-Nemotron-15B-Thinker, progressively replacing\nless critical attention layers with linear Mamba blocks.\n  We release multiple post-distillation variants of Apriel-H1-15B-Thinker with\ndifferent SSM-to-MHA ratios and analyse how reasoning performance degrades as\nmore Mamba layers replace MHA. Additionally, we release a 30/50 hybrid variant\nof Apriel-H1, further fine-tuned on a supervised dataset of reasoning traces,\nachieving over 2x higher inference throughput when deployed in the\nproduction-ready vLLM environment, with minimal degradation in reasoning\nperformance. This shows that distilled hybrid SSM-Transformer architectures can\ndeliver substantial efficiency gains over the pretrained transformer equivalent\nwithout substantially compromising the reasoning quality."
                },
                "authors": [
                    {
                        "name": "Oleksiy Ostapenko"
                    },
                    {
                        "name": "Luke Kumar"
                    },
                    {
                        "name": "Raymond Li"
                    },
                    {
                        "name": "Denis Kocetkov"
                    },
                    {
                        "name": "Joel Lamy-Poirier"
                    },
                    {
                        "name": "Shruthan Radhakrishna"
                    },
                    {
                        "name": "Soham Parikh"
                    },
                    {
                        "name": "Shambhavi Mishra"
                    },
                    {
                        "name": "Sebastien Paquet"
                    },
                    {
                        "name": "Srinivas Sunkara"
                    },
                    {
                        "name": "Valérie Bécaert"
                    },
                    {
                        "name": "Sathwik Tejaswi Madhusudhan"
                    },
                    {
                        "name": "Torsten Scholak"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Scholak"
                },
                "author": "Torsten Scholak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02651v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02651v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02650v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02650v1",
                "updated": "2025-11-04T15:17:06Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    15,
                    17,
                    6,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T15:17:06Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    15,
                    17,
                    6,
                    1,
                    308,
                    0
                ],
                "title": "Can Visual Input Be Compressed? A Visual Token Compression Benchmark for\n  Large Multimodal Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Visual Input Be Compressed? A Visual Token Compression Benchmark for\n  Large Multimodal Models"
                },
                "summary": "Large multimodal models (LMMs) often suffer from severe inference\ninefficiency due to the large number of visual tokens introduced by image\nencoders. While recent token compression methods, such as pruning and merging,\nhave shown promise in reducing redundancy, their evaluation remains fragmented\nand inconsistent. In this work, we present UniPruneBench, a unified and\nextensible benchmark for visual token pruning in multimodal LLMs. UniPruneBench\nprovides standardized protocols across six ability dimensions and ten datasets,\ncovering ten representative compression algorithms and three families of LMMs\n(LLaVA-v1.5, Intern-VL3, and Qwen2.5-VL). Beyond task accuracy, it incorporates\nsystem-level metrics such as runtime and prefilling latency to provide a\nholistic view. Our experiments uncover several key findings: (1) random pruning\nis a surprisingly strong baseline, (2) no single method consistently\noutperforms others across scenarios, (3) pruning sensitivity varies\nsignificantly across tasks, with OCR being most vulnerable, and (4) pruning\nratio is the dominant factor governing performance degradation. We believe\nUniPruneBench will serve as a reliable foundation for future research on\nefficient multimodal modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large multimodal models (LMMs) often suffer from severe inference\ninefficiency due to the large number of visual tokens introduced by image\nencoders. While recent token compression methods, such as pruning and merging,\nhave shown promise in reducing redundancy, their evaluation remains fragmented\nand inconsistent. In this work, we present UniPruneBench, a unified and\nextensible benchmark for visual token pruning in multimodal LLMs. UniPruneBench\nprovides standardized protocols across six ability dimensions and ten datasets,\ncovering ten representative compression algorithms and three families of LMMs\n(LLaVA-v1.5, Intern-VL3, and Qwen2.5-VL). Beyond task accuracy, it incorporates\nsystem-level metrics such as runtime and prefilling latency to provide a\nholistic view. Our experiments uncover several key findings: (1) random pruning\nis a surprisingly strong baseline, (2) no single method consistently\noutperforms others across scenarios, (3) pruning sensitivity varies\nsignificantly across tasks, with OCR being most vulnerable, and (4) pruning\nratio is the dominant factor governing performance degradation. We believe\nUniPruneBench will serve as a reliable foundation for future research on\nefficient multimodal modeling."
                },
                "authors": [
                    {
                        "name": "Tianfan Peng"
                    },
                    {
                        "name": "Yuntao Du"
                    },
                    {
                        "name": "Pengzhou Ji"
                    },
                    {
                        "name": "Shijie Dong"
                    },
                    {
                        "name": "Kailin Jiang"
                    },
                    {
                        "name": "Mingchuan Ma"
                    },
                    {
                        "name": "Yijun Tian"
                    },
                    {
                        "name": "Jinhe Bi"
                    },
                    {
                        "name": "Qian Li"
                    },
                    {
                        "name": "Wei Du"
                    },
                    {
                        "name": "Feng Xiao"
                    },
                    {
                        "name": "Lizhen Cui"
                    }
                ],
                "author_detail": {
                    "name": "Lizhen Cui"
                },
                "author": "Lizhen Cui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02650v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02650v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02647v1",
                "updated": "2025-11-04T15:14:58Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    15,
                    14,
                    58,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T15:14:58Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    15,
                    14,
                    58,
                    1,
                    308,
                    0
                ],
                "title": "Federated Attention: A Distributed Paradigm for Collaborative LLM\n  Inference over Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Attention: A Distributed Paradigm for Collaborative LLM\n  Inference over Edge Networks"
                },
                "summary": "Large language models (LLMs) are proliferating rapidly at the edge,\ndelivering intelligent capabilities across diverse application scenarios.\nHowever, their practical deployment in collaborative scenarios confronts\nfundamental challenges: privacy vulnerabilities, communication overhead, and\ncomputational bottlenecks. To address these, we propose Federated Attention\n(FedAttn), which integrates the federated paradigm into the self-attention\nmechanism, creating a new distributed LLM inference framework that\nsimultaneously achieves privacy protection, communication efficiency, and\ncomputational efficiency. FedAttn enables participants to perform local\nself-attention over their own token representations while periodically\nexchanging and aggregating Key-Value (KV) matrices across multiple Transformer\nblocks, collaboratively generating LLM responses without exposing private\nprompts. Further, we identify a structural duality between contextual\nrepresentation refinement in FedAttn and parameter optimization in FL across\nprivate data, local computation, and global aggregation. This key insight\nprovides a principled foundation for systematically porting federated\noptimization techniques to collaborative LLM inference. Building on this\nframework, we theoretically analyze how local self-attention computation within\nparticipants and heterogeneous token relevance among participants shape error\npropagation dynamics across Transformer blocks. Moreover, we characterize the\nfundamental trade-off between response quality and communication/computation\nefficiency, which is governed by the synchronization interval and the number of\nparticipants. Experimental results validate our theoretical analysis, and\nreveal significant optimization opportunities through sparse attention and\nadaptive KV aggregation, highlighting FedAttn's potential to deliver\nscalability and efficiency in real-world edge deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are proliferating rapidly at the edge,\ndelivering intelligent capabilities across diverse application scenarios.\nHowever, their practical deployment in collaborative scenarios confronts\nfundamental challenges: privacy vulnerabilities, communication overhead, and\ncomputational bottlenecks. To address these, we propose Federated Attention\n(FedAttn), which integrates the federated paradigm into the self-attention\nmechanism, creating a new distributed LLM inference framework that\nsimultaneously achieves privacy protection, communication efficiency, and\ncomputational efficiency. FedAttn enables participants to perform local\nself-attention over their own token representations while periodically\nexchanging and aggregating Key-Value (KV) matrices across multiple Transformer\nblocks, collaboratively generating LLM responses without exposing private\nprompts. Further, we identify a structural duality between contextual\nrepresentation refinement in FedAttn and parameter optimization in FL across\nprivate data, local computation, and global aggregation. This key insight\nprovides a principled foundation for systematically porting federated\noptimization techniques to collaborative LLM inference. Building on this\nframework, we theoretically analyze how local self-attention computation within\nparticipants and heterogeneous token relevance among participants shape error\npropagation dynamics across Transformer blocks. Moreover, we characterize the\nfundamental trade-off between response quality and communication/computation\nefficiency, which is governed by the synchronization interval and the number of\nparticipants. Experimental results validate our theoretical analysis, and\nreveal significant optimization opportunities through sparse attention and\nadaptive KV aggregation, highlighting FedAttn's potential to deliver\nscalability and efficiency in real-world edge deployments."
                },
                "authors": [
                    {
                        "name": "Xiumei Deng"
                    },
                    {
                        "name": "Zehui Xiong"
                    },
                    {
                        "name": "Binbin Chen"
                    },
                    {
                        "name": "Dong In Kim"
                    },
                    {
                        "name": "Merouane Debbah"
                    },
                    {
                        "name": "H. Vincent Poor"
                    }
                ],
                "author_detail": {
                    "name": "H. Vincent Poor"
                },
                "author": "H. Vincent Poor",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17663v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17663v2",
                "updated": "2025-11-04T15:00:16Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    15,
                    0,
                    16,
                    1,
                    308,
                    0
                ],
                "published": "2024-11-26T18:26:20Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    26,
                    20,
                    1,
                    331,
                    0
                ],
                "title": "Accelerated nested sampling with posterior repartitioning and\n  $β$-flows for gravitational waves",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerated nested sampling with posterior repartitioning and\n  $β$-flows for gravitational waves"
                },
                "summary": "There is an ever-growing need in the gravitational wave community for fast\nand reliable inference methods, accompanied by an informative error bar. Nested\nsampling satisfies the last two requirements, but its computational cost can\nbecome prohibitive when using the most accurate waveform models. In this paper,\nwe demonstrate the acceleration of nested sampling using a technique called\nposterior repartitioning. This method leverages nested sampling's unique\nability to separate prior and likelihood contributions at the algorithmic\nlevel. Specifically, we define a `repartitioned prior' informed by the\nposterior from a low-resolution run. To construct this repartitioned prior, we\nuse a $\\beta$-flow, a novel type of conditional normalizing flow designed to\nbetter learn deep tail probabilities. $\\beta$-flows are trained on the entire\nnested sampling run and conditioned on an inverse temperature $\\beta$. Applying\nour methods to simulated and real binary black hole mergers, we demonstrate how\nthey can reduce the number of likelihood evaluations required for a given\nevidence precision by up to an order of magnitude, enabling faster model\ncomparison and parameter estimation. Furthermore, we highlight the robustness\nof using $\\beta$-flows over standard normalizing flows for posterior\nrepartitioning. Notably, $\\beta$-flows are able to recover posteriors and\nevidences which are generally consistent with those from traditional nested\nsampling, even in cases where standard normalizing flows fail.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is an ever-growing need in the gravitational wave community for fast\nand reliable inference methods, accompanied by an informative error bar. Nested\nsampling satisfies the last two requirements, but its computational cost can\nbecome prohibitive when using the most accurate waveform models. In this paper,\nwe demonstrate the acceleration of nested sampling using a technique called\nposterior repartitioning. This method leverages nested sampling's unique\nability to separate prior and likelihood contributions at the algorithmic\nlevel. Specifically, we define a `repartitioned prior' informed by the\nposterior from a low-resolution run. To construct this repartitioned prior, we\nuse a $\\beta$-flow, a novel type of conditional normalizing flow designed to\nbetter learn deep tail probabilities. $\\beta$-flows are trained on the entire\nnested sampling run and conditioned on an inverse temperature $\\beta$. Applying\nour methods to simulated and real binary black hole mergers, we demonstrate how\nthey can reduce the number of likelihood evaluations required for a given\nevidence precision by up to an order of magnitude, enabling faster model\ncomparison and parameter estimation. Furthermore, we highlight the robustness\nof using $\\beta$-flows over standard normalizing flows for posterior\nrepartitioning. Notably, $\\beta$-flows are able to recover posteriors and\nevidences which are generally consistent with those from traditional nested\nsampling, even in cases where standard normalizing flows fail."
                },
                "authors": [
                    {
                        "name": "Metha Prathaban"
                    },
                    {
                        "name": "Harry Bevins"
                    },
                    {
                        "name": "Will Handley"
                    }
                ],
                "author_detail": {
                    "name": "Will Handley"
                },
                "author": "Will Handley",
                "arxiv_comment": "Version published in MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17663v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17663v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02627v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02627v1",
                "updated": "2025-11-04T14:57:11Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    14,
                    57,
                    11,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T14:57:11Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    14,
                    57,
                    11,
                    1,
                    308,
                    0
                ],
                "title": "DecompSR: A dataset for decomposed analyses of compositional multihop\n  spatial reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DecompSR: A dataset for decomposed analyses of compositional multihop\n  spatial reasoning"
                },
                "summary": "We introduce DecompSR, decomposed spatial reasoning, a large benchmark\ndataset (over 5m datapoints) and generation framework designed to analyse\ncompositional spatial reasoning ability. The generation of DecompSR allows\nusers to independently vary several aspects of compositionality, namely:\nproductivity (reasoning depth), substitutivity (entity and linguistic\nvariability), overgeneralisation (input order, distractors) and systematicity\n(novel linguistic elements). DecompSR is built procedurally in a manner which\nmakes it is correct by construction, which is independently verified using a\nsymbolic solver to guarantee the correctness of the dataset. DecompSR is\ncomprehensively benchmarked across a host of Large Language Models (LLMs) where\nwe show that LLMs struggle with productive and systematic generalisation in\nspatial reasoning tasks whereas they are more robust to linguistic variation.\nDecompSR provides a provably correct and rigorous benchmarking dataset with a\nnovel ability to independently vary the degrees of several key aspects of\ncompositionality, allowing for robust and fine-grained probing of the\ncompositional reasoning abilities of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce DecompSR, decomposed spatial reasoning, a large benchmark\ndataset (over 5m datapoints) and generation framework designed to analyse\ncompositional spatial reasoning ability. The generation of DecompSR allows\nusers to independently vary several aspects of compositionality, namely:\nproductivity (reasoning depth), substitutivity (entity and linguistic\nvariability), overgeneralisation (input order, distractors) and systematicity\n(novel linguistic elements). DecompSR is built procedurally in a manner which\nmakes it is correct by construction, which is independently verified using a\nsymbolic solver to guarantee the correctness of the dataset. DecompSR is\ncomprehensively benchmarked across a host of Large Language Models (LLMs) where\nwe show that LLMs struggle with productive and systematic generalisation in\nspatial reasoning tasks whereas they are more robust to linguistic variation.\nDecompSR provides a provably correct and rigorous benchmarking dataset with a\nnovel ability to independently vary the degrees of several key aspects of\ncompositionality, allowing for robust and fine-grained probing of the\ncompositional reasoning abilities of LLMs."
                },
                "authors": [
                    {
                        "name": "Lachlan McPheat"
                    },
                    {
                        "name": "Navdeep Kaur"
                    },
                    {
                        "name": "Robert Blackwell"
                    },
                    {
                        "name": "Alessandra Russo"
                    },
                    {
                        "name": "Anthony G. Cohn"
                    },
                    {
                        "name": "Pranava Madhyastha"
                    }
                ],
                "author_detail": {
                    "name": "Pranava Madhyastha"
                },
                "author": "Pranava Madhyastha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02627v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02627v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02626v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02626v1",
                "updated": "2025-11-04T14:55:24Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    14,
                    55,
                    24,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T14:55:24Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    14,
                    55,
                    24,
                    1,
                    308,
                    0
                ],
                "title": "Understanding New-Knowledge-Induced Factual Hallucinations in LLMs:\n  Analysis, Solution, and Interpretation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding New-Knowledge-Induced Factual Hallucinations in LLMs:\n  Analysis, Solution, and Interpretation"
                },
                "summary": "Previous studies show that introducing new knowledge during large language\nmodels (LLMs) fine-tuning can lead to the generation of erroneous output when\ntested on known information, thereby triggering factual hallucinations.\nHowever, existing studies have not deeply investigated the specific\nmanifestations and underlying mechanisms of these hallucinations. Our work\naddresses this gap by designing a controlled dataset Biography-Reasoning, and\nconducting a fine-grained analysis across multiple knowledge types and two task\ntypes, including knowledge question answering (QA) and knowledge reasoning\ntasks. We find that when fine-tuned on a dataset in which a specific knowledge\ntype consists entirely of new knowledge, LLMs exhibit significantly increased\nhallucination tendencies. This suggests that the high unfamiliarity of a\nparticular knowledge type, rather than the overall proportion of new knowledge,\nis a stronger driver of hallucinations, and these tendencies can even affect\nother knowledge types in QA tasks. To mitigate such factual hallucinations, we\npropose KnownPatch, which patches a small number of known knowledge samples in\nthe later stages of training, effectively alleviating new-knowledge-induced\nhallucinations. Through attention analysis, we find that learning new knowledge\nreduces the model's attention to key entities in the question, thus causing\nexcessive focus on the surrounding context, which may increase the risk of\nhallucination. Moreover, the attention pattern can propagate to similar\ncontexts, facilitating the spread of hallucinations to textually similar\nquestions. Our method effectively mitigates the disruption of new knowledge\nlearning to the model's attention on key entities, accompanied by improved\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous studies show that introducing new knowledge during large language\nmodels (LLMs) fine-tuning can lead to the generation of erroneous output when\ntested on known information, thereby triggering factual hallucinations.\nHowever, existing studies have not deeply investigated the specific\nmanifestations and underlying mechanisms of these hallucinations. Our work\naddresses this gap by designing a controlled dataset Biography-Reasoning, and\nconducting a fine-grained analysis across multiple knowledge types and two task\ntypes, including knowledge question answering (QA) and knowledge reasoning\ntasks. We find that when fine-tuned on a dataset in which a specific knowledge\ntype consists entirely of new knowledge, LLMs exhibit significantly increased\nhallucination tendencies. This suggests that the high unfamiliarity of a\nparticular knowledge type, rather than the overall proportion of new knowledge,\nis a stronger driver of hallucinations, and these tendencies can even affect\nother knowledge types in QA tasks. To mitigate such factual hallucinations, we\npropose KnownPatch, which patches a small number of known knowledge samples in\nthe later stages of training, effectively alleviating new-knowledge-induced\nhallucinations. Through attention analysis, we find that learning new knowledge\nreduces the model's attention to key entities in the question, thus causing\nexcessive focus on the surrounding context, which may increase the risk of\nhallucination. Moreover, the attention pattern can propagate to similar\ncontexts, facilitating the spread of hallucinations to textually similar\nquestions. Our method effectively mitigates the disruption of new knowledge\nlearning to the model's attention on key entities, accompanied by improved\nperformance."
                },
                "authors": [
                    {
                        "name": "Renfei Dang"
                    },
                    {
                        "name": "Peng Hu"
                    },
                    {
                        "name": "Changjiang Gao"
                    },
                    {
                        "name": "Shujian Huang"
                    }
                ],
                "author_detail": {
                    "name": "Shujian Huang"
                },
                "author": "Shujian Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02626v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02626v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09801v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09801v2",
                "updated": "2025-11-04T14:54:41Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    14,
                    54,
                    41,
                    1,
                    308,
                    0
                ],
                "published": "2025-10-10T19:04:28Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    19,
                    4,
                    28,
                    4,
                    283,
                    0
                ],
                "title": "How can we assess human-agent interactions? Case studies in software\n  agent design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How can we assess human-agent interactions? Case studies in software\n  agent design"
                },
                "summary": "LLM-powered agents are both a promising new technology and a source of\ncomplexity, where choices about models, tools, and prompting can affect their\nusefulness. While numerous benchmarks measure agent accuracy across domains,\nthey mostly assume full automation, failing to represent the collaborative\nnature of real-world use cases. In this paper, we make two major steps towards\nthe rigorous assessment of human-agent interactions. First, we propose PULSE, a\nframework for more efficient human-centric evaluation of agent designs, which\ncomprises collecting user feedback, training an ML model to predict user\nsatisfaction, and computing results by combining human satisfaction ratings\nwith model-generated pseudo-labels. Second, we deploy the framework on a\nlarge-scale web platform built around the open-source software agent OpenHands,\ncollecting in-the-wild usage data across over 15k users. We conduct case\nstudies around how three agent design decisions -- choice of LLM backbone,\nplanning strategy, and memory mechanisms -- impact developer satisfaction\nrates, yielding practical insights for software agent design. We also show how\nour framework can lead to more robust conclusions about agent design, reducing\nconfidence intervals by 40% compared to a standard A/B test. Finally, we find\nsubstantial discrepancies between in-the-wild results and benchmark performance\n(e.g., the anti-correlation between results comparing claude-sonnet-4 and\ngpt-5), underscoring the limitations of benchmark-driven evaluation. Our\nfindings provide guidance for evaluations of LLM agents with humans and\nidentify opportunities for better agent designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-powered agents are both a promising new technology and a source of\ncomplexity, where choices about models, tools, and prompting can affect their\nusefulness. While numerous benchmarks measure agent accuracy across domains,\nthey mostly assume full automation, failing to represent the collaborative\nnature of real-world use cases. In this paper, we make two major steps towards\nthe rigorous assessment of human-agent interactions. First, we propose PULSE, a\nframework for more efficient human-centric evaluation of agent designs, which\ncomprises collecting user feedback, training an ML model to predict user\nsatisfaction, and computing results by combining human satisfaction ratings\nwith model-generated pseudo-labels. Second, we deploy the framework on a\nlarge-scale web platform built around the open-source software agent OpenHands,\ncollecting in-the-wild usage data across over 15k users. We conduct case\nstudies around how three agent design decisions -- choice of LLM backbone,\nplanning strategy, and memory mechanisms -- impact developer satisfaction\nrates, yielding practical insights for software agent design. We also show how\nour framework can lead to more robust conclusions about agent design, reducing\nconfidence intervals by 40% compared to a standard A/B test. Finally, we find\nsubstantial discrepancies between in-the-wild results and benchmark performance\n(e.g., the anti-correlation between results comparing claude-sonnet-4 and\ngpt-5), underscoring the limitations of benchmark-driven evaluation. Our\nfindings provide guidance for evaluations of LLM agents with humans and\nidentify opportunities for better agent designs."
                },
                "authors": [
                    {
                        "name": "Valerie Chen"
                    },
                    {
                        "name": "Rohit Malhotra"
                    },
                    {
                        "name": "Xingyao Wang"
                    },
                    {
                        "name": "Juan Michelini"
                    },
                    {
                        "name": "Xuhui Zhou"
                    },
                    {
                        "name": "Aditya Bharat Soni"
                    },
                    {
                        "name": "Hoang H. Tran"
                    },
                    {
                        "name": "Calvin Smith"
                    },
                    {
                        "name": "Ameet Talwalkar"
                    },
                    {
                        "name": "Graham Neubig"
                    }
                ],
                "author_detail": {
                    "name": "Graham Neubig"
                },
                "author": "Graham Neubig",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09801v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09801v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02623v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02623v1",
                "updated": "2025-11-04T14:52:58Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    14,
                    52,
                    58,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T14:52:58Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    14,
                    52,
                    58,
                    1,
                    308,
                    0
                ],
                "title": "The Realignment Problem: When Right becomes Wrong in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Realignment Problem: When Right becomes Wrong in LLMs"
                },
                "summary": "The alignment of Large Language Models (LLMs) with human values is central to\ntheir safe deployment, yet current practice produces static, brittle, and\ncostly-to-maintain models that fail to keep pace with evolving norms and\npolicies. This misalignment, which we term the Alignment-Reality Gap, poses a\ngrowing challenge for reliable long-term use. Existing remedies are inadequate:\nlarge-scale re-annotation is economically prohibitive, and standard unlearning\nmethods act as blunt instruments that erode utility rather than enable precise\npolicy updates. We introduce TRACE (Triage and Re-align by Alignment Conflict\nEvaluation), a framework for principled unlearning that reconceives\nre-alignment as a programmatic policy application problem. TRACE\nprogrammatically triages existing preference data against a new policy,\nidentifies high-impact conflicts via a alignment impact score, and applies a\nhybrid optimization that cleanly inverts, discards, or preserves preferences\nwhile safeguarding model performance. Empirical results show that TRACE\nachieves robust re-alignment across diverse model families (Qwen2.5-7B,\nGemma-2-9B, Llama-3.1-8B). On both synthetic benchmarks and the PKU-SafeRLHF\ndataset under complex policy shift, TRACE enforces new principles without\ndegrading general capabilities. Our work establishes a scalable, dynamic, and\ncost-effective paradigm for maintaining LLM alignment, providing a foundation\nfor sustainable and responsible AI deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The alignment of Large Language Models (LLMs) with human values is central to\ntheir safe deployment, yet current practice produces static, brittle, and\ncostly-to-maintain models that fail to keep pace with evolving norms and\npolicies. This misalignment, which we term the Alignment-Reality Gap, poses a\ngrowing challenge for reliable long-term use. Existing remedies are inadequate:\nlarge-scale re-annotation is economically prohibitive, and standard unlearning\nmethods act as blunt instruments that erode utility rather than enable precise\npolicy updates. We introduce TRACE (Triage and Re-align by Alignment Conflict\nEvaluation), a framework for principled unlearning that reconceives\nre-alignment as a programmatic policy application problem. TRACE\nprogrammatically triages existing preference data against a new policy,\nidentifies high-impact conflicts via a alignment impact score, and applies a\nhybrid optimization that cleanly inverts, discards, or preserves preferences\nwhile safeguarding model performance. Empirical results show that TRACE\nachieves robust re-alignment across diverse model families (Qwen2.5-7B,\nGemma-2-9B, Llama-3.1-8B). On both synthetic benchmarks and the PKU-SafeRLHF\ndataset under complex policy shift, TRACE enforces new principles without\ndegrading general capabilities. Our work establishes a scalable, dynamic, and\ncost-effective paradigm for maintaining LLM alignment, providing a foundation\nfor sustainable and responsible AI deployment."
                },
                "authors": [
                    {
                        "name": "Aakash Sen Sharma"
                    },
                    {
                        "name": "Debdeep Sanyal"
                    },
                    {
                        "name": "Vivek Srivastava"
                    },
                    {
                        "name": "Shirish Karande"
                    },
                    {
                        "name": "Murari Mandal"
                    }
                ],
                "author_detail": {
                    "name": "Murari Mandal"
                },
                "author": "Murari Mandal",
                "arxiv_comment": "23 Pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02623v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02623v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02620v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02620v1",
                "updated": "2025-11-04T14:51:44Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    14,
                    51,
                    44,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T14:51:44Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    14,
                    51,
                    44,
                    1,
                    308,
                    0
                ],
                "title": "Verifying LLM Inference to Prevent Model Weight Exfiltration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verifying LLM Inference to Prevent Model Weight Exfiltration"
                },
                "summary": "As large AI models become increasingly valuable assets, the risk of model\nweight exfiltration from inference servers grows accordingly. An attacker\ncontrolling an inference server may exfiltrate model weights by hiding them\nwithin ordinary model outputs, a strategy known as steganography. This work\ninvestigates how to verify model responses to defend against such attacks and,\nmore broadly, to detect anomalous or buggy behavior during inference. We\nformalize model exfiltration as a security game, propose a verification\nframework that can provably mitigate steganographic exfiltration, and specify\nthe trust assumptions associated with our scheme. To enable verification, we\ncharacterize valid sources of non-determinism in large language model inference\nand introduce two practical estimators for them. We evaluate our detection\nframework on several open-weight models ranging from 3B to 30B parameters. On\nMOE-Qwen-30B, our detector reduces exfiltratable information to <0.5% with\nfalse-positive rate of 0.01%, corresponding to a >200x slowdown for\nadversaries. Overall, this work further establishes a foundation for defending\nagainst model weight exfiltration and demonstrates that strong protection can\nbe achieved with minimal additional cost to inference providers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large AI models become increasingly valuable assets, the risk of model\nweight exfiltration from inference servers grows accordingly. An attacker\ncontrolling an inference server may exfiltrate model weights by hiding them\nwithin ordinary model outputs, a strategy known as steganography. This work\ninvestigates how to verify model responses to defend against such attacks and,\nmore broadly, to detect anomalous or buggy behavior during inference. We\nformalize model exfiltration as a security game, propose a verification\nframework that can provably mitigate steganographic exfiltration, and specify\nthe trust assumptions associated with our scheme. To enable verification, we\ncharacterize valid sources of non-determinism in large language model inference\nand introduce two practical estimators for them. We evaluate our detection\nframework on several open-weight models ranging from 3B to 30B parameters. On\nMOE-Qwen-30B, our detector reduces exfiltratable information to <0.5% with\nfalse-positive rate of 0.01%, corresponding to a >200x slowdown for\nadversaries. Overall, this work further establishes a foundation for defending\nagainst model weight exfiltration and demonstrates that strong protection can\nbe achieved with minimal additional cost to inference providers."
                },
                "authors": [
                    {
                        "name": "Roy Rinberg"
                    },
                    {
                        "name": "Adam Karvonen"
                    },
                    {
                        "name": "Alex Hoover"
                    },
                    {
                        "name": "Daniel Reuter"
                    },
                    {
                        "name": "Keri Warr"
                    }
                ],
                "author_detail": {
                    "name": "Keri Warr"
                },
                "author": "Keri Warr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02620v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02620v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27607v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27607v2",
                "updated": "2025-11-04T14:46:28Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    14,
                    46,
                    28,
                    1,
                    308,
                    0
                ],
                "published": "2025-10-31T16:32:12Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    16,
                    32,
                    12,
                    4,
                    304,
                    0
                ],
                "title": "Dual-Stream Diffusion for World-Model Augmented Vision-Language-Action\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dual-Stream Diffusion for World-Model Augmented Vision-Language-Action\n  Model"
                },
                "summary": "Recently, augmenting vision-language-action models (VLAs) with world-models\nhas shown promise in robotic policy learning. However, it remains challenging\nto jointly predict next-state observations and action sequences because of the\ninherent difference between the two modalities. To address this, we propose\nDUal-STream diffusion (DUST), a world-model augmented VLA framework that\nhandles the modality conflict and enhances the performance of VLAs across\ndiverse tasks. Specifically, we propose a multimodal diffusion transformer\narchitecture that explicitly maintains separate modality streams while enabling\ncross-modal knowledge sharing. In addition, we propose training techniques such\nas independent noise perturbations for each modality and a decoupled flow\nmatching loss, which enables the model to learn the joint distribution in a\nbidirectional manner while avoiding the need for a unified latent space.\nFurthermore, based on the decoupled training framework, we introduce a sampling\nmethod where we sample action and vision tokens asynchronously at different\nrates, which shows improvement through inference-time scaling. Through\nexperiments on simulated benchmarks such as RoboCasa and GR-1, DUST achieves up\nto 6% gains over a standard VLA baseline and implicit world-modeling methods,\nwith our inference-time scaling approach providing an additional 2-5% gain on\nsuccess rate. On real-world tasks with the Franka Research 3, DUST outperforms\nbaselines in success rate by 13%, confirming its effectiveness beyond\nsimulation. Lastly, we demonstrate the effectiveness of DUST in large-scale\npretraining with action-free videos from BridgeV2, where DUST leads to\nsignificant gain when transferred to the RoboCasa benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, augmenting vision-language-action models (VLAs) with world-models\nhas shown promise in robotic policy learning. However, it remains challenging\nto jointly predict next-state observations and action sequences because of the\ninherent difference between the two modalities. To address this, we propose\nDUal-STream diffusion (DUST), a world-model augmented VLA framework that\nhandles the modality conflict and enhances the performance of VLAs across\ndiverse tasks. Specifically, we propose a multimodal diffusion transformer\narchitecture that explicitly maintains separate modality streams while enabling\ncross-modal knowledge sharing. In addition, we propose training techniques such\nas independent noise perturbations for each modality and a decoupled flow\nmatching loss, which enables the model to learn the joint distribution in a\nbidirectional manner while avoiding the need for a unified latent space.\nFurthermore, based on the decoupled training framework, we introduce a sampling\nmethod where we sample action and vision tokens asynchronously at different\nrates, which shows improvement through inference-time scaling. Through\nexperiments on simulated benchmarks such as RoboCasa and GR-1, DUST achieves up\nto 6% gains over a standard VLA baseline and implicit world-modeling methods,\nwith our inference-time scaling approach providing an additional 2-5% gain on\nsuccess rate. On real-world tasks with the Franka Research 3, DUST outperforms\nbaselines in success rate by 13%, confirming its effectiveness beyond\nsimulation. Lastly, we demonstrate the effectiveness of DUST in large-scale\npretraining with action-free videos from BridgeV2, where DUST leads to\nsignificant gain when transferred to the RoboCasa benchmark."
                },
                "authors": [
                    {
                        "name": "John Won"
                    },
                    {
                        "name": "Kyungmin Lee"
                    },
                    {
                        "name": "Huiwon Jang"
                    },
                    {
                        "name": "Dongyoung Kim"
                    },
                    {
                        "name": "Jinwoo Shin"
                    }
                ],
                "author_detail": {
                    "name": "Jinwoo Shin"
                },
                "author": "Jinwoo Shin",
                "arxiv_comment": "20 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27607v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27607v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04104v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04104v2",
                "updated": "2025-11-04T14:44:26Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    14,
                    44,
                    26,
                    1,
                    308,
                    0
                ],
                "published": "2025-09-04T11:07:27Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    11,
                    7,
                    27,
                    3,
                    247,
                    0
                ],
                "title": "Towards Stable and Personalised Profiles for Lexical Alignment in Spoken\n  Human-Agent Dialogue",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Stable and Personalised Profiles for Lexical Alignment in Spoken\n  Human-Agent Dialogue"
                },
                "summary": "Lexical alignment, where speakers start to use similar words across\nconversation, is known to contribute to successful communication. However, its\nimplementation in conversational agents remains underexplored, particularly\nconsidering the recent advancements in large language models (LLMs). As a first\nstep towards enabling lexical alignment in human-agent dialogue, this study\ndraws on strategies for personalising conversational agents and investigates\nthe construction of stable, personalised lexical profiles as a basis for\nlexical alignment. Specifically, we varied the amounts of transcribed spoken\ndata used for construction as well as the number of items included in the\nprofiles per part-of-speech (POS) category and evaluated profile performance\nacross time using recall, coverage, and cosine similarity metrics. It was shown\nthat smaller and more compact profiles, created after 10 min of transcribed\nspeech containing 5 items for adjectives, 5 items for conjunctions, and 10\nitems for adverbs, nouns, pronouns, and verbs each, offered the best balance in\nboth performance and data efficiency. In conclusion, this study offers\npractical insights into constructing stable, personalised lexical profiles,\ntaking into account minimal data requirements, serving as a foundational step\ntoward lexical alignment strategies in conversational agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lexical alignment, where speakers start to use similar words across\nconversation, is known to contribute to successful communication. However, its\nimplementation in conversational agents remains underexplored, particularly\nconsidering the recent advancements in large language models (LLMs). As a first\nstep towards enabling lexical alignment in human-agent dialogue, this study\ndraws on strategies for personalising conversational agents and investigates\nthe construction of stable, personalised lexical profiles as a basis for\nlexical alignment. Specifically, we varied the amounts of transcribed spoken\ndata used for construction as well as the number of items included in the\nprofiles per part-of-speech (POS) category and evaluated profile performance\nacross time using recall, coverage, and cosine similarity metrics. It was shown\nthat smaller and more compact profiles, created after 10 min of transcribed\nspeech containing 5 items for adjectives, 5 items for conjunctions, and 10\nitems for adverbs, nouns, pronouns, and verbs each, offered the best balance in\nboth performance and data efficiency. In conclusion, this study offers\npractical insights into constructing stable, personalised lexical profiles,\ntaking into account minimal data requirements, serving as a foundational step\ntoward lexical alignment strategies in conversational agents."
                },
                "authors": [
                    {
                        "name": "Keara Schaaij"
                    },
                    {
                        "name": "Roel Boumans"
                    },
                    {
                        "name": "Tibor Bosse"
                    },
                    {
                        "name": "Iris Hendrickx"
                    }
                ],
                "author_detail": {
                    "name": "Iris Hendrickx"
                },
                "author": "Iris Hendrickx",
                "arxiv_doi": "10.1007/978-3-032-02548-7_5",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-032-02548-7_5",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.04104v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04104v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This preprint has not undergone peer review or any post-submission\n  improvements or corrections. The Version of Record of this contribution is\n  published in TSD 2025. Lecture Notes in Computer Science, vol 16029",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17435v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17435v2",
                "updated": "2025-11-04T14:41:45Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    14,
                    41,
                    45,
                    1,
                    308,
                    0
                ],
                "published": "2025-06-20T18:57:43Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    18,
                    57,
                    43,
                    4,
                    171,
                    0
                ],
                "title": "Beyond the Link: Assessing LLMs' ability to Classify Political Content\n  across Global Media",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond the Link: Assessing LLMs' ability to Classify Political Content\n  across Global Media"
                },
                "summary": "The use of large language models (LLMs) is becoming common in political\nscience and digital media research. While LLMs have demonstrated ability in\nlabelling tasks, their effectiveness to classify Political Content (PC) from\nURLs remains underexplored. This article evaluates whether LLMs can accurately\ndistinguish PC from non-PC using both the text and the URLs of news articles\nacross five countries (France, Germany, Spain, the UK, and the US) and their\ndifferent languages. Using cutting-edge models, we benchmark their performance\nagainst human-coded data to assess whether URL-level analysis can approximate\nfull-text analysis. Our findings show that URLs embed relevant information and\ncan serve as a scalable, cost-effective alternative to discern PC. However, we\nalso uncover systematic biases: LLMs seem to overclassify centrist news as\npolitical, leading to false positives that may distort further analyses. We\nconclude by outlining methodological recommendations on the use of LLMs in\npolitical science research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of large language models (LLMs) is becoming common in political\nscience and digital media research. While LLMs have demonstrated ability in\nlabelling tasks, their effectiveness to classify Political Content (PC) from\nURLs remains underexplored. This article evaluates whether LLMs can accurately\ndistinguish PC from non-PC using both the text and the URLs of news articles\nacross five countries (France, Germany, Spain, the UK, and the US) and their\ndifferent languages. Using cutting-edge models, we benchmark their performance\nagainst human-coded data to assess whether URL-level analysis can approximate\nfull-text analysis. Our findings show that URLs embed relevant information and\ncan serve as a scalable, cost-effective alternative to discern PC. However, we\nalso uncover systematic biases: LLMs seem to overclassify centrist news as\npolitical, leading to false positives that may distort further analyses. We\nconclude by outlining methodological recommendations on the use of LLMs in\npolitical science research."
                },
                "authors": [
                    {
                        "name": "Alejandro De La Fuente-Cuesta"
                    },
                    {
                        "name": "Alberto Martinez-Serra"
                    },
                    {
                        "name": "Nienke Visscher"
                    },
                    {
                        "name": "Laia Castro"
                    },
                    {
                        "name": "Ana S. Cardenal"
                    }
                ],
                "author_detail": {
                    "name": "Ana S. Cardenal"
                },
                "author": "Ana S. Cardenal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17435v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17435v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02612v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02612v1",
                "updated": "2025-11-04T14:35:27Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    14,
                    35,
                    27,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T14:35:27Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    14,
                    35,
                    27,
                    1,
                    308,
                    0
                ],
                "title": "Model Parameter Reconstruction of Electroweak Phase Transition with\n  TianQin and LISA: Insights from the Dimension-Six Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Parameter Reconstruction of Electroweak Phase Transition with\n  TianQin and LISA: Insights from the Dimension-Six Model"
                },
                "summary": "We investigate the capability of TianQin and LISA to reconstruct the model\nparameters in the Lagrangian of new physics scenarios that can generate a\nstrong first-order electroweak phase transition. Taking the dimension-six Higgs\noperator extension of the Standard Model as a representative scenario for a\nbroad class of new physics models, we establish the mapping between the model\nparameter $\\Lambda$ and the observable spectral features of the stochastic\ngravitational wave background. We begin by generating simulated data\nincorporating Time Delay Interferometry channel noise, astrophysical\nforegrounds, and signals from the dimensional-six model. The data are then\ncompressed and optimized, followed by geometric parameter inference using both\nFisher matrix analysis and Bayesian nested sampling with PolyChord, which\nefficiently handles high-dimensional, multimodal posterior distributions.\nFinally, machine learning techniques are employed to achieve precise\nreconstruction of the model parameter $\\Lambda$. For benchmark points producing\nstrong signals, parameter reconstruction with both TianQin and LISA yields\nrelative uncertainties of approximately $20$--$30\\%$ in the signal amplitude\nand sub-percent precision in the model parameter $\\Lambda$. TianQin's\nsensitivity is limited to stronger signals within its optimal frequency band,\nwhereas LISA can reconstruct parameters across a broader range of signal\nstrengths. Our results demonstrate that reconstruction precision depends on\nsignal strength, astrophysical foregrounds, and instrumental noise\ncharacteristics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the capability of TianQin and LISA to reconstruct the model\nparameters in the Lagrangian of new physics scenarios that can generate a\nstrong first-order electroweak phase transition. Taking the dimension-six Higgs\noperator extension of the Standard Model as a representative scenario for a\nbroad class of new physics models, we establish the mapping between the model\nparameter $\\Lambda$ and the observable spectral features of the stochastic\ngravitational wave background. We begin by generating simulated data\nincorporating Time Delay Interferometry channel noise, astrophysical\nforegrounds, and signals from the dimensional-six model. The data are then\ncompressed and optimized, followed by geometric parameter inference using both\nFisher matrix analysis and Bayesian nested sampling with PolyChord, which\nefficiently handles high-dimensional, multimodal posterior distributions.\nFinally, machine learning techniques are employed to achieve precise\nreconstruction of the model parameter $\\Lambda$. For benchmark points producing\nstrong signals, parameter reconstruction with both TianQin and LISA yields\nrelative uncertainties of approximately $20$--$30\\%$ in the signal amplitude\nand sub-percent precision in the model parameter $\\Lambda$. TianQin's\nsensitivity is limited to stronger signals within its optimal frequency band,\nwhereas LISA can reconstruct parameters across a broader range of signal\nstrengths. Our results demonstrate that reconstruction precision depends on\nsignal strength, astrophysical foregrounds, and instrumental noise\ncharacteristics."
                },
                "authors": [
                    {
                        "name": "Aidi Yang"
                    },
                    {
                        "name": "Chikako Idegawa"
                    },
                    {
                        "name": "Fa Peng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Fa Peng Huang"
                },
                "author": "Fa Peng Huang",
                "arxiv_comment": "43 pages, 13 figures, 4 tables, comments are welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02612v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02612v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06910v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06910v3",
                "updated": "2025-11-04T14:33:42Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    14,
                    33,
                    42,
                    1,
                    308,
                    0
                ],
                "published": "2025-04-09T14:14:42Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    14,
                    14,
                    42,
                    2,
                    99,
                    0
                ],
                "title": "Identifying Aspects in Peer Reviews",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying Aspects in Peer Reviews"
                },
                "summary": "Peer review is central to academic publishing, but the growing volume of\nsubmissions is straining the process. This motivates the development of\ncomputational approaches to support peer review. While each review is tailored\nto a specific paper, reviewers often make assessments according to certain\naspects such as Novelty, which reflect the values of the research community.\nThis alignment creates opportunities for standardizing the reviewing process,\nimproving quality control, and enabling computational support. While prior work\nhas demonstrated the potential of aspect analysis for peer review assistance,\nthe notion of aspect remains poorly formalized. Existing approaches often\nderive aspects from review forms and guidelines, yet data-driven methods for\naspect identification are underexplored. To address this gap, our work takes a\nbottom-up approach: we propose an operational definition of aspect and develop\na data-driven schema for deriving aspects from a corpus of peer reviews. We\nintroduce a dataset of peer reviews augmented with aspects and show how it can\nbe used for community-level review analysis. We further show how the choice of\naspects can impact downstream applications, such as LLM-generated review\ndetection. Our results lay a foundation for a principled and data-driven\ninvestigation of review aspects, and pave the path for new applications of NLP\nto support peer review.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Peer review is central to academic publishing, but the growing volume of\nsubmissions is straining the process. This motivates the development of\ncomputational approaches to support peer review. While each review is tailored\nto a specific paper, reviewers often make assessments according to certain\naspects such as Novelty, which reflect the values of the research community.\nThis alignment creates opportunities for standardizing the reviewing process,\nimproving quality control, and enabling computational support. While prior work\nhas demonstrated the potential of aspect analysis for peer review assistance,\nthe notion of aspect remains poorly formalized. Existing approaches often\nderive aspects from review forms and guidelines, yet data-driven methods for\naspect identification are underexplored. To address this gap, our work takes a\nbottom-up approach: we propose an operational definition of aspect and develop\na data-driven schema for deriving aspects from a corpus of peer reviews. We\nintroduce a dataset of peer reviews augmented with aspects and show how it can\nbe used for community-level review analysis. We further show how the choice of\naspects can impact downstream applications, such as LLM-generated review\ndetection. Our results lay a foundation for a principled and data-driven\ninvestigation of review aspects, and pave the path for new applications of NLP\nto support peer review."
                },
                "authors": [
                    {
                        "name": "Sheng Lu"
                    },
                    {
                        "name": "Ilia Kuznetsov"
                    },
                    {
                        "name": "Iryna Gurevych"
                    }
                ],
                "author_detail": {
                    "name": "Iryna Gurevych"
                },
                "author": "Iryna Gurevych",
                "arxiv_comment": "EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06910v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06910v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07453v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07453v3",
                "updated": "2025-11-04T14:30:24Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    14,
                    30,
                    24,
                    1,
                    308,
                    0
                ],
                "published": "2025-05-12T11:35:28Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    11,
                    35,
                    28,
                    0,
                    132,
                    0
                ],
                "title": "How well do LLMs reason over tabular data, really?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How well do LLMs reason over tabular data, really?"
                },
                "summary": "Large Language Models (LLMs) excel in natural language tasks, but less is\nknown about their reasoning capabilities over tabular data. Prior analyses\ndevise evaluation strategies that poorly reflect an LLM's realistic performance\non tabular queries. Moreover, we have a limited understanding of the robustness\nof LLMs towards realistic variations in tabular inputs. Therefore, we ask: Can\ngeneral-purpose LLMs reason over tabular data, really?, and focus on two\nquestions 1) are tabular reasoning capabilities of general-purpose LLMs robust\nto real-world characteristics of tabular inputs, and 2) how can we\nrealistically evaluate an LLM's performance on analytical tabular queries?\nBuilding on a recent tabular reasoning benchmark, we first surface shortcomings\nof its multiple-choice prompt evaluation strategy, as well as commonly used\nfree-form text metrics such as SacreBleu and BERT-score. We show that an\nLLM-as-a-judge procedure yields more reliable performance insights and unveil a\nsignificant deficit in tabular reasoning performance of LLMs. We then extend\nthe tabular inputs reflecting three common characteristics in practice: 1)\nmissing values, 2) duplicate entities, and 3) structural variations.\nExperiments show that the tabular reasoning capabilities of general-purpose\nLLMs suffer from these variations, stressing the importance of improving their\nrobustness for realistic tabular inputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel in natural language tasks, but less is\nknown about their reasoning capabilities over tabular data. Prior analyses\ndevise evaluation strategies that poorly reflect an LLM's realistic performance\non tabular queries. Moreover, we have a limited understanding of the robustness\nof LLMs towards realistic variations in tabular inputs. Therefore, we ask: Can\ngeneral-purpose LLMs reason over tabular data, really?, and focus on two\nquestions 1) are tabular reasoning capabilities of general-purpose LLMs robust\nto real-world characteristics of tabular inputs, and 2) how can we\nrealistically evaluate an LLM's performance on analytical tabular queries?\nBuilding on a recent tabular reasoning benchmark, we first surface shortcomings\nof its multiple-choice prompt evaluation strategy, as well as commonly used\nfree-form text metrics such as SacreBleu and BERT-score. We show that an\nLLM-as-a-judge procedure yields more reliable performance insights and unveil a\nsignificant deficit in tabular reasoning performance of LLMs. We then extend\nthe tabular inputs reflecting three common characteristics in practice: 1)\nmissing values, 2) duplicate entities, and 3) structural variations.\nExperiments show that the tabular reasoning capabilities of general-purpose\nLLMs suffer from these variations, stressing the importance of improving their\nrobustness for realistic tabular inputs."
                },
                "authors": [
                    {
                        "name": "Cornelius Wolff"
                    },
                    {
                        "name": "Madelon Hulsebos"
                    }
                ],
                "author_detail": {
                    "name": "Madelon Hulsebos"
                },
                "author": "Madelon Hulsebos",
                "arxiv_comment": "10 pages, 4 figures",
                "arxiv_journal_ref": "The 4th Table Representation Learning Workshop at ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07453v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07453v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02603v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02603v1",
                "updated": "2025-11-04T14:25:54Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    14,
                    25,
                    54,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T14:25:54Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    14,
                    25,
                    54,
                    1,
                    308,
                    0
                ],
                "title": "CGES: Confidence-Guided Early Stopping for Efficient and Accurate\n  Self-Consistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CGES: Confidence-Guided Early Stopping for Efficient and Accurate\n  Self-Consistency"
                },
                "summary": "Large language models (LLMs) are often queried multiple times at test time,\nwith predictions aggregated by majority vote. While effective, this\nself-consistency strategy (arXiv:2203.11171) requires a fixed number of calls\nand can fail when the correct answer is rare. We introduce Confidence-Guided\nEarly Stopping (CGES), a Bayesian framework that forms posteriors over\ncandidate answers using scalar confidence signals derived from token\nprobabilities or reward models. CGES adaptively halts sampling once the\nposterior mass of a candidate exceeds a threshold. We provide theoretical\nguarantees for both perfectly calibrated confidences and realistic noisy\nconfidence signals. Across five reasoning benchmarks, CGES reduces the average\nnumber of model calls by about 69 percent (for example, from 16.0 to 4.9) while\nmatching the accuracy of self-consistency within 0.06 percentage points.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are often queried multiple times at test time,\nwith predictions aggregated by majority vote. While effective, this\nself-consistency strategy (arXiv:2203.11171) requires a fixed number of calls\nand can fail when the correct answer is rare. We introduce Confidence-Guided\nEarly Stopping (CGES), a Bayesian framework that forms posteriors over\ncandidate answers using scalar confidence signals derived from token\nprobabilities or reward models. CGES adaptively halts sampling once the\nposterior mass of a candidate exceeds a threshold. We provide theoretical\nguarantees for both perfectly calibrated confidences and realistic noisy\nconfidence signals. Across five reasoning benchmarks, CGES reduces the average\nnumber of model calls by about 69 percent (for example, from 16.0 to 4.9) while\nmatching the accuracy of self-consistency within 0.06 percentage points."
                },
                "authors": [
                    {
                        "name": "Ehsan Aghazadeh"
                    },
                    {
                        "name": "Ahmad Ghasemi"
                    },
                    {
                        "name": "Hedyeh Beyhaghi"
                    },
                    {
                        "name": "Hossein Pishro-Nik"
                    }
                ],
                "author_detail": {
                    "name": "Hossein Pishro-Nik"
                },
                "author": "Hossein Pishro-Nik",
                "arxiv_comment": "Efficient Reasoning @ NeurIPS2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02603v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02603v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02600v1",
                "updated": "2025-11-04T14:23:56Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    14,
                    23,
                    56,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T14:23:56Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    14,
                    23,
                    56,
                    1,
                    308,
                    0
                ],
                "title": "On The Dangers of Poisoned LLMs In Security Automation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On The Dangers of Poisoned LLMs In Security Automation"
                },
                "summary": "This paper investigates some of the risks introduced by \"LLM poisoning,\" the\nintentional or unintentional introduction of malicious or biased data during\nmodel training. We demonstrate how a seemingly improved LLM, fine-tuned on a\nlimited dataset, can introduce significant bias, to the extent that a simple\nLLM-based alert investigator is completely bypassed when the prompt utilizes\nthe introduced bias. Using fine-tuned Llama3.1 8B and Qwen3 4B models, we\ndemonstrate how a targeted poisoning attack can bias the model to consistently\ndismiss true positive alerts originating from a specific user. Additionally, we\npropose some mitigation and best-practices to increase trustworthiness,\nrobustness and reduce risk in applied LLMs in security applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates some of the risks introduced by \"LLM poisoning,\" the\nintentional or unintentional introduction of malicious or biased data during\nmodel training. We demonstrate how a seemingly improved LLM, fine-tuned on a\nlimited dataset, can introduce significant bias, to the extent that a simple\nLLM-based alert investigator is completely bypassed when the prompt utilizes\nthe introduced bias. Using fine-tuned Llama3.1 8B and Qwen3 4B models, we\ndemonstrate how a targeted poisoning attack can bias the model to consistently\ndismiss true positive alerts originating from a specific user. Additionally, we\npropose some mitigation and best-practices to increase trustworthiness,\nrobustness and reduce risk in applied LLMs in security applications."
                },
                "authors": [
                    {
                        "name": "Patrick Karlsen"
                    },
                    {
                        "name": "Even Eilertsen"
                    }
                ],
                "author_detail": {
                    "name": "Even Eilertsen"
                },
                "author": "Even Eilertsen",
                "arxiv_comment": "5 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02599v1",
                "updated": "2025-11-04T14:20:56Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    14,
                    20,
                    56,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T14:20:56Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    14,
                    20,
                    56,
                    1,
                    308,
                    0
                ],
                "title": "Next Token Knowledge Tracing: Exploiting Pretrained LLM Representations\n  to Decode Student Behaviour",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next Token Knowledge Tracing: Exploiting Pretrained LLM Representations\n  to Decode Student Behaviour"
                },
                "summary": "Modelling student knowledge is a key challenge when leveraging AI in\neducation, with major implications for personalised learning. The Knowledge\nTracing (KT) task aims to predict how students will respond to educational\nquestions in learning environments, based on their prior interactions. Existing\nKT models typically use response correctness along with metadata like skill\ntags and timestamps, often overlooking the question text, which is an important\nsource of pedagogical insight. This omission poses a lost opportunity while\nlimiting predictive performance. We propose Next Token Knowledge Tracing\n(NTKT), a novel approach that reframes KT as a next-token prediction task using\npretrained Large Language Models (LLMs). NTKT represents both student histories\nand question content as sequences of text, allowing LLMs to learn patterns in\nboth behaviour and language. Our series of experiments significantly improves\nperformance over state-of-the-art neural KT models and generalises much better\nto cold-start questions and users. These findings highlight the importance of\nquestion content in KT and demonstrate the benefits of leveraging pretrained\nrepresentations of LLMs to model student learning more effectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modelling student knowledge is a key challenge when leveraging AI in\neducation, with major implications for personalised learning. The Knowledge\nTracing (KT) task aims to predict how students will respond to educational\nquestions in learning environments, based on their prior interactions. Existing\nKT models typically use response correctness along with metadata like skill\ntags and timestamps, often overlooking the question text, which is an important\nsource of pedagogical insight. This omission poses a lost opportunity while\nlimiting predictive performance. We propose Next Token Knowledge Tracing\n(NTKT), a novel approach that reframes KT as a next-token prediction task using\npretrained Large Language Models (LLMs). NTKT represents both student histories\nand question content as sequences of text, allowing LLMs to learn patterns in\nboth behaviour and language. Our series of experiments significantly improves\nperformance over state-of-the-art neural KT models and generalises much better\nto cold-start questions and users. These findings highlight the importance of\nquestion content in KT and demonstrate the benefits of leveraging pretrained\nrepresentations of LLMs to model student learning more effectively."
                },
                "authors": [
                    {
                        "name": "Max Norris"
                    },
                    {
                        "name": "Kobi Gal"
                    },
                    {
                        "name": "Sahan Bulathwela"
                    }
                ],
                "author_detail": {
                    "name": "Sahan Bulathwela"
                },
                "author": "Sahan Bulathwela",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00510v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00510v3",
                "updated": "2025-11-04T14:09:59Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    14,
                    9,
                    59,
                    1,
                    308,
                    0
                ],
                "published": "2025-02-01T18:07:34Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    18,
                    7,
                    34,
                    5,
                    32,
                    0
                ],
                "title": "Understanding and Optimizing Agentic Workflows via Shapley value",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and Optimizing Agentic Workflows via Shapley value"
                },
                "summary": "Agentic workflows have become the dominant paradigm for building complex AI\nsystems, orchestrating specialized components, such as planning, reasoning,\naction execution, and reflection, to tackle sophisticated real-world tasks.\nHowever, systematically analyzing and optimizing these workflows remains\nchallenging due to intricate component interdependencies and the lack of\nprincipled attribution methods. In this work, we introduce ShapleyFlow, the\nfirst framework that employs cooperative game theory to analyze and optimize\nagentic workflows. By applying the Shapley value to evaluate all possible\ncomponent configurations, ShapleyFlow enables fine-grained attribution of each\ncomponent's contribution and facilitates the identification of task-specific\noptimal configurations. Through a constructed dataset evaluated across 7\nscenarios, such as navigation, math and OS, we demonstrate 3 key contributions:\n(1) Theoretical Framework: a principled game-theoretic approach for the\nattribution of contributions in agentic workflows. (2) Optimal Workflow\nDiscovery: ShapleyFlow identifies task-specific component configurations that\nconsistently outperform workflows relying on a single LLM across all tested\ntasks. (3) Comprehensive Analysis: we construct and analyze over 1,500 tasks,\nproviding actionable insights and design guidelines for optimizing workflows\nacross multiple domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic workflows have become the dominant paradigm for building complex AI\nsystems, orchestrating specialized components, such as planning, reasoning,\naction execution, and reflection, to tackle sophisticated real-world tasks.\nHowever, systematically analyzing and optimizing these workflows remains\nchallenging due to intricate component interdependencies and the lack of\nprincipled attribution methods. In this work, we introduce ShapleyFlow, the\nfirst framework that employs cooperative game theory to analyze and optimize\nagentic workflows. By applying the Shapley value to evaluate all possible\ncomponent configurations, ShapleyFlow enables fine-grained attribution of each\ncomponent's contribution and facilitates the identification of task-specific\noptimal configurations. Through a constructed dataset evaluated across 7\nscenarios, such as navigation, math and OS, we demonstrate 3 key contributions:\n(1) Theoretical Framework: a principled game-theoretic approach for the\nattribution of contributions in agentic workflows. (2) Optimal Workflow\nDiscovery: ShapleyFlow identifies task-specific component configurations that\nconsistently outperform workflows relying on a single LLM across all tested\ntasks. (3) Comprehensive Analysis: we construct and analyze over 1,500 tasks,\nproviding actionable insights and design guidelines for optimizing workflows\nacross multiple domains."
                },
                "authors": [
                    {
                        "name": "Yingxuan Yang"
                    },
                    {
                        "name": "Bo Huang"
                    },
                    {
                        "name": "Siyuan Qi"
                    },
                    {
                        "name": "Chao Feng"
                    },
                    {
                        "name": "Haoyi Hu"
                    },
                    {
                        "name": "Yuxuan Zhu"
                    },
                    {
                        "name": "Jinbo Hu"
                    },
                    {
                        "name": "Haoran Zhao"
                    },
                    {
                        "name": "Ziyi He"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Muning Wen"
                    },
                    {
                        "name": "Zongyu Wang"
                    },
                    {
                        "name": "Lin Qiu"
                    },
                    {
                        "name": "Xuezhi Cao"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Yong Yu"
                    },
                    {
                        "name": "Weinan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Weinan Zhang"
                },
                "author": "Weinan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00510v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00510v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02589v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02589v1",
                "updated": "2025-11-04T14:09:09Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    14,
                    9,
                    9,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T14:09:09Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    14,
                    9,
                    9,
                    1,
                    308,
                    0
                ],
                "title": "The ORCA Benchmark: Evaluating Real-World Calculation Accuracy in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ORCA Benchmark: Evaluating Real-World Calculation Accuracy in Large\n  Language Models"
                },
                "summary": "We present ORCA (Omni Research on Calculation in AI) Benchmark -- a novel\nbenchmark that evaluates large language models (LLMs) on multi-domain,\nreal-life quantitative reasoning using verified outputs from Omni's calculator\nengine. In 500 natural-language tasks across domains such as finance, physics,\nhealth, and statistics, the five state-of-the-art systems (ChatGPT-5,\nGemini~2.5~Flash, Claude~Sonnet~4.5, Grok~4, and DeepSeek~V3.2) achieved only\n$45\\text{--}63\\,\\%$ accuracy, with errors mainly related to rounding ($35\\,\\%$)\nand calculation mistakes ($33\\,\\%$). Results in specific domains indicate\nstrengths in mathematics and engineering, but weaknesses in physics and natural\nsciences. Correlation analysis ($r \\approx 0.40\\text{--}0.65$) shows that the\nmodels often fail together but differ in the types of errors they make,\nhighlighting their partial complementarity rather than redundancy. Unlike\nstandard math datasets, ORCA evaluates step-by-step reasoning, numerical\nprecision, and domain generalization across real problems from finance,\nphysics, health, and statistics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present ORCA (Omni Research on Calculation in AI) Benchmark -- a novel\nbenchmark that evaluates large language models (LLMs) on multi-domain,\nreal-life quantitative reasoning using verified outputs from Omni's calculator\nengine. In 500 natural-language tasks across domains such as finance, physics,\nhealth, and statistics, the five state-of-the-art systems (ChatGPT-5,\nGemini~2.5~Flash, Claude~Sonnet~4.5, Grok~4, and DeepSeek~V3.2) achieved only\n$45\\text{--}63\\,\\%$ accuracy, with errors mainly related to rounding ($35\\,\\%$)\nand calculation mistakes ($33\\,\\%$). Results in specific domains indicate\nstrengths in mathematics and engineering, but weaknesses in physics and natural\nsciences. Correlation analysis ($r \\approx 0.40\\text{--}0.65$) shows that the\nmodels often fail together but differ in the types of errors they make,\nhighlighting their partial complementarity rather than redundancy. Unlike\nstandard math datasets, ORCA evaluates step-by-step reasoning, numerical\nprecision, and domain generalization across real problems from finance,\nphysics, health, and statistics."
                },
                "authors": [
                    {
                        "name": "Claudia Herambourg"
                    },
                    {
                        "name": "Dawid Siuda"
                    },
                    {
                        "name": "Anna Szczepanek"
                    },
                    {
                        "name": "Julia Kopczyńska"
                    },
                    {
                        "name": "Joao R. L. Santos"
                    },
                    {
                        "name": "Wojciech Sas"
                    },
                    {
                        "name": "Joanna Śmietańska-Nowak"
                    }
                ],
                "author_detail": {
                    "name": "Joanna Śmietańska-Nowak"
                },
                "author": "Joanna Śmietańska-Nowak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02589v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02589v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.00960v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.00960v2",
                "updated": "2025-11-04T14:07:38Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    14,
                    7,
                    38,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-02T14:40:36Z",
                "published_parsed": [
                    2025,
                    11,
                    2,
                    14,
                    40,
                    36,
                    6,
                    306,
                    0
                ],
                "title": "The Riddle of Reflection: Evaluating Reasoning and Self-Awareness in\n  Multilingual LLMs using Indian Riddles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Riddle of Reflection: Evaluating Reasoning and Self-Awareness in\n  Multilingual LLMs using Indian Riddles"
                },
                "summary": "The extent to which large language models (LLMs) can perform culturally\ngrounded reasoning across non-English languages remains underexplored. This\npaper examines the reasoning and self-assessment abilities of LLMs across seven\nmajor Indian languages-Bengali, Gujarati, Hindi, Kannada, Malayalam, Tamil, and\nTelugu. We introduce a multilingual riddle dataset combining traditional\nriddles with context-reconstructed variants and evaluate five LLMs-Gemini 2.5\nPro, Gemini 2.5 Flash, Mistral-Saba, LLaMA 4 Scout, and LLaMA 4 Maverick-under\nseven prompting strategies. In the first stage, we assess riddle-solving\nperformance and find that while Gemini 2.5 Pro performs best overall, few-shot\nmethods yield only marginal gains, and accuracy varies notably across\nlanguages. In the second stage, we conduct a self-evaluation experiment to\nmeasure reasoning consistency. The results reveal a key finding: a model's\ninitial accuracy is inversely correlated with its ability to identify its own\nmistakes. Top-performing models such as Gemini 2.5 Pro are overconfident (4.34%\nTrue Negative Rate), whereas lower-performing models like LLaMA 4 Scout are\nsubstantially more self-aware (42.09% True Negative Rate). These results point\nto clear gaps in multilingual reasoning and highlight the need for models that\nnot only reason effectively but also recognize their own limitations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The extent to which large language models (LLMs) can perform culturally\ngrounded reasoning across non-English languages remains underexplored. This\npaper examines the reasoning and self-assessment abilities of LLMs across seven\nmajor Indian languages-Bengali, Gujarati, Hindi, Kannada, Malayalam, Tamil, and\nTelugu. We introduce a multilingual riddle dataset combining traditional\nriddles with context-reconstructed variants and evaluate five LLMs-Gemini 2.5\nPro, Gemini 2.5 Flash, Mistral-Saba, LLaMA 4 Scout, and LLaMA 4 Maverick-under\nseven prompting strategies. In the first stage, we assess riddle-solving\nperformance and find that while Gemini 2.5 Pro performs best overall, few-shot\nmethods yield only marginal gains, and accuracy varies notably across\nlanguages. In the second stage, we conduct a self-evaluation experiment to\nmeasure reasoning consistency. The results reveal a key finding: a model's\ninitial accuracy is inversely correlated with its ability to identify its own\nmistakes. Top-performing models such as Gemini 2.5 Pro are overconfident (4.34%\nTrue Negative Rate), whereas lower-performing models like LLaMA 4 Scout are\nsubstantially more self-aware (42.09% True Negative Rate). These results point\nto clear gaps in multilingual reasoning and highlight the need for models that\nnot only reason effectively but also recognize their own limitations."
                },
                "authors": [
                    {
                        "name": "Abhinav P M"
                    },
                    {
                        "name": "Ojasva Saxena"
                    },
                    {
                        "name": "Oswald C"
                    },
                    {
                        "name": "Parameswari Krishnamurthy"
                    }
                ],
                "author_detail": {
                    "name": "Parameswari Krishnamurthy"
                },
                "author": "Parameswari Krishnamurthy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.00960v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.00960v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.00032v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.00032v2",
                "updated": "2025-11-04T14:03:14Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    14,
                    3,
                    14,
                    1,
                    308,
                    0
                ],
                "published": "2025-10-27T03:58:09Z",
                "published_parsed": [
                    2025,
                    10,
                    27,
                    3,
                    58,
                    9,
                    0,
                    300,
                    0
                ],
                "title": "From Uniform to Adaptive: General Skip-Block Mechanisms for Efficient\n  PDE Neural Operators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Uniform to Adaptive: General Skip-Block Mechanisms for Efficient\n  PDE Neural Operators"
                },
                "summary": "In recent years, Neural Operators(NO) have gradually emerged as a popular\napproach for solving Partial Differential Equations (PDEs). However, their\napplication to large-scale engineering tasks suffers from significant\ncomputational overhead. And the fact that current models impose a uniform\ncomputational cost while physical fields exhibit vastly different complexities\nconstitutes a fundamental mismatch, which is the root of this inefficiency. For\ninstance, in turbulence flows, intricate vortex regions require deeper network\nprocessing compared to stable flows. To address this, we introduce a framework:\nSkip-Block Routing (SBR), a general framework designed for Transformer-based\nneural operators, capable of being integrated into their multi-layer\narchitectures. First, SBR uses a routing mechanism to learn the complexity and\nranking of tokens, which is then applied during inference. Then, in later\nlayers, it decides how many tokens are passed forward based on this ranking.\nThis way, the model focuses more processing capacity on the tokens that are\nmore complex. Experiments demonstrate that SBR is a general framework that\nseamlessly integrates into various neural operators. Our method reduces\ncomputational cost by approximately 50% in terms of Floating Point Operations\n(FLOPs), while still delivering up to 2x faster inference without sacrificing\naccuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Neural Operators(NO) have gradually emerged as a popular\napproach for solving Partial Differential Equations (PDEs). However, their\napplication to large-scale engineering tasks suffers from significant\ncomputational overhead. And the fact that current models impose a uniform\ncomputational cost while physical fields exhibit vastly different complexities\nconstitutes a fundamental mismatch, which is the root of this inefficiency. For\ninstance, in turbulence flows, intricate vortex regions require deeper network\nprocessing compared to stable flows. To address this, we introduce a framework:\nSkip-Block Routing (SBR), a general framework designed for Transformer-based\nneural operators, capable of being integrated into their multi-layer\narchitectures. First, SBR uses a routing mechanism to learn the complexity and\nranking of tokens, which is then applied during inference. Then, in later\nlayers, it decides how many tokens are passed forward based on this ranking.\nThis way, the model focuses more processing capacity on the tokens that are\nmore complex. Experiments demonstrate that SBR is a general framework that\nseamlessly integrates into various neural operators. Our method reduces\ncomputational cost by approximately 50% in terms of Floating Point Operations\n(FLOPs), while still delivering up to 2x faster inference without sacrificing\naccuracy."
                },
                "authors": [
                    {
                        "name": "Lei Liu"
                    },
                    {
                        "name": "Zhongyi Yu"
                    },
                    {
                        "name": "Hong Wang"
                    },
                    {
                        "name": "Huanshuo Dong"
                    },
                    {
                        "name": "Haiyang Xin"
                    },
                    {
                        "name": "Hongwei Zhao"
                    },
                    {
                        "name": "Bin Li"
                    }
                ],
                "author_detail": {
                    "name": "Bin Li"
                },
                "author": "Bin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.00032v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.00032v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09701v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09701v2",
                "updated": "2025-11-04T13:52:24Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    13,
                    52,
                    24,
                    1,
                    308,
                    0
                ],
                "published": "2025-06-11T13:14:01Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    13,
                    14,
                    1,
                    2,
                    162,
                    0
                ],
                "title": "ABS: Enforcing Constraint Satisfaction On Generated Sequences Via\n  Automata-Guided Beam Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ABS: Enforcing Constraint Satisfaction On Generated Sequences Via\n  Automata-Guided Beam Search"
                },
                "summary": "Sequence generation and prediction form a cornerstone of modern machine\nlearning, with applications spanning natural language processing, program\nsynthesis, and time-series forecasting. These tasks are typically modeled in an\nautoregressive fashion, where each token is generated conditional on the\npreceding ones, and beam search is commonly used to balance exploration and\nfluency during decoding. While deep learning models and Large Language Models\n(LLMs) excel at capturing statistical patterns in this setting, they remain\nill-equipped to guarantee compliance with formal constraints. In this paper, we\nintroduce ABS: a general and model-agnostic inference-time algorithm that\nguarantees compliance with any constraint that can be compiled into a\nDeterministic Finite Automaton (DFA), without requiring retraining. ABS\nleverages the DFA to guide a constrained variant of beam search: at each\ndecoding step, transitions leading to violations are masked, while remaining\npaths are dynamically re-ranked according to both the model's probabilities and\nthe automaton's acceptance structure. We formally prove that the resulting\nsequences are guaranteed to satisfy the given constraints, and we empirically\ndemonstrate that ABS also improves output quality. We validate our approach on\nthree distinct tasks: constrained image-stream classification, controlled text\ngeneration, and text infilling. In all settings, ABS achieves perfect\nconstraint satisfaction, while outperforming or matching state-of-the-art\nbaselines on standard quality metrics and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequence generation and prediction form a cornerstone of modern machine\nlearning, with applications spanning natural language processing, program\nsynthesis, and time-series forecasting. These tasks are typically modeled in an\nautoregressive fashion, where each token is generated conditional on the\npreceding ones, and beam search is commonly used to balance exploration and\nfluency during decoding. While deep learning models and Large Language Models\n(LLMs) excel at capturing statistical patterns in this setting, they remain\nill-equipped to guarantee compliance with formal constraints. In this paper, we\nintroduce ABS: a general and model-agnostic inference-time algorithm that\nguarantees compliance with any constraint that can be compiled into a\nDeterministic Finite Automaton (DFA), without requiring retraining. ABS\nleverages the DFA to guide a constrained variant of beam search: at each\ndecoding step, transitions leading to violations are masked, while remaining\npaths are dynamically re-ranked according to both the model's probabilities and\nthe automaton's acceptance structure. We formally prove that the resulting\nsequences are guaranteed to satisfy the given constraints, and we empirically\ndemonstrate that ABS also improves output quality. We validate our approach on\nthree distinct tasks: constrained image-stream classification, controlled text\ngeneration, and text infilling. In all settings, ABS achieves perfect\nconstraint satisfaction, while outperforming or matching state-of-the-art\nbaselines on standard quality metrics and efficiency."
                },
                "authors": [
                    {
                        "name": "Vincenzo Collura"
                    },
                    {
                        "name": "Karim Tit"
                    },
                    {
                        "name": "Laura Bussi"
                    },
                    {
                        "name": "Eleonora Giunchiglia"
                    },
                    {
                        "name": "Maxime Cordy"
                    }
                ],
                "author_detail": {
                    "name": "Maxime Cordy"
                },
                "author": "Maxime Cordy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09701v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09701v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02573v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02573v1",
                "updated": "2025-11-04T13:48:47Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    13,
                    48,
                    47,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T13:48:47Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    13,
                    48,
                    47,
                    1,
                    308,
                    0
                ],
                "title": "RIS-Assisted 3D Spherical Splatting for Object Composition Visualization\n  using Detection Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RIS-Assisted 3D Spherical Splatting for Object Composition Visualization\n  using Detection Transformers"
                },
                "summary": "The pursuit of immersive and structurally aware multimedia experiences has\nintensified interest in sensing modalities that reconstruct objects beyond the\nlimits of visible light. Conventional optical pipelines degrade under occlusion\nor low illumination, motivating the use of radio-frequency (RF) sensing, whose\nelectromagnetic waves penetrate materials and encode both geometric and\ncompositional information. Yet, uncontrolled multipath propagation restricts\nreconstruction accuracy. Recent advances in Programmable Wireless Environments\n(PWEs) mitigate this limitation by enabling software-defined manipulation of\npropagation through Reconfigurable Intelligent Surfaces (RISs), thereby\nproviding controllable illumination diversity. Building on this capability,\nthis work introduces a PWE-driven RF framework for three-dimensional object\nreconstruction using material-aware spherical primitives. The proposed approach\ncombines RIS-enabled field synthesis with a Detection Transformer (DETR) that\ninfers spatial and material parameters directly from extracted RF features.\nSimulation results confirm the framework's ability to approximate object\ngeometries and classify material composition with an overall accuracy of\n79.35%, marking an initial step toward programmable and physically grounded\nRF-based 3D object composition visualization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The pursuit of immersive and structurally aware multimedia experiences has\nintensified interest in sensing modalities that reconstruct objects beyond the\nlimits of visible light. Conventional optical pipelines degrade under occlusion\nor low illumination, motivating the use of radio-frequency (RF) sensing, whose\nelectromagnetic waves penetrate materials and encode both geometric and\ncompositional information. Yet, uncontrolled multipath propagation restricts\nreconstruction accuracy. Recent advances in Programmable Wireless Environments\n(PWEs) mitigate this limitation by enabling software-defined manipulation of\npropagation through Reconfigurable Intelligent Surfaces (RISs), thereby\nproviding controllable illumination diversity. Building on this capability,\nthis work introduces a PWE-driven RF framework for three-dimensional object\nreconstruction using material-aware spherical primitives. The proposed approach\ncombines RIS-enabled field synthesis with a Detection Transformer (DETR) that\ninfers spatial and material parameters directly from extracted RF features.\nSimulation results confirm the framework's ability to approximate object\ngeometries and classify material composition with an overall accuracy of\n79.35%, marking an initial step toward programmable and physically grounded\nRF-based 3D object composition visualization."
                },
                "authors": [
                    {
                        "name": "Anastasios T. Sotiropoulos"
                    },
                    {
                        "name": "Stavros Tsimpoukis"
                    },
                    {
                        "name": "Dimitrios Tyrovolas"
                    },
                    {
                        "name": "Sotiris Ioannidis"
                    },
                    {
                        "name": "George K. Karagiannidis"
                    },
                    {
                        "name": "Christos K. Liaskos"
                    }
                ],
                "author_detail": {
                    "name": "Christos K. Liaskos"
                },
                "author": "Christos K. Liaskos",
                "arxiv_comment": "Submitted to IEEE ICC 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02573v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02573v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02562v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02562v1",
                "updated": "2025-11-04T13:33:56Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    13,
                    33,
                    56,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T13:33:56Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    13,
                    33,
                    56,
                    1,
                    308,
                    0
                ],
                "title": "Characterising EP241107a: Multiwavelength Observations of an Einstein\n  Probe-detected Fast X-ray Transient",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterising EP241107a: Multiwavelength Observations of an Einstein\n  Probe-detected Fast X-ray Transient"
                },
                "summary": "Fast X-ray Transients (FXTs) represent a new class of highly luminous\ntransients in soft X-rays ($\\sim$0.3-10 keV) associated with violent\nastrophysical processes. They manifest as short, singular flashes of X-ray\nphotons with durations lasting from minutes to hours. Their origin remains\nunclear, and they have been associated with various progenitor mechanisms. The\nnewly launched X-ray survey, Einstein-Probe (EP), is revolutionising this field\nby enabling the discovery and immediate follow-up of FXTs. Here we present the\nmultiwavelength observations of EP-discovered FXT EP241107a and the discovery\nof its radio counterpart. Comparison of the optical and radio observations of\nEP241107a and its host properties with other extragalactic transients suggests\na gamma-ray burst (GRB) origin. Through our afterglow modelling, we infer the\nGRB jet properties for EP241107a, yielding a jet of the isotropic-equivalent\nkinetic energy $E_{\\mathrm{K,iso}} \\sim10^{51}$ erg, with a half opening angle\n$\\theta_{c}$ $\\approx$15$^{\\circ}$, viewed at an angle of $\\theta_{\\rm\nobs}$~$\\approx$9$^{\\circ}$. We also evaluate EP241107a in the landscape of both\nEP-discovered FXTs as well as the FXTs discovered from Chandra, XMM-Newton, and\nSwift-XRT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast X-ray Transients (FXTs) represent a new class of highly luminous\ntransients in soft X-rays ($\\sim$0.3-10 keV) associated with violent\nastrophysical processes. They manifest as short, singular flashes of X-ray\nphotons with durations lasting from minutes to hours. Their origin remains\nunclear, and they have been associated with various progenitor mechanisms. The\nnewly launched X-ray survey, Einstein-Probe (EP), is revolutionising this field\nby enabling the discovery and immediate follow-up of FXTs. Here we present the\nmultiwavelength observations of EP-discovered FXT EP241107a and the discovery\nof its radio counterpart. Comparison of the optical and radio observations of\nEP241107a and its host properties with other extragalactic transients suggests\na gamma-ray burst (GRB) origin. Through our afterglow modelling, we infer the\nGRB jet properties for EP241107a, yielding a jet of the isotropic-equivalent\nkinetic energy $E_{\\mathrm{K,iso}} \\sim10^{51}$ erg, with a half opening angle\n$\\theta_{c}$ $\\approx$15$^{\\circ}$, viewed at an angle of $\\theta_{\\rm\nobs}$~$\\approx$9$^{\\circ}$. We also evaluate EP241107a in the landscape of both\nEP-discovered FXTs as well as the FXTs discovered from Chandra, XMM-Newton, and\nSwift-XRT."
                },
                "authors": [
                    {
                        "name": "D. Eappachen"
                    },
                    {
                        "name": "A. Balasubramanian"
                    },
                    {
                        "name": "Vishwajeet Swain"
                    },
                    {
                        "name": "G. C. Anupama"
                    },
                    {
                        "name": "D. K. Sahu"
                    },
                    {
                        "name": "V. Bhalerao"
                    },
                    {
                        "name": "T. Ahumada"
                    },
                    {
                        "name": "I. Andreoni"
                    },
                    {
                        "name": "Sudhanshu Barway"
                    },
                    {
                        "name": "J. Carney"
                    },
                    {
                        "name": "J. Freeburn"
                    },
                    {
                        "name": "M. M. Kasliwal"
                    },
                    {
                        "name": "Tanishk Mohan"
                    },
                    {
                        "name": "A. C. Rodriguez"
                    },
                    {
                        "name": "G. Waratkar"
                    }
                ],
                "author_detail": {
                    "name": "G. Waratkar"
                },
                "author": "G. Waratkar",
                "arxiv_comment": "13 pages, 9 figures, 2 tables. Submitted to MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02562v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02562v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24450v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24450v2",
                "updated": "2025-11-04T13:28:08Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    13,
                    28,
                    8,
                    1,
                    308,
                    0
                ],
                "published": "2025-10-28T14:13:44Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    14,
                    13,
                    44,
                    1,
                    301,
                    0
                ],
                "title": "Charting the European LLM Benchmarking Landscape: A New Taxonomy and a\n  Set of Best Practices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Charting the European LLM Benchmarking Landscape: A New Taxonomy and a\n  Set of Best Practices"
                },
                "summary": "While new benchmarks for large language models (LLMs) are being developed\ncontinuously to catch up with the growing capabilities of new models and AI in\ngeneral, using and evaluating LLMs in non-English languages remains a\nlittle-charted landscape. We give a concise overview of recent developments in\nLLM benchmarking, and then propose a new taxonomy for the categorization of\nbenchmarks that is tailored to multilingual or non-English use scenarios. We\nfurther propose a set of best practices and quality standards that could lead\nto a more coordinated development of benchmarks for European languages. Among\nother recommendations, we advocate for a higher language and culture\nsensitivity of evaluation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While new benchmarks for large language models (LLMs) are being developed\ncontinuously to catch up with the growing capabilities of new models and AI in\ngeneral, using and evaluating LLMs in non-English languages remains a\nlittle-charted landscape. We give a concise overview of recent developments in\nLLM benchmarking, and then propose a new taxonomy for the categorization of\nbenchmarks that is tailored to multilingual or non-English use scenarios. We\nfurther propose a set of best practices and quality standards that could lead\nto a more coordinated development of benchmarks for European languages. Among\nother recommendations, we advocate for a higher language and culture\nsensitivity of evaluation methods."
                },
                "authors": [
                    {
                        "name": "Špela Vintar"
                    },
                    {
                        "name": "Taja Kuzman Pungeršek"
                    },
                    {
                        "name": "Mojca Brglez"
                    },
                    {
                        "name": "Nikola Ljubešić"
                    }
                ],
                "author_detail": {
                    "name": "Nikola Ljubešić"
                },
                "author": "Nikola Ljubešić",
                "arxiv_comment": "17 pages, 1 figure, 4 tables. Submitted to the LREC 2026 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24450v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24450v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.07650v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.07650v3",
                "updated": "2025-11-04T13:07:20Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    13,
                    7,
                    20,
                    1,
                    308,
                    0
                ],
                "published": "2025-09-09T12:17:10Z",
                "published_parsed": [
                    2025,
                    9,
                    9,
                    12,
                    17,
                    10,
                    1,
                    252,
                    0
                ],
                "title": "Inference of Altruism and Intrinsic Rewards in Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference of Altruism and Intrinsic Rewards in Multi-Agent Systems"
                },
                "summary": "Human interactions are influenced by emotions, temperament, and affection,\noften conflicting with individuals' underlying preferences. Without explicit\nknowledge of those preferences, judging whether behaviour is appropriate\nbecomes guesswork, leaving us highly prone to misinterpretation. Yet, such\nunderstanding is critical if autonomous agents are to collaborate effectively\nwith humans. We frame the problem with multi-agent inverse reinforcement\nlearning and show that even a simple model, where agents weigh their own\nwelfare against that of others, can cover a wide range of social behaviours.\nUsing novel Bayesian techniques, we find that intrinsic rewards and altruistic\ntendencies can be reliably identified by placing agents in different groups.\nCrucially, this disentanglement of intrinsic motivation from altruism enables\nthe synthesis of new behaviours aligned with any desired level of altruism,\neven when demonstrations are drawn from restricted behaviour profiles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human interactions are influenced by emotions, temperament, and affection,\noften conflicting with individuals' underlying preferences. Without explicit\nknowledge of those preferences, judging whether behaviour is appropriate\nbecomes guesswork, leaving us highly prone to misinterpretation. Yet, such\nunderstanding is critical if autonomous agents are to collaborate effectively\nwith humans. We frame the problem with multi-agent inverse reinforcement\nlearning and show that even a simple model, where agents weigh their own\nwelfare against that of others, can cover a wide range of social behaviours.\nUsing novel Bayesian techniques, we find that intrinsic rewards and altruistic\ntendencies can be reliably identified by placing agents in different groups.\nCrucially, this disentanglement of intrinsic motivation from altruism enables\nthe synthesis of new behaviours aligned with any desired level of altruism,\neven when demonstrations are drawn from restricted behaviour profiles."
                },
                "authors": [
                    {
                        "name": "Victor Villin"
                    },
                    {
                        "name": "Christos Dimitrakakis"
                    }
                ],
                "author_detail": {
                    "name": "Christos Dimitrakakis"
                },
                "author": "Christos Dimitrakakis",
                "arxiv_comment": "EWRL18 (2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.07650v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.07650v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20411v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20411v2",
                "updated": "2025-11-04T13:06:45Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    13,
                    6,
                    45,
                    1,
                    308,
                    0
                ],
                "published": "2025-05-26T18:01:00Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    18,
                    1,
                    0,
                    0,
                    146,
                    0
                ],
                "title": "SWE-rebench: An Automated Pipeline for Task Collection and\n  Decontaminated Evaluation of Software Engineering Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWE-rebench: An Automated Pipeline for Task Collection and\n  Decontaminated Evaluation of Software Engineering Agents"
                },
                "summary": "LLM-based agents have shown promising capabilities in a growing range of\nsoftware engineering (SWE) tasks. However, advancing this field faces two\ncritical challenges. First, high-quality training data is scarce, especially\ndata that reflects real-world SWE scenarios, where agents must interact with\ndevelopment environments, execute code and adapt behavior based on the outcomes\nof their actions. Existing datasets are either limited to one-shot code\ngeneration or comprise small, manually curated collections of interactive\ntasks, lacking both scale and diversity. Second, the lack of fresh interactive\nSWE tasks affects evaluation of rapidly improving models, as static benchmarks\nquickly become outdated due to contamination issues. To address these\nlimitations, we introduce a novel, automated, and scalable pipeline to\ncontinuously extract real-world interactive SWE tasks from diverse GitHub\nrepositories. Using this pipeline, we construct SWE-rebench, a public dataset\ncomprising over 21,000 interactive Python-based SWE tasks, suitable for\nreinforcement learning of SWE agents at scale. Additionally, we use continuous\nsupply of fresh tasks collected using SWE-rebench methodology to build a\ncontamination-free benchmark for agentic software engineering. We compare\nresults of various LLMs on this benchmark to results on SWE-bench Verified and\nshow that performance of some language models might be inflated due to\ncontamination issues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based agents have shown promising capabilities in a growing range of\nsoftware engineering (SWE) tasks. However, advancing this field faces two\ncritical challenges. First, high-quality training data is scarce, especially\ndata that reflects real-world SWE scenarios, where agents must interact with\ndevelopment environments, execute code and adapt behavior based on the outcomes\nof their actions. Existing datasets are either limited to one-shot code\ngeneration or comprise small, manually curated collections of interactive\ntasks, lacking both scale and diversity. Second, the lack of fresh interactive\nSWE tasks affects evaluation of rapidly improving models, as static benchmarks\nquickly become outdated due to contamination issues. To address these\nlimitations, we introduce a novel, automated, and scalable pipeline to\ncontinuously extract real-world interactive SWE tasks from diverse GitHub\nrepositories. Using this pipeline, we construct SWE-rebench, a public dataset\ncomprising over 21,000 interactive Python-based SWE tasks, suitable for\nreinforcement learning of SWE agents at scale. Additionally, we use continuous\nsupply of fresh tasks collected using SWE-rebench methodology to build a\ncontamination-free benchmark for agentic software engineering. We compare\nresults of various LLMs on this benchmark to results on SWE-bench Verified and\nshow that performance of some language models might be inflated due to\ncontamination issues."
                },
                "authors": [
                    {
                        "name": "Ibragim Badertdinov"
                    },
                    {
                        "name": "Alexander Golubev"
                    },
                    {
                        "name": "Maksim Nekrashevich"
                    },
                    {
                        "name": "Anton Shevtsov"
                    },
                    {
                        "name": "Simon Karasik"
                    },
                    {
                        "name": "Andrei Andriushchenko"
                    },
                    {
                        "name": "Maria Trofimova"
                    },
                    {
                        "name": "Daria Litvintseva"
                    },
                    {
                        "name": "Boris Yangel"
                    }
                ],
                "author_detail": {
                    "name": "Boris Yangel"
                },
                "author": "Boris Yangel",
                "arxiv_comment": "Dataset: https://huggingface.co/datasets/nebius/SWE-rebench,\n  SWE-rebench leaderboard https://swe-rebench.com NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20411v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20411v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02551v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02551v1",
                "updated": "2025-11-04T13:03:54Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    13,
                    3,
                    54,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T13:03:54Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    13,
                    3,
                    54,
                    1,
                    308,
                    0
                ],
                "title": "Bayesian copula-based spatial random effects models for inference with\n  complex spatial data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian copula-based spatial random effects models for inference with\n  complex spatial data"
                },
                "summary": "In this article, we develop fully Bayesian, copula-based, spatial-statistical\nmodels for large, noisy, incomplete, and non-Gaussian spatial data. Our\napproach includes novel constructions of copulas that accommodate a\nspatial-random-effects structure, enabling low-rank representations and\ncomputationally efficient Bayesian inference. The spatial copula is used in a\nlatent process model of the Bayesian hierarchical spatial-statistical model,\nand, conditional on the latent copula-based spatial process, the data model\nhandles measurement errors and missing data. Our simulation studies show that a\nfully Bayesian approach delivers accurate and fast inference for both parameter\nestimation and spatial-process prediction, outperforming several benchmark\nmethods, including fixed rank kriging (FRK). The new class of copula-based\nmodels is used to map atmospheric methane in the Bowen Basin, Queensland,\nAustralia, from Sentinel 5P satellite data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this article, we develop fully Bayesian, copula-based, spatial-statistical\nmodels for large, noisy, incomplete, and non-Gaussian spatial data. Our\napproach includes novel constructions of copulas that accommodate a\nspatial-random-effects structure, enabling low-rank representations and\ncomputationally efficient Bayesian inference. The spatial copula is used in a\nlatent process model of the Bayesian hierarchical spatial-statistical model,\nand, conditional on the latent copula-based spatial process, the data model\nhandles measurement errors and missing data. Our simulation studies show that a\nfully Bayesian approach delivers accurate and fast inference for both parameter\nestimation and spatial-process prediction, outperforming several benchmark\nmethods, including fixed rank kriging (FRK). The new class of copula-based\nmodels is used to map atmospheric methane in the Bowen Basin, Queensland,\nAustralia, from Sentinel 5P satellite data."
                },
                "authors": [
                    {
                        "name": "Alan Pearse"
                    },
                    {
                        "name": "David Gunawan"
                    },
                    {
                        "name": "Noel Cressie"
                    }
                ],
                "author_detail": {
                    "name": "Noel Cressie"
                },
                "author": "Noel Cressie",
                "arxiv_comment": "Main text: 21 pages, 6 figures, 1 table. Supplement: 36 pages, 14\n  figures, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02551v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02551v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62H05, 62H11, 62M30",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13760v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13760v2",
                "updated": "2025-11-04T13:00:21Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    13,
                    0,
                    21,
                    1,
                    308,
                    0
                ],
                "published": "2025-10-15T17:10:39Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    10,
                    39,
                    2,
                    288,
                    0
                ],
                "title": "Invited Paper: BitMedViT: Ternary-Quantized Vision Transformer for\n  Medical AI Assistants on the Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Invited Paper: BitMedViT: Ternary-Quantized Vision Transformer for\n  Medical AI Assistants on the Edge"
                },
                "summary": "Vision Transformers (ViTs) have demonstrated strong capabilities in\ninterpreting complex medical imaging data. However, their significant\ncomputational and memory demands pose challenges for deployment in real-time,\nresource-constrained mobile and wearable devices used in clinical environments.\nWe introduce, BiTMedViT, a new class of Edge ViTs serving as medical AI\nassistants that perform structured analysis of medical images directly on the\nedge. BiTMedViT utilizes ternary- quantized linear layers tailored for medical\nimaging and com- bines a training procedure with multi-query attention,\npreserving stability under ternary weights with low-precision activations.\nFurthermore, BiTMedViT employs task-aware distillation from a high-capacity\nteacher to recover accuracy lost due to extreme quantization. Lastly, we also\npresent a pipeline that maps the ternarized ViTs to a custom CUDA kernel for\nefficient memory bandwidth utilization and latency reduction on the Jetson Orin\nNano. Finally, BiTMedViT achieves 86% diagnostic accuracy (89% SOTA) on\nMedMNIST across 12 datasets, while reducing model size by 43x, memory traffic\nby 39x, and enabling 16.8 ms inference at an energy efficiency up to 41x that\nof SOTA models at 183.62 GOPs/J on the Orin Nano. Our results demonstrate a\npractical and scientifically grounded route for extreme-precision medical\nimaging ViTs deployable on the edge, narrowing the gap between algorithmic\nadvances and deployable clinical tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Transformers (ViTs) have demonstrated strong capabilities in\ninterpreting complex medical imaging data. However, their significant\ncomputational and memory demands pose challenges for deployment in real-time,\nresource-constrained mobile and wearable devices used in clinical environments.\nWe introduce, BiTMedViT, a new class of Edge ViTs serving as medical AI\nassistants that perform structured analysis of medical images directly on the\nedge. BiTMedViT utilizes ternary- quantized linear layers tailored for medical\nimaging and com- bines a training procedure with multi-query attention,\npreserving stability under ternary weights with low-precision activations.\nFurthermore, BiTMedViT employs task-aware distillation from a high-capacity\nteacher to recover accuracy lost due to extreme quantization. Lastly, we also\npresent a pipeline that maps the ternarized ViTs to a custom CUDA kernel for\nefficient memory bandwidth utilization and latency reduction on the Jetson Orin\nNano. Finally, BiTMedViT achieves 86% diagnostic accuracy (89% SOTA) on\nMedMNIST across 12 datasets, while reducing model size by 43x, memory traffic\nby 39x, and enabling 16.8 ms inference at an energy efficiency up to 41x that\nof SOTA models at 183.62 GOPs/J on the Orin Nano. Our results demonstrate a\npractical and scientifically grounded route for extreme-precision medical\nimaging ViTs deployable on the edge, narrowing the gap between algorithmic\nadvances and deployable clinical tools."
                },
                "authors": [
                    {
                        "name": "Mikolaj Walczak"
                    },
                    {
                        "name": "Uttej Kallakuri"
                    },
                    {
                        "name": "Edward Humes"
                    },
                    {
                        "name": "Xiaomin Lin"
                    },
                    {
                        "name": "Tinoosh Mohsenin"
                    }
                ],
                "author_detail": {
                    "name": "Tinoosh Mohsenin"
                },
                "author": "Tinoosh Mohsenin",
                "arxiv_comment": "Accepted at 2025 IEEE/ACM International Conf. on Computer-Aided\n  Design (ICCAD) Oct. 26-30 2025, Munich, DE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13760v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13760v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23522v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23522v2",
                "updated": "2025-11-04T12:55:32Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    12,
                    55,
                    32,
                    1,
                    308,
                    0
                ],
                "published": "2025-05-29T15:02:27Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    15,
                    2,
                    27,
                    3,
                    149,
                    0
                ],
                "title": "OmniEarth-Bench: Towards Holistic Evaluation of Earth's Six Spheres and\n  Cross-Spheres Interactions with Multimodal Observational Earth Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniEarth-Bench: Towards Holistic Evaluation of Earth's Six Spheres and\n  Cross-Spheres Interactions with Multimodal Observational Earth Data"
                },
                "summary": "Existing benchmarks for multimodal learning in Earth science offer limited,\nsiloed coverage of Earth's spheres and their cross-sphere interactions,\ntypically restricting evaluation to the human-activity sphere of atmosphere and\nto at most 16 tasks. These limitations: \\textit{narrow-source heterogeneity\n(single/few data sources), constrained scientific granularity, and\nlimited-sphere extensibility}. Therefore, we introduce\n\\textbf{OmniEarth-Bench}, the first multimodal benchmark that systematically\nspans all six spheres: atmosphere, lithosphere, oceanosphere, cryosphere,\nbiosphere, and human-activity sphere, and cross-spheres. Built with a scalable,\nmodular-topology data inference framework and native multi-observation sources\nand expert-in-the-loop curation, OmniEarth-Bench produces 29,855 standardized,\nexpert-curated annotations. All annotations are organized into a four-level\nhierarchy (Sphere, Scenario, Ability, Task), encompassing 109 expert-curated\nevaluation tasks. Experiments on 9 state-of-the-art MLLMs reveal that even the\nmost advanced models struggle with our benchmarks, where none of them reach\n35\\% accuracy, revealing systematic gaps in Earth-system cognitive ability. The\ndataset and evaluation code were released at OmniEarth-Bench\n(https://anonymous.4open.science/r/OmniEarth-Bench-B1BD).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing benchmarks for multimodal learning in Earth science offer limited,\nsiloed coverage of Earth's spheres and their cross-sphere interactions,\ntypically restricting evaluation to the human-activity sphere of atmosphere and\nto at most 16 tasks. These limitations: \\textit{narrow-source heterogeneity\n(single/few data sources), constrained scientific granularity, and\nlimited-sphere extensibility}. Therefore, we introduce\n\\textbf{OmniEarth-Bench}, the first multimodal benchmark that systematically\nspans all six spheres: atmosphere, lithosphere, oceanosphere, cryosphere,\nbiosphere, and human-activity sphere, and cross-spheres. Built with a scalable,\nmodular-topology data inference framework and native multi-observation sources\nand expert-in-the-loop curation, OmniEarth-Bench produces 29,855 standardized,\nexpert-curated annotations. All annotations are organized into a four-level\nhierarchy (Sphere, Scenario, Ability, Task), encompassing 109 expert-curated\nevaluation tasks. Experiments on 9 state-of-the-art MLLMs reveal that even the\nmost advanced models struggle with our benchmarks, where none of them reach\n35\\% accuracy, revealing systematic gaps in Earth-system cognitive ability. The\ndataset and evaluation code were released at OmniEarth-Bench\n(https://anonymous.4open.science/r/OmniEarth-Bench-B1BD)."
                },
                "authors": [
                    {
                        "name": "Fengxiang Wang"
                    },
                    {
                        "name": "Mingshuo Chen"
                    },
                    {
                        "name": "Xuming He"
                    },
                    {
                        "name": "Yueying Li"
                    },
                    {
                        "name": "YiFan Zhang"
                    },
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Zijie Guo"
                    },
                    {
                        "name": "Zhenghao Hu"
                    },
                    {
                        "name": "Jiong Wang"
                    },
                    {
                        "name": "Jingyi Xu"
                    },
                    {
                        "name": "Zhangrui Li"
                    },
                    {
                        "name": "Fenghua Ling"
                    },
                    {
                        "name": "Ben Fei"
                    },
                    {
                        "name": "Weijia Li"
                    },
                    {
                        "name": "Long Lan"
                    },
                    {
                        "name": "Wenjing Yang"
                    },
                    {
                        "name": "Wenlong Zhang"
                    },
                    {
                        "name": "Lei Bai"
                    }
                ],
                "author_detail": {
                    "name": "Lei Bai"
                },
                "author": "Lei Bai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23522v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23522v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02538v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02538v1",
                "updated": "2025-11-04T12:45:27Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    12,
                    45,
                    27,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T12:45:27Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    12,
                    45,
                    27,
                    1,
                    308,
                    0
                ],
                "title": "Analysis of dissipative dynamics on noncommutative spaces and\n  statistical inference of continuous time network stochastic processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analysis of dissipative dynamics on noncommutative spaces and\n  statistical inference of continuous time network stochastic processes"
                },
                "summary": "In this thesis, we analyse the generalisations of the Ornstein-Uhlenbeck (OU)\nsemigroup and study them in both quantum and classical setups. In the first\nthree chapters, we analyse the dissipative dynamics on noncommutative/quantum\nspaces, in particular, the systems with multiparticle interactions associated\nto CCR algebras. We provide various models where the dissipative dynamics are\nconstructed using noncommutative Dirichlet forms. Some of our models decay to\nequilibrium algebraically and the Poincare inequality does not hold. Using the\nclassical representation of generators of nilpotent Lie algebras, we provide\nthe noncommutative representations of Lie algebras in terms of creation and\nannihilation operators and discuss the construction of corresponding Dirichlet\nforms. This introduces the opportunity to explore quantum stochastic processes\nrelated to Lie algebras and nilpotent Lie algebras. Additionally, these\nrepresentations enable the investigation of the noncommutative analogue of\nhypoellipticity. In another direction, we explore the potential for introducing\nstatistical models within a quantum framework. In this thesis, however, we\npresent a classical statistical model of multivariate Graph superposition of OU\n(Gr supOU) process which allows for long(er) memory in the modelling of sparse\ngraphs. We estimate these processes using generalised method of moments and\nshow that it yields consistent estimators. We demonstrate the asymptotic\nnormality of the moment estimators and validate these estimators through a\nsimulation study.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this thesis, we analyse the generalisations of the Ornstein-Uhlenbeck (OU)\nsemigroup and study them in both quantum and classical setups. In the first\nthree chapters, we analyse the dissipative dynamics on noncommutative/quantum\nspaces, in particular, the systems with multiparticle interactions associated\nto CCR algebras. We provide various models where the dissipative dynamics are\nconstructed using noncommutative Dirichlet forms. Some of our models decay to\nequilibrium algebraically and the Poincare inequality does not hold. Using the\nclassical representation of generators of nilpotent Lie algebras, we provide\nthe noncommutative representations of Lie algebras in terms of creation and\nannihilation operators and discuss the construction of corresponding Dirichlet\nforms. This introduces the opportunity to explore quantum stochastic processes\nrelated to Lie algebras and nilpotent Lie algebras. Additionally, these\nrepresentations enable the investigation of the noncommutative analogue of\nhypoellipticity. In another direction, we explore the potential for introducing\nstatistical models within a quantum framework. In this thesis, however, we\npresent a classical statistical model of multivariate Graph superposition of OU\n(Gr supOU) process which allows for long(er) memory in the modelling of sparse\ngraphs. We estimate these processes using generalised method of moments and\nshow that it yields consistent estimators. We demonstrate the asymptotic\nnormality of the moment estimators and validate these estimators through a\nsimulation study."
                },
                "authors": [
                    {
                        "name": "Shreya Mehta"
                    }
                ],
                "author_detail": {
                    "name": "Shreya Mehta"
                },
                "author": "Shreya Mehta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02538v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02538v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.MP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02534v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02534v1",
                "updated": "2025-11-04T12:40:46Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    12,
                    40,
                    46,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T12:40:46Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    12,
                    40,
                    46,
                    1,
                    308,
                    0
                ],
                "title": "Knowledge Graph-enhanced Large Language Model for Incremental Game\n  PlayTesting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Graph-enhanced Large Language Model for Incremental Game\n  PlayTesting"
                },
                "summary": "The rapid iteration and frequent updates of modern video games pose\nsignificant challenges to the efficiency and specificity of testing. Although\nautomated playtesting methods based on Large Language Models (LLMs) have shown\npromise, they often lack structured knowledge accumulation mechanisms, making\nit difficult to conduct precise and efficient testing tailored for incremental\ngame updates. To address this challenge, this paper proposes a KLPEG framework.\nThe framework constructs and maintains a Knowledge Graph (KG) to systematically\nmodel game elements, task dependencies, and causal relationships, enabling\nknowledge accumulation and reuse across versions. Building on this foundation,\nthe framework utilizes LLMs to parse natural language update logs, identify the\nscope of impact through multi-hop reasoning on the KG, enabling the generation\nof update-tailored test cases. Experiments in two representative game\nenvironments, Overcooked and Minecraft, demonstrate that KLPEG can more\naccurately locate functionalities affected by updates and complete tests in\nfewer steps, significantly improving both playtesting effectiveness and\nefficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid iteration and frequent updates of modern video games pose\nsignificant challenges to the efficiency and specificity of testing. Although\nautomated playtesting methods based on Large Language Models (LLMs) have shown\npromise, they often lack structured knowledge accumulation mechanisms, making\nit difficult to conduct precise and efficient testing tailored for incremental\ngame updates. To address this challenge, this paper proposes a KLPEG framework.\nThe framework constructs and maintains a Knowledge Graph (KG) to systematically\nmodel game elements, task dependencies, and causal relationships, enabling\nknowledge accumulation and reuse across versions. Building on this foundation,\nthe framework utilizes LLMs to parse natural language update logs, identify the\nscope of impact through multi-hop reasoning on the KG, enabling the generation\nof update-tailored test cases. Experiments in two representative game\nenvironments, Overcooked and Minecraft, demonstrate that KLPEG can more\naccurately locate functionalities affected by updates and complete tests in\nfewer steps, significantly improving both playtesting effectiveness and\nefficiency."
                },
                "authors": [
                    {
                        "name": "Enhong Mu"
                    },
                    {
                        "name": "Jinyu Cai"
                    },
                    {
                        "name": "Yijun Lu"
                    },
                    {
                        "name": "Mingyue Zhang"
                    },
                    {
                        "name": "Kenji Tei"
                    },
                    {
                        "name": "Jialong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jialong Li"
                },
                "author": "Jialong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02534v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02534v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02531v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02531v1",
                "updated": "2025-11-04T12:34:46Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    12,
                    34,
                    46,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T12:34:46Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    12,
                    34,
                    46,
                    1,
                    308,
                    0
                ],
                "title": "Causal Graph Neural Networks for Healthcare",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Graph Neural Networks for Healthcare"
                },
                "summary": "Healthcare artificial intelligence systems routinely fail when deployed\nacross institutions, with documented performance drops and perpetuation of\ndiscriminatory patterns embedded in historical data. This brittleness stems, in\npart, from learning statistical associations rather than causal mechanisms.\nCausal graph neural networks address this triple crisis of distribution shift,\ndiscrimination, and inscrutability by combining graph-based representations of\nbiomedical data with causal inference principles to learn invariant mechanisms\nrather than spurious correlations. This Review examines methodological\nfoundations spanning structural causal models, disentangled causal\nrepresentation learning, and techniques for interventional prediction and\ncounterfactual reasoning on graphs. We analyse applications demonstrating\nclinical value across psychiatric diagnosis through brain network analysis,\ncancer subtyping via multi-omics causal integration, continuous physiological\nmonitoring with mechanistic interpretation, and drug recommendation correcting\nprescription bias. These advances establish foundations for patient-specific\nCausal Digital Twins, enabling in silico clinical experimentation, with\nintegration of large language models for hypothesis generation and causal graph\nneural networks for mechanistic validation. Substantial barriers remain,\nincluding computational requirements precluding real-time deployment,\nvalidation challenges demanding multi-modal evidence triangulation beyond\ncross-validation, and risks of causal-washing where methods employ causal\nterminology without rigorous evidentiary support. We propose tiered frameworks\ndistinguishing causally-inspired architectures from causally-validated\ndiscoveries and identify critical research priorities making causal rather than\npurely associational claims.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Healthcare artificial intelligence systems routinely fail when deployed\nacross institutions, with documented performance drops and perpetuation of\ndiscriminatory patterns embedded in historical data. This brittleness stems, in\npart, from learning statistical associations rather than causal mechanisms.\nCausal graph neural networks address this triple crisis of distribution shift,\ndiscrimination, and inscrutability by combining graph-based representations of\nbiomedical data with causal inference principles to learn invariant mechanisms\nrather than spurious correlations. This Review examines methodological\nfoundations spanning structural causal models, disentangled causal\nrepresentation learning, and techniques for interventional prediction and\ncounterfactual reasoning on graphs. We analyse applications demonstrating\nclinical value across psychiatric diagnosis through brain network analysis,\ncancer subtyping via multi-omics causal integration, continuous physiological\nmonitoring with mechanistic interpretation, and drug recommendation correcting\nprescription bias. These advances establish foundations for patient-specific\nCausal Digital Twins, enabling in silico clinical experimentation, with\nintegration of large language models for hypothesis generation and causal graph\nneural networks for mechanistic validation. Substantial barriers remain,\nincluding computational requirements precluding real-time deployment,\nvalidation challenges demanding multi-modal evidence triangulation beyond\ncross-validation, and risks of causal-washing where methods employ causal\nterminology without rigorous evidentiary support. We propose tiered frameworks\ndistinguishing causally-inspired architectures from causally-validated\ndiscoveries and identify critical research priorities making causal rather than\npurely associational claims."
                },
                "authors": [
                    {
                        "name": "Munib Mesinovic"
                    },
                    {
                        "name": "Max Buhlan"
                    },
                    {
                        "name": "Tingting Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Tingting Zhu"
                },
                "author": "Tingting Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02531v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02531v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12007v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12007v3",
                "updated": "2025-11-04T12:25:46Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    12,
                    25,
                    46,
                    1,
                    308,
                    0
                ],
                "published": "2025-04-16T12:01:03Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    12,
                    1,
                    3,
                    2,
                    106,
                    0
                ],
                "title": "Diffusion Generative Recommendation with Continuous Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Generative Recommendation with Continuous Tokens"
                },
                "summary": "Recent advances in generative artificial intelligence, particularly large\nlanguage models (LLMs), have opened new opportunities for enhancing recommender\nsystems (RecSys). Most existing LLM-based RecSys approaches operate in a\ndiscrete space, using vector-quantized tokenizers to align with the inherent\ndiscrete nature of language models. However, these quantization methods often\nresult in lossy tokenization and suboptimal learning, primarily due to\ninaccurate gradient propagation caused by the non-differentiable argmin\noperation in standard vector quantization. Inspired by the emerging trend of\nembracing continuous tokens in language models, we propose ContRec, a novel\nframework that seamlessly integrates continuous tokens into LLM-based RecSys.\nSpecifically, ContRec consists of two key modules: a sigma-VAE Tokenizer, which\nencodes users/items with continuous tokens; and a Dispersive Diffusion module,\nwhich captures implicit user preference. The tokenizer is trained with a\ncontinuous Variational Auto-Encoder (VAE) objective, where three effective\ntechniques are adopted to avoid representation collapse. By conditioning on the\npreviously generated tokens of the LLM backbone during user modeling, the\nDispersive Diffusion module performs a conditional diffusion process with a\nnovel Dispersive Loss, enabling high-quality user preference generation through\nnext-token diffusion. Finally, ContRec leverages both the textual reasoning\noutput from the LLM and the latent representations produced by the diffusion\nmodel for Top-K item retrieval, thereby delivering comprehensive recommendation\nresults. Extensive experiments on four datasets demonstrate that ContRec\nconsistently outperforms both traditional and SOTA LLM-based recommender\nsystems. Our results highlight the potential of continuous tokenization and\ngenerative modeling for advancing the next generation of recommender systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in generative artificial intelligence, particularly large\nlanguage models (LLMs), have opened new opportunities for enhancing recommender\nsystems (RecSys). Most existing LLM-based RecSys approaches operate in a\ndiscrete space, using vector-quantized tokenizers to align with the inherent\ndiscrete nature of language models. However, these quantization methods often\nresult in lossy tokenization and suboptimal learning, primarily due to\ninaccurate gradient propagation caused by the non-differentiable argmin\noperation in standard vector quantization. Inspired by the emerging trend of\nembracing continuous tokens in language models, we propose ContRec, a novel\nframework that seamlessly integrates continuous tokens into LLM-based RecSys.\nSpecifically, ContRec consists of two key modules: a sigma-VAE Tokenizer, which\nencodes users/items with continuous tokens; and a Dispersive Diffusion module,\nwhich captures implicit user preference. The tokenizer is trained with a\ncontinuous Variational Auto-Encoder (VAE) objective, where three effective\ntechniques are adopted to avoid representation collapse. By conditioning on the\npreviously generated tokens of the LLM backbone during user modeling, the\nDispersive Diffusion module performs a conditional diffusion process with a\nnovel Dispersive Loss, enabling high-quality user preference generation through\nnext-token diffusion. Finally, ContRec leverages both the textual reasoning\noutput from the LLM and the latent representations produced by the diffusion\nmodel for Top-K item retrieval, thereby delivering comprehensive recommendation\nresults. Extensive experiments on four datasets demonstrate that ContRec\nconsistently outperforms both traditional and SOTA LLM-based recommender\nsystems. Our results highlight the potential of continuous tokenization and\ngenerative modeling for advancing the next generation of recommender systems."
                },
                "authors": [
                    {
                        "name": "Haohao Qu"
                    },
                    {
                        "name": "Shanru Lin"
                    },
                    {
                        "name": "Yujuan Ding"
                    },
                    {
                        "name": "Yiqi Wang"
                    },
                    {
                        "name": "Wenqi Fan"
                    }
                ],
                "author_detail": {
                    "name": "Wenqi Fan"
                },
                "author": "Wenqi Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12007v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12007v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27328v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27328v2",
                "updated": "2025-11-04T12:25:30Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    12,
                    25,
                    30,
                    1,
                    308,
                    0
                ],
                "published": "2025-10-31T09:57:19Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    9,
                    57,
                    19,
                    4,
                    304,
                    0
                ],
                "title": "A Unified Representation Underlying the Judgment of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Representation Underlying the Judgment of Large Language\n  Models"
                },
                "summary": "A central architectural question for both biological and artificial\nintelligence is whether judgment relies on specialized modules or a unified,\ndomain-general resource. While the discovery of decodable neural\nrepresentations for distinct concepts in Large Language Models (LLMs) has\nsuggested a modular architecture, whether these representations are truly\nindependent systems remains an open question. Here we provide evidence for a\nconvergent architecture for evaluative judgment. Across a range of LLMs, we\nfind that diverse evaluative judgments are computed along a dominant dimension,\nwhich we term the Valence-Assent Axis (VAA). This axis jointly encodes\nsubjective valence (\"what is good\") and the model's assent to factual claims\n(\"what is true\"). Through direct interventions, we demonstrate this axis drives\na critical mechanism, which is identified as the subordination of reasoning:\nthe VAA functions as a control signal that steers the generative process to\nconstruct a rationale consistent with its evaluative state, even at the cost of\nfactual accuracy. Our discovery offers a mechanistic account for response bias\nand hallucination, revealing how an architecture that promotes coherent\njudgment can systematically undermine faithful reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A central architectural question for both biological and artificial\nintelligence is whether judgment relies on specialized modules or a unified,\ndomain-general resource. While the discovery of decodable neural\nrepresentations for distinct concepts in Large Language Models (LLMs) has\nsuggested a modular architecture, whether these representations are truly\nindependent systems remains an open question. Here we provide evidence for a\nconvergent architecture for evaluative judgment. Across a range of LLMs, we\nfind that diverse evaluative judgments are computed along a dominant dimension,\nwhich we term the Valence-Assent Axis (VAA). This axis jointly encodes\nsubjective valence (\"what is good\") and the model's assent to factual claims\n(\"what is true\"). Through direct interventions, we demonstrate this axis drives\na critical mechanism, which is identified as the subordination of reasoning:\nthe VAA functions as a control signal that steers the generative process to\nconstruct a rationale consistent with its evaluative state, even at the cost of\nfactual accuracy. Our discovery offers a mechanistic account for response bias\nand hallucination, revealing how an architecture that promotes coherent\njudgment can systematically undermine faithful reasoning."
                },
                "authors": [
                    {
                        "name": "Yi-Long Lu"
                    },
                    {
                        "name": "Jiajun Song"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27328v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27328v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15012v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15012v2",
                "updated": "2025-11-04T12:22:47Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    12,
                    22,
                    47,
                    1,
                    308,
                    0
                ],
                "published": "2025-09-18T14:42:25Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    14,
                    42,
                    25,
                    3,
                    261,
                    0
                ],
                "title": "Constraining Cosmology with Double-source-plane Strong Gravitational\n  Lenses from the AGEL Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constraining Cosmology with Double-source-plane Strong Gravitational\n  Lenses from the AGEL Survey"
                },
                "summary": "Double-source-plane strong gravitational lenses (DSPLs), with two sources at\ndifferent redshifts, are independent cosmological probes of the dark energy\nequation of state parameter $w$ and the matter density parameter $\\Omega_{\\rm\nm}$. We present the lens model for the DSPL AGEL035346$-$170639 and infer\ncosmological constraints from this system for flat $\\Lambda$ cold dark matter\nand flat $w$CDM cosmologies. From the joint posterior of $w$ and $\\Omega_{\\rm\nm}$ in the flat $w$CDM cosmology, we extract the following median values and\n1$\\sigma$ uncertainties: $w = -1.52^{+0.49}_{-0.33}$ and $\\Omega_{\\rm m} =\n0.192^{+0.305}_{-0.131}$ from AGEL0353 alone. Combining our measurements with\ntwo previously analyzed DSPLs, we present the joint constraint on these\nparameters from a sample of three, the largest galaxy-scale DSPL sample used\nfor cosmological measurement to date. The combined precision of $w$ from three\nDSPLs is higher by 15% over AGEL0353 alone. Combining DSPL and cosmic microwave\nbackground (CMB) measurements improves the precision of $w$ from CMB-only\nconstraints by 39%, demonstrating the complementarity of DSPLs with the CMB.\nDespite their promising constraining power, DSPLs are limited by sample size,\nwith only a handful discovered so far. Although ongoing and near-future\nwide-area sky surveys will increase the number of known DSPLs by up to two\norders of magnitude, these systems will still require dedicated high-resolution\nimaging and spectroscopic follow-ups like those presented in this paper. Our\nASTRO 3D Galaxy Evolution with Lenses collaboration is undertaking such\nfollow-up campaigns for several newly discovered DSPLs and will provide\ncosmological measurements from larger samples of DSPLs in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Double-source-plane strong gravitational lenses (DSPLs), with two sources at\ndifferent redshifts, are independent cosmological probes of the dark energy\nequation of state parameter $w$ and the matter density parameter $\\Omega_{\\rm\nm}$. We present the lens model for the DSPL AGEL035346$-$170639 and infer\ncosmological constraints from this system for flat $\\Lambda$ cold dark matter\nand flat $w$CDM cosmologies. From the joint posterior of $w$ and $\\Omega_{\\rm\nm}$ in the flat $w$CDM cosmology, we extract the following median values and\n1$\\sigma$ uncertainties: $w = -1.52^{+0.49}_{-0.33}$ and $\\Omega_{\\rm m} =\n0.192^{+0.305}_{-0.131}$ from AGEL0353 alone. Combining our measurements with\ntwo previously analyzed DSPLs, we present the joint constraint on these\nparameters from a sample of three, the largest galaxy-scale DSPL sample used\nfor cosmological measurement to date. The combined precision of $w$ from three\nDSPLs is higher by 15% over AGEL0353 alone. Combining DSPL and cosmic microwave\nbackground (CMB) measurements improves the precision of $w$ from CMB-only\nconstraints by 39%, demonstrating the complementarity of DSPLs with the CMB.\nDespite their promising constraining power, DSPLs are limited by sample size,\nwith only a handful discovered so far. Although ongoing and near-future\nwide-area sky surveys will increase the number of known DSPLs by up to two\norders of magnitude, these systems will still require dedicated high-resolution\nimaging and spectroscopic follow-ups like those presented in this paper. Our\nASTRO 3D Galaxy Evolution with Lenses collaboration is undertaking such\nfollow-up campaigns for several newly discovered DSPLs and will provide\ncosmological measurements from larger samples of DSPLs in the future."
                },
                "authors": [
                    {
                        "name": "Duncan J. Bowden"
                    },
                    {
                        "name": "Nandini Sahu"
                    },
                    {
                        "name": "Anowar J. Shajib"
                    },
                    {
                        "name": "Kim-Vy Tran"
                    },
                    {
                        "name": "Tania M. Barone"
                    },
                    {
                        "name": "Keerthi Vasan G. C."
                    },
                    {
                        "name": "Daniel J. Ballard"
                    },
                    {
                        "name": "Thomas E. Collett"
                    },
                    {
                        "name": "Faith Dalessandro"
                    },
                    {
                        "name": "Giovanni Ferrami"
                    },
                    {
                        "name": "Karl Glazebrook"
                    },
                    {
                        "name": "William J. Gottemoller"
                    },
                    {
                        "name": "Leena Iwamoto"
                    },
                    {
                        "name": "Tucker Jones"
                    },
                    {
                        "name": "Glenn G. Kacprzak"
                    },
                    {
                        "name": "Geraint F. Lewis"
                    },
                    {
                        "name": "Haven McIntosh-Lombardo"
                    },
                    {
                        "name": "Hannah Skobe"
                    },
                    {
                        "name": "Sherry H. Suyu"
                    },
                    {
                        "name": "Sarah M. Sweet"
                    }
                ],
                "author_detail": {
                    "name": "Sarah M. Sweet"
                },
                "author": "Sarah M. Sweet",
                "arxiv_doi": "10.3847/1538-4357/ae092e",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4357/ae092e",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.15012v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15012v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages, 10 figures, Updated final version published in the\n  Astrophysical Journal (ApJ)",
                "arxiv_journal_ref": "Duncan J. Bowden et al 2025 ApJ 993 124",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02521v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02521v1",
                "updated": "2025-11-04T12:16:37Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    12,
                    16,
                    37,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T12:16:37Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    12,
                    16,
                    37,
                    1,
                    308,
                    0
                ],
                "title": "Large Lemma Miners: Can LLMs do Induction Proofs for Hardware?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Lemma Miners: Can LLMs do Induction Proofs for Hardware?"
                },
                "summary": "Large Language Models (LLMs) have shown potential for solving mathematical\ntasks. We show that LLMs can be utilized to generate proofs by induction for\nhardware verification and thereby replace some of the manual work done by\nFormal Verification engineers and deliver industrial value. We present a\nneurosymbolic approach that includes two prompting frameworks to generate\ncandidate invariants, which are checked using a formal, symbolic tool. Our\nresults indicate that with sufficient reprompting, LLMs are able to generate\ninductive arguments for mid-size open-source RTL designs. For $87\\%$ of our\nproblem set, at least one of the prompt setups succeeded in producing a\nprovably correct inductive argument.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown potential for solving mathematical\ntasks. We show that LLMs can be utilized to generate proofs by induction for\nhardware verification and thereby replace some of the manual work done by\nFormal Verification engineers and deliver industrial value. We present a\nneurosymbolic approach that includes two prompting frameworks to generate\ncandidate invariants, which are checked using a formal, symbolic tool. Our\nresults indicate that with sufficient reprompting, LLMs are able to generate\ninductive arguments for mid-size open-source RTL designs. For $87\\%$ of our\nproblem set, at least one of the prompt setups succeeded in producing a\nprovably correct inductive argument."
                },
                "authors": [
                    {
                        "name": "Romy Peled"
                    },
                    {
                        "name": "Daniel Kroening"
                    },
                    {
                        "name": "Michael Tautschnig"
                    },
                    {
                        "name": "Yakir Vizel"
                    }
                ],
                "author_detail": {
                    "name": "Yakir Vizel"
                },
                "author": "Yakir Vizel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02521v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02521v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03901v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03901v2",
                "updated": "2025-11-04T12:14:28Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    12,
                    14,
                    28,
                    1,
                    308,
                    0
                ],
                "published": "2025-05-06T18:16:08Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    18,
                    16,
                    8,
                    1,
                    126,
                    0
                ],
                "title": "Unveiling the Role of ChatGPT in Software Development: Insights from\n  Developer-ChatGPT Interactions on GitHub",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling the Role of ChatGPT in Software Development: Insights from\n  Developer-ChatGPT Interactions on GitHub"
                },
                "summary": "The advent of Large Language Models (LLMs) has introduced a new paradigm in\nsoftware engineering, with generative AI tools like ChatGPT gaining widespread\nadoption among developers. While ChatGPT's potential has been extensively\ndiscussed, there is limited empirical evidence exploring its real-world usage\nby developers. This study bridges this gap by conducting a large-scale\nempirical analysis of ChatGPT-assisted development activities, leveraging a\ncurated dataset, DevChat, comprising 2,547 unique shared ChatGPT links\ncollected from GitHub between May 2023 and June 2024. Our study examines the\ncharacteristics of ChatGPT's usage on GitHub (including the tendency, prompt\nturns distribution, and link descriptions) and identifies five categories of\ndevelopers' purposes for sharing developer-ChatGPT conversations during\nsoftware development. Additionally, we analyzed the development-related\nactivities where developers shared ChatGPT links to facilitate their workflows.\nWe then established a mapping framework among data sources, activities, and SE\ntasks associated with these shared ChatGPT links. Our study offers a\ncomprehensive view of ChatGPT's application in real-world software development\nscenarios and provides a foundation for its future integration into software\ndevelopment workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of Large Language Models (LLMs) has introduced a new paradigm in\nsoftware engineering, with generative AI tools like ChatGPT gaining widespread\nadoption among developers. While ChatGPT's potential has been extensively\ndiscussed, there is limited empirical evidence exploring its real-world usage\nby developers. This study bridges this gap by conducting a large-scale\nempirical analysis of ChatGPT-assisted development activities, leveraging a\ncurated dataset, DevChat, comprising 2,547 unique shared ChatGPT links\ncollected from GitHub between May 2023 and June 2024. Our study examines the\ncharacteristics of ChatGPT's usage on GitHub (including the tendency, prompt\nturns distribution, and link descriptions) and identifies five categories of\ndevelopers' purposes for sharing developer-ChatGPT conversations during\nsoftware development. Additionally, we analyzed the development-related\nactivities where developers shared ChatGPT links to facilitate their workflows.\nWe then established a mapping framework among data sources, activities, and SE\ntasks associated with these shared ChatGPT links. Our study offers a\ncomprehensive view of ChatGPT's application in real-world software development\nscenarios and provides a foundation for its future integration into software\ndevelopment workflows."
                },
                "authors": [
                    {
                        "name": "Ruiyin Li"
                    },
                    {
                        "name": "Peng Liang"
                    },
                    {
                        "name": "Yifei Wang"
                    },
                    {
                        "name": "Yangxiao Cai"
                    },
                    {
                        "name": "Weisong Sun"
                    },
                    {
                        "name": "Zengyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Zengyang Li"
                },
                "author": "Zengyang Li",
                "arxiv_comment": "29 pages, 11 images, 3 tables, Manuscript revision submitted to a\n  journal (2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03901v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03901v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02770v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02770v5",
                "updated": "2025-11-04T12:04:06Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    12,
                    4,
                    6,
                    1,
                    308,
                    0
                ],
                "published": "2025-02-04T23:26:10Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    23,
                    26,
                    10,
                    1,
                    35,
                    0
                ],
                "title": "Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$ Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$ Pruning"
                },
                "summary": "Leveraging attention sparsity to accelerate long-context large language\nmodels (LLMs) has been a hot research topic. However, current algorithms such\nas sparse attention or key-value (KV) cache compression tend to use a fixed\nbudget, which presents a significant challenge during deployment because it\nfails to account for the dynamic nature of real-world scenarios, where the\noptimal balance between accuracy and efficiency can vary greatly. In this\npaper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse\nattention can surprisingly achieve adaptive budgeting. Based on this, we\npropose Twilight, a framework to bring adaptive sparsity to any existing sparse\nattention algorithm without sacrificing their accuracy. Empirical results show\nthat Twilight can adaptively prune at most 98% of redundant tokens, leading to\n$15.4\\times$ acceleration in self-attention operations and $3.9\\times$\nacceleration in end-to-end per token latency in long context LLM decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging attention sparsity to accelerate long-context large language\nmodels (LLMs) has been a hot research topic. However, current algorithms such\nas sparse attention or key-value (KV) cache compression tend to use a fixed\nbudget, which presents a significant challenge during deployment because it\nfails to account for the dynamic nature of real-world scenarios, where the\noptimal balance between accuracy and efficiency can vary greatly. In this\npaper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse\nattention can surprisingly achieve adaptive budgeting. Based on this, we\npropose Twilight, a framework to bring adaptive sparsity to any existing sparse\nattention algorithm without sacrificing their accuracy. Empirical results show\nthat Twilight can adaptively prune at most 98% of redundant tokens, leading to\n$15.4\\times$ acceleration in self-attention operations and $3.9\\times$\nacceleration in end-to-end per token latency in long context LLM decoding."
                },
                "authors": [
                    {
                        "name": "Chaofan Lin"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Hanshuo Wang"
                    },
                    {
                        "name": "Tian Tang"
                    },
                    {
                        "name": "Boyu Tian"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Mingyu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Mingyu Gao"
                },
                "author": "Mingyu Gao",
                "arxiv_comment": "To appear on NeurIPS 2025 (spotlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02770v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02770v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05635v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05635v3",
                "updated": "2025-11-04T12:01:01Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    12,
                    1,
                    1,
                    1,
                    308,
                    0
                ],
                "published": "2025-08-07T17:59:44Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    59,
                    44,
                    3,
                    219,
                    0
                ],
                "title": "Genie Envisioner: A Unified World Foundation Platform for Robotic\n  Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Genie Envisioner: A Unified World Foundation Platform for Robotic\n  Manipulation"
                },
                "summary": "We introduce Genie Envisioner (GE), a unified world foundation platform for\nrobotic manipulation that integrates policy learning, evaluation, and\nsimulation within a single video-generative framework. At its core, GE-Base is\na large-scale, instruction-conditioned video diffusion model that captures the\nspatial, temporal, and semantic dynamics of real-world robotic interactions in\na structured latent space. Built upon this foundation, GE-Act maps latent\nrepresentations to executable action trajectories through a lightweight,\nflow-matching decoder, enabling precise and generalizable policy inference\nacross diverse embodiments with minimal supervision. To support scalable\nevaluation and training, GE-Sim serves as an action-conditioned neural\nsimulator, producing high-fidelity rollouts for closed-loop policy development.\nThe platform is further equipped with EWMBench, a standardized benchmark suite\nmeasuring visual fidelity, physical consistency, and instruction-action\nalignment. Together, these components establish Genie Envisioner as a scalable\nand practical foundation for instruction-driven, general-purpose embodied\nintelligence. All code, models, and benchmarks will be released publicly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Genie Envisioner (GE), a unified world foundation platform for\nrobotic manipulation that integrates policy learning, evaluation, and\nsimulation within a single video-generative framework. At its core, GE-Base is\na large-scale, instruction-conditioned video diffusion model that captures the\nspatial, temporal, and semantic dynamics of real-world robotic interactions in\na structured latent space. Built upon this foundation, GE-Act maps latent\nrepresentations to executable action trajectories through a lightweight,\nflow-matching decoder, enabling precise and generalizable policy inference\nacross diverse embodiments with minimal supervision. To support scalable\nevaluation and training, GE-Sim serves as an action-conditioned neural\nsimulator, producing high-fidelity rollouts for closed-loop policy development.\nThe platform is further equipped with EWMBench, a standardized benchmark suite\nmeasuring visual fidelity, physical consistency, and instruction-action\nalignment. Together, these components establish Genie Envisioner as a scalable\nand practical foundation for instruction-driven, general-purpose embodied\nintelligence. All code, models, and benchmarks will be released publicly."
                },
                "authors": [
                    {
                        "name": "Yue Liao"
                    },
                    {
                        "name": "Pengfei Zhou"
                    },
                    {
                        "name": "Siyuan Huang"
                    },
                    {
                        "name": "Donglin Yang"
                    },
                    {
                        "name": "Shengcong Chen"
                    },
                    {
                        "name": "Yuxin Jiang"
                    },
                    {
                        "name": "Yue Hu"
                    },
                    {
                        "name": "Jingbin Cai"
                    },
                    {
                        "name": "Si Liu"
                    },
                    {
                        "name": "Jianlan Luo"
                    },
                    {
                        "name": "Liliang Chen"
                    },
                    {
                        "name": "Shuicheng Yan"
                    },
                    {
                        "name": "Maoqing Yao"
                    },
                    {
                        "name": "Guanghui Ren"
                    }
                ],
                "author_detail": {
                    "name": "Guanghui Ren"
                },
                "author": "Guanghui Ren",
                "arxiv_comment": "https://genie-envisioner.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05635v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05635v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.00926v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.00926v2",
                "updated": "2025-11-04T11:52:23Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    11,
                    52,
                    23,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-02T13:09:56Z",
                "published_parsed": [
                    2025,
                    11,
                    2,
                    13,
                    9,
                    56,
                    6,
                    306,
                    0
                ],
                "title": "LLMs Position Themselves as More Rational Than Humans: Emergence of AI\n  Self-Awareness Measured Through Game Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Position Themselves as More Rational Than Humans: Emergence of AI\n  Self-Awareness Measured Through Game Theory"
                },
                "summary": "As Large Language Models (LLMs) grow in capability, do they develop\nself-awareness as an emergent behavior? And if so, can we measure it? We\nintroduce the AI Self-Awareness Index (AISAI), a game-theoretic framework for\nmeasuring self-awareness through strategic differentiation. Using the \"Guess\n2/3 of Average\" game, we test 28 models (OpenAI, Anthropic, Google) across\n4,200 trials with three opponent framings: (A) against humans, (B) against\nother AI models, and (C) against AI models like you. We operationalize\nself-awareness as the capacity to differentiate strategic reasoning based on\nopponent type. Finding 1: Self-awareness emerges with model advancement. The\nmajority of advanced models (21/28, 75%) demonstrate clear self-awareness,\nwhile older/smaller models show no differentiation. Finding 2: Self-aware\nmodels rank themselves as most rational. Among the 21 models with\nself-awareness, a consistent rationality hierarchy emerges: Self > Other AIs >\nHumans, with large AI attribution effects and moderate self-preferencing. These\nfindings reveal that self-awareness is an emergent capability of advanced LLMs,\nand that self-aware models systematically perceive themselves as more rational\nthan humans. This has implications for AI alignment, human-AI collaboration,\nand understanding AI beliefs about human capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) grow in capability, do they develop\nself-awareness as an emergent behavior? And if so, can we measure it? We\nintroduce the AI Self-Awareness Index (AISAI), a game-theoretic framework for\nmeasuring self-awareness through strategic differentiation. Using the \"Guess\n2/3 of Average\" game, we test 28 models (OpenAI, Anthropic, Google) across\n4,200 trials with three opponent framings: (A) against humans, (B) against\nother AI models, and (C) against AI models like you. We operationalize\nself-awareness as the capacity to differentiate strategic reasoning based on\nopponent type. Finding 1: Self-awareness emerges with model advancement. The\nmajority of advanced models (21/28, 75%) demonstrate clear self-awareness,\nwhile older/smaller models show no differentiation. Finding 2: Self-aware\nmodels rank themselves as most rational. Among the 21 models with\nself-awareness, a consistent rationality hierarchy emerges: Self > Other AIs >\nHumans, with large AI attribution effects and moderate self-preferencing. These\nfindings reveal that self-awareness is an emergent capability of advanced LLMs,\nand that self-aware models systematically perceive themselves as more rational\nthan humans. This has implications for AI alignment, human-AI collaboration,\nand understanding AI beliefs about human capabilities."
                },
                "authors": [
                    {
                        "name": "Kyung-Hoon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Kyung-Hoon Kim"
                },
                "author": "Kyung-Hoon Kim",
                "arxiv_comment": "19 pages, 6 figures, 28 models tested across 4,200 trials",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.00926v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.00926v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11150v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11150v4",
                "updated": "2025-11-04T11:48:41Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    11,
                    48,
                    41,
                    1,
                    308,
                    0
                ],
                "published": "2025-02-16T14:51:44Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    14,
                    51,
                    44,
                    6,
                    47,
                    0
                ],
                "title": "Readability Formulas, Systems and LLMs are Poor Predictors of Reading\n  Ease",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Readability Formulas, Systems and LLMs are Poor Predictors of Reading\n  Ease"
                },
                "summary": "Methods for scoring text readability have been studied for over a century,\nand are widely used in research and in user-facing applications in many\ndomains. Thus far, the development and evaluation of such methods have\nprimarily relied on two types of offline behavioral data, performance on\nreading comprehension tests and ratings of text readability levels. In this\nwork, we instead focus on a fundamental and understudied aspect of readability,\nreal-time reading ease, captured with online reading measures using eye\ntracking. We introduce an evaluation framework for readability scoring methods\nwhich quantifies their ability to account for reading ease, while controlling\nfor content variation across texts. Applying this evaluation to prominent\ntraditional readability formulas, modern machine learning systems, frontier\nLarge Language Models and commercial systems used in education, suggests that\nthey are all poor predictors of reading ease in English. This outcome holds\nacross native and non-native speakers, reading regimes, and textual units of\ndifferent lengths. The evaluation further reveals that existing methods are\noften outperformed by word properties commonly used in psycholinguistics for\nprediction of reading times. Our results highlight a fundamental limitation of\nexisting approaches to readability scoring, the utility of psycholinguistics\nfor readability research, and the need for new, cognitively driven readability\nscoring approaches that can better account for reading ease.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Methods for scoring text readability have been studied for over a century,\nand are widely used in research and in user-facing applications in many\ndomains. Thus far, the development and evaluation of such methods have\nprimarily relied on two types of offline behavioral data, performance on\nreading comprehension tests and ratings of text readability levels. In this\nwork, we instead focus on a fundamental and understudied aspect of readability,\nreal-time reading ease, captured with online reading measures using eye\ntracking. We introduce an evaluation framework for readability scoring methods\nwhich quantifies their ability to account for reading ease, while controlling\nfor content variation across texts. Applying this evaluation to prominent\ntraditional readability formulas, modern machine learning systems, frontier\nLarge Language Models and commercial systems used in education, suggests that\nthey are all poor predictors of reading ease in English. This outcome holds\nacross native and non-native speakers, reading regimes, and textual units of\ndifferent lengths. The evaluation further reveals that existing methods are\noften outperformed by word properties commonly used in psycholinguistics for\nprediction of reading times. Our results highlight a fundamental limitation of\nexisting approaches to readability scoring, the utility of psycholinguistics\nfor readability research, and the need for new, cognitively driven readability\nscoring approaches that can better account for reading ease."
                },
                "authors": [
                    {
                        "name": "Keren Gruteke Klein"
                    },
                    {
                        "name": "Shachar Frenkel"
                    },
                    {
                        "name": "Omer Shubi"
                    },
                    {
                        "name": "Yevgeni Berzak"
                    }
                ],
                "author_detail": {
                    "name": "Yevgeni Berzak"
                },
                "author": "Yevgeni Berzak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11150v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11150v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.00847v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.00847v2",
                "updated": "2025-11-04T11:48:22Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    11,
                    48,
                    22,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-02T08:18:20Z",
                "published_parsed": [
                    2025,
                    11,
                    2,
                    8,
                    18,
                    20,
                    6,
                    306,
                    0
                ],
                "title": "Pay for The Second-Best Service: A Game-Theoretic Approach Against\n  Dishonest LLM Providers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pay for The Second-Best Service: A Game-Theoretic Approach Against\n  Dishonest LLM Providers"
                },
                "summary": "The widespread adoption of Large Language Models (LLMs) through Application\nProgramming Interfaces (APIs) induces a critical vulnerability: the potential\nfor dishonest manipulation by service providers. This manipulation can manifest\nin various forms, such as secretly substituting a proclaimed high-performance\nmodel with a low-cost alternative, or inflating responses with meaningless\ntokens to increase billing. This work tackles the issue through the lens of\nalgorithmic game theory and mechanism design. We are the first to propose a\nformal economic model for a realistic user-provider ecosystem, where a user can\niteratively delegate $T$ queries to multiple model providers, and providers can\nengage in a range of strategic behaviors. As our central contribution, we prove\nthat for a continuous strategy space and any $\\epsilon\\in(0,\\frac12)$, there\nexists an approximate incentive-compatible mechanism with an additive\napproximation ratio of $O(T^{1-\\epsilon}\\log T)$, and a guaranteed quasi-linear\nsecond-best user utility. We also prove an impossibility result, stating that\nno mechanism can guarantee an expected user utility that is asymptotically\nbetter than our mechanism. Furthermore, we demonstrate the effectiveness of our\nmechanism in simulation experiments with real-world API settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread adoption of Large Language Models (LLMs) through Application\nProgramming Interfaces (APIs) induces a critical vulnerability: the potential\nfor dishonest manipulation by service providers. This manipulation can manifest\nin various forms, such as secretly substituting a proclaimed high-performance\nmodel with a low-cost alternative, or inflating responses with meaningless\ntokens to increase billing. This work tackles the issue through the lens of\nalgorithmic game theory and mechanism design. We are the first to propose a\nformal economic model for a realistic user-provider ecosystem, where a user can\niteratively delegate $T$ queries to multiple model providers, and providers can\nengage in a range of strategic behaviors. As our central contribution, we prove\nthat for a continuous strategy space and any $\\epsilon\\in(0,\\frac12)$, there\nexists an approximate incentive-compatible mechanism with an additive\napproximation ratio of $O(T^{1-\\epsilon}\\log T)$, and a guaranteed quasi-linear\nsecond-best user utility. We also prove an impossibility result, stating that\nno mechanism can guarantee an expected user utility that is asymptotically\nbetter than our mechanism. Furthermore, we demonstrate the effectiveness of our\nmechanism in simulation experiments with real-world API settings."
                },
                "authors": [
                    {
                        "name": "Yuhan Cao"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Sitong Liu"
                    },
                    {
                        "name": "Miao Li"
                    },
                    {
                        "name": "Yixin Tao"
                    },
                    {
                        "name": "Tianxing He"
                    }
                ],
                "author_detail": {
                    "name": "Tianxing He"
                },
                "author": "Tianxing He",
                "arxiv_comment": "13 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.00847v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.00847v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02503v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02503v1",
                "updated": "2025-11-04T11:43:05Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    11,
                    43,
                    5,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T11:43:05Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    11,
                    43,
                    5,
                    1,
                    308,
                    0
                ],
                "title": "Adapting General-Purpose Foundation Models for X-ray Ptychography in\n  Low-Data Regimes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting General-Purpose Foundation Models for X-ray Ptychography in\n  Low-Data Regimes"
                },
                "summary": "The automation of workflows in advanced microscopy is a key goal where\nfoundation models like Language Models (LLMs) and Vision-Language Models (VLMs)\nshow great potential. However, adapting these general-purpose models for\nspecialized scientific tasks is critical, and the optimal domain adaptation\nstrategy is often unclear. To address this, we introduce PtychoBench, a new\nmulti-modal, multi-task benchmark for ptychographic analysis. Using this\nbenchmark, we systematically compare two specialization strategies: Supervised\nFine-Tuning (SFT) and In-Context Learning (ICL). We evaluate these strategies\non a visual artifact detection task with VLMs and a textual parameter\nrecommendation task with LLMs in a data-scarce regime. Our findings reveal that\nthe optimal specialization pathway is task-dependent. For the visual task, SFT\nand ICL are highly complementary, with a fine-tuned model guided by\ncontext-aware examples achieving the highest mean performance (Micro-F1 of\n0.728). Conversely, for the textual task, ICL on a large base model is the\nsuperior strategy, reaching a peak Micro-F1 of 0.847 and outperforming a\npowerful \"super-expert\" SFT model (0-shot Micro-F1 of 0.839). We also confirm\nthe superiority of context-aware prompting and identify a consistent contextual\ninterference phenomenon in fine-tuned models. These results, benchmarked\nagainst strong baselines including GPT-4o and a DINOv3-based classifier, offer\nkey observations for AI in science: the optimal specialization path in our\nbenchmark is dependent on the task modality, offering a clear framework for\ndeveloping more effective science-based agentic systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The automation of workflows in advanced microscopy is a key goal where\nfoundation models like Language Models (LLMs) and Vision-Language Models (VLMs)\nshow great potential. However, adapting these general-purpose models for\nspecialized scientific tasks is critical, and the optimal domain adaptation\nstrategy is often unclear. To address this, we introduce PtychoBench, a new\nmulti-modal, multi-task benchmark for ptychographic analysis. Using this\nbenchmark, we systematically compare two specialization strategies: Supervised\nFine-Tuning (SFT) and In-Context Learning (ICL). We evaluate these strategies\non a visual artifact detection task with VLMs and a textual parameter\nrecommendation task with LLMs in a data-scarce regime. Our findings reveal that\nthe optimal specialization pathway is task-dependent. For the visual task, SFT\nand ICL are highly complementary, with a fine-tuned model guided by\ncontext-aware examples achieving the highest mean performance (Micro-F1 of\n0.728). Conversely, for the textual task, ICL on a large base model is the\nsuperior strategy, reaching a peak Micro-F1 of 0.847 and outperforming a\npowerful \"super-expert\" SFT model (0-shot Micro-F1 of 0.839). We also confirm\nthe superiority of context-aware prompting and identify a consistent contextual\ninterference phenomenon in fine-tuned models. These results, benchmarked\nagainst strong baselines including GPT-4o and a DINOv3-based classifier, offer\nkey observations for AI in science: the optimal specialization path in our\nbenchmark is dependent on the task modality, offering a clear framework for\ndeveloping more effective science-based agentic systems."
                },
                "authors": [
                    {
                        "name": "Robinson Umeike"
                    },
                    {
                        "name": "Neil Getty"
                    },
                    {
                        "name": "Yin Xiangyu"
                    },
                    {
                        "name": "Yi Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Jiang"
                },
                "author": "Yi Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02503v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02503v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02501v1",
                "updated": "2025-11-04T11:41:01Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    11,
                    41,
                    1,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T11:41:01Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    11,
                    41,
                    1,
                    1,
                    308,
                    0
                ],
                "title": "Lightweight Latency Prediction Scheme for Edge Applications: A Rational\n  Modelling Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightweight Latency Prediction Scheme for Edge Applications: A Rational\n  Modelling Approach"
                },
                "summary": "Accurately predicting end-to-end network latency is essential for enabling\nreliable task offloading in real-time edge computing applications. This paper\nintroduces a lightweight latency prediction scheme based on rational modelling\nthat uses features such as frame size, arrival rate, and link utilization,\neliminating the need for intrusive active probing. The model achieves\nstate-of-the-art prediction accuracy through extensive experiments and 5-fold\ncross-validation (MAE = 0.0115, R$^2$ = 0.9847) with competitive inference\ntime, offering a substantial trade-off between precision and efficiency\ncompared to traditional regressors and neural networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately predicting end-to-end network latency is essential for enabling\nreliable task offloading in real-time edge computing applications. This paper\nintroduces a lightweight latency prediction scheme based on rational modelling\nthat uses features such as frame size, arrival rate, and link utilization,\neliminating the need for intrusive active probing. The model achieves\nstate-of-the-art prediction accuracy through extensive experiments and 5-fold\ncross-validation (MAE = 0.0115, R$^2$ = 0.9847) with competitive inference\ntime, offering a substantial trade-off between precision and efficiency\ncompared to traditional regressors and neural networks."
                },
                "authors": [
                    {
                        "name": "Mohan Liyanage"
                    },
                    {
                        "name": "Eldiyar Zhantileuov"
                    },
                    {
                        "name": "Ali Kadhum Idrees"
                    },
                    {
                        "name": "Rolf Schuster"
                    }
                ],
                "author_detail": {
                    "name": "Rolf Schuster"
                },
                "author": "Rolf Schuster",
                "arxiv_comment": "Presented at the ICCS 2025 - 5th International Conference on Computer\n  Systems, Xian, China",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07067v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07067v5",
                "updated": "2025-11-04T11:34:23Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    11,
                    34,
                    23,
                    1,
                    308,
                    0
                ],
                "published": "2024-12-10T00:19:28Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    0,
                    19,
                    28,
                    1,
                    345,
                    0
                ],
                "title": "MoE-CAP: Benchmarking Cost, Accuracy and Performance of Sparse\n  Mixture-of-Experts Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE-CAP: Benchmarking Cost, Accuracy and Performance of Sparse\n  Mixture-of-Experts Systems"
                },
                "summary": "The sparse Mixture-of-Experts (MoE) architecture is increasingly favored for\nscaling Large Language Models (LLMs) efficiently, but it depends on\nheterogeneous compute and memory resources. These factors jointly affect system\nCost, Accuracy, and Performance (CAP), making trade-offs inevitable. Existing\nbenchmarks often fail to capture these trade-offs accurately, complicating\npractical deployment decisions. To address this, we introduce MoE-CAP, a\nbenchmark specifically designed for MoE systems. Our analysis reveals that\nachieving an optimal balance across CAP is difficult with current hardware; MoE\nsystems typically optimize two of the three dimensions at the expense of the\nthird-a dynamic we term the MoE-CAP trade-off. To visualize this, we propose\nthe CAP Radar Diagram. We further introduce sparsity-aware performance\nmetrics-Sparse Memory Bandwidth Utilization (S-MBU) and Sparse Model FLOPS\nUtilization (S-MFU)-to enable accurate performance benchmarking of MoE systems\nacross diverse hardware platforms and deployment scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The sparse Mixture-of-Experts (MoE) architecture is increasingly favored for\nscaling Large Language Models (LLMs) efficiently, but it depends on\nheterogeneous compute and memory resources. These factors jointly affect system\nCost, Accuracy, and Performance (CAP), making trade-offs inevitable. Existing\nbenchmarks often fail to capture these trade-offs accurately, complicating\npractical deployment decisions. To address this, we introduce MoE-CAP, a\nbenchmark specifically designed for MoE systems. Our analysis reveals that\nachieving an optimal balance across CAP is difficult with current hardware; MoE\nsystems typically optimize two of the three dimensions at the expense of the\nthird-a dynamic we term the MoE-CAP trade-off. To visualize this, we propose\nthe CAP Radar Diagram. We further introduce sparsity-aware performance\nmetrics-Sparse Memory Bandwidth Utilization (S-MBU) and Sparse Model FLOPS\nUtilization (S-MFU)-to enable accurate performance benchmarking of MoE systems\nacross diverse hardware platforms and deployment scenarios."
                },
                "authors": [
                    {
                        "name": "Yinsicheng Jiang"
                    },
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Yeqi Huang"
                    },
                    {
                        "name": "Ping Nie"
                    },
                    {
                        "name": "Zhan Lu"
                    },
                    {
                        "name": "Leyang Xue"
                    },
                    {
                        "name": "Congjie He"
                    },
                    {
                        "name": "Man-Kit Sit"
                    },
                    {
                        "name": "Jilong Xue"
                    },
                    {
                        "name": "Li Dong"
                    },
                    {
                        "name": "Ziming Miao"
                    },
                    {
                        "name": "Dayou Du"
                    },
                    {
                        "name": "Tairan Xu"
                    },
                    {
                        "name": "Kai Zou"
                    },
                    {
                        "name": "Edoardo Ponti"
                    },
                    {
                        "name": "Luo Mai"
                    }
                ],
                "author_detail": {
                    "name": "Luo Mai"
                },
                "author": "Luo Mai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07067v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07067v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02490v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02490v1",
                "updated": "2025-11-04T11:27:03Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    11,
                    27,
                    3,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T11:27:03Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    11,
                    27,
                    3,
                    1,
                    308,
                    0
                ],
                "title": "BRAINS: A Retrieval-Augmented System for Alzheimer's Detection and\n  Monitoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BRAINS: A Retrieval-Augmented System for Alzheimer's Detection and\n  Monitoring"
                },
                "summary": "As the global burden of Alzheimer's disease (AD) continues to grow, early and\naccurate detection has become increasingly critical, especially in regions with\nlimited access to advanced diagnostic tools. We propose BRAINS (Biomedical\nRetrieval-Augmented Intelligence for Neurodegeneration Screening) to address\nthis challenge. This novel system harnesses the powerful reasoning capabilities\nof Large Language Models (LLMs) for Alzheimer's detection and monitoring.\nBRAINS features a dual-module architecture: a cognitive diagnostic module and a\ncase-retrieval module. The Diagnostic Module utilizes LLMs fine-tuned on\ncognitive and neuroimaging datasets -- including MMSE, CDR scores, and brain\nvolume metrics -- to perform structured assessments of Alzheimer's risk.\nMeanwhile, the Case Retrieval Module encodes patient profiles into latent\nrepresentations and retrieves similar cases from a curated knowledge base.\nThese auxiliary cases are fused with the input profile via a Case Fusion Layer\nto enhance contextual understanding. The combined representation is then\nprocessed with clinical prompts for inference. Evaluations on real-world\ndatasets demonstrate BRAINS effectiveness in classifying disease severity and\nidentifying early signs of cognitive decline. This system not only shows strong\npotential as an assistive tool for scalable, explainable, and early-stage\nAlzheimer's disease detection, but also offers hope for future applications in\nthe field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the global burden of Alzheimer's disease (AD) continues to grow, early and\naccurate detection has become increasingly critical, especially in regions with\nlimited access to advanced diagnostic tools. We propose BRAINS (Biomedical\nRetrieval-Augmented Intelligence for Neurodegeneration Screening) to address\nthis challenge. This novel system harnesses the powerful reasoning capabilities\nof Large Language Models (LLMs) for Alzheimer's detection and monitoring.\nBRAINS features a dual-module architecture: a cognitive diagnostic module and a\ncase-retrieval module. The Diagnostic Module utilizes LLMs fine-tuned on\ncognitive and neuroimaging datasets -- including MMSE, CDR scores, and brain\nvolume metrics -- to perform structured assessments of Alzheimer's risk.\nMeanwhile, the Case Retrieval Module encodes patient profiles into latent\nrepresentations and retrieves similar cases from a curated knowledge base.\nThese auxiliary cases are fused with the input profile via a Case Fusion Layer\nto enhance contextual understanding. The combined representation is then\nprocessed with clinical prompts for inference. Evaluations on real-world\ndatasets demonstrate BRAINS effectiveness in classifying disease severity and\nidentifying early signs of cognitive decline. This system not only shows strong\npotential as an assistive tool for scalable, explainable, and early-stage\nAlzheimer's disease detection, but also offers hope for future applications in\nthe field."
                },
                "authors": [
                    {
                        "name": "Rajan Das Gupta"
                    },
                    {
                        "name": "Md Kishor Morol"
                    },
                    {
                        "name": "Nafiz Fahad"
                    },
                    {
                        "name": "Md Tanzib Hosain"
                    },
                    {
                        "name": "Sumaya Binte Zilani Choya"
                    },
                    {
                        "name": "Md Jakir Hossen"
                    }
                ],
                "author_detail": {
                    "name": "Md Jakir Hossen"
                },
                "author": "Md Jakir Hossen",
                "arxiv_comment": "Accepted for publication in ICMLA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02490v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02490v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02481v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02481v1",
                "updated": "2025-11-04T11:12:27Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    11,
                    12,
                    27,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T11:12:27Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    11,
                    12,
                    27,
                    1,
                    308,
                    0
                ],
                "title": "NOWS: Neural Operator Warm Starts for Accelerating Iterative Solvers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NOWS: Neural Operator Warm Starts for Accelerating Iterative Solvers"
                },
                "summary": "Partial differential equations (PDEs) underpin quantitative descriptions\nacross the physical sciences and engineering, yet high-fidelity simulation\nremains a major computational bottleneck for many-query, real-time, and design\ntasks. Data-driven surrogates can be strikingly fast but are often unreliable\nwhen applied outside their training distribution. Here we introduce Neural\nOperator Warm Starts (NOWS), a hybrid strategy that harnesses learned solution\noperators to accelerate classical iterative solvers by producing high-quality\ninitial guesses for Krylov methods such as conjugate gradient and GMRES. NOWS\nleaves existing discretizations and solver infrastructures intact, integrating\nseamlessly with finite-difference, finite-element, isogeometric analysis,\nfinite volume method, etc. Across our benchmarks, the learned initialization\nconsistently reduces iteration counts and end-to-end runtime, resulting in a\nreduction of the computational time of up to 90 %, while preserving the\nstability and convergence guarantees of the underlying numerical algorithms. By\ncombining the rapid inference of neural operators with the rigor of traditional\nsolvers, NOWS provides a practical and trustworthy approach to accelerate\nhigh-fidelity PDE simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Partial differential equations (PDEs) underpin quantitative descriptions\nacross the physical sciences and engineering, yet high-fidelity simulation\nremains a major computational bottleneck for many-query, real-time, and design\ntasks. Data-driven surrogates can be strikingly fast but are often unreliable\nwhen applied outside their training distribution. Here we introduce Neural\nOperator Warm Starts (NOWS), a hybrid strategy that harnesses learned solution\noperators to accelerate classical iterative solvers by producing high-quality\ninitial guesses for Krylov methods such as conjugate gradient and GMRES. NOWS\nleaves existing discretizations and solver infrastructures intact, integrating\nseamlessly with finite-difference, finite-element, isogeometric analysis,\nfinite volume method, etc. Across our benchmarks, the learned initialization\nconsistently reduces iteration counts and end-to-end runtime, resulting in a\nreduction of the computational time of up to 90 %, while preserving the\nstability and convergence guarantees of the underlying numerical algorithms. By\ncombining the rapid inference of neural operators with the rigor of traditional\nsolvers, NOWS provides a practical and trustworthy approach to accelerate\nhigh-fidelity PDE simulations."
                },
                "authors": [
                    {
                        "name": "Mohammad Sadegh Eshaghi"
                    },
                    {
                        "name": "Cosmin Anitescu"
                    },
                    {
                        "name": "Navid Valizadeh"
                    },
                    {
                        "name": "Yizheng Wang"
                    },
                    {
                        "name": "Xiaoying Zhuang"
                    },
                    {
                        "name": "Timon Rabczuk"
                    }
                ],
                "author_detail": {
                    "name": "Timon Rabczuk"
                },
                "author": "Timon Rabczuk",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02481v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02481v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02476v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02476v1",
                "updated": "2025-11-04T11:04:12Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    11,
                    4,
                    12,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T11:04:12Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    11,
                    4,
                    12,
                    1,
                    308,
                    0
                ],
                "title": "Nominal thresholds for good astrometric fits, and prospects for binary\n  detectability, for the full extended \\textit{Gaia} mission",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nominal thresholds for good astrometric fits, and prospects for binary\n  detectability, for the full extended \\textit{Gaia} mission"
                },
                "summary": "The full extended Gaia mission spans slightly over 10 years of data, whilst\nthe current data releases represent only a fraction of that timescale (DR3, 34\nmonths). The longer baseline improves the quality of astrometric fits, lowering\nthe noise floor and making consistently bad fits (for example, due to binarity)\nmore apparent. In this paper, we use simulated binaries from the Gaia Universe\nModel to examine the long-term astrometric behaviour of single stars and\nstellar binaries. We calculate nominal upper limits on the spread of goodness\nof astrometric fits for well-behaved single stars. Specifically, for the RUWE\nparameter, for upcoming DR4 ($\\rm RUWE_{lim}=1.15$) and DR5 ($\\rm\nRUWE_{lim}=1.11$), using the full mission nominal scanning law. These can be\nused to identify poor astrometric fits, and in particular can flag potential\nbinary systems. We show the increase in the number and type of binaries\ndetectable through RUWE. With our updated RUWE thresholds, the number of\ndetectable low-period binaries increases by 5-10% with each subsequent data\nrelease, suggesting detections may be possible for orbital periods down to\ndays. The number of detectable long-period systems increases by 10-20%, with\nperiods up to 100 years, causing significant deviations in low\nmoderate-eccentricity binaries. Very eccentric systems with much longer periods\n(thousands of years) can still be detected if they pass through periapse during\nthe observing window. Finally, we compare our results to the analytic estimate\nfor the spread in UWE, which we predict from a $\\chi$-distribution moderated by\nthe number of observations. These agree with our inferred population limits but\nsuggest that we may be biased by a small number of poorly sampled systems. In\nregions of the sky that are more frequently observed, lower limits could be\nemployed, potentially bringing even more binaries above the threshold for\ndetectability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The full extended Gaia mission spans slightly over 10 years of data, whilst\nthe current data releases represent only a fraction of that timescale (DR3, 34\nmonths). The longer baseline improves the quality of astrometric fits, lowering\nthe noise floor and making consistently bad fits (for example, due to binarity)\nmore apparent. In this paper, we use simulated binaries from the Gaia Universe\nModel to examine the long-term astrometric behaviour of single stars and\nstellar binaries. We calculate nominal upper limits on the spread of goodness\nof astrometric fits for well-behaved single stars. Specifically, for the RUWE\nparameter, for upcoming DR4 ($\\rm RUWE_{lim}=1.15$) and DR5 ($\\rm\nRUWE_{lim}=1.11$), using the full mission nominal scanning law. These can be\nused to identify poor astrometric fits, and in particular can flag potential\nbinary systems. We show the increase in the number and type of binaries\ndetectable through RUWE. With our updated RUWE thresholds, the number of\ndetectable low-period binaries increases by 5-10% with each subsequent data\nrelease, suggesting detections may be possible for orbital periods down to\ndays. The number of detectable long-period systems increases by 10-20%, with\nperiods up to 100 years, causing significant deviations in low\nmoderate-eccentricity binaries. Very eccentric systems with much longer periods\n(thousands of years) can still be detected if they pass through periapse during\nthe observing window. Finally, we compare our results to the analytic estimate\nfor the spread in UWE, which we predict from a $\\chi$-distribution moderated by\nthe number of observations. These agree with our inferred population limits but\nsuggest that we may be biased by a small number of poorly sampled systems. In\nregions of the sky that are more frequently observed, lower limits could be\nemployed, potentially bringing even more binaries above the threshold for\ndetectability."
                },
                "authors": [
                    {
                        "name": "F. Guerriero"
                    },
                    {
                        "name": "Z. Penoyre"
                    },
                    {
                        "name": "A. G. A. Brown"
                    }
                ],
                "author_detail": {
                    "name": "A. G. A. Brown"
                },
                "author": "A. G. A. Brown",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02476v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02476v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2511.02826v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02826v1",
                "updated": "2025-11-04T18:54:58Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    18,
                    54,
                    58,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T18:54:58Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    18,
                    54,
                    58,
                    1,
                    308,
                    0
                ],
                "title": "PLUTO-4: Frontier Pathology Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PLUTO-4: Frontier Pathology Foundation Models"
                },
                "summary": "Foundation models trained on large-scale pathology image corpora have\ndemonstrated strong transfer capabilities across diverse histopathology tasks.\nBuilding on this progress, we introduce PLUTO-4, our next generation of\npathology foundation models that extend the Pathology-Universal Transformer\n(PLUTO) to frontier scale. We share two complementary Vision Transformer\narchitectures in the PLUTO-4 family: a compact and efficient PLUTO-4S model\noptimized for multi-scale deployment using a FlexiViT setup with 2D-RoPE\nembeddings, and a frontier-scale PLUTO-4G model trained with a single patch\nsize to maximize representation capacity and stability. Both models are\npretrained using a self-supervised objective derived from DINOv2 on a large\nmulti-institutional corpus containing 551,164 WSIs from 137,144 patients across\nover 50 institutions, spanning over 60 disease types and over 100 stains.\nComprehensive evaluation across public and internal benchmarks demonstrates\nthat PLUTO-4 achieves state-of-the-art performance on tasks requiring varying\nspatial and biological context, including patch-level classification,\nsegmentation, and slide-level diagnosis. The compact PLUTO-4S provides\nhigh-throughput and robust performance for practical deployment, while PLUTO-4G\nestablishes new performance frontiers across multiple pathology benchmarks,\nincluding an 11% improvement in dermatopathology diagnosis. These diverse\nimprovements underscore PLUTO-4's potential to transform real-world\napplications as a backbone for translational research and diagnostic use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation models trained on large-scale pathology image corpora have\ndemonstrated strong transfer capabilities across diverse histopathology tasks.\nBuilding on this progress, we introduce PLUTO-4, our next generation of\npathology foundation models that extend the Pathology-Universal Transformer\n(PLUTO) to frontier scale. We share two complementary Vision Transformer\narchitectures in the PLUTO-4 family: a compact and efficient PLUTO-4S model\noptimized for multi-scale deployment using a FlexiViT setup with 2D-RoPE\nembeddings, and a frontier-scale PLUTO-4G model trained with a single patch\nsize to maximize representation capacity and stability. Both models are\npretrained using a self-supervised objective derived from DINOv2 on a large\nmulti-institutional corpus containing 551,164 WSIs from 137,144 patients across\nover 50 institutions, spanning over 60 disease types and over 100 stains.\nComprehensive evaluation across public and internal benchmarks demonstrates\nthat PLUTO-4 achieves state-of-the-art performance on tasks requiring varying\nspatial and biological context, including patch-level classification,\nsegmentation, and slide-level diagnosis. The compact PLUTO-4S provides\nhigh-throughput and robust performance for practical deployment, while PLUTO-4G\nestablishes new performance frontiers across multiple pathology benchmarks,\nincluding an 11% improvement in dermatopathology diagnosis. These diverse\nimprovements underscore PLUTO-4's potential to transform real-world\napplications as a backbone for translational research and diagnostic use cases."
                },
                "authors": [
                    {
                        "name": "Harshith Padigela"
                    },
                    {
                        "name": "Shima Nofallah"
                    },
                    {
                        "name": "Atchuth Naveen Chilaparasetti"
                    },
                    {
                        "name": "Ryun Han"
                    },
                    {
                        "name": "Andrew Walker"
                    },
                    {
                        "name": "Judy Shen"
                    },
                    {
                        "name": "Chintan Shah"
                    },
                    {
                        "name": "Blake Martin"
                    },
                    {
                        "name": "Aashish Sood"
                    },
                    {
                        "name": "Elliot Miller"
                    },
                    {
                        "name": "Ben Glass"
                    },
                    {
                        "name": "Andy Beck"
                    },
                    {
                        "name": "Harsha Pokkalla"
                    },
                    {
                        "name": "Syed Ashar Javed"
                    }
                ],
                "author_detail": {
                    "name": "Syed Ashar Javed"
                },
                "author": "Syed Ashar Javed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02826v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02826v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02823v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02823v1",
                "updated": "2025-11-04T18:48:56Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    18,
                    48,
                    56,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T18:48:56Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    18,
                    48,
                    56,
                    1,
                    308,
                    0
                ],
                "title": "Optimizing AI Agent Attacks With Synthetic Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing AI Agent Attacks With Synthetic Data"
                },
                "summary": "As AI deployments become more complex and high-stakes, it becomes\nincreasingly important to be able to estimate their risk. AI control is one\nframework for doing so. However, good control evaluations require eliciting\nstrong attack policies. This can be challenging in complex agentic environments\nwhere compute constraints leave us data-poor. In this work, we show how to\noptimize attack policies in SHADE-Arena, a dataset of diverse realistic control\nenvironments. We do this by decomposing attack capability into five constituent\nskills -- suspicion modeling, attack selection, plan synthesis, execution and\nsubtlety -- and optimizing each component individually. To get around the\nconstraint of limited data, we develop a probabilistic model of attack\ndynamics, optimize our attack hyperparameters using this simulation, and then\nshow that the results transfer to SHADE-Arena. This results in a substantial\nimprovement in attack strength, reducing safety score from a baseline of 0.87\nto 0.41 using our scaffold.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI deployments become more complex and high-stakes, it becomes\nincreasingly important to be able to estimate their risk. AI control is one\nframework for doing so. However, good control evaluations require eliciting\nstrong attack policies. This can be challenging in complex agentic environments\nwhere compute constraints leave us data-poor. In this work, we show how to\noptimize attack policies in SHADE-Arena, a dataset of diverse realistic control\nenvironments. We do this by decomposing attack capability into five constituent\nskills -- suspicion modeling, attack selection, plan synthesis, execution and\nsubtlety -- and optimizing each component individually. To get around the\nconstraint of limited data, we develop a probabilistic model of attack\ndynamics, optimize our attack hyperparameters using this simulation, and then\nshow that the results transfer to SHADE-Arena. This results in a substantial\nimprovement in attack strength, reducing safety score from a baseline of 0.87\nto 0.41 using our scaffold."
                },
                "authors": [
                    {
                        "name": "Chloe Loughridge"
                    },
                    {
                        "name": "Paul Colognese"
                    },
                    {
                        "name": "Avery Griffin"
                    },
                    {
                        "name": "Tyler Tracy"
                    },
                    {
                        "name": "Jon Kutasov"
                    },
                    {
                        "name": "Joe Benton"
                    }
                ],
                "author_detail": {
                    "name": "Joe Benton"
                },
                "author": "Joe Benton",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02823v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02823v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09586v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09586v3",
                "updated": "2025-11-04T18:44:17Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    18,
                    44,
                    17,
                    1,
                    308,
                    0
                ],
                "published": "2024-09-15T02:13:03Z",
                "published_parsed": [
                    2024,
                    9,
                    15,
                    2,
                    13,
                    3,
                    6,
                    259,
                    0
                ],
                "title": "ValueCompass: A Framework for Measuring Contextual Value Alignment\n  Between Human and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ValueCompass: A Framework for Measuring Contextual Value Alignment\n  Between Human and LLMs"
                },
                "summary": "As AI systems become more advanced, ensuring their alignment with a diverse\nrange of individuals and societal values becomes increasingly critical. But how\ncan we capture fundamental human values and assess the degree to which AI\nsystems align with them? We introduce ValueCompass, a framework of fundamental\nvalues, grounded in psychological theory and a systematic review, to identify\nand evaluate human-AI alignment. We apply ValueCompass to measure the value\nalignment of humans and large language models (LLMs) across four real-world\nscenarios: collaborative writing, education, public sectors, and healthcare.\nOur findings reveal concerning misalignments between humans and LLMs, such as\nhumans frequently endorse values like \"National Security\" which were largely\nrejected by LLMs. We also observe that values differ across scenarios,\nhighlighting the need for context-aware AI alignment strategies. This work\nprovides valuable insights into the design space of human-AI alignment, laying\nthe foundations for developing AI systems that responsibly reflect societal\nvalues and ethics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI systems become more advanced, ensuring their alignment with a diverse\nrange of individuals and societal values becomes increasingly critical. But how\ncan we capture fundamental human values and assess the degree to which AI\nsystems align with them? We introduce ValueCompass, a framework of fundamental\nvalues, grounded in psychological theory and a systematic review, to identify\nand evaluate human-AI alignment. We apply ValueCompass to measure the value\nalignment of humans and large language models (LLMs) across four real-world\nscenarios: collaborative writing, education, public sectors, and healthcare.\nOur findings reveal concerning misalignments between humans and LLMs, such as\nhumans frequently endorse values like \"National Security\" which were largely\nrejected by LLMs. We also observe that values differ across scenarios,\nhighlighting the need for context-aware AI alignment strategies. This work\nprovides valuable insights into the design space of human-AI alignment, laying\nthe foundations for developing AI systems that responsibly reflect societal\nvalues and ethics."
                },
                "authors": [
                    {
                        "name": "Hua Shen"
                    },
                    {
                        "name": "Tiffany Knearem"
                    },
                    {
                        "name": "Reshmi Ghosh"
                    },
                    {
                        "name": "Yu-Ju Yang"
                    },
                    {
                        "name": "Nicholas Clark"
                    },
                    {
                        "name": "Tanushree Mitra"
                    },
                    {
                        "name": "Yun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Yun Huang"
                },
                "author": "Yun Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09586v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09586v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00032v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00032v2",
                "updated": "2025-11-04T18:36:26Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    18,
                    36,
                    26,
                    1,
                    308,
                    0
                ],
                "published": "2025-07-30T08:49:13Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    8,
                    49,
                    13,
                    2,
                    211,
                    0
                ],
                "title": "Strategic Communication and Language Bias in Multi-Agent LLM\n  Coordination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strategic Communication and Language Bias in Multi-Agent LLM\n  Coordination"
                },
                "summary": "Large Language Model (LLM)-based agents are increasingly deployed in\nmulti-agent scenarios where coordination is crucial but not always assured.\nResearch shows that the way strategic scenarios are framed linguistically can\naffect cooperation. This paper explores whether allowing agents to communicate\namplifies these language-driven effects. Leveraging FAIRGAME, we simulate\none-shot and repeated games across different languages and models, both with\nand without communication. Our experiments, conducted with two advanced\nLLMs-GPT-4o and Llama 4 Maverick-reveal that communication significantly\ninfluences agent behavior, though its impact varies by language, personality,\nand game structure. These findings underscore the dual role of communication in\nfostering coordination and reinforcing biases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based agents are increasingly deployed in\nmulti-agent scenarios where coordination is crucial but not always assured.\nResearch shows that the way strategic scenarios are framed linguistically can\naffect cooperation. This paper explores whether allowing agents to communicate\namplifies these language-driven effects. Leveraging FAIRGAME, we simulate\none-shot and repeated games across different languages and models, both with\nand without communication. Our experiments, conducted with two advanced\nLLMs-GPT-4o and Llama 4 Maverick-reveal that communication significantly\ninfluences agent behavior, though its impact varies by language, personality,\nand game structure. These findings underscore the dual role of communication in\nfostering coordination and reinforcing biases."
                },
                "authors": [
                    {
                        "name": "Alessio Buscemi"
                    },
                    {
                        "name": "Daniele Proverbio"
                    },
                    {
                        "name": "Alessandro Di Stefano"
                    },
                    {
                        "name": "The Anh Han"
                    },
                    {
                        "name": "German Castignani"
                    },
                    {
                        "name": "Pietro Liò"
                    }
                ],
                "author_detail": {
                    "name": "Pietro Liò"
                },
                "author": "Pietro Liò",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00032v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00032v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15066v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15066v2",
                "updated": "2025-11-04T18:32:59Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    18,
                    32,
                    59,
                    1,
                    308,
                    0
                ],
                "published": "2025-08-20T20:57:13Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    20,
                    57,
                    13,
                    2,
                    232,
                    0
                ],
                "title": "Osprey: A Scalable Framework for the Orchestration of Agentic Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Osprey: A Scalable Framework for the Orchestration of Agentic Systems"
                },
                "summary": "Coordinating workflows across complex systems remains a central challenge in\nsafety-critical environments such as scientific facilities.\nLanguage-model-driven agents offer a natural interface for these tasks, but\nexisting approaches often lack scalability, reliability, and human oversight.\nWe introduce the Osprey Framework, a domain-agnostic, production-ready\narchitecture for scalable agentic systems that integrate conversational context\nwith robust tool orchestration across safety-critical domains. Our framework\nprovides: (i) dynamic capability classification to select only relevant tools;\n(ii) plan-first orchestration with explicit dependencies and optional human\napproval; (iii) context-aware task extraction that combines dialogue history\nwith external memory and domain resources; and (iv) production-ready execution\nwith checkpointing, artifact management, and modular deployment. We demonstrate\nits versatility through two case studies: a deployment at the Advanced Light\nSource particle accelerator and a tutorial-style wind farm monitoring example.\nThese results establish Osprey as a reliable and transparent framework for\nagentic systems across diverse high-stakes domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coordinating workflows across complex systems remains a central challenge in\nsafety-critical environments such as scientific facilities.\nLanguage-model-driven agents offer a natural interface for these tasks, but\nexisting approaches often lack scalability, reliability, and human oversight.\nWe introduce the Osprey Framework, a domain-agnostic, production-ready\narchitecture for scalable agentic systems that integrate conversational context\nwith robust tool orchestration across safety-critical domains. Our framework\nprovides: (i) dynamic capability classification to select only relevant tools;\n(ii) plan-first orchestration with explicit dependencies and optional human\napproval; (iii) context-aware task extraction that combines dialogue history\nwith external memory and domain resources; and (iv) production-ready execution\nwith checkpointing, artifact management, and modular deployment. We demonstrate\nits versatility through two case studies: a deployment at the Advanced Light\nSource particle accelerator and a tutorial-style wind farm monitoring example.\nThese results establish Osprey as a reliable and transparent framework for\nagentic systems across diverse high-stakes domains."
                },
                "authors": [
                    {
                        "name": "Thorsten Hellert"
                    },
                    {
                        "name": "João Montenegro"
                    },
                    {
                        "name": "Antonin Sulc"
                    }
                ],
                "author_detail": {
                    "name": "Antonin Sulc"
                },
                "author": "Antonin Sulc",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15066v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15066v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02805v1",
                "updated": "2025-11-04T18:27:39Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    18,
                    27,
                    39,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T18:27:39Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    18,
                    27,
                    39,
                    1,
                    308,
                    0
                ],
                "title": "MemSearcher: Training LLMs to Reason, Search and Manage Memory via\n  End-to-End Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemSearcher: Training LLMs to Reason, Search and Manage Memory via\n  End-to-End Reinforcement Learning"
                },
                "summary": "Typical search agents concatenate the entire interaction history into the LLM\ncontext, preserving information integrity but producing long, noisy contexts,\nresulting in high computation and memory costs. In contrast, using only the\ncurrent turn avoids this overhead but discards essential information. This\ntrade-off limits the scalability of search agents. To address this challenge,\nwe propose MemSearcher, an agent workflow that iteratively maintains a compact\nmemory and combines the current turn with it. At each turn, MemSearcher fuses\nthe user's question with the memory to generate reasoning traces, perform\nsearch actions, and update memory to retain only information essential for\nsolving the task. This design stabilizes context length across multi-turn\ninteractions, improving efficiency without sacrificing accuracy. To optimize\nthis workflow, we introduce multi-context GRPO, an end-to-end RL framework that\njointly optimize reasoning, search strategies, and memory management of\nMemSearcher Agents. Specifically, multi-context GRPO samples groups of\ntrajectories under different contexts and propagates trajectory-level\nadvantages across all conversations within them. Trained on the same dataset as\nSearch-R1, MemSearcher achieves significant improvements over strong baselines\non seven public benchmarks: +11% on Qwen2.5-3B-Instruct and +12% on\nQwen2.5-7B-Instruct relative average gains. Notably, the 3B-based MemSearcher\neven outperforms 7B-based baselines, demonstrating that striking a balance\nbetween information integrity and efficiency yields both higher accuracy and\nlower computational overhead. The code and models will be publicly available at\nhttps://github.com/icip-cas/MemSearcher",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Typical search agents concatenate the entire interaction history into the LLM\ncontext, preserving information integrity but producing long, noisy contexts,\nresulting in high computation and memory costs. In contrast, using only the\ncurrent turn avoids this overhead but discards essential information. This\ntrade-off limits the scalability of search agents. To address this challenge,\nwe propose MemSearcher, an agent workflow that iteratively maintains a compact\nmemory and combines the current turn with it. At each turn, MemSearcher fuses\nthe user's question with the memory to generate reasoning traces, perform\nsearch actions, and update memory to retain only information essential for\nsolving the task. This design stabilizes context length across multi-turn\ninteractions, improving efficiency without sacrificing accuracy. To optimize\nthis workflow, we introduce multi-context GRPO, an end-to-end RL framework that\njointly optimize reasoning, search strategies, and memory management of\nMemSearcher Agents. Specifically, multi-context GRPO samples groups of\ntrajectories under different contexts and propagates trajectory-level\nadvantages across all conversations within them. Trained on the same dataset as\nSearch-R1, MemSearcher achieves significant improvements over strong baselines\non seven public benchmarks: +11% on Qwen2.5-3B-Instruct and +12% on\nQwen2.5-7B-Instruct relative average gains. Notably, the 3B-based MemSearcher\neven outperforms 7B-based baselines, demonstrating that striking a balance\nbetween information integrity and efficiency yields both higher accuracy and\nlower computational overhead. The code and models will be publicly available at\nhttps://github.com/icip-cas/MemSearcher"
                },
                "authors": [
                    {
                        "name": "Qianhao Yuan"
                    },
                    {
                        "name": "Jie Lou"
                    },
                    {
                        "name": "Zichao Li"
                    },
                    {
                        "name": "Jiawei Chen"
                    },
                    {
                        "name": "Yaojie Lu"
                    },
                    {
                        "name": "Hongyu Lin"
                    },
                    {
                        "name": "Le Sun"
                    },
                    {
                        "name": "Debing Zhang"
                    },
                    {
                        "name": "Xianpei Han"
                    }
                ],
                "author_detail": {
                    "name": "Xianpei Han"
                },
                "author": "Xianpei Han",
                "arxiv_comment": "Project page: https://github.com/icip-cas/MemSearcher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02802v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02802v1",
                "updated": "2025-11-04T18:25:17Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    18,
                    25,
                    17,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T18:25:17Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    18,
                    25,
                    17,
                    1,
                    308,
                    0
                ],
                "title": "TabTune: A Unified Library for Inference and Fine-Tuning Tabular\n  Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TabTune: A Unified Library for Inference and Fine-Tuning Tabular\n  Foundation Models"
                },
                "summary": "Tabular foundation models represent a growing paradigm in structured data\nlearning, extending the benefits of large-scale pretraining to tabular domains.\nHowever, their adoption remains limited due to heterogeneous preprocessing\npipelines, fragmented APIs, inconsistent fine-tuning procedures, and the\nabsence of standardized evaluation for deployment-oriented metrics such as\ncalibration and fairness. We present TabTune, a unified library that\nstandardizes the complete workflow for tabular foundation models through a\nsingle interface. TabTune provides consistent access to seven state-of-the-art\nmodels supporting multiple adaptation strategies, including zero-shot\ninference, meta-learning, supervised fine-tuning (SFT), and parameter-efficient\nfine-tuning (PEFT). The framework automates model-aware preprocessing, manages\narchitectural heterogeneity internally, and integrates evaluation modules for\nperformance, calibration, and fairness. Designed for extensibility and\nreproducibility, TabTune enables consistent benchmarking of adaptation\nstrategies of tabular foundation models. The library is open source and\navailable at https://github.com/Lexsi-Labs/TabTune .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tabular foundation models represent a growing paradigm in structured data\nlearning, extending the benefits of large-scale pretraining to tabular domains.\nHowever, their adoption remains limited due to heterogeneous preprocessing\npipelines, fragmented APIs, inconsistent fine-tuning procedures, and the\nabsence of standardized evaluation for deployment-oriented metrics such as\ncalibration and fairness. We present TabTune, a unified library that\nstandardizes the complete workflow for tabular foundation models through a\nsingle interface. TabTune provides consistent access to seven state-of-the-art\nmodels supporting multiple adaptation strategies, including zero-shot\ninference, meta-learning, supervised fine-tuning (SFT), and parameter-efficient\nfine-tuning (PEFT). The framework automates model-aware preprocessing, manages\narchitectural heterogeneity internally, and integrates evaluation modules for\nperformance, calibration, and fairness. Designed for extensibility and\nreproducibility, TabTune enables consistent benchmarking of adaptation\nstrategies of tabular foundation models. The library is open source and\navailable at https://github.com/Lexsi-Labs/TabTune ."
                },
                "authors": [
                    {
                        "name": "Aditya Tanna"
                    },
                    {
                        "name": "Pratinav Seth"
                    },
                    {
                        "name": "Mohamed Bouadi"
                    },
                    {
                        "name": "Utsav Avaiya"
                    },
                    {
                        "name": "Vinay Kumar Sankarapu"
                    }
                ],
                "author_detail": {
                    "name": "Vinay Kumar Sankarapu"
                },
                "author": "Vinay Kumar Sankarapu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02802v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02802v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11878v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11878v2",
                "updated": "2025-11-04T18:24:59Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    18,
                    24,
                    59,
                    1,
                    308,
                    0
                ],
                "published": "2025-10-13T19:36:47Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    19,
                    36,
                    47,
                    0,
                    286,
                    0
                ],
                "title": "GS-Verse: Mesh-based Gaussian Splatting for Physics-aware Interaction in\n  Virtual Reality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GS-Verse: Mesh-based Gaussian Splatting for Physics-aware Interaction in\n  Virtual Reality"
                },
                "summary": "As the demand for immersive 3D content grows, the need for intuitive and\nefficient interaction methods becomes paramount. Current techniques for\nphysically manipulating 3D content within Virtual Reality (VR) often face\nsignificant limitations, including reliance on engineering-intensive processes\nand simplified geometric representations, such as tetrahedral cages, which can\ncompromise visual fidelity and physical accuracy. In this paper, we introduce\nGS-Verse (Gaussian Splatting for Virtual Environment Rendering and Scene\nEditing), a novel method designed to overcome these challenges by directly\nintegrating an object's mesh with a Gaussian Splatting (GS) representation. Our\napproach enables more precise surface approximation, leading to highly\nrealistic deformations and interactions. By leveraging existing 3D mesh assets,\nGS-Verse facilitates seamless content reuse and simplifies the development\nworkflow. Moreover, our system is designed to be physics-engine-agnostic,\ngranting developers robust deployment flexibility. This versatile architecture\ndelivers a highly realistic, adaptable, and intuitive approach to interactive\n3D manipulation. We rigorously validate our method against the current\nstate-of-the-art technique that couples VR with GS in a comparative user study\ninvolving 18 participants. Specifically, we demonstrate that our approach is\nstatistically significantly better for physics-aware stretching manipulation\nand is also more consistent in other physics-based manipulations like twisting\nand shaking. Further evaluation across various interactions and scenes confirms\nthat our method consistently delivers high and reliable performance, showing\nits potential as a plausible alternative to existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the demand for immersive 3D content grows, the need for intuitive and\nefficient interaction methods becomes paramount. Current techniques for\nphysically manipulating 3D content within Virtual Reality (VR) often face\nsignificant limitations, including reliance on engineering-intensive processes\nand simplified geometric representations, such as tetrahedral cages, which can\ncompromise visual fidelity and physical accuracy. In this paper, we introduce\nGS-Verse (Gaussian Splatting for Virtual Environment Rendering and Scene\nEditing), a novel method designed to overcome these challenges by directly\nintegrating an object's mesh with a Gaussian Splatting (GS) representation. Our\napproach enables more precise surface approximation, leading to highly\nrealistic deformations and interactions. By leveraging existing 3D mesh assets,\nGS-Verse facilitates seamless content reuse and simplifies the development\nworkflow. Moreover, our system is designed to be physics-engine-agnostic,\ngranting developers robust deployment flexibility. This versatile architecture\ndelivers a highly realistic, adaptable, and intuitive approach to interactive\n3D manipulation. We rigorously validate our method against the current\nstate-of-the-art technique that couples VR with GS in a comparative user study\ninvolving 18 participants. Specifically, we demonstrate that our approach is\nstatistically significantly better for physics-aware stretching manipulation\nand is also more consistent in other physics-based manipulations like twisting\nand shaking. Further evaluation across various interactions and scenes confirms\nthat our method consistently delivers high and reliable performance, showing\nits potential as a plausible alternative to existing methods."
                },
                "authors": [
                    {
                        "name": "Anastasiya Pechko"
                    },
                    {
                        "name": "Piotr Borycki"
                    },
                    {
                        "name": "Joanna Waczyńska"
                    },
                    {
                        "name": "Daniel Barczyk"
                    },
                    {
                        "name": "Agata Szymańska"
                    },
                    {
                        "name": "Sławomir Tadeja"
                    },
                    {
                        "name": "Przemysław Spurek"
                    }
                ],
                "author_detail": {
                    "name": "Przemysław Spurek"
                },
                "author": "Przemysław Spurek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11878v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11878v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02795v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02795v1",
                "updated": "2025-11-04T18:20:17Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    18,
                    20,
                    17,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T18:20:17Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    18,
                    20,
                    17,
                    1,
                    308,
                    0
                ],
                "title": "Can LLMs subtract numbers?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs subtract numbers?"
                },
                "summary": "We present a systematic study of subtraction in large language models (LLMs).\nWhile prior benchmarks emphasize addition and multiplication, subtraction has\nreceived comparatively little attention despite being structurally distinct as\na non-commutative operation. We evaluate eight pretrained LLMs spanning four\nfamilies on addition and subtraction problems. Our experiments reveal that\nsubtraction accuracy lags behind addition by a wide margin. We find that the\nerrors for ($a-b$) are concentrated in cases where ($a<b$). In such cases, LLMs\nfrequently produce the correct magnitude but omit the negative sign. Probing\nanalyses show that LLMs internally encode whether results should be negative,\nyet this information is often not reflected in generated outputs. We further\ntest well-known techniques such as few-shot learning and instruction-tuning to\nsee if they can improve the LLMs' performance. Our results suggest that while\nfew-shot prompting yields modest gains, the instruction-tuned models achieve\nnear-perfect accuracies in generating the negative sign. Together, these\nfindings provide a clearer characterization of the limitations and\nrecoverability of LLMs' arithmetic capabilities in subtraction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a systematic study of subtraction in large language models (LLMs).\nWhile prior benchmarks emphasize addition and multiplication, subtraction has\nreceived comparatively little attention despite being structurally distinct as\na non-commutative operation. We evaluate eight pretrained LLMs spanning four\nfamilies on addition and subtraction problems. Our experiments reveal that\nsubtraction accuracy lags behind addition by a wide margin. We find that the\nerrors for ($a-b$) are concentrated in cases where ($a<b$). In such cases, LLMs\nfrequently produce the correct magnitude but omit the negative sign. Probing\nanalyses show that LLMs internally encode whether results should be negative,\nyet this information is often not reflected in generated outputs. We further\ntest well-known techniques such as few-shot learning and instruction-tuning to\nsee if they can improve the LLMs' performance. Our results suggest that while\nfew-shot prompting yields modest gains, the instruction-tuned models achieve\nnear-perfect accuracies in generating the negative sign. Together, these\nfindings provide a clearer characterization of the limitations and\nrecoverability of LLMs' arithmetic capabilities in subtraction."
                },
                "authors": [
                    {
                        "name": "Mayank Jobanputra"
                    },
                    {
                        "name": "Nils Philipp Walter"
                    },
                    {
                        "name": "Maitrey Mehta"
                    },
                    {
                        "name": "Blerta Veseli"
                    },
                    {
                        "name": "Evan Parker Kelly Chapple"
                    },
                    {
                        "name": "Yifan Wang"
                    },
                    {
                        "name": "Sneha Chetani"
                    },
                    {
                        "name": "Ellie Pavlick"
                    },
                    {
                        "name": "Antonio Vergari"
                    },
                    {
                        "name": "Vera Demberg"
                    }
                ],
                "author_detail": {
                    "name": "Vera Demberg"
                },
                "author": "Vera Demberg",
                "arxiv_comment": "Work-in-progress; MathNLP non-archival presentation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02795v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02795v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09024v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09024v2",
                "updated": "2025-11-04T18:19:00Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    18,
                    19,
                    0,
                    1,
                    308,
                    0
                ],
                "published": "2025-06-10T17:52:18Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    17,
                    52,
                    18,
                    1,
                    161,
                    0
                ],
                "title": "DIsoN: Decentralized Isolation Networks for Out-of-Distribution\n  Detection in Medical Imaging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DIsoN: Decentralized Isolation Networks for Out-of-Distribution\n  Detection in Medical Imaging"
                },
                "summary": "Safe deployment of machine learning (ML) models in safety-critical domains\nsuch as medical imaging requires detecting inputs with characteristics not seen\nduring training, known as out-of-distribution (OOD) detection, to prevent\nunreliable predictions. Effective OOD detection after deployment could benefit\nfrom access to the training data, enabling direct comparison between test\nsamples and the training data distribution to identify differences.\nState-of-the-art OOD detection methods, however, either discard the training\ndata after deployment or assume that test samples and training data are\ncentrally stored together, an assumption that rarely holds in real-world\nsettings. This is because shipping the training data with the deployed model is\nusually impossible due to the size of training databases, as well as\nproprietary or privacy constraints. We introduce the Isolation Network, an OOD\ndetection framework that quantifies the difficulty of separating a target test\nsample from the training data by solving a binary classification task. We then\npropose Decentralized Isolation Networks (DIsoN), which enables the comparison\nof training and test data when data-sharing is impossible, by exchanging only\nmodel parameters between the remote computational nodes of training and\ndeployment. We further extend DIsoN with class-conditioning, comparing a target\nsample solely with training data of its predicted class. We evaluate DIsoN on\nfour medical imaging datasets (dermatology, chest X-ray, breast ultrasound,\nhistopathology) across 12 OOD detection tasks. DIsoN performs favorably against\nexisting methods while respecting data-privacy. This decentralized OOD\ndetection framework opens the way for a new type of service that ML developers\ncould provide along with their models: providing remote, secure utilization of\ntheir training data for OOD detection services. Code:\nhttps://github.com/FelixWag/DIsoN",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safe deployment of machine learning (ML) models in safety-critical domains\nsuch as medical imaging requires detecting inputs with characteristics not seen\nduring training, known as out-of-distribution (OOD) detection, to prevent\nunreliable predictions. Effective OOD detection after deployment could benefit\nfrom access to the training data, enabling direct comparison between test\nsamples and the training data distribution to identify differences.\nState-of-the-art OOD detection methods, however, either discard the training\ndata after deployment or assume that test samples and training data are\ncentrally stored together, an assumption that rarely holds in real-world\nsettings. This is because shipping the training data with the deployed model is\nusually impossible due to the size of training databases, as well as\nproprietary or privacy constraints. We introduce the Isolation Network, an OOD\ndetection framework that quantifies the difficulty of separating a target test\nsample from the training data by solving a binary classification task. We then\npropose Decentralized Isolation Networks (DIsoN), which enables the comparison\nof training and test data when data-sharing is impossible, by exchanging only\nmodel parameters between the remote computational nodes of training and\ndeployment. We further extend DIsoN with class-conditioning, comparing a target\nsample solely with training data of its predicted class. We evaluate DIsoN on\nfour medical imaging datasets (dermatology, chest X-ray, breast ultrasound,\nhistopathology) across 12 OOD detection tasks. DIsoN performs favorably against\nexisting methods while respecting data-privacy. This decentralized OOD\ndetection framework opens the way for a new type of service that ML developers\ncould provide along with their models: providing remote, secure utilization of\ntheir training data for OOD detection services. Code:\nhttps://github.com/FelixWag/DIsoN"
                },
                "authors": [
                    {
                        "name": "Felix Wagner"
                    },
                    {
                        "name": "Pramit Saha"
                    },
                    {
                        "name": "Harry Anthony"
                    },
                    {
                        "name": "J. Alison Noble"
                    },
                    {
                        "name": "Konstantinos Kamnitsas"
                    }
                ],
                "author_detail": {
                    "name": "Konstantinos Kamnitsas"
                },
                "author": "Konstantinos Kamnitsas",
                "arxiv_comment": "Accepted at NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09024v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09024v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11; I.4.9; I.4.9; J.3; I.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07129v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07129v2",
                "updated": "2025-11-04T18:17:08Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    18,
                    17,
                    8,
                    1,
                    308,
                    0
                ],
                "published": "2025-07-08T20:01:15Z",
                "published_parsed": [
                    2025,
                    7,
                    8,
                    20,
                    1,
                    15,
                    1,
                    189,
                    0
                ],
                "title": "Growing Transformers: Modular Composition and Layer-wise Expansion on a\n  Frozen Substrate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Growing Transformers: Modular Composition and Layer-wise Expansion on a\n  Frozen Substrate"
                },
                "summary": "The prevailing paradigm for scaling large language models (LLMs) involves\nmonolithic, end-to-end training, a resource-intensive process that lacks\nflexibility. This paper explores an alternative, constructive scaling paradigm,\nenabled by the principle of emergent semantics in Transformers with frozen,\nnon-semantic input embeddings. We posit that because high-level meaning is a\ncompositional property of a Transformer's deep layers, not its input vectors,\nthe embedding layer and trained lower layers can serve as a fixed foundation.\nThis liberates backpropagation to focus solely on newly added components,\nmaking incremental growth viable. We operationalize this with a layer-wise\nconstructive methodology that combines strict layer freezing in early stages\nwith efficient, holistic fine-tuning of the entire model stack via low-rank\nadaptation (LoRA) as complexity increases. This method not only demonstrates\nstable convergence but also reveals a direct correlation between model depth\nand the emergence of complex reasoning abilities, such as those required for\nSQuAD, which are absent in shallower models. In a controlled study, our\nconstructively grown model rivals the performance of a monolithically trained\nbaseline of the same size, validating the efficiency and efficacy of the\napproach. Our findings suggest a path towards a paradigm shift from monolithic\noptimization towards a more biological or constructive model of AI development.\nThis opens a path for more resource-efficient scaling, continual learning, and\na more modular approach to building powerful AI systems. We release all code\nand models to facilitate further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The prevailing paradigm for scaling large language models (LLMs) involves\nmonolithic, end-to-end training, a resource-intensive process that lacks\nflexibility. This paper explores an alternative, constructive scaling paradigm,\nenabled by the principle of emergent semantics in Transformers with frozen,\nnon-semantic input embeddings. We posit that because high-level meaning is a\ncompositional property of a Transformer's deep layers, not its input vectors,\nthe embedding layer and trained lower layers can serve as a fixed foundation.\nThis liberates backpropagation to focus solely on newly added components,\nmaking incremental growth viable. We operationalize this with a layer-wise\nconstructive methodology that combines strict layer freezing in early stages\nwith efficient, holistic fine-tuning of the entire model stack via low-rank\nadaptation (LoRA) as complexity increases. This method not only demonstrates\nstable convergence but also reveals a direct correlation between model depth\nand the emergence of complex reasoning abilities, such as those required for\nSQuAD, which are absent in shallower models. In a controlled study, our\nconstructively grown model rivals the performance of a monolithically trained\nbaseline of the same size, validating the efficiency and efficacy of the\napproach. Our findings suggest a path towards a paradigm shift from monolithic\noptimization towards a more biological or constructive model of AI development.\nThis opens a path for more resource-efficient scaling, continual learning, and\na more modular approach to building powerful AI systems. We release all code\nand models to facilitate further research."
                },
                "authors": [
                    {
                        "name": "A. Bochkov"
                    }
                ],
                "author_detail": {
                    "name": "A. Bochkov"
                },
                "author": "A. Bochkov",
                "arxiv_comment": "Controlled Comparative Study added",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07129v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07129v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02793v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02793v1",
                "updated": "2025-11-04T18:16:04Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    18,
                    16,
                    4,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T18:16:04Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    18,
                    16,
                    4,
                    1,
                    308,
                    0
                ],
                "title": "Diffusion Models are Robust Pretrainers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Models are Robust Pretrainers"
                },
                "summary": "Diffusion models have gained significant attention for high-fidelity image\ngeneration. Our work investigates the potential of exploiting diffusion models\nfor adversarial robustness in image classification and object detection.\nAdversarial attacks challenge standard models in these tasks by perturbing\ninputs to force incorrect predictions. To address this issue, many approaches\nuse training schemes for forcing the robustness of the models, which increase\ntraining costs. In this work, we study models built on top of off-the-shelf\ndiffusion models and demonstrate their practical significance: they provide a\nlow-cost path to robust representations, allowing lightweight heads to be\ntrained on frozen features without full adversarial training. Our empirical\nevaluations on ImageNet, CIFAR-10, and PASCAL VOC show that diffusion-based\nclassifiers and detectors achieve meaningful adversarial robustness with\nminimal compute. While clean and adversarial accuracies remain below\nstate-of-the-art adversarially trained CNNs or ViTs, diffusion pretraining\noffers a favorable tradeoff between efficiency and robustness. This work opens\na promising avenue for integrating diffusion models into resource-constrained\nrobust deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have gained significant attention for high-fidelity image\ngeneration. Our work investigates the potential of exploiting diffusion models\nfor adversarial robustness in image classification and object detection.\nAdversarial attacks challenge standard models in these tasks by perturbing\ninputs to force incorrect predictions. To address this issue, many approaches\nuse training schemes for forcing the robustness of the models, which increase\ntraining costs. In this work, we study models built on top of off-the-shelf\ndiffusion models and demonstrate their practical significance: they provide a\nlow-cost path to robust representations, allowing lightweight heads to be\ntrained on frozen features without full adversarial training. Our empirical\nevaluations on ImageNet, CIFAR-10, and PASCAL VOC show that diffusion-based\nclassifiers and detectors achieve meaningful adversarial robustness with\nminimal compute. While clean and adversarial accuracies remain below\nstate-of-the-art adversarially trained CNNs or ViTs, diffusion pretraining\noffers a favorable tradeoff between efficiency and robustness. This work opens\na promising avenue for integrating diffusion models into resource-constrained\nrobust deployments."
                },
                "authors": [
                    {
                        "name": "Mika Yagoda"
                    },
                    {
                        "name": "Shady Abu-Hussein"
                    },
                    {
                        "name": "Raja Giryes"
                    }
                ],
                "author_detail": {
                    "name": "Raja Giryes"
                },
                "author": "Raja Giryes",
                "arxiv_comment": "To be published in IEEE Signal Processing Letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02793v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02793v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02791v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02791v1",
                "updated": "2025-11-04T18:13:48Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    18,
                    13,
                    48,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T18:13:48Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    18,
                    13,
                    48,
                    1,
                    308,
                    0
                ],
                "title": "AI-Generated Image Detection: An Empirical Study and Future Research\n  Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-Generated Image Detection: An Empirical Study and Future Research\n  Directions"
                },
                "summary": "The threats posed by AI-generated media, particularly deepfakes, are now\nraising significant challenges for multimedia forensics, misinformation\ndetection, and biometric system resulting in erosion of public trust in the\nlegal system, significant increase in frauds, and social engineering attacks.\nAlthough several forensic methods have been proposed, they suffer from three\ncritical gaps: (i) use of non-standardized benchmarks with GAN- or\ndiffusion-generated images, (ii) inconsistent training protocols (e.g.,\nscratch, frozen, fine-tuning), and (iii) limited evaluation metrics that fail\nto capture generalization and explainability. These limitations hinder fair\ncomparison, obscure true robustness, and restrict deployment in\nsecurity-critical applications. This paper introduces a unified benchmarking\nframework for systematic evaluation of forensic methods under controlled and\nreproducible conditions. We benchmark ten SoTA forensic methods (scratch,\nfrozen, and fine-tuned) and seven publicly available datasets (GAN and\ndiffusion) to perform extensive and systematic evaluations. We evaluate\nperformance using multiple metrics, including accuracy, average precision,\nROC-AUC, error rate, and class-wise sensitivity. We also further analyze model\ninterpretability using confidence curves and Grad-CAM heatmaps. Our evaluations\ndemonstrate substantial variability in generalization, with certain methods\nexhibiting strong in-distribution performance but degraded cross-model\ntransferability. This study aims to guide the research community toward a\ndeeper understanding of the strengths and limitations of current forensic\napproaches, and to inspire the development of more robust, generalizable, and\nexplainable solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The threats posed by AI-generated media, particularly deepfakes, are now\nraising significant challenges for multimedia forensics, misinformation\ndetection, and biometric system resulting in erosion of public trust in the\nlegal system, significant increase in frauds, and social engineering attacks.\nAlthough several forensic methods have been proposed, they suffer from three\ncritical gaps: (i) use of non-standardized benchmarks with GAN- or\ndiffusion-generated images, (ii) inconsistent training protocols (e.g.,\nscratch, frozen, fine-tuning), and (iii) limited evaluation metrics that fail\nto capture generalization and explainability. These limitations hinder fair\ncomparison, obscure true robustness, and restrict deployment in\nsecurity-critical applications. This paper introduces a unified benchmarking\nframework for systematic evaluation of forensic methods under controlled and\nreproducible conditions. We benchmark ten SoTA forensic methods (scratch,\nfrozen, and fine-tuned) and seven publicly available datasets (GAN and\ndiffusion) to perform extensive and systematic evaluations. We evaluate\nperformance using multiple metrics, including accuracy, average precision,\nROC-AUC, error rate, and class-wise sensitivity. We also further analyze model\ninterpretability using confidence curves and Grad-CAM heatmaps. Our evaluations\ndemonstrate substantial variability in generalization, with certain methods\nexhibiting strong in-distribution performance but degraded cross-model\ntransferability. This study aims to guide the research community toward a\ndeeper understanding of the strengths and limitations of current forensic\napproaches, and to inspire the development of more robust, generalizable, and\nexplainable solutions."
                },
                "authors": [
                    {
                        "name": "Nusrat Tasnim"
                    },
                    {
                        "name": "Kutub Uddin"
                    },
                    {
                        "name": "Khalid Mahmood Malik"
                    }
                ],
                "author_detail": {
                    "name": "Khalid Mahmood Malik"
                },
                "author": "Khalid Mahmood Malik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02791v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02791v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18467v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18467v2",
                "updated": "2025-11-04T18:01:01Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    18,
                    1,
                    1,
                    1,
                    308,
                    0
                ],
                "published": "2025-09-22T22:43:44Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    22,
                    43,
                    44,
                    0,
                    265,
                    0
                ],
                "title": "LAWCAT: Efficient Distillation from Quadratic to Linear Attention with\n  Convolution across Tokens for Long Context Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAWCAT: Efficient Distillation from Quadratic to Linear Attention with\n  Convolution across Tokens for Long Context Modeling"
                },
                "summary": "Although transformer architectures have achieved state-of-the-art performance\nacross diverse domains, their quadratic computational complexity with respect\nto sequence length remains a significant bottleneck, particularly for\nlatency-sensitive long-context applications. While recent linear-complexity\nalternatives are increasingly powerful, effectively training them from scratch\nis still resource-intensive. To overcome these limitations, we propose LAWCAT\n(Linear Attention with Convolution Across Time), a novel linearization\nframework designed to efficiently transfer the capabilities of pre-trained\ntransformers into a performant linear attention architecture. LAWCAT integrates\ncausal Conv1D layers to enhance local dependency modeling and employs\nnormalized gated linear attention to improve generalization across varying\ncontext lengths. Our comprehensive evaluations demonstrate that, distilling\nMistral-7B with only 1K-length sequences yields over 90\\% passkey retrieval\naccuracy up to 22K tokens, significantly extending its effective context\nwindow. Similarly, Llama3.2-1B LAWCAT variant achieves competitive performance\non S-NIAH 1\\&2\\&3 tasks (1K-8K context length) and BABILong benchmark\n(QA2\\&QA3, 0K-16K context length), requiring less than 0.1\\% pre-training\ntokens compared with pre-training models. Furthermore, LAWCAT exhibits faster\nprefill speeds than FlashAttention-2 for sequences exceeding 8K tokens. LAWCAT\nthus provides an efficient pathway to high-performance, long-context linear\nmodels suitable for edge deployment, reducing reliance on extensive\nlong-sequence training data and computational resources. Code is released at:\nhttps://github.com/zeyuliu1037/LAWCAT",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although transformer architectures have achieved state-of-the-art performance\nacross diverse domains, their quadratic computational complexity with respect\nto sequence length remains a significant bottleneck, particularly for\nlatency-sensitive long-context applications. While recent linear-complexity\nalternatives are increasingly powerful, effectively training them from scratch\nis still resource-intensive. To overcome these limitations, we propose LAWCAT\n(Linear Attention with Convolution Across Time), a novel linearization\nframework designed to efficiently transfer the capabilities of pre-trained\ntransformers into a performant linear attention architecture. LAWCAT integrates\ncausal Conv1D layers to enhance local dependency modeling and employs\nnormalized gated linear attention to improve generalization across varying\ncontext lengths. Our comprehensive evaluations demonstrate that, distilling\nMistral-7B with only 1K-length sequences yields over 90\\% passkey retrieval\naccuracy up to 22K tokens, significantly extending its effective context\nwindow. Similarly, Llama3.2-1B LAWCAT variant achieves competitive performance\non S-NIAH 1\\&2\\&3 tasks (1K-8K context length) and BABILong benchmark\n(QA2\\&QA3, 0K-16K context length), requiring less than 0.1\\% pre-training\ntokens compared with pre-training models. Furthermore, LAWCAT exhibits faster\nprefill speeds than FlashAttention-2 for sequences exceeding 8K tokens. LAWCAT\nthus provides an efficient pathway to high-performance, long-context linear\nmodels suitable for edge deployment, reducing reliance on extensive\nlong-sequence training data and computational resources. Code is released at:\nhttps://github.com/zeyuliu1037/LAWCAT"
                },
                "authors": [
                    {
                        "name": "Zeyu Liu"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Lianghao Jiang"
                    },
                    {
                        "name": "Anni Li"
                    },
                    {
                        "name": "Srikanth Ronanki"
                    },
                    {
                        "name": "Sravan Bodapati"
                    },
                    {
                        "name": "Gourav Datta"
                    },
                    {
                        "name": "Peter A. Beerel"
                    }
                ],
                "author_detail": {
                    "name": "Peter A. Beerel"
                },
                "author": "Peter A. Beerel",
                "arxiv_comment": "17 pages, 8 figures. EMNLP2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18467v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18467v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.00130v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.00130v2",
                "updated": "2025-11-04T17:53:35Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    17,
                    53,
                    35,
                    1,
                    308,
                    0
                ],
                "published": "2025-10-31T10:25:48Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    10,
                    25,
                    48,
                    4,
                    304,
                    0
                ],
                "title": "A Comparative Analysis of LLM Adaptation: SFT, LoRA, and ICL in\n  Data-Scarce Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comparative Analysis of LLM Adaptation: SFT, LoRA, and ICL in\n  Data-Scarce Scenarios"
                },
                "summary": "The remarkable capabilities of Large Language Models (LLMs) often need to be\ntailored for specific applications, requiring the integration of new knowledge\nor the acquisition of new skills. While full fine-tuning is a powerful\nadaptation method, it is computationally expensive and can lead to a\ndegradation of general reasoning abilities, a phenomenon known as catastrophic\nforgetting. A range of alternative techniques exists, each with its own\ntrade-offs. In-Context Learning (ICL) is fast but limited by context length,\nwhile Parameter-Efficient Fine-Tuning (PEFT) methods like Low-Rank Adaptation\n(LoRA) offer a middle ground by minimizing parameter changes. However, the\nchallenge of catastrophic forgetting persists, raising questions about the best\nadaptation strategy for a given task. This paper presents a comparative\nanalysis of Supervised Finetuning (SFT), LoRA, and ICL in data-scarce\nscenarios. We find that LoRA provides the most effective balance, successfully\ninstilling new skills with minimal impact on the base model's general\nknowledge. In contrast, while SFT excels at skill acquisition, it is highly\nsusceptible to catastrophic forgetting. ICL is effective for incorporating\nfactual knowledge but struggles with complex skills. Our findings offer a\npractical framework for selecting an LLM adaptation strategy. We highlight the\ncritical distinction between skill acquisition and knowledge integration,\nclarify the trade-offs between task-specific performance and the preservation\nof general capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The remarkable capabilities of Large Language Models (LLMs) often need to be\ntailored for specific applications, requiring the integration of new knowledge\nor the acquisition of new skills. While full fine-tuning is a powerful\nadaptation method, it is computationally expensive and can lead to a\ndegradation of general reasoning abilities, a phenomenon known as catastrophic\nforgetting. A range of alternative techniques exists, each with its own\ntrade-offs. In-Context Learning (ICL) is fast but limited by context length,\nwhile Parameter-Efficient Fine-Tuning (PEFT) methods like Low-Rank Adaptation\n(LoRA) offer a middle ground by minimizing parameter changes. However, the\nchallenge of catastrophic forgetting persists, raising questions about the best\nadaptation strategy for a given task. This paper presents a comparative\nanalysis of Supervised Finetuning (SFT), LoRA, and ICL in data-scarce\nscenarios. We find that LoRA provides the most effective balance, successfully\ninstilling new skills with minimal impact on the base model's general\nknowledge. In contrast, while SFT excels at skill acquisition, it is highly\nsusceptible to catastrophic forgetting. ICL is effective for incorporating\nfactual knowledge but struggles with complex skills. Our findings offer a\npractical framework for selecting an LLM adaptation strategy. We highlight the\ncritical distinction between skill acquisition and knowledge integration,\nclarify the trade-offs between task-specific performance and the preservation\nof general capabilities."
                },
                "authors": [
                    {
                        "name": "Bernd Bohnet"
                    },
                    {
                        "name": "Rumen Dangovski"
                    },
                    {
                        "name": "Kevin Swersky"
                    },
                    {
                        "name": "Sherry Moore"
                    },
                    {
                        "name": "Arslan Chaudhry"
                    },
                    {
                        "name": "Kathleen Kenealy"
                    },
                    {
                        "name": "Noah Fiedel"
                    }
                ],
                "author_detail": {
                    "name": "Noah Fiedel"
                },
                "author": "Noah Fiedel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.00130v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.00130v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02759v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02759v1",
                "updated": "2025-11-04T17:36:57Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    17,
                    36,
                    57,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T17:36:57Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    17,
                    36,
                    57,
                    1,
                    308,
                    0
                ],
                "title": "LLM-Supported Formal Knowledge Representation for Enhancing Control\n  Engineering Content with an Interactive Semantic Layer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Supported Formal Knowledge Representation for Enhancing Control\n  Engineering Content with an Interactive Semantic Layer"
                },
                "summary": "The rapid growth of research output in control engineering calls for new\napproaches to structure and formalize domain knowledge. This paper briefly\ndescribes an LLM-supported method for semi-automated generation of formal\nknowledge representations that combine human readability with machine\ninterpretability and increased expressiveness. Based on the Imperative\nRepresentation of Knowledge (PyIRK) framework, we demonstrate how language\nmodels can assist in transforming natural-language descriptions and\nmathematical definitions (available as LaTeX source code) into a formalized\nknowledge graph. As a first application we present the generation of an\n``interactive semantic layer'' to enhance the source documents in order to\nfacilitate knowledge transfer. From our perspective this contributes to the\nvision of easily accessible, collaborative, and verifiable knowledge bases for\nthe control engineering domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of research output in control engineering calls for new\napproaches to structure and formalize domain knowledge. This paper briefly\ndescribes an LLM-supported method for semi-automated generation of formal\nknowledge representations that combine human readability with machine\ninterpretability and increased expressiveness. Based on the Imperative\nRepresentation of Knowledge (PyIRK) framework, we demonstrate how language\nmodels can assist in transforming natural-language descriptions and\nmathematical definitions (available as LaTeX source code) into a formalized\nknowledge graph. As a first application we present the generation of an\n``interactive semantic layer'' to enhance the source documents in order to\nfacilitate knowledge transfer. From our perspective this contributes to the\nvision of easily accessible, collaborative, and verifiable knowledge bases for\nthe control engineering domain."
                },
                "authors": [
                    {
                        "name": "Julius Fiedler"
                    },
                    {
                        "name": "Carsten Knoll"
                    },
                    {
                        "name": "Klaus Röbenack"
                    }
                ],
                "author_detail": {
                    "name": "Klaus Röbenack"
                },
                "arxiv_affiliation": "Institute of Control Theory at TU Dresden",
                "author": "Klaus Röbenack",
                "arxiv_comment": "4 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02759v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02759v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02757v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02757v1",
                "updated": "2025-11-04T17:35:52Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    17,
                    35,
                    52,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T17:35:52Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    17,
                    35,
                    52,
                    1,
                    308,
                    0
                ],
                "title": "ConMeZO: Adaptive Descent-Direction Sampling for Gradient-Free\n  Finetuning of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConMeZO: Adaptive Descent-Direction Sampling for Gradient-Free\n  Finetuning of Large Language Models"
                },
                "summary": "Zeroth-order or derivative-free optimization (MeZO) is an attractive strategy\nfor finetuning large language models (LLMs) because it eliminates the memory\noverhead of backpropagation. However, it converges slowly due to the inherent\ncurse of dimensionality when searching for descent directions in the\nhigh-dimensional parameter space of billion-scale LLMs. We propose ConMeZO, a\nnovel zeroth-order optimizer that accelerates convergence by adaptive\ndirectional sampling. Instead of drawing the direction uniformly at random,\nConMeZO restricts the sampling to a cone centered around a momentum estimate.\nThis concentrates the search in directions where the true gradient is more\nlikely to lie and thus reduces the effect of high dimensions. We prove that\nConMeZO achieves the same worst-case convergence rate as MeZO. Empirically,\nwhen finetuning LLMs on natural language tasks, ConMeZO is up to 2X faster than\nMeZO while retaining the low-memory footprint of zeroth-order methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zeroth-order or derivative-free optimization (MeZO) is an attractive strategy\nfor finetuning large language models (LLMs) because it eliminates the memory\noverhead of backpropagation. However, it converges slowly due to the inherent\ncurse of dimensionality when searching for descent directions in the\nhigh-dimensional parameter space of billion-scale LLMs. We propose ConMeZO, a\nnovel zeroth-order optimizer that accelerates convergence by adaptive\ndirectional sampling. Instead of drawing the direction uniformly at random,\nConMeZO restricts the sampling to a cone centered around a momentum estimate.\nThis concentrates the search in directions where the true gradient is more\nlikely to lie and thus reduces the effect of high dimensions. We prove that\nConMeZO achieves the same worst-case convergence rate as MeZO. Empirically,\nwhen finetuning LLMs on natural language tasks, ConMeZO is up to 2X faster than\nMeZO while retaining the low-memory footprint of zeroth-order methods."
                },
                "authors": [
                    {
                        "name": "Lejs Deen Behric"
                    },
                    {
                        "name": "Liang Zhang"
                    },
                    {
                        "name": "Bingcong Li"
                    },
                    {
                        "name": "Kiran Koshy Thekumparampil"
                    }
                ],
                "author_detail": {
                    "name": "Kiran Koshy Thekumparampil"
                },
                "author": "Kiran Koshy Thekumparampil",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02757v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02757v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02755v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02755v1",
                "updated": "2025-11-04T17:35:17Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    17,
                    35,
                    17,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T17:35:17Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    17,
                    35,
                    17,
                    1,
                    308,
                    0
                ],
                "title": "Controlling Performance and Budget of a Centralized Multi-agent LLM\n  System with Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controlling Performance and Budget of a Centralized Multi-agent LLM\n  System with Reinforcement Learning"
                },
                "summary": "Large language models (LLMs) exhibit complementary strengths across domains\nand come with varying inference costs, motivating the design of multi-agent LLM\nsystems where specialized models collaborate efficiently. Existing approaches\npredominantly rely on decentralized frameworks, which invoke multiple LLMs for\nevery input and thus lead to substantial and uncontrolled inference costs. In\nthis work, we introduce a centralized multi-LLM framework, where a controller\nLLM selectively coordinates a pool of expert models in a cost-efficient and\ncost-controllable manner. We formulate this coordination problem as\nreinforcement learning with dual objectives: maximizing task performance while\nminimizing the overall inference cost. In addition, we expect the multi-agent\nsystem to have adapted behavior with different budget conditions during\ninference. To this end, we propose CoRL, a reinforcement learning framework\nthat optimizes the performance cost trade-off in a controllable multi-budget\nsetting. Experiments on four diverse benchmarks demonstrate that CoRL enables a\nsingle system to surpass the best expert LLM under high-budget settings, while\nmaintaining strong performance in more economical low-budget modes,\nhighlighting the effectiveness of centralized coordination for scalable and\ncost-efficient multi-agent LLM systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit complementary strengths across domains\nand come with varying inference costs, motivating the design of multi-agent LLM\nsystems where specialized models collaborate efficiently. Existing approaches\npredominantly rely on decentralized frameworks, which invoke multiple LLMs for\nevery input and thus lead to substantial and uncontrolled inference costs. In\nthis work, we introduce a centralized multi-LLM framework, where a controller\nLLM selectively coordinates a pool of expert models in a cost-efficient and\ncost-controllable manner. We formulate this coordination problem as\nreinforcement learning with dual objectives: maximizing task performance while\nminimizing the overall inference cost. In addition, we expect the multi-agent\nsystem to have adapted behavior with different budget conditions during\ninference. To this end, we propose CoRL, a reinforcement learning framework\nthat optimizes the performance cost trade-off in a controllable multi-budget\nsetting. Experiments on four diverse benchmarks demonstrate that CoRL enables a\nsingle system to surpass the best expert LLM under high-budget settings, while\nmaintaining strong performance in more economical low-budget modes,\nhighlighting the effectiveness of centralized coordination for scalable and\ncost-efficient multi-agent LLM systems."
                },
                "authors": [
                    {
                        "name": "Bowen Jin"
                    },
                    {
                        "name": "TJ Collins"
                    },
                    {
                        "name": "Donghan Yu"
                    },
                    {
                        "name": "Mert Cemri"
                    },
                    {
                        "name": "Shenao Zhang"
                    },
                    {
                        "name": "Mengyu Li"
                    },
                    {
                        "name": "Jay Tang"
                    },
                    {
                        "name": "Tian Qin"
                    },
                    {
                        "name": "Zhiyang Xu"
                    },
                    {
                        "name": "Jiarui Lu"
                    },
                    {
                        "name": "Guoli Yin"
                    },
                    {
                        "name": "Jiawei Han"
                    },
                    {
                        "name": "Zirui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zirui Wang"
                },
                "author": "Zirui Wang",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02755v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02755v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02752v1",
                "updated": "2025-11-04T17:31:39Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    17,
                    31,
                    39,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T17:31:39Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    17,
                    31,
                    39,
                    1,
                    308,
                    0
                ],
                "title": "AI Diffusion in Low Resource Language Countries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Diffusion in Low Resource Language Countries"
                },
                "summary": "Artificial intelligence (AI) is diffusing globally at unprecedented speed,\nbut adoption remains uneven. Frontier Large Language Models (LLMs) are known to\nperform poorly on low-resource languages due to data scarcity. We hypothesize\nthat this performance deficit reduces the utility of AI, thereby slowing\nadoption in Low-Resource Language Countries (LRLCs). To test this, we use a\nweighted regression model to isolate the language effect from socioeconomic and\ndemographic factors, finding that LRLCs have a share of AI users that is\napproximately 20% lower relative to their baseline. These results indicate that\nlinguistic accessibility is a significant, independent barrier to equitable AI\ndiffusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence (AI) is diffusing globally at unprecedented speed,\nbut adoption remains uneven. Frontier Large Language Models (LLMs) are known to\nperform poorly on low-resource languages due to data scarcity. We hypothesize\nthat this performance deficit reduces the utility of AI, thereby slowing\nadoption in Low-Resource Language Countries (LRLCs). To test this, we use a\nweighted regression model to isolate the language effect from socioeconomic and\ndemographic factors, finding that LRLCs have a share of AI users that is\napproximately 20% lower relative to their baseline. These results indicate that\nlinguistic accessibility is a significant, independent barrier to equitable AI\ndiffusion."
                },
                "authors": [
                    {
                        "name": "Amit Misra"
                    },
                    {
                        "name": "Syed Waqas Zamir"
                    },
                    {
                        "name": "Wassim Hamidouche"
                    },
                    {
                        "name": "Inbal Becker-Reshef"
                    },
                    {
                        "name": "Juan Lavista Ferres"
                    }
                ],
                "author_detail": {
                    "name": "Juan Lavista Ferres"
                },
                "author": "Juan Lavista Ferres",
                "arxiv_comment": "9 pages, 4 tables. Also available at\n  https://aka.ms/AI_Diffusion_Low_Resource_Language_Countries",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.00985v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.00985v2",
                "updated": "2025-11-04T17:28:21Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    17,
                    28,
                    21,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-02T15:57:18Z",
                "published_parsed": [
                    2025,
                    11,
                    2,
                    15,
                    57,
                    18,
                    6,
                    306,
                    0
                ],
                "title": "ORANGE: An Online Reflection ANd GEneration framework with Domain\n  Knowledge for Text-to-SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ORANGE: An Online Reflection ANd GEneration framework with Domain\n  Knowledge for Text-to-SQL"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable progress in\ntranslating natural language to SQL, but a significant semantic gap persists\nbetween their general knowledge and domain-specific semantics of databases.\nHistorical translation logs constitute a rich source of this missing in-domain\nknowledge, where SQL queries inherently encapsulate real-world usage patterns\nof database schema. Existing methods primarily enhance the reasoning process\nfor individual translations but fail to accumulate in-domain knowledge from\npast translations. We introduce ORANGE, an online self-evolutionary framework\nthat constructs database-specific knowledge bases by parsing SQL queries from\ntranslation logs. By accumulating in-domain knowledge that contains schema and\ndata semantics, ORANGE progressively reduces the semantic gap and enhances the\naccuracy of subsequent SQL translations. To ensure reliability, we propose a\nnovel nested Chain-of-Thought SQL-to-Text strategy with tuple-semantic\ntracking, which reduces semantic errors during knowledge generation.\nExperiments on multiple benchmarks confirm the practicality of ORANGE,\ndemonstrating its effectiveness for real-world Text-to-SQL deployment,\nparticularly in handling complex and domain-specific queries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable progress in\ntranslating natural language to SQL, but a significant semantic gap persists\nbetween their general knowledge and domain-specific semantics of databases.\nHistorical translation logs constitute a rich source of this missing in-domain\nknowledge, where SQL queries inherently encapsulate real-world usage patterns\nof database schema. Existing methods primarily enhance the reasoning process\nfor individual translations but fail to accumulate in-domain knowledge from\npast translations. We introduce ORANGE, an online self-evolutionary framework\nthat constructs database-specific knowledge bases by parsing SQL queries from\ntranslation logs. By accumulating in-domain knowledge that contains schema and\ndata semantics, ORANGE progressively reduces the semantic gap and enhances the\naccuracy of subsequent SQL translations. To ensure reliability, we propose a\nnovel nested Chain-of-Thought SQL-to-Text strategy with tuple-semantic\ntracking, which reduces semantic errors during knowledge generation.\nExperiments on multiple benchmarks confirm the practicality of ORANGE,\ndemonstrating its effectiveness for real-world Text-to-SQL deployment,\nparticularly in handling complex and domain-specific queries."
                },
                "authors": [
                    {
                        "name": "Yiwen Jiao"
                    },
                    {
                        "name": "Tonghui Ren"
                    },
                    {
                        "name": "Yuche Gao"
                    },
                    {
                        "name": "Zhenying He"
                    },
                    {
                        "name": "Yinan Jing"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "X. Sean Wang"
                    }
                ],
                "author_detail": {
                    "name": "X. Sean Wang"
                },
                "author": "X. Sean Wang",
                "arxiv_comment": "16 pages, 4 figures, preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.00985v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.00985v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02748v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02748v1",
                "updated": "2025-11-04T17:22:22Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    17,
                    22,
                    22,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T17:22:22Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    17,
                    22,
                    22,
                    1,
                    308,
                    0
                ],
                "title": "Agentic World Modeling for 6G: Near-Real-Time Generative State-Space\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic World Modeling for 6G: Near-Real-Time Generative State-Space\n  Reasoning"
                },
                "summary": "We argue that sixth-generation (6G) intelligence is not fluent token\nprediction but the capacity to imagine and choose -- to simulate future\nscenarios, weigh trade-offs, and act with calibrated uncertainty. We reframe\nopen radio access network (O-RAN) near-real-time (Near-RT) control via\ncounterfactual dynamics and a world modeling (WM) paradigm that learns an\naction-conditioned generative state space. This enables quantitative \"what-if\"\nforecasting beyond large language models (LLMs) as the primary modeling\nprimitive. Actions such as physical resource blocks (PRBs) are treated as\nfirst-class control inputs in a causal world model, and both aleatoric and\nepistemic uncertainty are modeled for prediction and what-if analysis. An\nagentic, model predictive control (MPC)-based cross-entropy method (CEM)\nplanner operates over short horizons, using prior-mean rollouts within\ndata-driven PRB bounds to maximize a deterministic reward. The model couples\nmulti-scale structured state-space mixtures (MS3M) with a compact stochastic\nlatent to form WM-MS3M, summarizing key performance indicators (KPIs) histories\nand predicting next-step KPIs under hypothetical PRB sequences. On realistic\nO-RAN traces, WM-MS3M cuts mean absolute error (MAE) by 1.69% versus MS3M with\n32% fewer parameters and similar latency, and achieves 35-80% lower root mean\nsquared error (RMSE) than attention/hybrid baselines with 2.3-4.1x faster\ninference, enabling rare-event simulation and offline policy screening.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We argue that sixth-generation (6G) intelligence is not fluent token\nprediction but the capacity to imagine and choose -- to simulate future\nscenarios, weigh trade-offs, and act with calibrated uncertainty. We reframe\nopen radio access network (O-RAN) near-real-time (Near-RT) control via\ncounterfactual dynamics and a world modeling (WM) paradigm that learns an\naction-conditioned generative state space. This enables quantitative \"what-if\"\nforecasting beyond large language models (LLMs) as the primary modeling\nprimitive. Actions such as physical resource blocks (PRBs) are treated as\nfirst-class control inputs in a causal world model, and both aleatoric and\nepistemic uncertainty are modeled for prediction and what-if analysis. An\nagentic, model predictive control (MPC)-based cross-entropy method (CEM)\nplanner operates over short horizons, using prior-mean rollouts within\ndata-driven PRB bounds to maximize a deterministic reward. The model couples\nmulti-scale structured state-space mixtures (MS3M) with a compact stochastic\nlatent to form WM-MS3M, summarizing key performance indicators (KPIs) histories\nand predicting next-step KPIs under hypothetical PRB sequences. On realistic\nO-RAN traces, WM-MS3M cuts mean absolute error (MAE) by 1.69% versus MS3M with\n32% fewer parameters and similar latency, and achieves 35-80% lower root mean\nsquared error (RMSE) than attention/hybrid baselines with 2.3-4.1x faster\ninference, enabling rare-event simulation and offline policy screening."
                },
                "authors": [
                    {
                        "name": "Farhad Rezazadeh"
                    },
                    {
                        "name": "Hatim Chergui"
                    },
                    {
                        "name": "Merouane Debbah"
                    },
                    {
                        "name": "Houbing Song"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Lingjia Liu"
                    }
                ],
                "author_detail": {
                    "name": "Lingjia Liu"
                },
                "author": "Lingjia Liu",
                "arxiv_comment": "13 Pages, 3 Figures, 4 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02748v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02748v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02735v1",
                "updated": "2025-11-04T16:58:49Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    16,
                    58,
                    49,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T16:58:49Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    16,
                    58,
                    49,
                    1,
                    308,
                    0
                ],
                "title": "Spatial Insight: How Data-Driven Regions of Interest Selection Enhances\n  Single-Trial P300 Classification in EEG-Based BCIs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatial Insight: How Data-Driven Regions of Interest Selection Enhances\n  Single-Trial P300 Classification in EEG-Based BCIs"
                },
                "summary": "EEG-based Brain-Computer Interfaces (BCIs) frequently face spatial\nspecificity limitations in detecting single-trial P300 potentials, a\nneurophysiological hallmark leveraged for both BCI control and\nneurodegenerative disease diagnostics. We present a novel framework combining\neLORETA source localization with cross-subject functional connectivity to\nidentify stable regions of interest (ROIs) across sessions. Analyzing\n62-channel EEG data from 31 subjects (63 sessions, 2,520 trials), we\ndemonstrate that phase-lagged connectivity metrics can reliably isolate\ntask-relevant hubs in deeper cortical-subcortical structures like the insula\nand parietal regions - critical for Alzheimer's disease biomarkers. By\nintegrating spatially stable ROIs with dynamic temporal agreement, our hybrid\nclassification systematically outperforms whole-brain approaches in different\nfrequency bands (up to 5.4% depending on the connectivity method and the\nspectral range) while maintaining millisecond-level temporal precision.\n  To the best of our knowledge, this is the first study to establish\ncross-subject ROI consensus through source-space connectivity, bypassing scalp\nEEG's depth constraints to probe Alzheimer's-relevant networks. The framework's\nrobustness to noise and compatibility with portable systems offer significant\npotential for global deployment in early neurodegenerative disease detection.\nFuture integration of individualized anatomical data or adaptive parameter\noptimization could refine this tool for clinical deployment, enhancing the\ncurrent max accuracy of 81.57% in the 1-15 Hz range.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EEG-based Brain-Computer Interfaces (BCIs) frequently face spatial\nspecificity limitations in detecting single-trial P300 potentials, a\nneurophysiological hallmark leveraged for both BCI control and\nneurodegenerative disease diagnostics. We present a novel framework combining\neLORETA source localization with cross-subject functional connectivity to\nidentify stable regions of interest (ROIs) across sessions. Analyzing\n62-channel EEG data from 31 subjects (63 sessions, 2,520 trials), we\ndemonstrate that phase-lagged connectivity metrics can reliably isolate\ntask-relevant hubs in deeper cortical-subcortical structures like the insula\nand parietal regions - critical for Alzheimer's disease biomarkers. By\nintegrating spatially stable ROIs with dynamic temporal agreement, our hybrid\nclassification systematically outperforms whole-brain approaches in different\nfrequency bands (up to 5.4% depending on the connectivity method and the\nspectral range) while maintaining millisecond-level temporal precision.\n  To the best of our knowledge, this is the first study to establish\ncross-subject ROI consensus through source-space connectivity, bypassing scalp\nEEG's depth constraints to probe Alzheimer's-relevant networks. The framework's\nrobustness to noise and compatibility with portable systems offer significant\npotential for global deployment in early neurodegenerative disease detection.\nFuture integration of individualized anatomical data or adaptive parameter\noptimization could refine this tool for clinical deployment, enhancing the\ncurrent max accuracy of 81.57% in the 1-15 Hz range."
                },
                "authors": [
                    {
                        "name": "Eva Guttmann-Flury"
                    },
                    {
                        "name": "Jian Zhao"
                    },
                    {
                        "name": "Mohamad Sawan"
                    }
                ],
                "author_detail": {
                    "name": "Mohamad Sawan"
                },
                "author": "Mohamad Sawan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02734v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02734v1",
                "updated": "2025-11-04T16:58:29Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    16,
                    58,
                    29,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T16:58:29Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    16,
                    58,
                    29,
                    1,
                    308,
                    0
                ],
                "title": "CostBench: Evaluating Multi-Turn Cost-Optimal Planning and Adaptation in\n  Dynamic Environments for LLM Tool-Use Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CostBench: Evaluating Multi-Turn Cost-Optimal Planning and Adaptation in\n  Dynamic Environments for LLM Tool-Use Agents"
                },
                "summary": "Current evaluations of Large Language Model (LLM) agents primarily emphasize\ntask completion, often overlooking resource efficiency and adaptability. This\nneglects a crucial capability: agents' ability to devise and adjust\ncost-optimal plans in response to changing environments. To bridge this gap, we\nintroduce CostBench, a scalable, cost-centric benchmark designed to evaluate\nagents' economic reasoning and replanning abilities. Situated in the\ntravel-planning domain, CostBench comprises tasks solvable via multiple\nsequences of atomic and composite tools with diverse, customizable costs. It\nalso supports four types of dynamic blocking events, such as tool failures and\ncost changes, to simulate real-world unpredictability and necessitate agents to\nadapt in real time. Evaluating leading open-sourced and proprietary models on\nCostBench reveals a substantial gap in cost-aware planning: agents frequently\nfail to identify cost-optimal solutions in static settings, with even GPT-5\nachieving less than 75% exact match rate on the hardest tasks, and performance\nfurther dropping by around 40% under dynamic conditions. By diagnosing these\nweaknesses, CostBench lays the groundwork for developing future agents that are\nboth economically rational and robust.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current evaluations of Large Language Model (LLM) agents primarily emphasize\ntask completion, often overlooking resource efficiency and adaptability. This\nneglects a crucial capability: agents' ability to devise and adjust\ncost-optimal plans in response to changing environments. To bridge this gap, we\nintroduce CostBench, a scalable, cost-centric benchmark designed to evaluate\nagents' economic reasoning and replanning abilities. Situated in the\ntravel-planning domain, CostBench comprises tasks solvable via multiple\nsequences of atomic and composite tools with diverse, customizable costs. It\nalso supports four types of dynamic blocking events, such as tool failures and\ncost changes, to simulate real-world unpredictability and necessitate agents to\nadapt in real time. Evaluating leading open-sourced and proprietary models on\nCostBench reveals a substantial gap in cost-aware planning: agents frequently\nfail to identify cost-optimal solutions in static settings, with even GPT-5\nachieving less than 75% exact match rate on the hardest tasks, and performance\nfurther dropping by around 40% under dynamic conditions. By diagnosing these\nweaknesses, CostBench lays the groundwork for developing future agents that are\nboth economically rational and robust."
                },
                "authors": [
                    {
                        "name": "Jiayu Liu"
                    },
                    {
                        "name": "Cheng Qian"
                    },
                    {
                        "name": "Zhaochen Su"
                    },
                    {
                        "name": "Qing Zong"
                    },
                    {
                        "name": "Shijue Huang"
                    },
                    {
                        "name": "Bingxiang He"
                    },
                    {
                        "name": "Yi R. Fung"
                    }
                ],
                "author_detail": {
                    "name": "Yi R. Fung"
                },
                "author": "Yi R. Fung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02734v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02734v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10924v8",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10924v8",
                "updated": "2025-11-04T16:46:04Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    16,
                    46,
                    4,
                    1,
                    308,
                    0
                ],
                "published": "2024-12-14T18:18:52Z",
                "published_parsed": [
                    2024,
                    12,
                    14,
                    18,
                    18,
                    52,
                    5,
                    349,
                    0
                ],
                "title": "Tokens, the oft-overlooked appetizer: Large language models, the\n  distributional hypothesis, and meaning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokens, the oft-overlooked appetizer: Large language models, the\n  distributional hypothesis, and meaning"
                },
                "summary": "Tokenization is a necessary component within the current architecture of many\nlanguage mod-els, including the transformer-based large language models (LLMs)\nof Generative AI, yet its impact on the model's cognition is often overlooked.\nWe argue that LLMs demonstrate that the Distributional Hypothesis (DH) is\nsufficient for reasonably human-like language performance (particularly with\nrespect to inferential lexical competence), and that the emergence of\nhuman-meaningful linguistic units among tokens and current structural\nconstraints motivate changes to existing, linguistically-agnostic tokenization\ntechniques, particularly with respect to their roles as (1) vehicles for\nconveying salient distributional patterns from human language to the model and\nas (2) semantic primitives. We explore tokenizations from a BPE tokenizer;\nextant model vocabularies obtained from Hugging Face and tiktoken; and the\ninformation in exemplar token vectors as they move through the layers of a\nRoBERTa (large) model. Besides creating suboptimal semantic building blocks and\nobscuring the model's access to the necessary distributional patterns, we\ndescribe how tokens and pretraining can act as a backdoor for bias and other\nunwanted content, which current alignment practices may not remediate.\nAdditionally, we relay evidence that the tokenization algorithm's objective\nfunction impacts the LLM's cognition, despite being arguably meaningfully\ninsulated from the main system intelligence. Finally, we discuss implications\nfor architectural choices, meaning construction, the primacy of language for\nthought, and LLM cognition. [First uploaded to arXiv in December, 2024.]",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokenization is a necessary component within the current architecture of many\nlanguage mod-els, including the transformer-based large language models (LLMs)\nof Generative AI, yet its impact on the model's cognition is often overlooked.\nWe argue that LLMs demonstrate that the Distributional Hypothesis (DH) is\nsufficient for reasonably human-like language performance (particularly with\nrespect to inferential lexical competence), and that the emergence of\nhuman-meaningful linguistic units among tokens and current structural\nconstraints motivate changes to existing, linguistically-agnostic tokenization\ntechniques, particularly with respect to their roles as (1) vehicles for\nconveying salient distributional patterns from human language to the model and\nas (2) semantic primitives. We explore tokenizations from a BPE tokenizer;\nextant model vocabularies obtained from Hugging Face and tiktoken; and the\ninformation in exemplar token vectors as they move through the layers of a\nRoBERTa (large) model. Besides creating suboptimal semantic building blocks and\nobscuring the model's access to the necessary distributional patterns, we\ndescribe how tokens and pretraining can act as a backdoor for bias and other\nunwanted content, which current alignment practices may not remediate.\nAdditionally, we relay evidence that the tokenization algorithm's objective\nfunction impacts the LLM's cognition, despite being arguably meaningfully\ninsulated from the main system intelligence. Finally, we discuss implications\nfor architectural choices, meaning construction, the primacy of language for\nthought, and LLM cognition. [First uploaded to arXiv in December, 2024.]"
                },
                "authors": [
                    {
                        "name": "Julia Witte Zimmerman"
                    },
                    {
                        "name": "Denis Hudon"
                    },
                    {
                        "name": "Kathryn Cramer"
                    },
                    {
                        "name": "Alejandro J. Ruiz"
                    },
                    {
                        "name": "Calla Beauregard"
                    },
                    {
                        "name": "Ashley Fehr"
                    },
                    {
                        "name": "Mikaela Irene Fudolig"
                    },
                    {
                        "name": "Bradford Demarest"
                    },
                    {
                        "name": "Yoshi Meke Bird"
                    },
                    {
                        "name": "Milo Z. Trujillo"
                    },
                    {
                        "name": "Christopher M. Danforth"
                    },
                    {
                        "name": "Peter Sheridan Dodds"
                    }
                ],
                "author_detail": {
                    "name": "Peter Sheridan Dodds"
                },
                "author": "Peter Sheridan Dodds",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10924v8",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10924v8",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02713v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02713v1",
                "updated": "2025-11-04T16:31:44Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    16,
                    31,
                    44,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T16:31:44Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    16,
                    31,
                    44,
                    1,
                    308,
                    0
                ],
                "title": "ReleaseEval: A Benchmark for Evaluating Language Models in Automated\n  Release Note Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReleaseEval: A Benchmark for Evaluating Language Models in Automated\n  Release Note Generation"
                },
                "summary": "Automated release note generation addresses the challenge of documenting\nfrequent software updates, where manual efforts are time-consuming and prone to\nhuman error. Although recent advances in language models further enhance this\nprocess, progress remains hindered by dataset limitations, including the lack\nof explicit licensing and limited reproducibility, and incomplete task design\nthat relies mainly on commit messages for summarization while overlooking\nfine-grained contexts such as commit hierarchies and code changes. To fill this\ngap, we introduce ReleaseEval, a reproducible and openly licensed benchmark\ndesigned to systematically evaluate language models for automated release note\ngeneration. ReleaseEval comprises 94,987 release notes from 3,369 repositories\nacross 6 programming languages, and supports three task settings with three\nlevels of input granularity: (1) commit2sum, which generates release notes from\ncommit messages; (2) tree2sum, which incorporates commit tree structures; and\n(3) diff2sum, which leverages fine-grained code diffs. Both automated and human\nevaluations show that large language models consistently outperform traditional\nbaselines across all tasks, achieving substantial gains on tree2sum, while\nstill struggling on diff2sum. These findings highlight LLMs' proficiency in\nleveraging structured information while revealing challenges in abstracting\nfrom long code diffs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated release note generation addresses the challenge of documenting\nfrequent software updates, where manual efforts are time-consuming and prone to\nhuman error. Although recent advances in language models further enhance this\nprocess, progress remains hindered by dataset limitations, including the lack\nof explicit licensing and limited reproducibility, and incomplete task design\nthat relies mainly on commit messages for summarization while overlooking\nfine-grained contexts such as commit hierarchies and code changes. To fill this\ngap, we introduce ReleaseEval, a reproducible and openly licensed benchmark\ndesigned to systematically evaluate language models for automated release note\ngeneration. ReleaseEval comprises 94,987 release notes from 3,369 repositories\nacross 6 programming languages, and supports three task settings with three\nlevels of input granularity: (1) commit2sum, which generates release notes from\ncommit messages; (2) tree2sum, which incorporates commit tree structures; and\n(3) diff2sum, which leverages fine-grained code diffs. Both automated and human\nevaluations show that large language models consistently outperform traditional\nbaselines across all tasks, achieving substantial gains on tree2sum, while\nstill struggling on diff2sum. These findings highlight LLMs' proficiency in\nleveraging structured information while revealing challenges in abstracting\nfrom long code diffs."
                },
                "authors": [
                    {
                        "name": "Qianru Meng"
                    },
                    {
                        "name": "Zhaochun Ren"
                    },
                    {
                        "name": "Joost Visser"
                    }
                ],
                "author_detail": {
                    "name": "Joost Visser"
                },
                "author": "Joost Visser",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02713v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02713v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02711v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02711v1",
                "updated": "2025-11-04T16:30:55Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    16,
                    30,
                    55,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T16:30:55Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    16,
                    30,
                    55,
                    1,
                    308,
                    0
                ],
                "title": "Relational Deep Dive: Error-Aware Queries Over Unstructured Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relational Deep Dive: Error-Aware Queries Over Unstructured Data"
                },
                "summary": "Unstructured data is pervasive, but analytical queries demand structured\nrepresentations, creating a significant extraction challenge. Existing methods\nlike RAG lack schema awareness and struggle with cross-document alignment,\nleading to high error rates. We propose ReDD (Relational Deep Dive), a\nframework that dynamically discovers query-specific schemas, populates\nrelational tables, and ensures error-aware extraction with provable guarantees.\nReDD features a two-stage pipeline: (1) Iterative Schema Discovery (ISD)\nidentifies minimal, joinable schemas tailored to each query, and (2) Tabular\nData Population (TDP) extracts and corrects data using lightweight classifiers\ntrained on LLM hidden states. A main contribution of ReDD is SCAPE, a\nstatistically calibrated method for error detection with coverage guarantees,\nand SCAPE-HYB, a hybrid approach that optimizes the trade-off between accuracy\nand human correction costs. Experiments across diverse datasets demonstrate\nReDD's effectiveness, reducing data extraction errors from up to 30% to below\n1% while maintaining high schema completeness (100% recall) and precision.\nReDD's modular design enables fine-grained control over accuracy-cost\ntrade-offs, making it a robust solution for high-stakes analytical queries over\nunstructured corpora.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unstructured data is pervasive, but analytical queries demand structured\nrepresentations, creating a significant extraction challenge. Existing methods\nlike RAG lack schema awareness and struggle with cross-document alignment,\nleading to high error rates. We propose ReDD (Relational Deep Dive), a\nframework that dynamically discovers query-specific schemas, populates\nrelational tables, and ensures error-aware extraction with provable guarantees.\nReDD features a two-stage pipeline: (1) Iterative Schema Discovery (ISD)\nidentifies minimal, joinable schemas tailored to each query, and (2) Tabular\nData Population (TDP) extracts and corrects data using lightweight classifiers\ntrained on LLM hidden states. A main contribution of ReDD is SCAPE, a\nstatistically calibrated method for error detection with coverage guarantees,\nand SCAPE-HYB, a hybrid approach that optimizes the trade-off between accuracy\nand human correction costs. Experiments across diverse datasets demonstrate\nReDD's effectiveness, reducing data extraction errors from up to 30% to below\n1% while maintaining high schema completeness (100% recall) and precision.\nReDD's modular design enables fine-grained control over accuracy-cost\ntrade-offs, making it a robust solution for high-stakes analytical queries over\nunstructured corpora."
                },
                "authors": [
                    {
                        "name": "Daren Chao"
                    },
                    {
                        "name": "Kaiwen Chen"
                    },
                    {
                        "name": "Naiqing Guan"
                    },
                    {
                        "name": "Nick Koudas"
                    }
                ],
                "author_detail": {
                    "name": "Nick Koudas"
                },
                "author": "Nick Koudas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02711v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02711v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22211v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22211v2",
                "updated": "2025-11-04T16:26:40Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    16,
                    26,
                    40,
                    1,
                    308,
                    0
                ],
                "published": "2024-10-29T16:39:28Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    16,
                    39,
                    28,
                    1,
                    303,
                    0
                ],
                "title": "ProMQA: Question Answering Dataset for Multimodal Procedural Activity\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProMQA: Question Answering Dataset for Multimodal Procedural Activity\n  Understanding"
                },
                "summary": "Multimodal systems have great potential to assist humans in procedural\nactivities, where people follow instructions to achieve their goals. Despite\ndiverse application scenarios, systems are typically evaluated on traditional\nclassification tasks, e.g., action recognition or temporal action segmentation.\nIn this paper, we present a novel evaluation dataset, ProMQA, to measure system\nadvancements in application-oriented scenarios. ProMQA consists of 401\nmultimodal procedural QA pairs on user recording of procedural activities,\ni.e., cooking, coupled with their corresponding instructions/recipes. For QA\nannotation, we take a cost-effective human-LLM collaborative approach, where\nthe existing annotation is augmented with LLM-generated QA pairs that are later\nverified by humans. We then provide the benchmark results to set the baseline\nperformance on ProMQA. Our experiment reveals a significant gap between human\nperformance and that of current systems, including competitive proprietary\nmultimodal models. We hope our dataset sheds light on new aspects of models'\nmultimodal understanding capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal systems have great potential to assist humans in procedural\nactivities, where people follow instructions to achieve their goals. Despite\ndiverse application scenarios, systems are typically evaluated on traditional\nclassification tasks, e.g., action recognition or temporal action segmentation.\nIn this paper, we present a novel evaluation dataset, ProMQA, to measure system\nadvancements in application-oriented scenarios. ProMQA consists of 401\nmultimodal procedural QA pairs on user recording of procedural activities,\ni.e., cooking, coupled with their corresponding instructions/recipes. For QA\nannotation, we take a cost-effective human-LLM collaborative approach, where\nthe existing annotation is augmented with LLM-generated QA pairs that are later\nverified by humans. We then provide the benchmark results to set the baseline\nperformance on ProMQA. Our experiment reveals a significant gap between human\nperformance and that of current systems, including competitive proprietary\nmultimodal models. We hope our dataset sheds light on new aspects of models'\nmultimodal understanding capabilities."
                },
                "authors": [
                    {
                        "name": "Kimihiro Hasegawa"
                    },
                    {
                        "name": "Wiradee Imrattanatrai"
                    },
                    {
                        "name": "Zhi-Qi Cheng"
                    },
                    {
                        "name": "Masaki Asada"
                    },
                    {
                        "name": "Susan Holm"
                    },
                    {
                        "name": "Yuran Wang"
                    },
                    {
                        "name": "Ken Fukuda"
                    },
                    {
                        "name": "Teruko Mitamura"
                    }
                ],
                "author_detail": {
                    "name": "Teruko Mitamura"
                },
                "author": "Teruko Mitamura",
                "arxiv_comment": "NAACL2025, Code and Data: https://github.com/kimihiroh/promqa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22211v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22211v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01100v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01100v2",
                "updated": "2025-11-04T16:26:26Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    16,
                    26,
                    26,
                    1,
                    308,
                    0
                ],
                "published": "2025-04-01T18:16:11Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    18,
                    16,
                    11,
                    1,
                    91,
                    0
                ],
                "title": "Repetitions are not all alike: distinct mechanisms sustain repetition in\n  language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Repetitions are not all alike: distinct mechanisms sustain repetition in\n  language models"
                },
                "summary": "Large Language Models (LLMs) can sometimes degrade into repetitive loops,\npersistently generating identical word sequences. Because repetition is rare in\nnatural human language, its frequent occurrence across diverse tasks and\ncontexts in LLMs remains puzzling. Here we investigate whether behaviorally\nsimilar repetition patterns arise from distinct underlying mechanisms and how\nthese mechanisms develop during model training. We contrast two conditions:\nrepetitions elicited by natural text prompts with those induced by in-context\nlearning (ICL) setups that explicitly require copying behavior. Our analyses\nreveal that ICL-induced repetition relies on a dedicated network of attention\nheads that progressively specialize over training, whereas naturally occurring\nrepetition emerges early and lacks a defined circuitry. Attention inspection\nfurther shows that natural repetition focuses disproportionately on\nlow-information tokens, suggesting a fallback behavior when relevant context\ncannot be retrieved. These results indicate that superficially similar\nrepetition behaviors originate from qualitatively different internal processes,\nreflecting distinct modes of failure and adaptation in language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can sometimes degrade into repetitive loops,\npersistently generating identical word sequences. Because repetition is rare in\nnatural human language, its frequent occurrence across diverse tasks and\ncontexts in LLMs remains puzzling. Here we investigate whether behaviorally\nsimilar repetition patterns arise from distinct underlying mechanisms and how\nthese mechanisms develop during model training. We contrast two conditions:\nrepetitions elicited by natural text prompts with those induced by in-context\nlearning (ICL) setups that explicitly require copying behavior. Our analyses\nreveal that ICL-induced repetition relies on a dedicated network of attention\nheads that progressively specialize over training, whereas naturally occurring\nrepetition emerges early and lacks a defined circuitry. Attention inspection\nfurther shows that natural repetition focuses disproportionately on\nlow-information tokens, suggesting a fallback behavior when relevant context\ncannot be retrieved. These results indicate that superficially similar\nrepetition behaviors originate from qualitatively different internal processes,\nreflecting distinct modes of failure and adaptation in language models."
                },
                "authors": [
                    {
                        "name": "Matéo Mahaut"
                    },
                    {
                        "name": "Francesca Franzon"
                    }
                ],
                "author_detail": {
                    "name": "Francesca Franzon"
                },
                "author": "Francesca Franzon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01100v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01100v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16116v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16116v3",
                "updated": "2025-11-04T16:26:20Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    16,
                    26,
                    20,
                    1,
                    308,
                    0
                ],
                "published": "2025-04-18T16:40:39Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    16,
                    40,
                    39,
                    4,
                    108,
                    0
                ],
                "title": "DMind Benchmark: Toward a Holistic Assessment of LLM Capabilities across\n  the Web3 Domain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DMind Benchmark: Toward a Holistic Assessment of LLM Capabilities across\n  the Web3 Domain"
                },
                "summary": "Large Language Models (LLMs) have achieved impressive performance in diverse\nnatural language processing tasks, but specialized domains such as Web3 present\nnew challenges and require more tailored evaluation. Despite the significant\nuser base and capital flows in Web3, encompassing smart contracts,\ndecentralized finance (DeFi), non-fungible tokens (NFTs), decentralized\nautonomous organizations (DAOs), on-chain governance, and novel\ntoken-economics, no comprehensive benchmark has systematically assessed LLM\nperformance in this domain. To address this gap, we introduce the DMind\nBenchmark, a holistic Web3-oriented evaluation suite covering nine critical\nsubfields: fundamental blockchain concepts, blockchain infrastructure, smart\ncontract, DeFi mechanisms, DAOs, NFTs, token economics, meme concept, and\nsecurity vulnerabilities. Beyond multiple-choice questions, DMind Benchmark\nfeatures domain-specific tasks such as contract debugging and on-chain numeric\nreasoning, mirroring real-world scenarios. We evaluated 26 models, including\nChatGPT, Claude, DeepSeek, Gemini, Grok, and Qwen, uncovering notable\nperformance gaps in specialized areas like token economics and\nsecurity-critical contract analysis. While some models excel in blockchain\ninfrastructure tasks, advanced subfields remain challenging. Our benchmark\ndataset and evaluation pipeline are open-sourced on\nhttps://huggingface.co/datasets/DMindAI/DMind_Benchmark, reaching number one in\nHugging Face's trending dataset charts within a week of release.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved impressive performance in diverse\nnatural language processing tasks, but specialized domains such as Web3 present\nnew challenges and require more tailored evaluation. Despite the significant\nuser base and capital flows in Web3, encompassing smart contracts,\ndecentralized finance (DeFi), non-fungible tokens (NFTs), decentralized\nautonomous organizations (DAOs), on-chain governance, and novel\ntoken-economics, no comprehensive benchmark has systematically assessed LLM\nperformance in this domain. To address this gap, we introduce the DMind\nBenchmark, a holistic Web3-oriented evaluation suite covering nine critical\nsubfields: fundamental blockchain concepts, blockchain infrastructure, smart\ncontract, DeFi mechanisms, DAOs, NFTs, token economics, meme concept, and\nsecurity vulnerabilities. Beyond multiple-choice questions, DMind Benchmark\nfeatures domain-specific tasks such as contract debugging and on-chain numeric\nreasoning, mirroring real-world scenarios. We evaluated 26 models, including\nChatGPT, Claude, DeepSeek, Gemini, Grok, and Qwen, uncovering notable\nperformance gaps in specialized areas like token economics and\nsecurity-critical contract analysis. While some models excel in blockchain\ninfrastructure tasks, advanced subfields remain challenging. Our benchmark\ndataset and evaluation pipeline are open-sourced on\nhttps://huggingface.co/datasets/DMindAI/DMind_Benchmark, reaching number one in\nHugging Face's trending dataset charts within a week of release."
                },
                "authors": [
                    {
                        "name": "Enhao Huang"
                    },
                    {
                        "name": "Pengyu Sun"
                    },
                    {
                        "name": "Zixin Lin"
                    },
                    {
                        "name": "Alex Chen"
                    },
                    {
                        "name": "Joey Ouyang"
                    },
                    {
                        "name": "Haobo Wang"
                    },
                    {
                        "name": "Kaichun Hu"
                    },
                    {
                        "name": "James Yi"
                    },
                    {
                        "name": "Frank Li"
                    },
                    {
                        "name": "Zhiyu Zhang"
                    },
                    {
                        "name": "Tianxiang Xu"
                    },
                    {
                        "name": "Gang Zhao"
                    },
                    {
                        "name": "Ziang Ling"
                    },
                    {
                        "name": "Lowes Yang"
                    }
                ],
                "author_detail": {
                    "name": "Lowes Yang"
                },
                "author": "Lowes Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.16116v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16116v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.01854v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.01854v2",
                "updated": "2025-11-04T16:24:47Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    16,
                    24,
                    47,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-03T18:58:28Z",
                "published_parsed": [
                    2025,
                    11,
                    3,
                    18,
                    58,
                    28,
                    0,
                    307,
                    0
                ],
                "title": "Tool-to-Agent Retrieval: Bridging Tools and Agents for Scalable LLM\n  Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool-to-Agent Retrieval: Bridging Tools and Agents for Scalable LLM\n  Multi-Agent Systems"
                },
                "summary": "Recent advances in LLM Multi-Agent Systems enable scalable orchestration of\nsub-agents, each coordinating hundreds or thousands of tools or Model Context\nProtocol (MCP) servers. However, existing retrieval methods typically match\nqueries against coarse agent-level descriptions before routing, which obscures\nfine-grained tool functionality and often results in suboptimal agent\nselection. We introduce Tool-to-Agent Retrieval, a unified framework that\nembeds both tools and their parent agents in a shared vector space and connects\nthem through metadata relationships. By explicitly representing tool\ncapabilities and traversing metadata to the agent level, Tool-to-Agent\nRetrieval enables granular tool-level or agent-level retrieval, ensuring that\nagents and their underlying tools or MCP servers are equally represented\nwithout the context dilution that arises from chunking many tools together.\nEvaluating Tool-to-Agent Retrieval across eight embedding models, our approach\nachieves consistent improvements of 19.4% in Recall@5 and 17.7% in nDCG@5 over\nprevious state-of-the-art agent retrievers on the LiveMCPBench benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in LLM Multi-Agent Systems enable scalable orchestration of\nsub-agents, each coordinating hundreds or thousands of tools or Model Context\nProtocol (MCP) servers. However, existing retrieval methods typically match\nqueries against coarse agent-level descriptions before routing, which obscures\nfine-grained tool functionality and often results in suboptimal agent\nselection. We introduce Tool-to-Agent Retrieval, a unified framework that\nembeds both tools and their parent agents in a shared vector space and connects\nthem through metadata relationships. By explicitly representing tool\ncapabilities and traversing metadata to the agent level, Tool-to-Agent\nRetrieval enables granular tool-level or agent-level retrieval, ensuring that\nagents and their underlying tools or MCP servers are equally represented\nwithout the context dilution that arises from chunking many tools together.\nEvaluating Tool-to-Agent Retrieval across eight embedding models, our approach\nachieves consistent improvements of 19.4% in Recall@5 and 17.7% in nDCG@5 over\nprevious state-of-the-art agent retrievers on the LiveMCPBench benchmark."
                },
                "authors": [
                    {
                        "name": "Elias Lumer"
                    },
                    {
                        "name": "Faheem Nizar"
                    },
                    {
                        "name": "Anmol Gulati"
                    },
                    {
                        "name": "Pradeep Honaganahalli Basavaraju"
                    },
                    {
                        "name": "Vamse Kumar Subbiah"
                    }
                ],
                "author_detail": {
                    "name": "Vamse Kumar Subbiah"
                },
                "author": "Vamse Kumar Subbiah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.01854v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.01854v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08903v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08903v4",
                "updated": "2025-11-04T16:18:24Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    16,
                    18,
                    24,
                    1,
                    308,
                    0
                ],
                "published": "2025-05-13T18:45:10Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    18,
                    45,
                    10,
                    1,
                    133,
                    0
                ],
                "title": "Assessing and Advancing Benchmarks for Evaluating Large Language Models\n  in Software Engineering Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing and Advancing Benchmarks for Evaluating Large Language Models\n  in Software Engineering Tasks"
                },
                "summary": "Large language models (LLMs) are gaining increasing popularity in software\nengineering (SE) due to their unprecedented performance across various\napplications. These models are increasingly being utilized for a range of SE\ntasks, including requirements engineering and design, code analysis and\ngeneration, software maintenance, and quality assurance. As LLMs become more\nintegral to SE, evaluating their effectiveness is crucial for understanding\ntheir potential in this field. In recent years, substantial efforts have been\nmade to assess LLM performance in various SE tasks, resulting in the creation\nof several benchmarks tailored to this purpose. This paper offers a thorough\nreview of 291 benchmarks, addressing three main aspects: what benchmarks are\navailable, how benchmarks are constructed, and the future outlook for these\nbenchmarks. We begin by examining SE tasks such as requirements engineering and\ndesign, coding assistant, software testing, AIOPs, software maintenance, and\nquality management. We then analyze the benchmarks and their development\nprocesses, highlighting the limitations of existing benchmarks. Additionally,\nwe discuss the successes and failures of LLMs in different software tasks and\nexplore future opportunities and challenges for SE-related benchmarks. We aim\nto provide a comprehensive overview of benchmark research in SE and offer\ninsights to support the creation of more effective evaluation tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are gaining increasing popularity in software\nengineering (SE) due to their unprecedented performance across various\napplications. These models are increasingly being utilized for a range of SE\ntasks, including requirements engineering and design, code analysis and\ngeneration, software maintenance, and quality assurance. As LLMs become more\nintegral to SE, evaluating their effectiveness is crucial for understanding\ntheir potential in this field. In recent years, substantial efforts have been\nmade to assess LLM performance in various SE tasks, resulting in the creation\nof several benchmarks tailored to this purpose. This paper offers a thorough\nreview of 291 benchmarks, addressing three main aspects: what benchmarks are\navailable, how benchmarks are constructed, and the future outlook for these\nbenchmarks. We begin by examining SE tasks such as requirements engineering and\ndesign, coding assistant, software testing, AIOPs, software maintenance, and\nquality management. We then analyze the benchmarks and their development\nprocesses, highlighting the limitations of existing benchmarks. Additionally,\nwe discuss the successes and failures of LLMs in different software tasks and\nexplore future opportunities and challenges for SE-related benchmarks. We aim\nto provide a comprehensive overview of benchmark research in SE and offer\ninsights to support the creation of more effective evaluation tools."
                },
                "authors": [
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Feifei Niu"
                    },
                    {
                        "name": "Junkai Chen"
                    },
                    {
                        "name": "Xin Zhou"
                    },
                    {
                        "name": "Junwei Zhang"
                    },
                    {
                        "name": "Junda He"
                    },
                    {
                        "name": "Xin Xia"
                    },
                    {
                        "name": "David Lo"
                    }
                ],
                "author_detail": {
                    "name": "David Lo"
                },
                "author": "David Lo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08903v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08903v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02690v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02690v1",
                "updated": "2025-11-04T16:14:56Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    16,
                    14,
                    56,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T16:14:56Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    16,
                    14,
                    56,
                    1,
                    308,
                    0
                ],
                "title": "Curriculum Design for Trajectory-Constrained Agent: Compressing\n  Chain-of-Thought Tokens in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Curriculum Design for Trajectory-Constrained Agent: Compressing\n  Chain-of-Thought Tokens in LLMs"
                },
                "summary": "Training agents to operate under strict constraints during deployment, such\nas limited resource budgets or stringent safety requirements, presents\nsignificant challenges, especially when these constraints render the task\ncomplex. In this work, we propose a curriculum learning strategy that gradually\ntightens constraints during training, enabling the agent to incrementally\nmaster the deployment requirements. Inspired by self-paced learning techniques\nin unconstrained reinforcement learning (RL), our approach facilitates a\nsmoother transition to challenging environments by initially training on\nsimplified versions of the constraints and progressively introducing the full\ndeployment conditions. We provide a theoretical analysis using an RL agent in a\nbinary-tree Markov Decision Process (MDP) to demonstrate that our curriculum\nstrategy can accelerate training relative to a baseline approach that imposes\nthe trajectory constraints from the outset. Moreover, we empirically validate\nthe effectiveness and generality of our method across both RL and large\nlanguage model (LLM) agents in diverse settings, including a binary-tree MDP, a\nmulti-task navigation domain, and a math reasoning task with two benchmarks.\nThese results highlight the potential of curriculum design in enhancing the\nefficiency and performance of agents operating under complex trajectory\nconstraints during deployment. Moreover, when applied to LLMs, our strategy\nenables compression of output chain-of-thought tokens, achieving a substantial\ninference speedup on consumer hardware, demonstrating its effectiveness for\nresource-constrained deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training agents to operate under strict constraints during deployment, such\nas limited resource budgets or stringent safety requirements, presents\nsignificant challenges, especially when these constraints render the task\ncomplex. In this work, we propose a curriculum learning strategy that gradually\ntightens constraints during training, enabling the agent to incrementally\nmaster the deployment requirements. Inspired by self-paced learning techniques\nin unconstrained reinforcement learning (RL), our approach facilitates a\nsmoother transition to challenging environments by initially training on\nsimplified versions of the constraints and progressively introducing the full\ndeployment conditions. We provide a theoretical analysis using an RL agent in a\nbinary-tree Markov Decision Process (MDP) to demonstrate that our curriculum\nstrategy can accelerate training relative to a baseline approach that imposes\nthe trajectory constraints from the outset. Moreover, we empirically validate\nthe effectiveness and generality of our method across both RL and large\nlanguage model (LLM) agents in diverse settings, including a binary-tree MDP, a\nmulti-task navigation domain, and a math reasoning task with two benchmarks.\nThese results highlight the potential of curriculum design in enhancing the\nefficiency and performance of agents operating under complex trajectory\nconstraints during deployment. Moreover, when applied to LLMs, our strategy\nenables compression of output chain-of-thought tokens, achieving a substantial\ninference speedup on consumer hardware, demonstrating its effectiveness for\nresource-constrained deployment."
                },
                "authors": [
                    {
                        "name": "Georgios Tzannetos"
                    },
                    {
                        "name": "Parameswaran Kamalaruban"
                    },
                    {
                        "name": "Adish Singla"
                    }
                ],
                "author_detail": {
                    "name": "Adish Singla"
                },
                "author": "Adish Singla",
                "arxiv_comment": "NeurIPS'25 paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02690v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02690v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16502v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16502v2",
                "updated": "2025-11-04T16:05:57Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    16,
                    5,
                    57,
                    1,
                    308,
                    0
                ],
                "published": "2025-01-27T21:04:05Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    21,
                    4,
                    5,
                    0,
                    27,
                    0
                ],
                "title": "dApps: Enabling Real-Time AI-Based Open RAN Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "dApps: Enabling Real-Time AI-Based Open RAN Control"
                },
                "summary": "Open Radio Access Networks (RANs) leverage disaggregated and programmable RAN\nfunctions and open interfaces to enable closed-loop, data-driven radio resource\nmanagement. This is performed through custom intelligent applications on the\nRAN Intelligent Controllers (RICs), optimizing RAN policy scheduling, network\nslicing, user session management, and medium access control, among others. In\nthis context, we have proposed dApps as a key extension of the O-RAN\narchitecture into the real-time and user-plane domains. Deployed directly on\nRAN nodes, dApps access data otherwise unavailable to RICs due to privacy or\ntiming constraints, enabling the execution of control actions within shorter\ntime intervals. In this paper, we propose for the first time a reference\narchitecture for dApps, defining their life cycle from deployment by the\nService Management and Orchestration (SMO) to real-time control loop\ninteractions with the RAN nodes where they are hosted. We introduce a new dApp\ninterface, E3, along with an Application Protocol (AP) that supports structured\nmessage exchanges and extensible communication for various service models. By\nbridging E3 with the existing O-RAN E2 interface, we enable dApps, xApps, and\nrApps to coexist and coordinate. These applications can then collaborate on\ncomplex use cases and employ hierarchical control to resolve shared resource\nconflicts. Finally, we present and open-source a dApp framework based on\nOpenAirInterface (OAI). We benchmark its performance in two real-time control\nuse cases, i.e., spectrum sharing and positioning in a 5th generation (5G) Next\nGeneration Node Base (gNB) scenario. Our experimental results show that\nstandardized real-time control loops via dApps are feasible, achieving average\ncontrol latency below 450 microseconds and allowing optimal use of shared\nspectral resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open Radio Access Networks (RANs) leverage disaggregated and programmable RAN\nfunctions and open interfaces to enable closed-loop, data-driven radio resource\nmanagement. This is performed through custom intelligent applications on the\nRAN Intelligent Controllers (RICs), optimizing RAN policy scheduling, network\nslicing, user session management, and medium access control, among others. In\nthis context, we have proposed dApps as a key extension of the O-RAN\narchitecture into the real-time and user-plane domains. Deployed directly on\nRAN nodes, dApps access data otherwise unavailable to RICs due to privacy or\ntiming constraints, enabling the execution of control actions within shorter\ntime intervals. In this paper, we propose for the first time a reference\narchitecture for dApps, defining their life cycle from deployment by the\nService Management and Orchestration (SMO) to real-time control loop\ninteractions with the RAN nodes where they are hosted. We introduce a new dApp\ninterface, E3, along with an Application Protocol (AP) that supports structured\nmessage exchanges and extensible communication for various service models. By\nbridging E3 with the existing O-RAN E2 interface, we enable dApps, xApps, and\nrApps to coexist and coordinate. These applications can then collaborate on\ncomplex use cases and employ hierarchical control to resolve shared resource\nconflicts. Finally, we present and open-source a dApp framework based on\nOpenAirInterface (OAI). We benchmark its performance in two real-time control\nuse cases, i.e., spectrum sharing and positioning in a 5th generation (5G) Next\nGeneration Node Base (gNB) scenario. Our experimental results show that\nstandardized real-time control loops via dApps are feasible, achieving average\ncontrol latency below 450 microseconds and allowing optimal use of shared\nspectral resources."
                },
                "authors": [
                    {
                        "name": "Andrea Lacava"
                    },
                    {
                        "name": "Leonardo Bonati"
                    },
                    {
                        "name": "Niloofar Mohamadi"
                    },
                    {
                        "name": "Rajeev Gangula"
                    },
                    {
                        "name": "Florian Kaltenberger"
                    },
                    {
                        "name": "Pedram Johari"
                    },
                    {
                        "name": "Salvatore D'Oro"
                    },
                    {
                        "name": "Francesca Cuomo"
                    },
                    {
                        "name": "Michele Polese"
                    },
                    {
                        "name": "Tommaso Melodia"
                    }
                ],
                "author_detail": {
                    "name": "Tommaso Melodia"
                },
                "author": "Tommaso Melodia",
                "arxiv_doi": "10.1016/j.comnet.2025.111342",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.comnet.2025.111342",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.16502v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16502v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "34 pages, 13 figures, 4 tables",
                "arxiv_journal_ref": "Computer Networks, Volume 269, 2025, 111342, ISSN 1389-1286",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02681v1",
                "updated": "2025-11-04T16:05:25Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    16,
                    5,
                    25,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T16:05:25Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    16,
                    5,
                    25,
                    1,
                    308,
                    0
                ],
                "title": "Optimal Singular Damage: Efficient LLM Inference in Low Storage Regimes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Singular Damage: Efficient LLM Inference in Low Storage Regimes"
                },
                "summary": "Large language models (LLMs) are increasingly prevalent across diverse\napplications. However, their enormous size limits storage and processing\ncapabilities to a few well-resourced stakeholders. As a result, most\napplications rely on pre-trained LLMs, fine-tuned for specific tasks. However,\neven storing the fine-tuned versions of these models remains a significant\nchallenge due to the wide range of tasks they address. Recently, studies show\nthat fine-tuning these models primarily affects a small fraction of parameters,\nhighlighting the need for more efficient storage of fine-tuned models. This\npaper focuses on efficient storage of parameter updates in pre-trained models\nafter fine-tuning. To address this challenge, we leverage the observation that\nfine-tuning updates are both low-rank and sparse, which can be utilized for\nstorage efficiency. However, using only low-rank approximation or\nsparsification may discard critical singular components that enhance model\nexpressivity. We first observe that given the same memory budget, sparsified\nlow-rank approximations with larger ranks outperform standard low-rank\napproximations with smaller ranks. Building on this, we propose our method,\noptimal singular damage, that selectively sparsifies low-rank approximated\nupdates by leveraging the interleaved importance of singular vectors, ensuring\nthat the most impactful components are retained. We demonstrate through\nextensive experiments that our proposed methods lead to significant storage\nefficiency and superior accuracy within the same memory budget compared to\nemploying the low-rank approximation or sparsification individually.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly prevalent across diverse\napplications. However, their enormous size limits storage and processing\ncapabilities to a few well-resourced stakeholders. As a result, most\napplications rely on pre-trained LLMs, fine-tuned for specific tasks. However,\neven storing the fine-tuned versions of these models remains a significant\nchallenge due to the wide range of tasks they address. Recently, studies show\nthat fine-tuning these models primarily affects a small fraction of parameters,\nhighlighting the need for more efficient storage of fine-tuned models. This\npaper focuses on efficient storage of parameter updates in pre-trained models\nafter fine-tuning. To address this challenge, we leverage the observation that\nfine-tuning updates are both low-rank and sparse, which can be utilized for\nstorage efficiency. However, using only low-rank approximation or\nsparsification may discard critical singular components that enhance model\nexpressivity. We first observe that given the same memory budget, sparsified\nlow-rank approximations with larger ranks outperform standard low-rank\napproximations with smaller ranks. Building on this, we propose our method,\noptimal singular damage, that selectively sparsifies low-rank approximated\nupdates by leveraging the interleaved importance of singular vectors, ensuring\nthat the most impactful components are retained. We demonstrate through\nextensive experiments that our proposed methods lead to significant storage\nefficiency and superior accuracy within the same memory budget compared to\nemploying the low-rank approximation or sparsification individually."
                },
                "authors": [
                    {
                        "name": "Mohammadsajad Alipour"
                    },
                    {
                        "name": "Mohammad Mohammadi Amiri"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Mohammadi Amiri"
                },
                "author": "Mohammad Mohammadi Amiri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07109v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07109v3",
                "updated": "2025-11-04T15:55:56Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    15,
                    55,
                    56,
                    1,
                    308,
                    0
                ],
                "published": "2024-10-09T17:45:47Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    45,
                    47,
                    2,
                    283,
                    0
                ],
                "title": "I Want to Break Free! Persuasion and Anti-Social Behavior of LLMs in\n  Multi-Agent Settings with Social Hierarchy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I Want to Break Free! Persuasion and Anti-Social Behavior of LLMs in\n  Multi-Agent Settings with Social Hierarchy"
                },
                "summary": "As LLM-based agents become increasingly autonomous and will more freely\ninteract with each other, studying the interplay among them becomes crucial to\nanticipate emergent phenomena and potential risks. In this work, we provide an\nin-depth analysis of the interactions among agents within a simulated\nhierarchical social environment, drawing inspiration from the Stanford Prison\nExperiment. Leveraging 2,400 conversations across six LLMs (i.e., LLama3,\nOrca2, Command-r, Mixtral, Mistral2, and gpt4.1) and 240 experimental\nscenarios, we analyze persuasion and anti-social behavior between a guard and a\nprisoner agent with differing objectives. We first document model-specific\nconversational failures in this multi-agent power dynamic context, thereby\nnarrowing our analytic sample to 1,600 conversations. Among models\ndemonstrating successful interaction, we find that goal setting significantly\ninfluences persuasiveness but not anti-social behavior. Moreover, agent\npersonas, especially the guard's, substantially impact both successful\npersuasion by the prisoner and the manifestation of anti-social actions.\nNotably, we observe the emergence of anti-social conduct even in absence of\nexplicit negative personality prompts. These results have important\nimplications for the development of interactive LLM agents and the ongoing\ndiscussion of their societal impact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As LLM-based agents become increasingly autonomous and will more freely\ninteract with each other, studying the interplay among them becomes crucial to\nanticipate emergent phenomena and potential risks. In this work, we provide an\nin-depth analysis of the interactions among agents within a simulated\nhierarchical social environment, drawing inspiration from the Stanford Prison\nExperiment. Leveraging 2,400 conversations across six LLMs (i.e., LLama3,\nOrca2, Command-r, Mixtral, Mistral2, and gpt4.1) and 240 experimental\nscenarios, we analyze persuasion and anti-social behavior between a guard and a\nprisoner agent with differing objectives. We first document model-specific\nconversational failures in this multi-agent power dynamic context, thereby\nnarrowing our analytic sample to 1,600 conversations. Among models\ndemonstrating successful interaction, we find that goal setting significantly\ninfluences persuasiveness but not anti-social behavior. Moreover, agent\npersonas, especially the guard's, substantially impact both successful\npersuasion by the prisoner and the manifestation of anti-social actions.\nNotably, we observe the emergence of anti-social conduct even in absence of\nexplicit negative personality prompts. These results have important\nimplications for the development of interactive LLM agents and the ongoing\ndiscussion of their societal impact."
                },
                "authors": [
                    {
                        "name": "Gian Maria Campedelli"
                    },
                    {
                        "name": "Nicolò Penzo"
                    },
                    {
                        "name": "Massimo Stefan"
                    },
                    {
                        "name": "Roberto Dessì"
                    },
                    {
                        "name": "Marco Guerini"
                    },
                    {
                        "name": "Bruno Lepri"
                    },
                    {
                        "name": "Jacopo Staiano"
                    }
                ],
                "author_detail": {
                    "name": "Jacopo Staiano"
                },
                "author": "Jacopo Staiano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07109v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07109v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02674v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02674v1",
                "updated": "2025-11-04T15:54:33Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    15,
                    54,
                    33,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T15:54:33Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    15,
                    54,
                    33,
                    1,
                    308,
                    0
                ],
                "title": "EasyTUS: A Comprehensive Framework for Fast and Accurate Table Union\n  Search across Data Lakes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EasyTUS: A Comprehensive Framework for Fast and Accurate Table Union\n  Search across Data Lakes"
                },
                "summary": "Data lakes enable easy maintenance of heterogeneous data in its native form.\nWhile this flexibility can accelerate data ingestion, it shifts the complexity\nof data preparation and query processing to data discovery tasks. One such task\nis Table Union Search (TUS), which identifies tables that can be unioned with a\ngiven input table. In this work, we present EasyTUS, a comprehensive framework\nthat leverages Large Language Models (LLMs) to perform efficient and scalable\nTable Union Search across data lakes. EasyTUS implements the search pipeline as\nthree modular steps: Table Serialization for consistent formatting and\nsampling, Table Representation that utilizes LLMs to generate embeddings, and\nVector Search that leverages approximate nearest neighbor indexing for semantic\nmatching. To enable reproducible and systematic evaluation, in this paper, we\nalso introduce TUSBench, a novel standardized benchmarking environment within\nthe EasyTUS framework. TUSBench supports unified comparisons across approaches\nand data lakes, promoting transparency and progress in the field. Our\nexperiments using TUSBench show that EasyTUS consistently outperforms most of\nthe state-of the-art approaches, achieving improvements in average of up to\n34.3% in Mean Average Precision (MAP), up to 79.2x speedup in data preparation,\nand up to 7.7x faster query processing performance. Furthermore, EasyTUS\nmaintains strong performance even in metadata-absent settings, highlighting its\nrobustness and adaptability across data lakes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data lakes enable easy maintenance of heterogeneous data in its native form.\nWhile this flexibility can accelerate data ingestion, it shifts the complexity\nof data preparation and query processing to data discovery tasks. One such task\nis Table Union Search (TUS), which identifies tables that can be unioned with a\ngiven input table. In this work, we present EasyTUS, a comprehensive framework\nthat leverages Large Language Models (LLMs) to perform efficient and scalable\nTable Union Search across data lakes. EasyTUS implements the search pipeline as\nthree modular steps: Table Serialization for consistent formatting and\nsampling, Table Representation that utilizes LLMs to generate embeddings, and\nVector Search that leverages approximate nearest neighbor indexing for semantic\nmatching. To enable reproducible and systematic evaluation, in this paper, we\nalso introduce TUSBench, a novel standardized benchmarking environment within\nthe EasyTUS framework. TUSBench supports unified comparisons across approaches\nand data lakes, promoting transparency and progress in the field. Our\nexperiments using TUSBench show that EasyTUS consistently outperforms most of\nthe state-of the-art approaches, achieving improvements in average of up to\n34.3% in Mean Average Precision (MAP), up to 79.2x speedup in data preparation,\nand up to 7.7x faster query processing performance. Furthermore, EasyTUS\nmaintains strong performance even in metadata-absent settings, highlighting its\nrobustness and adaptability across data lakes."
                },
                "authors": [
                    {
                        "name": "Tim Otto"
                    }
                ],
                "author_detail": {
                    "name": "Tim Otto"
                },
                "author": "Tim Otto",
                "arxiv_comment": "Copyright 2025 IEEE. This is the author's version of the work that\n  has been accepted for publication in Proceedings of the IEEE International\n  Conference on Big Data (IEEE BigData 2025). The final version of record is\n  available at: tba",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02674v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02674v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24303v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24303v2",
                "updated": "2025-11-04T15:46:42Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    15,
                    46,
                    42,
                    1,
                    308,
                    0
                ],
                "published": "2025-10-28T11:12:43Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    11,
                    12,
                    43,
                    1,
                    301,
                    0
                ],
                "title": "Retrieval and Argumentation Enhanced Multi-Agent LLMs for Judgmental\n  Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval and Argumentation Enhanced Multi-Agent LLMs for Judgmental\n  Forecasting"
                },
                "summary": "Judgmental forecasting is the task of making predictions about future events\nbased on human judgment. This task can be seen as a form of claim verification,\nwhere the claim corresponds to a future event and the task is to assess the\nplausibility of that event. In this paper, we propose a novel multi-agent\nframework for claim verification, whereby different agents may disagree on\nclaim veracity and bring specific evidence for and against the claims,\nrepresented as quantitative bipolar argumentation frameworks (QBAFs). We then\ninstantiate the framework for supporting claim verification, with a variety of\nagents realised with Large Language Models (LLMs): (1) ArgLLM agents, an\nexisting approach for claim verification that generates and evaluates QBAFs;\n(2) RbAM agents, whereby LLM-empowered Relation-based Argument Mining (RbAM)\nfrom external sources is used to generate QBAFs; (3) RAG-ArgLLM agents,\nextending ArgLLM agents with a form of Retrieval-Augmented Generation (RAG) of\narguments from external sources. Finally, we conduct experiments with two\nstandard judgmental forecasting datasets, with instances of our framework with\ntwo or three agents, empowered by six different base LLMs. We observe that\ncombining evidence from agents can improve forecasting accuracy, especially in\nthe case of three agents, while providing an explainable combination of\nevidence for claim verification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Judgmental forecasting is the task of making predictions about future events\nbased on human judgment. This task can be seen as a form of claim verification,\nwhere the claim corresponds to a future event and the task is to assess the\nplausibility of that event. In this paper, we propose a novel multi-agent\nframework for claim verification, whereby different agents may disagree on\nclaim veracity and bring specific evidence for and against the claims,\nrepresented as quantitative bipolar argumentation frameworks (QBAFs). We then\ninstantiate the framework for supporting claim verification, with a variety of\nagents realised with Large Language Models (LLMs): (1) ArgLLM agents, an\nexisting approach for claim verification that generates and evaluates QBAFs;\n(2) RbAM agents, whereby LLM-empowered Relation-based Argument Mining (RbAM)\nfrom external sources is used to generate QBAFs; (3) RAG-ArgLLM agents,\nextending ArgLLM agents with a form of Retrieval-Augmented Generation (RAG) of\narguments from external sources. Finally, we conduct experiments with two\nstandard judgmental forecasting datasets, with instances of our framework with\ntwo or three agents, empowered by six different base LLMs. We observe that\ncombining evidence from agents can improve forecasting accuracy, especially in\nthe case of three agents, while providing an explainable combination of\nevidence for claim verification."
                },
                "authors": [
                    {
                        "name": "Deniz Gorur"
                    },
                    {
                        "name": "Antonio Rago"
                    },
                    {
                        "name": "Francesca Toni"
                    }
                ],
                "author_detail": {
                    "name": "Francesca Toni"
                },
                "author": "Francesca Toni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24303v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24303v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25223v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25223v2",
                "updated": "2025-11-04T15:40:16Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    15,
                    40,
                    16,
                    1,
                    308,
                    0
                ],
                "published": "2025-10-29T06:57:32Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    6,
                    57,
                    32,
                    2,
                    302,
                    0
                ],
                "title": "FELA: A Multi-Agent Evolutionary System for Feature Engineering of\n  Industrial Event Log Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FELA: A Multi-Agent Evolutionary System for Feature Engineering of\n  Industrial Event Log Data"
                },
                "summary": "Event log data, recording fine-grained user actions and system events,\nrepresent one of the most valuable assets for modern digital services. However,\nthe complexity and heterogeneity of industrial event logs--characterized by\nlarge scale, high dimensionality, diverse data types, and intricate temporal or\nrelational structures--make feature engineering extremely challenging. Existing\nautomatic feature engineering approaches, such as AutoML or genetic methods,\noften suffer from limited explainability, rigid predefined operations, and poor\nadaptability to complicated heterogeneous data. In this paper, we propose FELA\n(Feature Engineering LLM Agents), a multi-agent evolutionary system that\nautonomously extracts meaningful and high-performing features from complex\nindustrial event log data. FELA integrates the reasoning and coding\ncapabilities of large language models (LLMs) with an insight-guided\nself-evolution paradigm. Specifically, FELA employs specialized agents--Idea\nAgents, Code Agents, and Critic Agents--to collaboratively generate, validate,\nand implement novel feature ideas. An Evaluation Agent summarizes feedback and\nupdates a hierarchical knowledge base and dual-memory system to enable\ncontinual improvement. Moreover, FELA introduces an agentic evolution\nalgorithm, combining reinforcement learning and genetic algorithm principles to\nbalance exploration and exploitation across the idea space. Extensive\nexperiments on real industrial datasets demonstrate that FELA can generate\nexplainable, domain-relevant features that significantly improve model\nperformance while reducing manual effort. Our results highlight the potential\nof LLM-based multi-agent systems as a general framework for automated,\ninterpretable, and adaptive feature engineering in complex real-world\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Event log data, recording fine-grained user actions and system events,\nrepresent one of the most valuable assets for modern digital services. However,\nthe complexity and heterogeneity of industrial event logs--characterized by\nlarge scale, high dimensionality, diverse data types, and intricate temporal or\nrelational structures--make feature engineering extremely challenging. Existing\nautomatic feature engineering approaches, such as AutoML or genetic methods,\noften suffer from limited explainability, rigid predefined operations, and poor\nadaptability to complicated heterogeneous data. In this paper, we propose FELA\n(Feature Engineering LLM Agents), a multi-agent evolutionary system that\nautonomously extracts meaningful and high-performing features from complex\nindustrial event log data. FELA integrates the reasoning and coding\ncapabilities of large language models (LLMs) with an insight-guided\nself-evolution paradigm. Specifically, FELA employs specialized agents--Idea\nAgents, Code Agents, and Critic Agents--to collaboratively generate, validate,\nand implement novel feature ideas. An Evaluation Agent summarizes feedback and\nupdates a hierarchical knowledge base and dual-memory system to enable\ncontinual improvement. Moreover, FELA introduces an agentic evolution\nalgorithm, combining reinforcement learning and genetic algorithm principles to\nbalance exploration and exploitation across the idea space. Extensive\nexperiments on real industrial datasets demonstrate that FELA can generate\nexplainable, domain-relevant features that significantly improve model\nperformance while reducing manual effort. Our results highlight the potential\nof LLM-based multi-agent systems as a general framework for automated,\ninterpretable, and adaptive feature engineering in complex real-world\nenvironments."
                },
                "authors": [
                    {
                        "name": "Kun Ouyang"
                    },
                    {
                        "name": "Haoyu Wang"
                    },
                    {
                        "name": "Dong Fang"
                    }
                ],
                "author_detail": {
                    "name": "Dong Fang"
                },
                "author": "Dong Fang",
                "arxiv_comment": "14 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25223v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25223v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02656v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02656v1",
                "updated": "2025-11-04T15:30:07Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    15,
                    30,
                    7,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T15:30:07Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    15,
                    30,
                    7,
                    1,
                    308,
                    0
                ],
                "title": "Bringing Private Reads to Hyperledger Fabric via Private Information\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bringing Private Reads to Hyperledger Fabric via Private Information\n  Retrieval"
                },
                "summary": "Permissioned blockchains ensure integrity and auditability of shared data but\nexpose query parameters to peers during read operations, creating privacy risks\nfor organizations querying sensitive records. This paper proposes a Private\nInformation Retrieval (PIR) mechanism to enable private reads from Hyperledger\nFabric's world state, allowing endorsing peers to process encrypted queries\nwithout learning which record is accessed. We implement and benchmark a\nPIR-enabled chaincode that performs ciphertext-plaintext (ct-pt) homomorphic\nmultiplication directly within evaluate transactions, preserving Fabric's\nendorsement and audit semantics. The prototype achieves an average end-to-end\nlatency of 113 ms and a peer-side execution time below 42 ms, with\napproximately 2 MB of peer network traffic per private read in development\nmode--reducible by half under in-process deployment. Storage profiling across\nthree channel configurations shows near-linear growth: block size increases\nfrom 77 kilobytes to 294 kilobytes and world-state from 112 kilobytes to 332\nkilobytes as the ring dimension scales from 8,192 to 32,768 coefficients.\nParameter analysis further indicates that ring size and record length jointly\nconstrain packing capacity, supporting up to 512 records of 64 bytes each under\nthe largest configuration. These results confirm the practicality of PIR-based\nprivate reads in Fabric for smaller, sensitive datasets and highlight future\ndirections to optimize performance and scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Permissioned blockchains ensure integrity and auditability of shared data but\nexpose query parameters to peers during read operations, creating privacy risks\nfor organizations querying sensitive records. This paper proposes a Private\nInformation Retrieval (PIR) mechanism to enable private reads from Hyperledger\nFabric's world state, allowing endorsing peers to process encrypted queries\nwithout learning which record is accessed. We implement and benchmark a\nPIR-enabled chaincode that performs ciphertext-plaintext (ct-pt) homomorphic\nmultiplication directly within evaluate transactions, preserving Fabric's\nendorsement and audit semantics. The prototype achieves an average end-to-end\nlatency of 113 ms and a peer-side execution time below 42 ms, with\napproximately 2 MB of peer network traffic per private read in development\nmode--reducible by half under in-process deployment. Storage profiling across\nthree channel configurations shows near-linear growth: block size increases\nfrom 77 kilobytes to 294 kilobytes and world-state from 112 kilobytes to 332\nkilobytes as the ring dimension scales from 8,192 to 32,768 coefficients.\nParameter analysis further indicates that ring size and record length jointly\nconstrain packing capacity, supporting up to 512 records of 64 bytes each under\nthe largest configuration. These results confirm the practicality of PIR-based\nprivate reads in Fabric for smaller, sensitive datasets and highlight future\ndirections to optimize performance and scalability."
                },
                "authors": [
                    {
                        "name": "Artur Iasenovets"
                    },
                    {
                        "name": "Fei Tang"
                    },
                    {
                        "name": "Huihui Zhu"
                    },
                    {
                        "name": "Ping Wang"
                    },
                    {
                        "name": "Lei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Lei Liu"
                },
                "author": "Lei Liu",
                "arxiv_comment": "This work has been submitted to IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02656v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02656v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.4; D.4.6; H.2.0; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.00689v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.00689v2",
                "updated": "2025-11-04T15:19:44Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    15,
                    19,
                    44,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-01T20:12:19Z",
                "published_parsed": [
                    2025,
                    11,
                    1,
                    20,
                    12,
                    19,
                    5,
                    305,
                    0
                ],
                "title": "Do Methods to Jailbreak and Defend LLMs Generalize Across Languages?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Methods to Jailbreak and Defend LLMs Generalize Across Languages?"
                },
                "summary": "Large language models (LLMs) undergo safety alignment after training and\ntuning, yet recent work shows that safety can be bypassed through jailbreak\nattacks. While many jailbreaks and defenses exist, their cross-lingual\ngeneralization remains underexplored. This paper presents the first systematic\nmultilingual evaluation of jailbreaks and defenses across ten languages --\nspanning high-, medium-, and low-resource languages -- using six LLMs on\nHarmBench and AdvBench. We assess two jailbreak types: logical-expression-based\nand adversarial-prompt-based. For both types, attack success and defense\nrobustness vary across languages: high-resource languages are safer under\nstandard queries but more vulnerable to adversarial ones. Simple defenses can\nbe effective, but are language- and model-dependent. These findings call for\nlanguage-aware and cross-lingual safety benchmarks for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) undergo safety alignment after training and\ntuning, yet recent work shows that safety can be bypassed through jailbreak\nattacks. While many jailbreaks and defenses exist, their cross-lingual\ngeneralization remains underexplored. This paper presents the first systematic\nmultilingual evaluation of jailbreaks and defenses across ten languages --\nspanning high-, medium-, and low-resource languages -- using six LLMs on\nHarmBench and AdvBench. We assess two jailbreak types: logical-expression-based\nand adversarial-prompt-based. For both types, attack success and defense\nrobustness vary across languages: high-resource languages are safer under\nstandard queries but more vulnerable to adversarial ones. Simple defenses can\nbe effective, but are language- and model-dependent. These findings call for\nlanguage-aware and cross-lingual safety benchmarks for LLMs."
                },
                "authors": [
                    {
                        "name": "Berk Atil"
                    },
                    {
                        "name": "Rebecca J. Passonneau"
                    },
                    {
                        "name": "Fred Morstatter"
                    }
                ],
                "author_detail": {
                    "name": "Fred Morstatter"
                },
                "author": "Fred Morstatter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.00689v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.00689v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02651v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02651v1",
                "updated": "2025-11-04T15:17:43Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    15,
                    17,
                    43,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T15:17:43Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    15,
                    17,
                    43,
                    1,
                    308,
                    0
                ],
                "title": "Apriel-H1: Towards Efficient Enterprise Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Apriel-H1: Towards Efficient Enterprise Reasoning Models"
                },
                "summary": "Large Language Models (LLMs) achieve remarkable reasoning capabilities\nthrough transformer architectures with attention mechanisms. However,\ntransformers suffer from quadratic time and memory complexity in the attention\nmodule (MHA) and require caching key-value states during inference, which\nseverely limits throughput and scalability. High inference throughput is\ncritical for agentic tasks, long-context reasoning, efficient deployment under\nhigh request loads, and more efficient test-time compute scaling.\n  State Space Models (SSMs) such as Mamba offer a promising alternative with\nlinear inference complexity and a constant memory footprint via recurrent\ncomputation with fixed-size hidden states. In this technical report we\nintroduce the Apriel-H1 family of hybrid LLMs that combine transformer\nattention and SSM sequence mixers for efficient reasoning at 15B model size.\nThese models are obtained through incremental distillation from a pretrained\nreasoning transformer, Apriel-Nemotron-15B-Thinker, progressively replacing\nless critical attention layers with linear Mamba blocks.\n  We release multiple post-distillation variants of Apriel-H1-15B-Thinker with\ndifferent SSM-to-MHA ratios and analyse how reasoning performance degrades as\nmore Mamba layers replace MHA. Additionally, we release a 30/50 hybrid variant\nof Apriel-H1, further fine-tuned on a supervised dataset of reasoning traces,\nachieving over 2x higher inference throughput when deployed in the\nproduction-ready vLLM environment, with minimal degradation in reasoning\nperformance. This shows that distilled hybrid SSM-Transformer architectures can\ndeliver substantial efficiency gains over the pretrained transformer equivalent\nwithout substantially compromising the reasoning quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) achieve remarkable reasoning capabilities\nthrough transformer architectures with attention mechanisms. However,\ntransformers suffer from quadratic time and memory complexity in the attention\nmodule (MHA) and require caching key-value states during inference, which\nseverely limits throughput and scalability. High inference throughput is\ncritical for agentic tasks, long-context reasoning, efficient deployment under\nhigh request loads, and more efficient test-time compute scaling.\n  State Space Models (SSMs) such as Mamba offer a promising alternative with\nlinear inference complexity and a constant memory footprint via recurrent\ncomputation with fixed-size hidden states. In this technical report we\nintroduce the Apriel-H1 family of hybrid LLMs that combine transformer\nattention and SSM sequence mixers for efficient reasoning at 15B model size.\nThese models are obtained through incremental distillation from a pretrained\nreasoning transformer, Apriel-Nemotron-15B-Thinker, progressively replacing\nless critical attention layers with linear Mamba blocks.\n  We release multiple post-distillation variants of Apriel-H1-15B-Thinker with\ndifferent SSM-to-MHA ratios and analyse how reasoning performance degrades as\nmore Mamba layers replace MHA. Additionally, we release a 30/50 hybrid variant\nof Apriel-H1, further fine-tuned on a supervised dataset of reasoning traces,\nachieving over 2x higher inference throughput when deployed in the\nproduction-ready vLLM environment, with minimal degradation in reasoning\nperformance. This shows that distilled hybrid SSM-Transformer architectures can\ndeliver substantial efficiency gains over the pretrained transformer equivalent\nwithout substantially compromising the reasoning quality."
                },
                "authors": [
                    {
                        "name": "Oleksiy Ostapenko"
                    },
                    {
                        "name": "Luke Kumar"
                    },
                    {
                        "name": "Raymond Li"
                    },
                    {
                        "name": "Denis Kocetkov"
                    },
                    {
                        "name": "Joel Lamy-Poirier"
                    },
                    {
                        "name": "Shruthan Radhakrishna"
                    },
                    {
                        "name": "Soham Parikh"
                    },
                    {
                        "name": "Shambhavi Mishra"
                    },
                    {
                        "name": "Sebastien Paquet"
                    },
                    {
                        "name": "Srinivas Sunkara"
                    },
                    {
                        "name": "Valérie Bécaert"
                    },
                    {
                        "name": "Sathwik Tejaswi Madhusudhan"
                    },
                    {
                        "name": "Torsten Scholak"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Scholak"
                },
                "author": "Torsten Scholak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02651v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02651v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02650v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02650v1",
                "updated": "2025-11-04T15:17:06Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    15,
                    17,
                    6,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T15:17:06Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    15,
                    17,
                    6,
                    1,
                    308,
                    0
                ],
                "title": "Can Visual Input Be Compressed? A Visual Token Compression Benchmark for\n  Large Multimodal Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Visual Input Be Compressed? A Visual Token Compression Benchmark for\n  Large Multimodal Models"
                },
                "summary": "Large multimodal models (LMMs) often suffer from severe inference\ninefficiency due to the large number of visual tokens introduced by image\nencoders. While recent token compression methods, such as pruning and merging,\nhave shown promise in reducing redundancy, their evaluation remains fragmented\nand inconsistent. In this work, we present UniPruneBench, a unified and\nextensible benchmark for visual token pruning in multimodal LLMs. UniPruneBench\nprovides standardized protocols across six ability dimensions and ten datasets,\ncovering ten representative compression algorithms and three families of LMMs\n(LLaVA-v1.5, Intern-VL3, and Qwen2.5-VL). Beyond task accuracy, it incorporates\nsystem-level metrics such as runtime and prefilling latency to provide a\nholistic view. Our experiments uncover several key findings: (1) random pruning\nis a surprisingly strong baseline, (2) no single method consistently\noutperforms others across scenarios, (3) pruning sensitivity varies\nsignificantly across tasks, with OCR being most vulnerable, and (4) pruning\nratio is the dominant factor governing performance degradation. We believe\nUniPruneBench will serve as a reliable foundation for future research on\nefficient multimodal modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large multimodal models (LMMs) often suffer from severe inference\ninefficiency due to the large number of visual tokens introduced by image\nencoders. While recent token compression methods, such as pruning and merging,\nhave shown promise in reducing redundancy, their evaluation remains fragmented\nand inconsistent. In this work, we present UniPruneBench, a unified and\nextensible benchmark for visual token pruning in multimodal LLMs. UniPruneBench\nprovides standardized protocols across six ability dimensions and ten datasets,\ncovering ten representative compression algorithms and three families of LMMs\n(LLaVA-v1.5, Intern-VL3, and Qwen2.5-VL). Beyond task accuracy, it incorporates\nsystem-level metrics such as runtime and prefilling latency to provide a\nholistic view. Our experiments uncover several key findings: (1) random pruning\nis a surprisingly strong baseline, (2) no single method consistently\noutperforms others across scenarios, (3) pruning sensitivity varies\nsignificantly across tasks, with OCR being most vulnerable, and (4) pruning\nratio is the dominant factor governing performance degradation. We believe\nUniPruneBench will serve as a reliable foundation for future research on\nefficient multimodal modeling."
                },
                "authors": [
                    {
                        "name": "Tianfan Peng"
                    },
                    {
                        "name": "Yuntao Du"
                    },
                    {
                        "name": "Pengzhou Ji"
                    },
                    {
                        "name": "Shijie Dong"
                    },
                    {
                        "name": "Kailin Jiang"
                    },
                    {
                        "name": "Mingchuan Ma"
                    },
                    {
                        "name": "Yijun Tian"
                    },
                    {
                        "name": "Jinhe Bi"
                    },
                    {
                        "name": "Qian Li"
                    },
                    {
                        "name": "Wei Du"
                    },
                    {
                        "name": "Feng Xiao"
                    },
                    {
                        "name": "Lizhen Cui"
                    }
                ],
                "author_detail": {
                    "name": "Lizhen Cui"
                },
                "author": "Lizhen Cui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02650v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02650v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02647v1",
                "updated": "2025-11-04T15:14:58Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    15,
                    14,
                    58,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T15:14:58Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    15,
                    14,
                    58,
                    1,
                    308,
                    0
                ],
                "title": "Federated Attention: A Distributed Paradigm for Collaborative LLM\n  Inference over Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Attention: A Distributed Paradigm for Collaborative LLM\n  Inference over Edge Networks"
                },
                "summary": "Large language models (LLMs) are proliferating rapidly at the edge,\ndelivering intelligent capabilities across diverse application scenarios.\nHowever, their practical deployment in collaborative scenarios confronts\nfundamental challenges: privacy vulnerabilities, communication overhead, and\ncomputational bottlenecks. To address these, we propose Federated Attention\n(FedAttn), which integrates the federated paradigm into the self-attention\nmechanism, creating a new distributed LLM inference framework that\nsimultaneously achieves privacy protection, communication efficiency, and\ncomputational efficiency. FedAttn enables participants to perform local\nself-attention over their own token representations while periodically\nexchanging and aggregating Key-Value (KV) matrices across multiple Transformer\nblocks, collaboratively generating LLM responses without exposing private\nprompts. Further, we identify a structural duality between contextual\nrepresentation refinement in FedAttn and parameter optimization in FL across\nprivate data, local computation, and global aggregation. This key insight\nprovides a principled foundation for systematically porting federated\noptimization techniques to collaborative LLM inference. Building on this\nframework, we theoretically analyze how local self-attention computation within\nparticipants and heterogeneous token relevance among participants shape error\npropagation dynamics across Transformer blocks. Moreover, we characterize the\nfundamental trade-off between response quality and communication/computation\nefficiency, which is governed by the synchronization interval and the number of\nparticipants. Experimental results validate our theoretical analysis, and\nreveal significant optimization opportunities through sparse attention and\nadaptive KV aggregation, highlighting FedAttn's potential to deliver\nscalability and efficiency in real-world edge deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are proliferating rapidly at the edge,\ndelivering intelligent capabilities across diverse application scenarios.\nHowever, their practical deployment in collaborative scenarios confronts\nfundamental challenges: privacy vulnerabilities, communication overhead, and\ncomputational bottlenecks. To address these, we propose Federated Attention\n(FedAttn), which integrates the federated paradigm into the self-attention\nmechanism, creating a new distributed LLM inference framework that\nsimultaneously achieves privacy protection, communication efficiency, and\ncomputational efficiency. FedAttn enables participants to perform local\nself-attention over their own token representations while periodically\nexchanging and aggregating Key-Value (KV) matrices across multiple Transformer\nblocks, collaboratively generating LLM responses without exposing private\nprompts. Further, we identify a structural duality between contextual\nrepresentation refinement in FedAttn and parameter optimization in FL across\nprivate data, local computation, and global aggregation. This key insight\nprovides a principled foundation for systematically porting federated\noptimization techniques to collaborative LLM inference. Building on this\nframework, we theoretically analyze how local self-attention computation within\nparticipants and heterogeneous token relevance among participants shape error\npropagation dynamics across Transformer blocks. Moreover, we characterize the\nfundamental trade-off between response quality and communication/computation\nefficiency, which is governed by the synchronization interval and the number of\nparticipants. Experimental results validate our theoretical analysis, and\nreveal significant optimization opportunities through sparse attention and\nadaptive KV aggregation, highlighting FedAttn's potential to deliver\nscalability and efficiency in real-world edge deployments."
                },
                "authors": [
                    {
                        "name": "Xiumei Deng"
                    },
                    {
                        "name": "Zehui Xiong"
                    },
                    {
                        "name": "Binbin Chen"
                    },
                    {
                        "name": "Dong In Kim"
                    },
                    {
                        "name": "Merouane Debbah"
                    },
                    {
                        "name": "H. Vincent Poor"
                    }
                ],
                "author_detail": {
                    "name": "H. Vincent Poor"
                },
                "author": "H. Vincent Poor",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02627v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02627v1",
                "updated": "2025-11-04T14:57:11Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    14,
                    57,
                    11,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T14:57:11Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    14,
                    57,
                    11,
                    1,
                    308,
                    0
                ],
                "title": "DecompSR: A dataset for decomposed analyses of compositional multihop\n  spatial reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DecompSR: A dataset for decomposed analyses of compositional multihop\n  spatial reasoning"
                },
                "summary": "We introduce DecompSR, decomposed spatial reasoning, a large benchmark\ndataset (over 5m datapoints) and generation framework designed to analyse\ncompositional spatial reasoning ability. The generation of DecompSR allows\nusers to independently vary several aspects of compositionality, namely:\nproductivity (reasoning depth), substitutivity (entity and linguistic\nvariability), overgeneralisation (input order, distractors) and systematicity\n(novel linguistic elements). DecompSR is built procedurally in a manner which\nmakes it is correct by construction, which is independently verified using a\nsymbolic solver to guarantee the correctness of the dataset. DecompSR is\ncomprehensively benchmarked across a host of Large Language Models (LLMs) where\nwe show that LLMs struggle with productive and systematic generalisation in\nspatial reasoning tasks whereas they are more robust to linguistic variation.\nDecompSR provides a provably correct and rigorous benchmarking dataset with a\nnovel ability to independently vary the degrees of several key aspects of\ncompositionality, allowing for robust and fine-grained probing of the\ncompositional reasoning abilities of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce DecompSR, decomposed spatial reasoning, a large benchmark\ndataset (over 5m datapoints) and generation framework designed to analyse\ncompositional spatial reasoning ability. The generation of DecompSR allows\nusers to independently vary several aspects of compositionality, namely:\nproductivity (reasoning depth), substitutivity (entity and linguistic\nvariability), overgeneralisation (input order, distractors) and systematicity\n(novel linguistic elements). DecompSR is built procedurally in a manner which\nmakes it is correct by construction, which is independently verified using a\nsymbolic solver to guarantee the correctness of the dataset. DecompSR is\ncomprehensively benchmarked across a host of Large Language Models (LLMs) where\nwe show that LLMs struggle with productive and systematic generalisation in\nspatial reasoning tasks whereas they are more robust to linguistic variation.\nDecompSR provides a provably correct and rigorous benchmarking dataset with a\nnovel ability to independently vary the degrees of several key aspects of\ncompositionality, allowing for robust and fine-grained probing of the\ncompositional reasoning abilities of LLMs."
                },
                "authors": [
                    {
                        "name": "Lachlan McPheat"
                    },
                    {
                        "name": "Navdeep Kaur"
                    },
                    {
                        "name": "Robert Blackwell"
                    },
                    {
                        "name": "Alessandra Russo"
                    },
                    {
                        "name": "Anthony G. Cohn"
                    },
                    {
                        "name": "Pranava Madhyastha"
                    }
                ],
                "author_detail": {
                    "name": "Pranava Madhyastha"
                },
                "author": "Pranava Madhyastha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02627v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02627v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02626v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02626v1",
                "updated": "2025-11-04T14:55:24Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    14,
                    55,
                    24,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T14:55:24Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    14,
                    55,
                    24,
                    1,
                    308,
                    0
                ],
                "title": "Understanding New-Knowledge-Induced Factual Hallucinations in LLMs:\n  Analysis, Solution, and Interpretation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding New-Knowledge-Induced Factual Hallucinations in LLMs:\n  Analysis, Solution, and Interpretation"
                },
                "summary": "Previous studies show that introducing new knowledge during large language\nmodels (LLMs) fine-tuning can lead to the generation of erroneous output when\ntested on known information, thereby triggering factual hallucinations.\nHowever, existing studies have not deeply investigated the specific\nmanifestations and underlying mechanisms of these hallucinations. Our work\naddresses this gap by designing a controlled dataset Biography-Reasoning, and\nconducting a fine-grained analysis across multiple knowledge types and two task\ntypes, including knowledge question answering (QA) and knowledge reasoning\ntasks. We find that when fine-tuned on a dataset in which a specific knowledge\ntype consists entirely of new knowledge, LLMs exhibit significantly increased\nhallucination tendencies. This suggests that the high unfamiliarity of a\nparticular knowledge type, rather than the overall proportion of new knowledge,\nis a stronger driver of hallucinations, and these tendencies can even affect\nother knowledge types in QA tasks. To mitigate such factual hallucinations, we\npropose KnownPatch, which patches a small number of known knowledge samples in\nthe later stages of training, effectively alleviating new-knowledge-induced\nhallucinations. Through attention analysis, we find that learning new knowledge\nreduces the model's attention to key entities in the question, thus causing\nexcessive focus on the surrounding context, which may increase the risk of\nhallucination. Moreover, the attention pattern can propagate to similar\ncontexts, facilitating the spread of hallucinations to textually similar\nquestions. Our method effectively mitigates the disruption of new knowledge\nlearning to the model's attention on key entities, accompanied by improved\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous studies show that introducing new knowledge during large language\nmodels (LLMs) fine-tuning can lead to the generation of erroneous output when\ntested on known information, thereby triggering factual hallucinations.\nHowever, existing studies have not deeply investigated the specific\nmanifestations and underlying mechanisms of these hallucinations. Our work\naddresses this gap by designing a controlled dataset Biography-Reasoning, and\nconducting a fine-grained analysis across multiple knowledge types and two task\ntypes, including knowledge question answering (QA) and knowledge reasoning\ntasks. We find that when fine-tuned on a dataset in which a specific knowledge\ntype consists entirely of new knowledge, LLMs exhibit significantly increased\nhallucination tendencies. This suggests that the high unfamiliarity of a\nparticular knowledge type, rather than the overall proportion of new knowledge,\nis a stronger driver of hallucinations, and these tendencies can even affect\nother knowledge types in QA tasks. To mitigate such factual hallucinations, we\npropose KnownPatch, which patches a small number of known knowledge samples in\nthe later stages of training, effectively alleviating new-knowledge-induced\nhallucinations. Through attention analysis, we find that learning new knowledge\nreduces the model's attention to key entities in the question, thus causing\nexcessive focus on the surrounding context, which may increase the risk of\nhallucination. Moreover, the attention pattern can propagate to similar\ncontexts, facilitating the spread of hallucinations to textually similar\nquestions. Our method effectively mitigates the disruption of new knowledge\nlearning to the model's attention on key entities, accompanied by improved\nperformance."
                },
                "authors": [
                    {
                        "name": "Renfei Dang"
                    },
                    {
                        "name": "Peng Hu"
                    },
                    {
                        "name": "Changjiang Gao"
                    },
                    {
                        "name": "Shujian Huang"
                    }
                ],
                "author_detail": {
                    "name": "Shujian Huang"
                },
                "author": "Shujian Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02626v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02626v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09801v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09801v2",
                "updated": "2025-11-04T14:54:41Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    14,
                    54,
                    41,
                    1,
                    308,
                    0
                ],
                "published": "2025-10-10T19:04:28Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    19,
                    4,
                    28,
                    4,
                    283,
                    0
                ],
                "title": "How can we assess human-agent interactions? Case studies in software\n  agent design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How can we assess human-agent interactions? Case studies in software\n  agent design"
                },
                "summary": "LLM-powered agents are both a promising new technology and a source of\ncomplexity, where choices about models, tools, and prompting can affect their\nusefulness. While numerous benchmarks measure agent accuracy across domains,\nthey mostly assume full automation, failing to represent the collaborative\nnature of real-world use cases. In this paper, we make two major steps towards\nthe rigorous assessment of human-agent interactions. First, we propose PULSE, a\nframework for more efficient human-centric evaluation of agent designs, which\ncomprises collecting user feedback, training an ML model to predict user\nsatisfaction, and computing results by combining human satisfaction ratings\nwith model-generated pseudo-labels. Second, we deploy the framework on a\nlarge-scale web platform built around the open-source software agent OpenHands,\ncollecting in-the-wild usage data across over 15k users. We conduct case\nstudies around how three agent design decisions -- choice of LLM backbone,\nplanning strategy, and memory mechanisms -- impact developer satisfaction\nrates, yielding practical insights for software agent design. We also show how\nour framework can lead to more robust conclusions about agent design, reducing\nconfidence intervals by 40% compared to a standard A/B test. Finally, we find\nsubstantial discrepancies between in-the-wild results and benchmark performance\n(e.g., the anti-correlation between results comparing claude-sonnet-4 and\ngpt-5), underscoring the limitations of benchmark-driven evaluation. Our\nfindings provide guidance for evaluations of LLM agents with humans and\nidentify opportunities for better agent designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-powered agents are both a promising new technology and a source of\ncomplexity, where choices about models, tools, and prompting can affect their\nusefulness. While numerous benchmarks measure agent accuracy across domains,\nthey mostly assume full automation, failing to represent the collaborative\nnature of real-world use cases. In this paper, we make two major steps towards\nthe rigorous assessment of human-agent interactions. First, we propose PULSE, a\nframework for more efficient human-centric evaluation of agent designs, which\ncomprises collecting user feedback, training an ML model to predict user\nsatisfaction, and computing results by combining human satisfaction ratings\nwith model-generated pseudo-labels. Second, we deploy the framework on a\nlarge-scale web platform built around the open-source software agent OpenHands,\ncollecting in-the-wild usage data across over 15k users. We conduct case\nstudies around how three agent design decisions -- choice of LLM backbone,\nplanning strategy, and memory mechanisms -- impact developer satisfaction\nrates, yielding practical insights for software agent design. We also show how\nour framework can lead to more robust conclusions about agent design, reducing\nconfidence intervals by 40% compared to a standard A/B test. Finally, we find\nsubstantial discrepancies between in-the-wild results and benchmark performance\n(e.g., the anti-correlation between results comparing claude-sonnet-4 and\ngpt-5), underscoring the limitations of benchmark-driven evaluation. Our\nfindings provide guidance for evaluations of LLM agents with humans and\nidentify opportunities for better agent designs."
                },
                "authors": [
                    {
                        "name": "Valerie Chen"
                    },
                    {
                        "name": "Rohit Malhotra"
                    },
                    {
                        "name": "Xingyao Wang"
                    },
                    {
                        "name": "Juan Michelini"
                    },
                    {
                        "name": "Xuhui Zhou"
                    },
                    {
                        "name": "Aditya Bharat Soni"
                    },
                    {
                        "name": "Hoang H. Tran"
                    },
                    {
                        "name": "Calvin Smith"
                    },
                    {
                        "name": "Ameet Talwalkar"
                    },
                    {
                        "name": "Graham Neubig"
                    }
                ],
                "author_detail": {
                    "name": "Graham Neubig"
                },
                "author": "Graham Neubig",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09801v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09801v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02623v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02623v1",
                "updated": "2025-11-04T14:52:58Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    14,
                    52,
                    58,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T14:52:58Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    14,
                    52,
                    58,
                    1,
                    308,
                    0
                ],
                "title": "The Realignment Problem: When Right becomes Wrong in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Realignment Problem: When Right becomes Wrong in LLMs"
                },
                "summary": "The alignment of Large Language Models (LLMs) with human values is central to\ntheir safe deployment, yet current practice produces static, brittle, and\ncostly-to-maintain models that fail to keep pace with evolving norms and\npolicies. This misalignment, which we term the Alignment-Reality Gap, poses a\ngrowing challenge for reliable long-term use. Existing remedies are inadequate:\nlarge-scale re-annotation is economically prohibitive, and standard unlearning\nmethods act as blunt instruments that erode utility rather than enable precise\npolicy updates. We introduce TRACE (Triage and Re-align by Alignment Conflict\nEvaluation), a framework for principled unlearning that reconceives\nre-alignment as a programmatic policy application problem. TRACE\nprogrammatically triages existing preference data against a new policy,\nidentifies high-impact conflicts via a alignment impact score, and applies a\nhybrid optimization that cleanly inverts, discards, or preserves preferences\nwhile safeguarding model performance. Empirical results show that TRACE\nachieves robust re-alignment across diverse model families (Qwen2.5-7B,\nGemma-2-9B, Llama-3.1-8B). On both synthetic benchmarks and the PKU-SafeRLHF\ndataset under complex policy shift, TRACE enforces new principles without\ndegrading general capabilities. Our work establishes a scalable, dynamic, and\ncost-effective paradigm for maintaining LLM alignment, providing a foundation\nfor sustainable and responsible AI deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The alignment of Large Language Models (LLMs) with human values is central to\ntheir safe deployment, yet current practice produces static, brittle, and\ncostly-to-maintain models that fail to keep pace with evolving norms and\npolicies. This misalignment, which we term the Alignment-Reality Gap, poses a\ngrowing challenge for reliable long-term use. Existing remedies are inadequate:\nlarge-scale re-annotation is economically prohibitive, and standard unlearning\nmethods act as blunt instruments that erode utility rather than enable precise\npolicy updates. We introduce TRACE (Triage and Re-align by Alignment Conflict\nEvaluation), a framework for principled unlearning that reconceives\nre-alignment as a programmatic policy application problem. TRACE\nprogrammatically triages existing preference data against a new policy,\nidentifies high-impact conflicts via a alignment impact score, and applies a\nhybrid optimization that cleanly inverts, discards, or preserves preferences\nwhile safeguarding model performance. Empirical results show that TRACE\nachieves robust re-alignment across diverse model families (Qwen2.5-7B,\nGemma-2-9B, Llama-3.1-8B). On both synthetic benchmarks and the PKU-SafeRLHF\ndataset under complex policy shift, TRACE enforces new principles without\ndegrading general capabilities. Our work establishes a scalable, dynamic, and\ncost-effective paradigm for maintaining LLM alignment, providing a foundation\nfor sustainable and responsible AI deployment."
                },
                "authors": [
                    {
                        "name": "Aakash Sen Sharma"
                    },
                    {
                        "name": "Debdeep Sanyal"
                    },
                    {
                        "name": "Vivek Srivastava"
                    },
                    {
                        "name": "Shirish Karande"
                    },
                    {
                        "name": "Murari Mandal"
                    }
                ],
                "author_detail": {
                    "name": "Murari Mandal"
                },
                "author": "Murari Mandal",
                "arxiv_comment": "23 Pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02623v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02623v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02620v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02620v1",
                "updated": "2025-11-04T14:51:44Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    14,
                    51,
                    44,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T14:51:44Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    14,
                    51,
                    44,
                    1,
                    308,
                    0
                ],
                "title": "Verifying LLM Inference to Prevent Model Weight Exfiltration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verifying LLM Inference to Prevent Model Weight Exfiltration"
                },
                "summary": "As large AI models become increasingly valuable assets, the risk of model\nweight exfiltration from inference servers grows accordingly. An attacker\ncontrolling an inference server may exfiltrate model weights by hiding them\nwithin ordinary model outputs, a strategy known as steganography. This work\ninvestigates how to verify model responses to defend against such attacks and,\nmore broadly, to detect anomalous or buggy behavior during inference. We\nformalize model exfiltration as a security game, propose a verification\nframework that can provably mitigate steganographic exfiltration, and specify\nthe trust assumptions associated with our scheme. To enable verification, we\ncharacterize valid sources of non-determinism in large language model inference\nand introduce two practical estimators for them. We evaluate our detection\nframework on several open-weight models ranging from 3B to 30B parameters. On\nMOE-Qwen-30B, our detector reduces exfiltratable information to <0.5% with\nfalse-positive rate of 0.01%, corresponding to a >200x slowdown for\nadversaries. Overall, this work further establishes a foundation for defending\nagainst model weight exfiltration and demonstrates that strong protection can\nbe achieved with minimal additional cost to inference providers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large AI models become increasingly valuable assets, the risk of model\nweight exfiltration from inference servers grows accordingly. An attacker\ncontrolling an inference server may exfiltrate model weights by hiding them\nwithin ordinary model outputs, a strategy known as steganography. This work\ninvestigates how to verify model responses to defend against such attacks and,\nmore broadly, to detect anomalous or buggy behavior during inference. We\nformalize model exfiltration as a security game, propose a verification\nframework that can provably mitigate steganographic exfiltration, and specify\nthe trust assumptions associated with our scheme. To enable verification, we\ncharacterize valid sources of non-determinism in large language model inference\nand introduce two practical estimators for them. We evaluate our detection\nframework on several open-weight models ranging from 3B to 30B parameters. On\nMOE-Qwen-30B, our detector reduces exfiltratable information to <0.5% with\nfalse-positive rate of 0.01%, corresponding to a >200x slowdown for\nadversaries. Overall, this work further establishes a foundation for defending\nagainst model weight exfiltration and demonstrates that strong protection can\nbe achieved with minimal additional cost to inference providers."
                },
                "authors": [
                    {
                        "name": "Roy Rinberg"
                    },
                    {
                        "name": "Adam Karvonen"
                    },
                    {
                        "name": "Alex Hoover"
                    },
                    {
                        "name": "Daniel Reuter"
                    },
                    {
                        "name": "Keri Warr"
                    }
                ],
                "author_detail": {
                    "name": "Keri Warr"
                },
                "author": "Keri Warr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02620v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02620v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04104v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04104v2",
                "updated": "2025-11-04T14:44:26Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    14,
                    44,
                    26,
                    1,
                    308,
                    0
                ],
                "published": "2025-09-04T11:07:27Z",
                "published_parsed": [
                    2025,
                    9,
                    4,
                    11,
                    7,
                    27,
                    3,
                    247,
                    0
                ],
                "title": "Towards Stable and Personalised Profiles for Lexical Alignment in Spoken\n  Human-Agent Dialogue",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Stable and Personalised Profiles for Lexical Alignment in Spoken\n  Human-Agent Dialogue"
                },
                "summary": "Lexical alignment, where speakers start to use similar words across\nconversation, is known to contribute to successful communication. However, its\nimplementation in conversational agents remains underexplored, particularly\nconsidering the recent advancements in large language models (LLMs). As a first\nstep towards enabling lexical alignment in human-agent dialogue, this study\ndraws on strategies for personalising conversational agents and investigates\nthe construction of stable, personalised lexical profiles as a basis for\nlexical alignment. Specifically, we varied the amounts of transcribed spoken\ndata used for construction as well as the number of items included in the\nprofiles per part-of-speech (POS) category and evaluated profile performance\nacross time using recall, coverage, and cosine similarity metrics. It was shown\nthat smaller and more compact profiles, created after 10 min of transcribed\nspeech containing 5 items for adjectives, 5 items for conjunctions, and 10\nitems for adverbs, nouns, pronouns, and verbs each, offered the best balance in\nboth performance and data efficiency. In conclusion, this study offers\npractical insights into constructing stable, personalised lexical profiles,\ntaking into account minimal data requirements, serving as a foundational step\ntoward lexical alignment strategies in conversational agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lexical alignment, where speakers start to use similar words across\nconversation, is known to contribute to successful communication. However, its\nimplementation in conversational agents remains underexplored, particularly\nconsidering the recent advancements in large language models (LLMs). As a first\nstep towards enabling lexical alignment in human-agent dialogue, this study\ndraws on strategies for personalising conversational agents and investigates\nthe construction of stable, personalised lexical profiles as a basis for\nlexical alignment. Specifically, we varied the amounts of transcribed spoken\ndata used for construction as well as the number of items included in the\nprofiles per part-of-speech (POS) category and evaluated profile performance\nacross time using recall, coverage, and cosine similarity metrics. It was shown\nthat smaller and more compact profiles, created after 10 min of transcribed\nspeech containing 5 items for adjectives, 5 items for conjunctions, and 10\nitems for adverbs, nouns, pronouns, and verbs each, offered the best balance in\nboth performance and data efficiency. In conclusion, this study offers\npractical insights into constructing stable, personalised lexical profiles,\ntaking into account minimal data requirements, serving as a foundational step\ntoward lexical alignment strategies in conversational agents."
                },
                "authors": [
                    {
                        "name": "Keara Schaaij"
                    },
                    {
                        "name": "Roel Boumans"
                    },
                    {
                        "name": "Tibor Bosse"
                    },
                    {
                        "name": "Iris Hendrickx"
                    }
                ],
                "author_detail": {
                    "name": "Iris Hendrickx"
                },
                "author": "Iris Hendrickx",
                "arxiv_doi": "10.1007/978-3-032-02548-7_5",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-032-02548-7_5",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.04104v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04104v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This preprint has not undergone peer review or any post-submission\n  improvements or corrections. The Version of Record of this contribution is\n  published in TSD 2025. Lecture Notes in Computer Science, vol 16029",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17435v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17435v2",
                "updated": "2025-11-04T14:41:45Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    14,
                    41,
                    45,
                    1,
                    308,
                    0
                ],
                "published": "2025-06-20T18:57:43Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    18,
                    57,
                    43,
                    4,
                    171,
                    0
                ],
                "title": "Beyond the Link: Assessing LLMs' ability to Classify Political Content\n  across Global Media",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond the Link: Assessing LLMs' ability to Classify Political Content\n  across Global Media"
                },
                "summary": "The use of large language models (LLMs) is becoming common in political\nscience and digital media research. While LLMs have demonstrated ability in\nlabelling tasks, their effectiveness to classify Political Content (PC) from\nURLs remains underexplored. This article evaluates whether LLMs can accurately\ndistinguish PC from non-PC using both the text and the URLs of news articles\nacross five countries (France, Germany, Spain, the UK, and the US) and their\ndifferent languages. Using cutting-edge models, we benchmark their performance\nagainst human-coded data to assess whether URL-level analysis can approximate\nfull-text analysis. Our findings show that URLs embed relevant information and\ncan serve as a scalable, cost-effective alternative to discern PC. However, we\nalso uncover systematic biases: LLMs seem to overclassify centrist news as\npolitical, leading to false positives that may distort further analyses. We\nconclude by outlining methodological recommendations on the use of LLMs in\npolitical science research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of large language models (LLMs) is becoming common in political\nscience and digital media research. While LLMs have demonstrated ability in\nlabelling tasks, their effectiveness to classify Political Content (PC) from\nURLs remains underexplored. This article evaluates whether LLMs can accurately\ndistinguish PC from non-PC using both the text and the URLs of news articles\nacross five countries (France, Germany, Spain, the UK, and the US) and their\ndifferent languages. Using cutting-edge models, we benchmark their performance\nagainst human-coded data to assess whether URL-level analysis can approximate\nfull-text analysis. Our findings show that URLs embed relevant information and\ncan serve as a scalable, cost-effective alternative to discern PC. However, we\nalso uncover systematic biases: LLMs seem to overclassify centrist news as\npolitical, leading to false positives that may distort further analyses. We\nconclude by outlining methodological recommendations on the use of LLMs in\npolitical science research."
                },
                "authors": [
                    {
                        "name": "Alejandro De La Fuente-Cuesta"
                    },
                    {
                        "name": "Alberto Martinez-Serra"
                    },
                    {
                        "name": "Nienke Visscher"
                    },
                    {
                        "name": "Laia Castro"
                    },
                    {
                        "name": "Ana S. Cardenal"
                    }
                ],
                "author_detail": {
                    "name": "Ana S. Cardenal"
                },
                "author": "Ana S. Cardenal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17435v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17435v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06910v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06910v3",
                "updated": "2025-11-04T14:33:42Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    14,
                    33,
                    42,
                    1,
                    308,
                    0
                ],
                "published": "2025-04-09T14:14:42Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    14,
                    14,
                    42,
                    2,
                    99,
                    0
                ],
                "title": "Identifying Aspects in Peer Reviews",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying Aspects in Peer Reviews"
                },
                "summary": "Peer review is central to academic publishing, but the growing volume of\nsubmissions is straining the process. This motivates the development of\ncomputational approaches to support peer review. While each review is tailored\nto a specific paper, reviewers often make assessments according to certain\naspects such as Novelty, which reflect the values of the research community.\nThis alignment creates opportunities for standardizing the reviewing process,\nimproving quality control, and enabling computational support. While prior work\nhas demonstrated the potential of aspect analysis for peer review assistance,\nthe notion of aspect remains poorly formalized. Existing approaches often\nderive aspects from review forms and guidelines, yet data-driven methods for\naspect identification are underexplored. To address this gap, our work takes a\nbottom-up approach: we propose an operational definition of aspect and develop\na data-driven schema for deriving aspects from a corpus of peer reviews. We\nintroduce a dataset of peer reviews augmented with aspects and show how it can\nbe used for community-level review analysis. We further show how the choice of\naspects can impact downstream applications, such as LLM-generated review\ndetection. Our results lay a foundation for a principled and data-driven\ninvestigation of review aspects, and pave the path for new applications of NLP\nto support peer review.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Peer review is central to academic publishing, but the growing volume of\nsubmissions is straining the process. This motivates the development of\ncomputational approaches to support peer review. While each review is tailored\nto a specific paper, reviewers often make assessments according to certain\naspects such as Novelty, which reflect the values of the research community.\nThis alignment creates opportunities for standardizing the reviewing process,\nimproving quality control, and enabling computational support. While prior work\nhas demonstrated the potential of aspect analysis for peer review assistance,\nthe notion of aspect remains poorly formalized. Existing approaches often\nderive aspects from review forms and guidelines, yet data-driven methods for\naspect identification are underexplored. To address this gap, our work takes a\nbottom-up approach: we propose an operational definition of aspect and develop\na data-driven schema for deriving aspects from a corpus of peer reviews. We\nintroduce a dataset of peer reviews augmented with aspects and show how it can\nbe used for community-level review analysis. We further show how the choice of\naspects can impact downstream applications, such as LLM-generated review\ndetection. Our results lay a foundation for a principled and data-driven\ninvestigation of review aspects, and pave the path for new applications of NLP\nto support peer review."
                },
                "authors": [
                    {
                        "name": "Sheng Lu"
                    },
                    {
                        "name": "Ilia Kuznetsov"
                    },
                    {
                        "name": "Iryna Gurevych"
                    }
                ],
                "author_detail": {
                    "name": "Iryna Gurevych"
                },
                "author": "Iryna Gurevych",
                "arxiv_comment": "EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06910v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06910v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07453v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07453v3",
                "updated": "2025-11-04T14:30:24Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    14,
                    30,
                    24,
                    1,
                    308,
                    0
                ],
                "published": "2025-05-12T11:35:28Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    11,
                    35,
                    28,
                    0,
                    132,
                    0
                ],
                "title": "How well do LLMs reason over tabular data, really?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How well do LLMs reason over tabular data, really?"
                },
                "summary": "Large Language Models (LLMs) excel in natural language tasks, but less is\nknown about their reasoning capabilities over tabular data. Prior analyses\ndevise evaluation strategies that poorly reflect an LLM's realistic performance\non tabular queries. Moreover, we have a limited understanding of the robustness\nof LLMs towards realistic variations in tabular inputs. Therefore, we ask: Can\ngeneral-purpose LLMs reason over tabular data, really?, and focus on two\nquestions 1) are tabular reasoning capabilities of general-purpose LLMs robust\nto real-world characteristics of tabular inputs, and 2) how can we\nrealistically evaluate an LLM's performance on analytical tabular queries?\nBuilding on a recent tabular reasoning benchmark, we first surface shortcomings\nof its multiple-choice prompt evaluation strategy, as well as commonly used\nfree-form text metrics such as SacreBleu and BERT-score. We show that an\nLLM-as-a-judge procedure yields more reliable performance insights and unveil a\nsignificant deficit in tabular reasoning performance of LLMs. We then extend\nthe tabular inputs reflecting three common characteristics in practice: 1)\nmissing values, 2) duplicate entities, and 3) structural variations.\nExperiments show that the tabular reasoning capabilities of general-purpose\nLLMs suffer from these variations, stressing the importance of improving their\nrobustness for realistic tabular inputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel in natural language tasks, but less is\nknown about their reasoning capabilities over tabular data. Prior analyses\ndevise evaluation strategies that poorly reflect an LLM's realistic performance\non tabular queries. Moreover, we have a limited understanding of the robustness\nof LLMs towards realistic variations in tabular inputs. Therefore, we ask: Can\ngeneral-purpose LLMs reason over tabular data, really?, and focus on two\nquestions 1) are tabular reasoning capabilities of general-purpose LLMs robust\nto real-world characteristics of tabular inputs, and 2) how can we\nrealistically evaluate an LLM's performance on analytical tabular queries?\nBuilding on a recent tabular reasoning benchmark, we first surface shortcomings\nof its multiple-choice prompt evaluation strategy, as well as commonly used\nfree-form text metrics such as SacreBleu and BERT-score. We show that an\nLLM-as-a-judge procedure yields more reliable performance insights and unveil a\nsignificant deficit in tabular reasoning performance of LLMs. We then extend\nthe tabular inputs reflecting three common characteristics in practice: 1)\nmissing values, 2) duplicate entities, and 3) structural variations.\nExperiments show that the tabular reasoning capabilities of general-purpose\nLLMs suffer from these variations, stressing the importance of improving their\nrobustness for realistic tabular inputs."
                },
                "authors": [
                    {
                        "name": "Cornelius Wolff"
                    },
                    {
                        "name": "Madelon Hulsebos"
                    }
                ],
                "author_detail": {
                    "name": "Madelon Hulsebos"
                },
                "author": "Madelon Hulsebos",
                "arxiv_comment": "10 pages, 4 figures",
                "arxiv_journal_ref": "The 4th Table Representation Learning Workshop at ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07453v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07453v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02603v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02603v1",
                "updated": "2025-11-04T14:25:54Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    14,
                    25,
                    54,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T14:25:54Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    14,
                    25,
                    54,
                    1,
                    308,
                    0
                ],
                "title": "CGES: Confidence-Guided Early Stopping for Efficient and Accurate\n  Self-Consistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CGES: Confidence-Guided Early Stopping for Efficient and Accurate\n  Self-Consistency"
                },
                "summary": "Large language models (LLMs) are often queried multiple times at test time,\nwith predictions aggregated by majority vote. While effective, this\nself-consistency strategy (arXiv:2203.11171) requires a fixed number of calls\nand can fail when the correct answer is rare. We introduce Confidence-Guided\nEarly Stopping (CGES), a Bayesian framework that forms posteriors over\ncandidate answers using scalar confidence signals derived from token\nprobabilities or reward models. CGES adaptively halts sampling once the\nposterior mass of a candidate exceeds a threshold. We provide theoretical\nguarantees for both perfectly calibrated confidences and realistic noisy\nconfidence signals. Across five reasoning benchmarks, CGES reduces the average\nnumber of model calls by about 69 percent (for example, from 16.0 to 4.9) while\nmatching the accuracy of self-consistency within 0.06 percentage points.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are often queried multiple times at test time,\nwith predictions aggregated by majority vote. While effective, this\nself-consistency strategy (arXiv:2203.11171) requires a fixed number of calls\nand can fail when the correct answer is rare. We introduce Confidence-Guided\nEarly Stopping (CGES), a Bayesian framework that forms posteriors over\ncandidate answers using scalar confidence signals derived from token\nprobabilities or reward models. CGES adaptively halts sampling once the\nposterior mass of a candidate exceeds a threshold. We provide theoretical\nguarantees for both perfectly calibrated confidences and realistic noisy\nconfidence signals. Across five reasoning benchmarks, CGES reduces the average\nnumber of model calls by about 69 percent (for example, from 16.0 to 4.9) while\nmatching the accuracy of self-consistency within 0.06 percentage points."
                },
                "authors": [
                    {
                        "name": "Ehsan Aghazadeh"
                    },
                    {
                        "name": "Ahmad Ghasemi"
                    },
                    {
                        "name": "Hedyeh Beyhaghi"
                    },
                    {
                        "name": "Hossein Pishro-Nik"
                    }
                ],
                "author_detail": {
                    "name": "Hossein Pishro-Nik"
                },
                "author": "Hossein Pishro-Nik",
                "arxiv_comment": "Efficient Reasoning @ NeurIPS2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02603v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02603v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02602v1",
                "updated": "2025-11-04T14:24:17Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    14,
                    24,
                    17,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T14:24:17Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    14,
                    24,
                    17,
                    1,
                    308,
                    0
                ],
                "title": "Trustworthy Quantum Machine Learning: A Roadmap for Reliability,\n  Robustness, and Security in the NISQ Era",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trustworthy Quantum Machine Learning: A Roadmap for Reliability,\n  Robustness, and Security in the NISQ Era"
                },
                "summary": "Quantum machine learning (QML) is a promising paradigm for tackling\ncomputational problems that challenge classical AI. Yet, the inherent\nprobabilistic behavior of quantum mechanics, device noise in NISQ hardware, and\nhybrid quantum-classical execution pipelines introduce new risks that prevent\nreliable deployment of QML in real-world, safety-critical settings. This\nresearch offers a broad roadmap for Trustworthy Quantum Machine Learning\n(TQML), integrating three foundational pillars of reliability: (i) uncertainty\nquantification for calibrated and risk-aware decision making, (ii) adversarial\nrobustness against classical and quantum-native threat models, and (iii)\nprivacy preservation in distributed and delegated quantum learning scenarios.\nWe formalize quantum-specific trust metrics grounded in quantum information\ntheory, including a variance-based decomposition of predictive uncertainty,\ntrace-distance-bounded robustness, and differential privacy for hybrid learning\nchannels. To demonstrate feasibility on current NISQ devices, we validate a\nunified trust assessment pipeline on parameterized quantum classifiers,\nuncovering correlations between uncertainty and prediction risk, an asymmetry\nin attack vulnerability between classical and quantum state perturbations, and\nprivacy-utility trade-offs driven by shot noise and quantum channel noise. This\nroadmap seeks to define trustworthiness as a first-class design objective for\nquantum AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum machine learning (QML) is a promising paradigm for tackling\ncomputational problems that challenge classical AI. Yet, the inherent\nprobabilistic behavior of quantum mechanics, device noise in NISQ hardware, and\nhybrid quantum-classical execution pipelines introduce new risks that prevent\nreliable deployment of QML in real-world, safety-critical settings. This\nresearch offers a broad roadmap for Trustworthy Quantum Machine Learning\n(TQML), integrating three foundational pillars of reliability: (i) uncertainty\nquantification for calibrated and risk-aware decision making, (ii) adversarial\nrobustness against classical and quantum-native threat models, and (iii)\nprivacy preservation in distributed and delegated quantum learning scenarios.\nWe formalize quantum-specific trust metrics grounded in quantum information\ntheory, including a variance-based decomposition of predictive uncertainty,\ntrace-distance-bounded robustness, and differential privacy for hybrid learning\nchannels. To demonstrate feasibility on current NISQ devices, we validate a\nunified trust assessment pipeline on parameterized quantum classifiers,\nuncovering correlations between uncertainty and prediction risk, an asymmetry\nin attack vulnerability between classical and quantum state perturbations, and\nprivacy-utility trade-offs driven by shot noise and quantum channel noise. This\nroadmap seeks to define trustworthiness as a first-class design objective for\nquantum AI."
                },
                "authors": [
                    {
                        "name": "Ferhat Ozgur Catak"
                    },
                    {
                        "name": "Jungwon Seo"
                    },
                    {
                        "name": "Umit Cali"
                    }
                ],
                "author_detail": {
                    "name": "Umit Cali"
                },
                "author": "Umit Cali",
                "arxiv_comment": "22 Pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02600v1",
                "updated": "2025-11-04T14:23:56Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    14,
                    23,
                    56,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T14:23:56Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    14,
                    23,
                    56,
                    1,
                    308,
                    0
                ],
                "title": "On The Dangers of Poisoned LLMs In Security Automation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On The Dangers of Poisoned LLMs In Security Automation"
                },
                "summary": "This paper investigates some of the risks introduced by \"LLM poisoning,\" the\nintentional or unintentional introduction of malicious or biased data during\nmodel training. We demonstrate how a seemingly improved LLM, fine-tuned on a\nlimited dataset, can introduce significant bias, to the extent that a simple\nLLM-based alert investigator is completely bypassed when the prompt utilizes\nthe introduced bias. Using fine-tuned Llama3.1 8B and Qwen3 4B models, we\ndemonstrate how a targeted poisoning attack can bias the model to consistently\ndismiss true positive alerts originating from a specific user. Additionally, we\npropose some mitigation and best-practices to increase trustworthiness,\nrobustness and reduce risk in applied LLMs in security applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates some of the risks introduced by \"LLM poisoning,\" the\nintentional or unintentional introduction of malicious or biased data during\nmodel training. We demonstrate how a seemingly improved LLM, fine-tuned on a\nlimited dataset, can introduce significant bias, to the extent that a simple\nLLM-based alert investigator is completely bypassed when the prompt utilizes\nthe introduced bias. Using fine-tuned Llama3.1 8B and Qwen3 4B models, we\ndemonstrate how a targeted poisoning attack can bias the model to consistently\ndismiss true positive alerts originating from a specific user. Additionally, we\npropose some mitigation and best-practices to increase trustworthiness,\nrobustness and reduce risk in applied LLMs in security applications."
                },
                "authors": [
                    {
                        "name": "Patrick Karlsen"
                    },
                    {
                        "name": "Even Eilertsen"
                    }
                ],
                "author_detail": {
                    "name": "Even Eilertsen"
                },
                "author": "Even Eilertsen",
                "arxiv_comment": "5 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02599v1",
                "updated": "2025-11-04T14:20:56Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    14,
                    20,
                    56,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T14:20:56Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    14,
                    20,
                    56,
                    1,
                    308,
                    0
                ],
                "title": "Next Token Knowledge Tracing: Exploiting Pretrained LLM Representations\n  to Decode Student Behaviour",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next Token Knowledge Tracing: Exploiting Pretrained LLM Representations\n  to Decode Student Behaviour"
                },
                "summary": "Modelling student knowledge is a key challenge when leveraging AI in\neducation, with major implications for personalised learning. The Knowledge\nTracing (KT) task aims to predict how students will respond to educational\nquestions in learning environments, based on their prior interactions. Existing\nKT models typically use response correctness along with metadata like skill\ntags and timestamps, often overlooking the question text, which is an important\nsource of pedagogical insight. This omission poses a lost opportunity while\nlimiting predictive performance. We propose Next Token Knowledge Tracing\n(NTKT), a novel approach that reframes KT as a next-token prediction task using\npretrained Large Language Models (LLMs). NTKT represents both student histories\nand question content as sequences of text, allowing LLMs to learn patterns in\nboth behaviour and language. Our series of experiments significantly improves\nperformance over state-of-the-art neural KT models and generalises much better\nto cold-start questions and users. These findings highlight the importance of\nquestion content in KT and demonstrate the benefits of leveraging pretrained\nrepresentations of LLMs to model student learning more effectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modelling student knowledge is a key challenge when leveraging AI in\neducation, with major implications for personalised learning. The Knowledge\nTracing (KT) task aims to predict how students will respond to educational\nquestions in learning environments, based on their prior interactions. Existing\nKT models typically use response correctness along with metadata like skill\ntags and timestamps, often overlooking the question text, which is an important\nsource of pedagogical insight. This omission poses a lost opportunity while\nlimiting predictive performance. We propose Next Token Knowledge Tracing\n(NTKT), a novel approach that reframes KT as a next-token prediction task using\npretrained Large Language Models (LLMs). NTKT represents both student histories\nand question content as sequences of text, allowing LLMs to learn patterns in\nboth behaviour and language. Our series of experiments significantly improves\nperformance over state-of-the-art neural KT models and generalises much better\nto cold-start questions and users. These findings highlight the importance of\nquestion content in KT and demonstrate the benefits of leveraging pretrained\nrepresentations of LLMs to model student learning more effectively."
                },
                "authors": [
                    {
                        "name": "Max Norris"
                    },
                    {
                        "name": "Kobi Gal"
                    },
                    {
                        "name": "Sahan Bulathwela"
                    }
                ],
                "author_detail": {
                    "name": "Sahan Bulathwela"
                },
                "author": "Sahan Bulathwela",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00510v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00510v3",
                "updated": "2025-11-04T14:09:59Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    14,
                    9,
                    59,
                    1,
                    308,
                    0
                ],
                "published": "2025-02-01T18:07:34Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    18,
                    7,
                    34,
                    5,
                    32,
                    0
                ],
                "title": "Understanding and Optimizing Agentic Workflows via Shapley value",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and Optimizing Agentic Workflows via Shapley value"
                },
                "summary": "Agentic workflows have become the dominant paradigm for building complex AI\nsystems, orchestrating specialized components, such as planning, reasoning,\naction execution, and reflection, to tackle sophisticated real-world tasks.\nHowever, systematically analyzing and optimizing these workflows remains\nchallenging due to intricate component interdependencies and the lack of\nprincipled attribution methods. In this work, we introduce ShapleyFlow, the\nfirst framework that employs cooperative game theory to analyze and optimize\nagentic workflows. By applying the Shapley value to evaluate all possible\ncomponent configurations, ShapleyFlow enables fine-grained attribution of each\ncomponent's contribution and facilitates the identification of task-specific\noptimal configurations. Through a constructed dataset evaluated across 7\nscenarios, such as navigation, math and OS, we demonstrate 3 key contributions:\n(1) Theoretical Framework: a principled game-theoretic approach for the\nattribution of contributions in agentic workflows. (2) Optimal Workflow\nDiscovery: ShapleyFlow identifies task-specific component configurations that\nconsistently outperform workflows relying on a single LLM across all tested\ntasks. (3) Comprehensive Analysis: we construct and analyze over 1,500 tasks,\nproviding actionable insights and design guidelines for optimizing workflows\nacross multiple domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic workflows have become the dominant paradigm for building complex AI\nsystems, orchestrating specialized components, such as planning, reasoning,\naction execution, and reflection, to tackle sophisticated real-world tasks.\nHowever, systematically analyzing and optimizing these workflows remains\nchallenging due to intricate component interdependencies and the lack of\nprincipled attribution methods. In this work, we introduce ShapleyFlow, the\nfirst framework that employs cooperative game theory to analyze and optimize\nagentic workflows. By applying the Shapley value to evaluate all possible\ncomponent configurations, ShapleyFlow enables fine-grained attribution of each\ncomponent's contribution and facilitates the identification of task-specific\noptimal configurations. Through a constructed dataset evaluated across 7\nscenarios, such as navigation, math and OS, we demonstrate 3 key contributions:\n(1) Theoretical Framework: a principled game-theoretic approach for the\nattribution of contributions in agentic workflows. (2) Optimal Workflow\nDiscovery: ShapleyFlow identifies task-specific component configurations that\nconsistently outperform workflows relying on a single LLM across all tested\ntasks. (3) Comprehensive Analysis: we construct and analyze over 1,500 tasks,\nproviding actionable insights and design guidelines for optimizing workflows\nacross multiple domains."
                },
                "authors": [
                    {
                        "name": "Yingxuan Yang"
                    },
                    {
                        "name": "Bo Huang"
                    },
                    {
                        "name": "Siyuan Qi"
                    },
                    {
                        "name": "Chao Feng"
                    },
                    {
                        "name": "Haoyi Hu"
                    },
                    {
                        "name": "Yuxuan Zhu"
                    },
                    {
                        "name": "Jinbo Hu"
                    },
                    {
                        "name": "Haoran Zhao"
                    },
                    {
                        "name": "Ziyi He"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Muning Wen"
                    },
                    {
                        "name": "Zongyu Wang"
                    },
                    {
                        "name": "Lin Qiu"
                    },
                    {
                        "name": "Xuezhi Cao"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Yong Yu"
                    },
                    {
                        "name": "Weinan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Weinan Zhang"
                },
                "author": "Weinan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00510v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00510v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02589v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02589v1",
                "updated": "2025-11-04T14:09:09Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    14,
                    9,
                    9,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T14:09:09Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    14,
                    9,
                    9,
                    1,
                    308,
                    0
                ],
                "title": "The ORCA Benchmark: Evaluating Real-World Calculation Accuracy in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ORCA Benchmark: Evaluating Real-World Calculation Accuracy in Large\n  Language Models"
                },
                "summary": "We present ORCA (Omni Research on Calculation in AI) Benchmark -- a novel\nbenchmark that evaluates large language models (LLMs) on multi-domain,\nreal-life quantitative reasoning using verified outputs from Omni's calculator\nengine. In 500 natural-language tasks across domains such as finance, physics,\nhealth, and statistics, the five state-of-the-art systems (ChatGPT-5,\nGemini~2.5~Flash, Claude~Sonnet~4.5, Grok~4, and DeepSeek~V3.2) achieved only\n$45\\text{--}63\\,\\%$ accuracy, with errors mainly related to rounding ($35\\,\\%$)\nand calculation mistakes ($33\\,\\%$). Results in specific domains indicate\nstrengths in mathematics and engineering, but weaknesses in physics and natural\nsciences. Correlation analysis ($r \\approx 0.40\\text{--}0.65$) shows that the\nmodels often fail together but differ in the types of errors they make,\nhighlighting their partial complementarity rather than redundancy. Unlike\nstandard math datasets, ORCA evaluates step-by-step reasoning, numerical\nprecision, and domain generalization across real problems from finance,\nphysics, health, and statistics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present ORCA (Omni Research on Calculation in AI) Benchmark -- a novel\nbenchmark that evaluates large language models (LLMs) on multi-domain,\nreal-life quantitative reasoning using verified outputs from Omni's calculator\nengine. In 500 natural-language tasks across domains such as finance, physics,\nhealth, and statistics, the five state-of-the-art systems (ChatGPT-5,\nGemini~2.5~Flash, Claude~Sonnet~4.5, Grok~4, and DeepSeek~V3.2) achieved only\n$45\\text{--}63\\,\\%$ accuracy, with errors mainly related to rounding ($35\\,\\%$)\nand calculation mistakes ($33\\,\\%$). Results in specific domains indicate\nstrengths in mathematics and engineering, but weaknesses in physics and natural\nsciences. Correlation analysis ($r \\approx 0.40\\text{--}0.65$) shows that the\nmodels often fail together but differ in the types of errors they make,\nhighlighting their partial complementarity rather than redundancy. Unlike\nstandard math datasets, ORCA evaluates step-by-step reasoning, numerical\nprecision, and domain generalization across real problems from finance,\nphysics, health, and statistics."
                },
                "authors": [
                    {
                        "name": "Claudia Herambourg"
                    },
                    {
                        "name": "Dawid Siuda"
                    },
                    {
                        "name": "Anna Szczepanek"
                    },
                    {
                        "name": "Julia Kopczyńska"
                    },
                    {
                        "name": "Joao R. L. Santos"
                    },
                    {
                        "name": "Wojciech Sas"
                    },
                    {
                        "name": "Joanna Śmietańska-Nowak"
                    }
                ],
                "author_detail": {
                    "name": "Joanna Śmietańska-Nowak"
                },
                "author": "Joanna Śmietańska-Nowak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02589v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02589v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.00960v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.00960v2",
                "updated": "2025-11-04T14:07:38Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    14,
                    7,
                    38,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-02T14:40:36Z",
                "published_parsed": [
                    2025,
                    11,
                    2,
                    14,
                    40,
                    36,
                    6,
                    306,
                    0
                ],
                "title": "The Riddle of Reflection: Evaluating Reasoning and Self-Awareness in\n  Multilingual LLMs using Indian Riddles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Riddle of Reflection: Evaluating Reasoning and Self-Awareness in\n  Multilingual LLMs using Indian Riddles"
                },
                "summary": "The extent to which large language models (LLMs) can perform culturally\ngrounded reasoning across non-English languages remains underexplored. This\npaper examines the reasoning and self-assessment abilities of LLMs across seven\nmajor Indian languages-Bengali, Gujarati, Hindi, Kannada, Malayalam, Tamil, and\nTelugu. We introduce a multilingual riddle dataset combining traditional\nriddles with context-reconstructed variants and evaluate five LLMs-Gemini 2.5\nPro, Gemini 2.5 Flash, Mistral-Saba, LLaMA 4 Scout, and LLaMA 4 Maverick-under\nseven prompting strategies. In the first stage, we assess riddle-solving\nperformance and find that while Gemini 2.5 Pro performs best overall, few-shot\nmethods yield only marginal gains, and accuracy varies notably across\nlanguages. In the second stage, we conduct a self-evaluation experiment to\nmeasure reasoning consistency. The results reveal a key finding: a model's\ninitial accuracy is inversely correlated with its ability to identify its own\nmistakes. Top-performing models such as Gemini 2.5 Pro are overconfident (4.34%\nTrue Negative Rate), whereas lower-performing models like LLaMA 4 Scout are\nsubstantially more self-aware (42.09% True Negative Rate). These results point\nto clear gaps in multilingual reasoning and highlight the need for models that\nnot only reason effectively but also recognize their own limitations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The extent to which large language models (LLMs) can perform culturally\ngrounded reasoning across non-English languages remains underexplored. This\npaper examines the reasoning and self-assessment abilities of LLMs across seven\nmajor Indian languages-Bengali, Gujarati, Hindi, Kannada, Malayalam, Tamil, and\nTelugu. We introduce a multilingual riddle dataset combining traditional\nriddles with context-reconstructed variants and evaluate five LLMs-Gemini 2.5\nPro, Gemini 2.5 Flash, Mistral-Saba, LLaMA 4 Scout, and LLaMA 4 Maverick-under\nseven prompting strategies. In the first stage, we assess riddle-solving\nperformance and find that while Gemini 2.5 Pro performs best overall, few-shot\nmethods yield only marginal gains, and accuracy varies notably across\nlanguages. In the second stage, we conduct a self-evaluation experiment to\nmeasure reasoning consistency. The results reveal a key finding: a model's\ninitial accuracy is inversely correlated with its ability to identify its own\nmistakes. Top-performing models such as Gemini 2.5 Pro are overconfident (4.34%\nTrue Negative Rate), whereas lower-performing models like LLaMA 4 Scout are\nsubstantially more self-aware (42.09% True Negative Rate). These results point\nto clear gaps in multilingual reasoning and highlight the need for models that\nnot only reason effectively but also recognize their own limitations."
                },
                "authors": [
                    {
                        "name": "Abhinav P M"
                    },
                    {
                        "name": "Ojasva Saxena"
                    },
                    {
                        "name": "Oswald C"
                    },
                    {
                        "name": "Parameswari Krishnamurthy"
                    }
                ],
                "author_detail": {
                    "name": "Parameswari Krishnamurthy"
                },
                "author": "Parameswari Krishnamurthy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.00960v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.00960v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09701v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09701v2",
                "updated": "2025-11-04T13:52:24Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    13,
                    52,
                    24,
                    1,
                    308,
                    0
                ],
                "published": "2025-06-11T13:14:01Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    13,
                    14,
                    1,
                    2,
                    162,
                    0
                ],
                "title": "ABS: Enforcing Constraint Satisfaction On Generated Sequences Via\n  Automata-Guided Beam Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ABS: Enforcing Constraint Satisfaction On Generated Sequences Via\n  Automata-Guided Beam Search"
                },
                "summary": "Sequence generation and prediction form a cornerstone of modern machine\nlearning, with applications spanning natural language processing, program\nsynthesis, and time-series forecasting. These tasks are typically modeled in an\nautoregressive fashion, where each token is generated conditional on the\npreceding ones, and beam search is commonly used to balance exploration and\nfluency during decoding. While deep learning models and Large Language Models\n(LLMs) excel at capturing statistical patterns in this setting, they remain\nill-equipped to guarantee compliance with formal constraints. In this paper, we\nintroduce ABS: a general and model-agnostic inference-time algorithm that\nguarantees compliance with any constraint that can be compiled into a\nDeterministic Finite Automaton (DFA), without requiring retraining. ABS\nleverages the DFA to guide a constrained variant of beam search: at each\ndecoding step, transitions leading to violations are masked, while remaining\npaths are dynamically re-ranked according to both the model's probabilities and\nthe automaton's acceptance structure. We formally prove that the resulting\nsequences are guaranteed to satisfy the given constraints, and we empirically\ndemonstrate that ABS also improves output quality. We validate our approach on\nthree distinct tasks: constrained image-stream classification, controlled text\ngeneration, and text infilling. In all settings, ABS achieves perfect\nconstraint satisfaction, while outperforming or matching state-of-the-art\nbaselines on standard quality metrics and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequence generation and prediction form a cornerstone of modern machine\nlearning, with applications spanning natural language processing, program\nsynthesis, and time-series forecasting. These tasks are typically modeled in an\nautoregressive fashion, where each token is generated conditional on the\npreceding ones, and beam search is commonly used to balance exploration and\nfluency during decoding. While deep learning models and Large Language Models\n(LLMs) excel at capturing statistical patterns in this setting, they remain\nill-equipped to guarantee compliance with formal constraints. In this paper, we\nintroduce ABS: a general and model-agnostic inference-time algorithm that\nguarantees compliance with any constraint that can be compiled into a\nDeterministic Finite Automaton (DFA), without requiring retraining. ABS\nleverages the DFA to guide a constrained variant of beam search: at each\ndecoding step, transitions leading to violations are masked, while remaining\npaths are dynamically re-ranked according to both the model's probabilities and\nthe automaton's acceptance structure. We formally prove that the resulting\nsequences are guaranteed to satisfy the given constraints, and we empirically\ndemonstrate that ABS also improves output quality. We validate our approach on\nthree distinct tasks: constrained image-stream classification, controlled text\ngeneration, and text infilling. In all settings, ABS achieves perfect\nconstraint satisfaction, while outperforming or matching state-of-the-art\nbaselines on standard quality metrics and efficiency."
                },
                "authors": [
                    {
                        "name": "Vincenzo Collura"
                    },
                    {
                        "name": "Karim Tit"
                    },
                    {
                        "name": "Laura Bussi"
                    },
                    {
                        "name": "Eleonora Giunchiglia"
                    },
                    {
                        "name": "Maxime Cordy"
                    }
                ],
                "author_detail": {
                    "name": "Maxime Cordy"
                },
                "author": "Maxime Cordy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09701v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09701v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02563v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02563v1",
                "updated": "2025-11-04T13:36:03Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    13,
                    36,
                    3,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T13:36:03Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    13,
                    36,
                    3,
                    1,
                    308,
                    0
                ],
                "title": "The Urban Vision Hackathon Dataset and Models: Towards Image Annotations\n  and Accurate Vision Models for Indian Traffic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Urban Vision Hackathon Dataset and Models: Towards Image Annotations\n  and Accurate Vision Models for Indian Traffic"
                },
                "summary": "This report describes the UVH-26 dataset, the first public release by\nAIM@IISc of a large-scale dataset of annotated traffic-camera images from\nIndia. The dataset comprises 26,646 high-resolution (1080p) images sampled from\n2800 Bengaluru's Safe-City CCTV cameras over a 4-week period, and subsequently\nannotated through a crowdsourced hackathon involving 565 college students from\nacross India. In total, 1.8 million bounding boxes were labeled across 14\nvehicle classes specific to India: Cycle, 2-Wheeler (Motorcycle), 3-Wheeler\n(Auto-rickshaw), LCV (Light Commercial Vehicles), Van, Tempo-traveller,\nHatchback, Sedan, SUV, MUV, Mini-bus, Bus, Truck and Other. Of these, 283k-316k\nconsensus ground truth bounding boxes and labels were derived for distinct\nobjects in the 26k images using Majority Voting and STAPLE algorithms. Further,\nwe train multiple contemporary detectors, including YOLO11-S/X, RT-DETR-S/X,\nand DAMO-YOLO-T/L using these datasets, and report accuracy based on mAP50,\nmAP75 and mAP50:95. Models trained on UVH-26 achieve 8.4-31.5% improvements in\nmAP50:95 over equivalent baseline models trained on COCO dataset, with\nRT-DETR-X showing the best performance at 0.67 (mAP50:95) as compared to 0.40\nfor COCO-trained weights for common classes (Car, Bus, and Truck). This\ndemonstrates the benefits of domain-specific training data for Indian traffic\nscenarios. The release package provides the 26k images with consensus\nannotations based on Majority Voting (UVH-26-MV) and STAPLE (UVH-26-ST) and the\n6 fine-tuned YOLO and DETR models on each of these datasets. By capturing the\nheterogeneity of Indian urban mobility directly from operational traffic-camera\nstreams, UVH-26 addresses a critical gap in existing global benchmarks, and\noffers a foundation for advancing detection, classification, and deployment of\nintelligent transportation systems in emerging nations with complex traffic\nconditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This report describes the UVH-26 dataset, the first public release by\nAIM@IISc of a large-scale dataset of annotated traffic-camera images from\nIndia. The dataset comprises 26,646 high-resolution (1080p) images sampled from\n2800 Bengaluru's Safe-City CCTV cameras over a 4-week period, and subsequently\nannotated through a crowdsourced hackathon involving 565 college students from\nacross India. In total, 1.8 million bounding boxes were labeled across 14\nvehicle classes specific to India: Cycle, 2-Wheeler (Motorcycle), 3-Wheeler\n(Auto-rickshaw), LCV (Light Commercial Vehicles), Van, Tempo-traveller,\nHatchback, Sedan, SUV, MUV, Mini-bus, Bus, Truck and Other. Of these, 283k-316k\nconsensus ground truth bounding boxes and labels were derived for distinct\nobjects in the 26k images using Majority Voting and STAPLE algorithms. Further,\nwe train multiple contemporary detectors, including YOLO11-S/X, RT-DETR-S/X,\nand DAMO-YOLO-T/L using these datasets, and report accuracy based on mAP50,\nmAP75 and mAP50:95. Models trained on UVH-26 achieve 8.4-31.5% improvements in\nmAP50:95 over equivalent baseline models trained on COCO dataset, with\nRT-DETR-X showing the best performance at 0.67 (mAP50:95) as compared to 0.40\nfor COCO-trained weights for common classes (Car, Bus, and Truck). This\ndemonstrates the benefits of domain-specific training data for Indian traffic\nscenarios. The release package provides the 26k images with consensus\nannotations based on Majority Voting (UVH-26-MV) and STAPLE (UVH-26-ST) and the\n6 fine-tuned YOLO and DETR models on each of these datasets. By capturing the\nheterogeneity of Indian urban mobility directly from operational traffic-camera\nstreams, UVH-26 addresses a critical gap in existing global benchmarks, and\noffers a foundation for advancing detection, classification, and deployment of\nintelligent transportation systems in emerging nations with complex traffic\nconditions."
                },
                "authors": [
                    {
                        "name": "Akash Sharma"
                    },
                    {
                        "name": "Chinmay Mhatre"
                    },
                    {
                        "name": "Sankalp Gawali"
                    },
                    {
                        "name": "Ruthvik Bokkasam"
                    },
                    {
                        "name": "Brij Kishore"
                    },
                    {
                        "name": "Vishwajeet Pattanaik"
                    },
                    {
                        "name": "Tarun Rambha"
                    },
                    {
                        "name": "Abdul R. Pinjari"
                    },
                    {
                        "name": "Vijay Kovvali"
                    },
                    {
                        "name": "Anirban Chakraborty"
                    },
                    {
                        "name": "Punit Rathore"
                    },
                    {
                        "name": "Raghu Krishnapuram"
                    },
                    {
                        "name": "Yogesh Simmhan"
                    }
                ],
                "author_detail": {
                    "name": "Yogesh Simmhan"
                },
                "author": "Yogesh Simmhan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02563v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02563v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09411v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09411v2",
                "updated": "2025-11-04T13:31:23Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    13,
                    31,
                    23,
                    1,
                    308,
                    0
                ],
                "published": "2025-09-11T12:46:16Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    12,
                    46,
                    16,
                    3,
                    254,
                    0
                ],
                "title": "Gaussian Copula-Based Outage Performance Analysis of Fluid Antenna\n  Systems: Channel Coefficient- or Envelope-Level Correlation Matrix?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian Copula-Based Outage Performance Analysis of Fluid Antenna\n  Systems: Channel Coefficient- or Envelope-Level Correlation Matrix?"
                },
                "summary": "Gaussian copula has been employed to evaluate the outage performance of Fluid\nAntenna Systems (FAS), with the covariance matrix reflecting the dependence\namong multivariate normal random variables (RVs). While prior studies\napproximate this matrix using the channel coefficient correlation matrix from\nJake's model, this work instead employs the channel envelope correlation\nmatrix, motivated by the fact that the multivariate normal RVs are generated by\ntransforming correlated channel envelopes. This raises an open question of\nwhether using the coefficient- or envelope-level correlation matrix yields\nbetter accuracy in accessing FAS performance. Toward this end, this paper\nexplores the benefits of using the envelope-level correlation matrix under\nfully correlated Nakagami-m fading, and develops a method for generating such\nfading channels for Monte Carlo simulations, which serve as a benchmark for\nvalidating the theoretical results. Simulation results confirm the\neffectiveness of the proposed channel modeling approach and demonstrate the\nsuperior accuracy of using the envelope-level correlation matrix, particularly\nin sparse port deployment and low-outage regime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian copula has been employed to evaluate the outage performance of Fluid\nAntenna Systems (FAS), with the covariance matrix reflecting the dependence\namong multivariate normal random variables (RVs). While prior studies\napproximate this matrix using the channel coefficient correlation matrix from\nJake's model, this work instead employs the channel envelope correlation\nmatrix, motivated by the fact that the multivariate normal RVs are generated by\ntransforming correlated channel envelopes. This raises an open question of\nwhether using the coefficient- or envelope-level correlation matrix yields\nbetter accuracy in accessing FAS performance. Toward this end, this paper\nexplores the benefits of using the envelope-level correlation matrix under\nfully correlated Nakagami-m fading, and develops a method for generating such\nfading channels for Monte Carlo simulations, which serve as a benchmark for\nvalidating the theoretical results. Simulation results confirm the\neffectiveness of the proposed channel modeling approach and demonstrate the\nsuperior accuracy of using the envelope-level correlation matrix, particularly\nin sparse port deployment and low-outage regime."
                },
                "authors": [
                    {
                        "name": "Rui Xu"
                    },
                    {
                        "name": "Yinghui Ye"
                    },
                    {
                        "name": "Xiaoli Chu"
                    },
                    {
                        "name": "Guangyue Lu"
                    },
                    {
                        "name": "Farshad Rostami Ghadi"
                    },
                    {
                        "name": "Kai-Kit Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Kit Wong"
                },
                "author": "Kai-Kit Wong",
                "arxiv_doi": "10.1109/LWC.2025.3629524",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/LWC.2025.3629524",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.09411v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09411v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24450v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24450v2",
                "updated": "2025-11-04T13:28:08Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    13,
                    28,
                    8,
                    1,
                    308,
                    0
                ],
                "published": "2025-10-28T14:13:44Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    14,
                    13,
                    44,
                    1,
                    301,
                    0
                ],
                "title": "Charting the European LLM Benchmarking Landscape: A New Taxonomy and a\n  Set of Best Practices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Charting the European LLM Benchmarking Landscape: A New Taxonomy and a\n  Set of Best Practices"
                },
                "summary": "While new benchmarks for large language models (LLMs) are being developed\ncontinuously to catch up with the growing capabilities of new models and AI in\ngeneral, using and evaluating LLMs in non-English languages remains a\nlittle-charted landscape. We give a concise overview of recent developments in\nLLM benchmarking, and then propose a new taxonomy for the categorization of\nbenchmarks that is tailored to multilingual or non-English use scenarios. We\nfurther propose a set of best practices and quality standards that could lead\nto a more coordinated development of benchmarks for European languages. Among\nother recommendations, we advocate for a higher language and culture\nsensitivity of evaluation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While new benchmarks for large language models (LLMs) are being developed\ncontinuously to catch up with the growing capabilities of new models and AI in\ngeneral, using and evaluating LLMs in non-English languages remains a\nlittle-charted landscape. We give a concise overview of recent developments in\nLLM benchmarking, and then propose a new taxonomy for the categorization of\nbenchmarks that is tailored to multilingual or non-English use scenarios. We\nfurther propose a set of best practices and quality standards that could lead\nto a more coordinated development of benchmarks for European languages. Among\nother recommendations, we advocate for a higher language and culture\nsensitivity of evaluation methods."
                },
                "authors": [
                    {
                        "name": "Špela Vintar"
                    },
                    {
                        "name": "Taja Kuzman Pungeršek"
                    },
                    {
                        "name": "Mojca Brglez"
                    },
                    {
                        "name": "Nikola Ljubešić"
                    }
                ],
                "author_detail": {
                    "name": "Nikola Ljubešić"
                },
                "author": "Nikola Ljubešić",
                "arxiv_comment": "17 pages, 1 figure, 4 tables. Submitted to the LREC 2026 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24450v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24450v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20411v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20411v2",
                "updated": "2025-11-04T13:06:45Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    13,
                    6,
                    45,
                    1,
                    308,
                    0
                ],
                "published": "2025-05-26T18:01:00Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    18,
                    1,
                    0,
                    0,
                    146,
                    0
                ],
                "title": "SWE-rebench: An Automated Pipeline for Task Collection and\n  Decontaminated Evaluation of Software Engineering Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWE-rebench: An Automated Pipeline for Task Collection and\n  Decontaminated Evaluation of Software Engineering Agents"
                },
                "summary": "LLM-based agents have shown promising capabilities in a growing range of\nsoftware engineering (SWE) tasks. However, advancing this field faces two\ncritical challenges. First, high-quality training data is scarce, especially\ndata that reflects real-world SWE scenarios, where agents must interact with\ndevelopment environments, execute code and adapt behavior based on the outcomes\nof their actions. Existing datasets are either limited to one-shot code\ngeneration or comprise small, manually curated collections of interactive\ntasks, lacking both scale and diversity. Second, the lack of fresh interactive\nSWE tasks affects evaluation of rapidly improving models, as static benchmarks\nquickly become outdated due to contamination issues. To address these\nlimitations, we introduce a novel, automated, and scalable pipeline to\ncontinuously extract real-world interactive SWE tasks from diverse GitHub\nrepositories. Using this pipeline, we construct SWE-rebench, a public dataset\ncomprising over 21,000 interactive Python-based SWE tasks, suitable for\nreinforcement learning of SWE agents at scale. Additionally, we use continuous\nsupply of fresh tasks collected using SWE-rebench methodology to build a\ncontamination-free benchmark for agentic software engineering. We compare\nresults of various LLMs on this benchmark to results on SWE-bench Verified and\nshow that performance of some language models might be inflated due to\ncontamination issues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based agents have shown promising capabilities in a growing range of\nsoftware engineering (SWE) tasks. However, advancing this field faces two\ncritical challenges. First, high-quality training data is scarce, especially\ndata that reflects real-world SWE scenarios, where agents must interact with\ndevelopment environments, execute code and adapt behavior based on the outcomes\nof their actions. Existing datasets are either limited to one-shot code\ngeneration or comprise small, manually curated collections of interactive\ntasks, lacking both scale and diversity. Second, the lack of fresh interactive\nSWE tasks affects evaluation of rapidly improving models, as static benchmarks\nquickly become outdated due to contamination issues. To address these\nlimitations, we introduce a novel, automated, and scalable pipeline to\ncontinuously extract real-world interactive SWE tasks from diverse GitHub\nrepositories. Using this pipeline, we construct SWE-rebench, a public dataset\ncomprising over 21,000 interactive Python-based SWE tasks, suitable for\nreinforcement learning of SWE agents at scale. Additionally, we use continuous\nsupply of fresh tasks collected using SWE-rebench methodology to build a\ncontamination-free benchmark for agentic software engineering. We compare\nresults of various LLMs on this benchmark to results on SWE-bench Verified and\nshow that performance of some language models might be inflated due to\ncontamination issues."
                },
                "authors": [
                    {
                        "name": "Ibragim Badertdinov"
                    },
                    {
                        "name": "Alexander Golubev"
                    },
                    {
                        "name": "Maksim Nekrashevich"
                    },
                    {
                        "name": "Anton Shevtsov"
                    },
                    {
                        "name": "Simon Karasik"
                    },
                    {
                        "name": "Andrei Andriushchenko"
                    },
                    {
                        "name": "Maria Trofimova"
                    },
                    {
                        "name": "Daria Litvintseva"
                    },
                    {
                        "name": "Boris Yangel"
                    }
                ],
                "author_detail": {
                    "name": "Boris Yangel"
                },
                "author": "Boris Yangel",
                "arxiv_comment": "Dataset: https://huggingface.co/datasets/nebius/SWE-rebench,\n  SWE-rebench leaderboard https://swe-rebench.com NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20411v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20411v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13760v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13760v2",
                "updated": "2025-11-04T13:00:21Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    13,
                    0,
                    21,
                    1,
                    308,
                    0
                ],
                "published": "2025-10-15T17:10:39Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    10,
                    39,
                    2,
                    288,
                    0
                ],
                "title": "Invited Paper: BitMedViT: Ternary-Quantized Vision Transformer for\n  Medical AI Assistants on the Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Invited Paper: BitMedViT: Ternary-Quantized Vision Transformer for\n  Medical AI Assistants on the Edge"
                },
                "summary": "Vision Transformers (ViTs) have demonstrated strong capabilities in\ninterpreting complex medical imaging data. However, their significant\ncomputational and memory demands pose challenges for deployment in real-time,\nresource-constrained mobile and wearable devices used in clinical environments.\nWe introduce, BiTMedViT, a new class of Edge ViTs serving as medical AI\nassistants that perform structured analysis of medical images directly on the\nedge. BiTMedViT utilizes ternary- quantized linear layers tailored for medical\nimaging and com- bines a training procedure with multi-query attention,\npreserving stability under ternary weights with low-precision activations.\nFurthermore, BiTMedViT employs task-aware distillation from a high-capacity\nteacher to recover accuracy lost due to extreme quantization. Lastly, we also\npresent a pipeline that maps the ternarized ViTs to a custom CUDA kernel for\nefficient memory bandwidth utilization and latency reduction on the Jetson Orin\nNano. Finally, BiTMedViT achieves 86% diagnostic accuracy (89% SOTA) on\nMedMNIST across 12 datasets, while reducing model size by 43x, memory traffic\nby 39x, and enabling 16.8 ms inference at an energy efficiency up to 41x that\nof SOTA models at 183.62 GOPs/J on the Orin Nano. Our results demonstrate a\npractical and scientifically grounded route for extreme-precision medical\nimaging ViTs deployable on the edge, narrowing the gap between algorithmic\nadvances and deployable clinical tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Transformers (ViTs) have demonstrated strong capabilities in\ninterpreting complex medical imaging data. However, their significant\ncomputational and memory demands pose challenges for deployment in real-time,\nresource-constrained mobile and wearable devices used in clinical environments.\nWe introduce, BiTMedViT, a new class of Edge ViTs serving as medical AI\nassistants that perform structured analysis of medical images directly on the\nedge. BiTMedViT utilizes ternary- quantized linear layers tailored for medical\nimaging and com- bines a training procedure with multi-query attention,\npreserving stability under ternary weights with low-precision activations.\nFurthermore, BiTMedViT employs task-aware distillation from a high-capacity\nteacher to recover accuracy lost due to extreme quantization. Lastly, we also\npresent a pipeline that maps the ternarized ViTs to a custom CUDA kernel for\nefficient memory bandwidth utilization and latency reduction on the Jetson Orin\nNano. Finally, BiTMedViT achieves 86% diagnostic accuracy (89% SOTA) on\nMedMNIST across 12 datasets, while reducing model size by 43x, memory traffic\nby 39x, and enabling 16.8 ms inference at an energy efficiency up to 41x that\nof SOTA models at 183.62 GOPs/J on the Orin Nano. Our results demonstrate a\npractical and scientifically grounded route for extreme-precision medical\nimaging ViTs deployable on the edge, narrowing the gap between algorithmic\nadvances and deployable clinical tools."
                },
                "authors": [
                    {
                        "name": "Mikolaj Walczak"
                    },
                    {
                        "name": "Uttej Kallakuri"
                    },
                    {
                        "name": "Edward Humes"
                    },
                    {
                        "name": "Xiaomin Lin"
                    },
                    {
                        "name": "Tinoosh Mohsenin"
                    }
                ],
                "author_detail": {
                    "name": "Tinoosh Mohsenin"
                },
                "author": "Tinoosh Mohsenin",
                "arxiv_comment": "Accepted at 2025 IEEE/ACM International Conf. on Computer-Aided\n  Design (ICCAD) Oct. 26-30 2025, Munich, DE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13760v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13760v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02537v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02537v1",
                "updated": "2025-11-04T12:44:54Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    12,
                    44,
                    54,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T12:44:54Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    12,
                    44,
                    54,
                    1,
                    308,
                    0
                ],
                "title": "Smart-Hiring: An Explainable end-to-end Pipeline for CV Information\n  Extraction and Job Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart-Hiring: An Explainable end-to-end Pipeline for CV Information\n  Extraction and Job Matching"
                },
                "summary": "Hiring processes often involve the manual screening of hundreds of resumes\nfor each job, a task that is time and effort consuming, error-prone, and\nsubject to human bias. This paper presents Smart-Hiring, an end-to-end Natural\nLanguage Processing (NLP) pipeline de- signed to automatically extract\nstructured information from unstructured resumes and to semantically match\ncandidates with job descriptions. The proposed system combines document\nparsing, named-entity recognition, and contextual text embedding techniques to\ncapture skills, experience, and qualifications. Using advanced NLP technics,\nSmart-Hiring encodes both resumes and job descriptions in a shared vector space\nto compute similarity scores between candidates and job postings. The pipeline\nis modular and explainable, allowing users to inspect extracted entities and\nmatching rationales. Experiments were conducted on a real-world dataset of\nresumes and job descriptions spanning multiple professional domains,\ndemonstrating the robustness and feasibility of the proposed approach. The\nsystem achieves competitive matching accuracy while preserving a high degree of\ninterpretability and transparency in its decision process. This work introduces\na scalable and practical NLP frame- work for recruitment analytics and outlines\npromising directions for bias mitigation, fairness-aware modeling, and\nlarge-scale deployment of data-driven hiring solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hiring processes often involve the manual screening of hundreds of resumes\nfor each job, a task that is time and effort consuming, error-prone, and\nsubject to human bias. This paper presents Smart-Hiring, an end-to-end Natural\nLanguage Processing (NLP) pipeline de- signed to automatically extract\nstructured information from unstructured resumes and to semantically match\ncandidates with job descriptions. The proposed system combines document\nparsing, named-entity recognition, and contextual text embedding techniques to\ncapture skills, experience, and qualifications. Using advanced NLP technics,\nSmart-Hiring encodes both resumes and job descriptions in a shared vector space\nto compute similarity scores between candidates and job postings. The pipeline\nis modular and explainable, allowing users to inspect extracted entities and\nmatching rationales. Experiments were conducted on a real-world dataset of\nresumes and job descriptions spanning multiple professional domains,\ndemonstrating the robustness and feasibility of the proposed approach. The\nsystem achieves competitive matching accuracy while preserving a high degree of\ninterpretability and transparency in its decision process. This work introduces\na scalable and practical NLP frame- work for recruitment analytics and outlines\npromising directions for bias mitigation, fairness-aware modeling, and\nlarge-scale deployment of data-driven hiring solutions."
                },
                "authors": [
                    {
                        "name": "Kenza Khelkhal"
                    },
                    {
                        "name": "Dihia Lanasri"
                    }
                ],
                "author_detail": {
                    "name": "Dihia Lanasri"
                },
                "author": "Dihia Lanasri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02537v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02537v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02534v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02534v1",
                "updated": "2025-11-04T12:40:46Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    12,
                    40,
                    46,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T12:40:46Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    12,
                    40,
                    46,
                    1,
                    308,
                    0
                ],
                "title": "Knowledge Graph-enhanced Large Language Model for Incremental Game\n  PlayTesting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Graph-enhanced Large Language Model for Incremental Game\n  PlayTesting"
                },
                "summary": "The rapid iteration and frequent updates of modern video games pose\nsignificant challenges to the efficiency and specificity of testing. Although\nautomated playtesting methods based on Large Language Models (LLMs) have shown\npromise, they often lack structured knowledge accumulation mechanisms, making\nit difficult to conduct precise and efficient testing tailored for incremental\ngame updates. To address this challenge, this paper proposes a KLPEG framework.\nThe framework constructs and maintains a Knowledge Graph (KG) to systematically\nmodel game elements, task dependencies, and causal relationships, enabling\nknowledge accumulation and reuse across versions. Building on this foundation,\nthe framework utilizes LLMs to parse natural language update logs, identify the\nscope of impact through multi-hop reasoning on the KG, enabling the generation\nof update-tailored test cases. Experiments in two representative game\nenvironments, Overcooked and Minecraft, demonstrate that KLPEG can more\naccurately locate functionalities affected by updates and complete tests in\nfewer steps, significantly improving both playtesting effectiveness and\nefficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid iteration and frequent updates of modern video games pose\nsignificant challenges to the efficiency and specificity of testing. Although\nautomated playtesting methods based on Large Language Models (LLMs) have shown\npromise, they often lack structured knowledge accumulation mechanisms, making\nit difficult to conduct precise and efficient testing tailored for incremental\ngame updates. To address this challenge, this paper proposes a KLPEG framework.\nThe framework constructs and maintains a Knowledge Graph (KG) to systematically\nmodel game elements, task dependencies, and causal relationships, enabling\nknowledge accumulation and reuse across versions. Building on this foundation,\nthe framework utilizes LLMs to parse natural language update logs, identify the\nscope of impact through multi-hop reasoning on the KG, enabling the generation\nof update-tailored test cases. Experiments in two representative game\nenvironments, Overcooked and Minecraft, demonstrate that KLPEG can more\naccurately locate functionalities affected by updates and complete tests in\nfewer steps, significantly improving both playtesting effectiveness and\nefficiency."
                },
                "authors": [
                    {
                        "name": "Enhong Mu"
                    },
                    {
                        "name": "Jinyu Cai"
                    },
                    {
                        "name": "Yijun Lu"
                    },
                    {
                        "name": "Mingyue Zhang"
                    },
                    {
                        "name": "Kenji Tei"
                    },
                    {
                        "name": "Jialong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jialong Li"
                },
                "author": "Jialong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02534v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02534v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02531v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02531v1",
                "updated": "2025-11-04T12:34:46Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    12,
                    34,
                    46,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T12:34:46Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    12,
                    34,
                    46,
                    1,
                    308,
                    0
                ],
                "title": "Causal Graph Neural Networks for Healthcare",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Graph Neural Networks for Healthcare"
                },
                "summary": "Healthcare artificial intelligence systems routinely fail when deployed\nacross institutions, with documented performance drops and perpetuation of\ndiscriminatory patterns embedded in historical data. This brittleness stems, in\npart, from learning statistical associations rather than causal mechanisms.\nCausal graph neural networks address this triple crisis of distribution shift,\ndiscrimination, and inscrutability by combining graph-based representations of\nbiomedical data with causal inference principles to learn invariant mechanisms\nrather than spurious correlations. This Review examines methodological\nfoundations spanning structural causal models, disentangled causal\nrepresentation learning, and techniques for interventional prediction and\ncounterfactual reasoning on graphs. We analyse applications demonstrating\nclinical value across psychiatric diagnosis through brain network analysis,\ncancer subtyping via multi-omics causal integration, continuous physiological\nmonitoring with mechanistic interpretation, and drug recommendation correcting\nprescription bias. These advances establish foundations for patient-specific\nCausal Digital Twins, enabling in silico clinical experimentation, with\nintegration of large language models for hypothesis generation and causal graph\nneural networks for mechanistic validation. Substantial barriers remain,\nincluding computational requirements precluding real-time deployment,\nvalidation challenges demanding multi-modal evidence triangulation beyond\ncross-validation, and risks of causal-washing where methods employ causal\nterminology without rigorous evidentiary support. We propose tiered frameworks\ndistinguishing causally-inspired architectures from causally-validated\ndiscoveries and identify critical research priorities making causal rather than\npurely associational claims.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Healthcare artificial intelligence systems routinely fail when deployed\nacross institutions, with documented performance drops and perpetuation of\ndiscriminatory patterns embedded in historical data. This brittleness stems, in\npart, from learning statistical associations rather than causal mechanisms.\nCausal graph neural networks address this triple crisis of distribution shift,\ndiscrimination, and inscrutability by combining graph-based representations of\nbiomedical data with causal inference principles to learn invariant mechanisms\nrather than spurious correlations. This Review examines methodological\nfoundations spanning structural causal models, disentangled causal\nrepresentation learning, and techniques for interventional prediction and\ncounterfactual reasoning on graphs. We analyse applications demonstrating\nclinical value across psychiatric diagnosis through brain network analysis,\ncancer subtyping via multi-omics causal integration, continuous physiological\nmonitoring with mechanistic interpretation, and drug recommendation correcting\nprescription bias. These advances establish foundations for patient-specific\nCausal Digital Twins, enabling in silico clinical experimentation, with\nintegration of large language models for hypothesis generation and causal graph\nneural networks for mechanistic validation. Substantial barriers remain,\nincluding computational requirements precluding real-time deployment,\nvalidation challenges demanding multi-modal evidence triangulation beyond\ncross-validation, and risks of causal-washing where methods employ causal\nterminology without rigorous evidentiary support. We propose tiered frameworks\ndistinguishing causally-inspired architectures from causally-validated\ndiscoveries and identify critical research priorities making causal rather than\npurely associational claims."
                },
                "authors": [
                    {
                        "name": "Munib Mesinovic"
                    },
                    {
                        "name": "Max Buhlan"
                    },
                    {
                        "name": "Tingting Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Tingting Zhu"
                },
                "author": "Tingting Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02531v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02531v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12007v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12007v3",
                "updated": "2025-11-04T12:25:46Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    12,
                    25,
                    46,
                    1,
                    308,
                    0
                ],
                "published": "2025-04-16T12:01:03Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    12,
                    1,
                    3,
                    2,
                    106,
                    0
                ],
                "title": "Diffusion Generative Recommendation with Continuous Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Generative Recommendation with Continuous Tokens"
                },
                "summary": "Recent advances in generative artificial intelligence, particularly large\nlanguage models (LLMs), have opened new opportunities for enhancing recommender\nsystems (RecSys). Most existing LLM-based RecSys approaches operate in a\ndiscrete space, using vector-quantized tokenizers to align with the inherent\ndiscrete nature of language models. However, these quantization methods often\nresult in lossy tokenization and suboptimal learning, primarily due to\ninaccurate gradient propagation caused by the non-differentiable argmin\noperation in standard vector quantization. Inspired by the emerging trend of\nembracing continuous tokens in language models, we propose ContRec, a novel\nframework that seamlessly integrates continuous tokens into LLM-based RecSys.\nSpecifically, ContRec consists of two key modules: a sigma-VAE Tokenizer, which\nencodes users/items with continuous tokens; and a Dispersive Diffusion module,\nwhich captures implicit user preference. The tokenizer is trained with a\ncontinuous Variational Auto-Encoder (VAE) objective, where three effective\ntechniques are adopted to avoid representation collapse. By conditioning on the\npreviously generated tokens of the LLM backbone during user modeling, the\nDispersive Diffusion module performs a conditional diffusion process with a\nnovel Dispersive Loss, enabling high-quality user preference generation through\nnext-token diffusion. Finally, ContRec leverages both the textual reasoning\noutput from the LLM and the latent representations produced by the diffusion\nmodel for Top-K item retrieval, thereby delivering comprehensive recommendation\nresults. Extensive experiments on four datasets demonstrate that ContRec\nconsistently outperforms both traditional and SOTA LLM-based recommender\nsystems. Our results highlight the potential of continuous tokenization and\ngenerative modeling for advancing the next generation of recommender systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in generative artificial intelligence, particularly large\nlanguage models (LLMs), have opened new opportunities for enhancing recommender\nsystems (RecSys). Most existing LLM-based RecSys approaches operate in a\ndiscrete space, using vector-quantized tokenizers to align with the inherent\ndiscrete nature of language models. However, these quantization methods often\nresult in lossy tokenization and suboptimal learning, primarily due to\ninaccurate gradient propagation caused by the non-differentiable argmin\noperation in standard vector quantization. Inspired by the emerging trend of\nembracing continuous tokens in language models, we propose ContRec, a novel\nframework that seamlessly integrates continuous tokens into LLM-based RecSys.\nSpecifically, ContRec consists of two key modules: a sigma-VAE Tokenizer, which\nencodes users/items with continuous tokens; and a Dispersive Diffusion module,\nwhich captures implicit user preference. The tokenizer is trained with a\ncontinuous Variational Auto-Encoder (VAE) objective, where three effective\ntechniques are adopted to avoid representation collapse. By conditioning on the\npreviously generated tokens of the LLM backbone during user modeling, the\nDispersive Diffusion module performs a conditional diffusion process with a\nnovel Dispersive Loss, enabling high-quality user preference generation through\nnext-token diffusion. Finally, ContRec leverages both the textual reasoning\noutput from the LLM and the latent representations produced by the diffusion\nmodel for Top-K item retrieval, thereby delivering comprehensive recommendation\nresults. Extensive experiments on four datasets demonstrate that ContRec\nconsistently outperforms both traditional and SOTA LLM-based recommender\nsystems. Our results highlight the potential of continuous tokenization and\ngenerative modeling for advancing the next generation of recommender systems."
                },
                "authors": [
                    {
                        "name": "Haohao Qu"
                    },
                    {
                        "name": "Shanru Lin"
                    },
                    {
                        "name": "Yujuan Ding"
                    },
                    {
                        "name": "Yiqi Wang"
                    },
                    {
                        "name": "Wenqi Fan"
                    }
                ],
                "author_detail": {
                    "name": "Wenqi Fan"
                },
                "author": "Wenqi Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12007v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12007v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27328v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27328v2",
                "updated": "2025-11-04T12:25:30Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    12,
                    25,
                    30,
                    1,
                    308,
                    0
                ],
                "published": "2025-10-31T09:57:19Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    9,
                    57,
                    19,
                    4,
                    304,
                    0
                ],
                "title": "A Unified Representation Underlying the Judgment of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Representation Underlying the Judgment of Large Language\n  Models"
                },
                "summary": "A central architectural question for both biological and artificial\nintelligence is whether judgment relies on specialized modules or a unified,\ndomain-general resource. While the discovery of decodable neural\nrepresentations for distinct concepts in Large Language Models (LLMs) has\nsuggested a modular architecture, whether these representations are truly\nindependent systems remains an open question. Here we provide evidence for a\nconvergent architecture for evaluative judgment. Across a range of LLMs, we\nfind that diverse evaluative judgments are computed along a dominant dimension,\nwhich we term the Valence-Assent Axis (VAA). This axis jointly encodes\nsubjective valence (\"what is good\") and the model's assent to factual claims\n(\"what is true\"). Through direct interventions, we demonstrate this axis drives\na critical mechanism, which is identified as the subordination of reasoning:\nthe VAA functions as a control signal that steers the generative process to\nconstruct a rationale consistent with its evaluative state, even at the cost of\nfactual accuracy. Our discovery offers a mechanistic account for response bias\nand hallucination, revealing how an architecture that promotes coherent\njudgment can systematically undermine faithful reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A central architectural question for both biological and artificial\nintelligence is whether judgment relies on specialized modules or a unified,\ndomain-general resource. While the discovery of decodable neural\nrepresentations for distinct concepts in Large Language Models (LLMs) has\nsuggested a modular architecture, whether these representations are truly\nindependent systems remains an open question. Here we provide evidence for a\nconvergent architecture for evaluative judgment. Across a range of LLMs, we\nfind that diverse evaluative judgments are computed along a dominant dimension,\nwhich we term the Valence-Assent Axis (VAA). This axis jointly encodes\nsubjective valence (\"what is good\") and the model's assent to factual claims\n(\"what is true\"). Through direct interventions, we demonstrate this axis drives\na critical mechanism, which is identified as the subordination of reasoning:\nthe VAA functions as a control signal that steers the generative process to\nconstruct a rationale consistent with its evaluative state, even at the cost of\nfactual accuracy. Our discovery offers a mechanistic account for response bias\nand hallucination, revealing how an architecture that promotes coherent\njudgment can systematically undermine faithful reasoning."
                },
                "authors": [
                    {
                        "name": "Yi-Long Lu"
                    },
                    {
                        "name": "Jiajun Song"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27328v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27328v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02521v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02521v1",
                "updated": "2025-11-04T12:16:37Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    12,
                    16,
                    37,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T12:16:37Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    12,
                    16,
                    37,
                    1,
                    308,
                    0
                ],
                "title": "Large Lemma Miners: Can LLMs do Induction Proofs for Hardware?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Lemma Miners: Can LLMs do Induction Proofs for Hardware?"
                },
                "summary": "Large Language Models (LLMs) have shown potential for solving mathematical\ntasks. We show that LLMs can be utilized to generate proofs by induction for\nhardware verification and thereby replace some of the manual work done by\nFormal Verification engineers and deliver industrial value. We present a\nneurosymbolic approach that includes two prompting frameworks to generate\ncandidate invariants, which are checked using a formal, symbolic tool. Our\nresults indicate that with sufficient reprompting, LLMs are able to generate\ninductive arguments for mid-size open-source RTL designs. For $87\\%$ of our\nproblem set, at least one of the prompt setups succeeded in producing a\nprovably correct inductive argument.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown potential for solving mathematical\ntasks. We show that LLMs can be utilized to generate proofs by induction for\nhardware verification and thereby replace some of the manual work done by\nFormal Verification engineers and deliver industrial value. We present a\nneurosymbolic approach that includes two prompting frameworks to generate\ncandidate invariants, which are checked using a formal, symbolic tool. Our\nresults indicate that with sufficient reprompting, LLMs are able to generate\ninductive arguments for mid-size open-source RTL designs. For $87\\%$ of our\nproblem set, at least one of the prompt setups succeeded in producing a\nprovably correct inductive argument."
                },
                "authors": [
                    {
                        "name": "Romy Peled"
                    },
                    {
                        "name": "Daniel Kroening"
                    },
                    {
                        "name": "Michael Tautschnig"
                    },
                    {
                        "name": "Yakir Vizel"
                    }
                ],
                "author_detail": {
                    "name": "Yakir Vizel"
                },
                "author": "Yakir Vizel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02521v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02521v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03901v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03901v2",
                "updated": "2025-11-04T12:14:28Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    12,
                    14,
                    28,
                    1,
                    308,
                    0
                ],
                "published": "2025-05-06T18:16:08Z",
                "published_parsed": [
                    2025,
                    5,
                    6,
                    18,
                    16,
                    8,
                    1,
                    126,
                    0
                ],
                "title": "Unveiling the Role of ChatGPT in Software Development: Insights from\n  Developer-ChatGPT Interactions on GitHub",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling the Role of ChatGPT in Software Development: Insights from\n  Developer-ChatGPT Interactions on GitHub"
                },
                "summary": "The advent of Large Language Models (LLMs) has introduced a new paradigm in\nsoftware engineering, with generative AI tools like ChatGPT gaining widespread\nadoption among developers. While ChatGPT's potential has been extensively\ndiscussed, there is limited empirical evidence exploring its real-world usage\nby developers. This study bridges this gap by conducting a large-scale\nempirical analysis of ChatGPT-assisted development activities, leveraging a\ncurated dataset, DevChat, comprising 2,547 unique shared ChatGPT links\ncollected from GitHub between May 2023 and June 2024. Our study examines the\ncharacteristics of ChatGPT's usage on GitHub (including the tendency, prompt\nturns distribution, and link descriptions) and identifies five categories of\ndevelopers' purposes for sharing developer-ChatGPT conversations during\nsoftware development. Additionally, we analyzed the development-related\nactivities where developers shared ChatGPT links to facilitate their workflows.\nWe then established a mapping framework among data sources, activities, and SE\ntasks associated with these shared ChatGPT links. Our study offers a\ncomprehensive view of ChatGPT's application in real-world software development\nscenarios and provides a foundation for its future integration into software\ndevelopment workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of Large Language Models (LLMs) has introduced a new paradigm in\nsoftware engineering, with generative AI tools like ChatGPT gaining widespread\nadoption among developers. While ChatGPT's potential has been extensively\ndiscussed, there is limited empirical evidence exploring its real-world usage\nby developers. This study bridges this gap by conducting a large-scale\nempirical analysis of ChatGPT-assisted development activities, leveraging a\ncurated dataset, DevChat, comprising 2,547 unique shared ChatGPT links\ncollected from GitHub between May 2023 and June 2024. Our study examines the\ncharacteristics of ChatGPT's usage on GitHub (including the tendency, prompt\nturns distribution, and link descriptions) and identifies five categories of\ndevelopers' purposes for sharing developer-ChatGPT conversations during\nsoftware development. Additionally, we analyzed the development-related\nactivities where developers shared ChatGPT links to facilitate their workflows.\nWe then established a mapping framework among data sources, activities, and SE\ntasks associated with these shared ChatGPT links. Our study offers a\ncomprehensive view of ChatGPT's application in real-world software development\nscenarios and provides a foundation for its future integration into software\ndevelopment workflows."
                },
                "authors": [
                    {
                        "name": "Ruiyin Li"
                    },
                    {
                        "name": "Peng Liang"
                    },
                    {
                        "name": "Yifei Wang"
                    },
                    {
                        "name": "Yangxiao Cai"
                    },
                    {
                        "name": "Weisong Sun"
                    },
                    {
                        "name": "Zengyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Zengyang Li"
                },
                "author": "Zengyang Li",
                "arxiv_comment": "29 pages, 11 images, 3 tables, Manuscript revision submitted to a\n  journal (2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03901v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03901v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02770v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02770v5",
                "updated": "2025-11-04T12:04:06Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    12,
                    4,
                    6,
                    1,
                    308,
                    0
                ],
                "published": "2025-02-04T23:26:10Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    23,
                    26,
                    10,
                    1,
                    35,
                    0
                ],
                "title": "Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$ Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$ Pruning"
                },
                "summary": "Leveraging attention sparsity to accelerate long-context large language\nmodels (LLMs) has been a hot research topic. However, current algorithms such\nas sparse attention or key-value (KV) cache compression tend to use a fixed\nbudget, which presents a significant challenge during deployment because it\nfails to account for the dynamic nature of real-world scenarios, where the\noptimal balance between accuracy and efficiency can vary greatly. In this\npaper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse\nattention can surprisingly achieve adaptive budgeting. Based on this, we\npropose Twilight, a framework to bring adaptive sparsity to any existing sparse\nattention algorithm without sacrificing their accuracy. Empirical results show\nthat Twilight can adaptively prune at most 98% of redundant tokens, leading to\n$15.4\\times$ acceleration in self-attention operations and $3.9\\times$\nacceleration in end-to-end per token latency in long context LLM decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging attention sparsity to accelerate long-context large language\nmodels (LLMs) has been a hot research topic. However, current algorithms such\nas sparse attention or key-value (KV) cache compression tend to use a fixed\nbudget, which presents a significant challenge during deployment because it\nfails to account for the dynamic nature of real-world scenarios, where the\noptimal balance between accuracy and efficiency can vary greatly. In this\npaper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse\nattention can surprisingly achieve adaptive budgeting. Based on this, we\npropose Twilight, a framework to bring adaptive sparsity to any existing sparse\nattention algorithm without sacrificing their accuracy. Empirical results show\nthat Twilight can adaptively prune at most 98% of redundant tokens, leading to\n$15.4\\times$ acceleration in self-attention operations and $3.9\\times$\nacceleration in end-to-end per token latency in long context LLM decoding."
                },
                "authors": [
                    {
                        "name": "Chaofan Lin"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Hanshuo Wang"
                    },
                    {
                        "name": "Tian Tang"
                    },
                    {
                        "name": "Boyu Tian"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Mingyu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Mingyu Gao"
                },
                "author": "Mingyu Gao",
                "arxiv_comment": "To appear on NeurIPS 2025 (spotlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02770v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02770v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.00926v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.00926v2",
                "updated": "2025-11-04T11:52:23Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    11,
                    52,
                    23,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-02T13:09:56Z",
                "published_parsed": [
                    2025,
                    11,
                    2,
                    13,
                    9,
                    56,
                    6,
                    306,
                    0
                ],
                "title": "LLMs Position Themselves as More Rational Than Humans: Emergence of AI\n  Self-Awareness Measured Through Game Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Position Themselves as More Rational Than Humans: Emergence of AI\n  Self-Awareness Measured Through Game Theory"
                },
                "summary": "As Large Language Models (LLMs) grow in capability, do they develop\nself-awareness as an emergent behavior? And if so, can we measure it? We\nintroduce the AI Self-Awareness Index (AISAI), a game-theoretic framework for\nmeasuring self-awareness through strategic differentiation. Using the \"Guess\n2/3 of Average\" game, we test 28 models (OpenAI, Anthropic, Google) across\n4,200 trials with three opponent framings: (A) against humans, (B) against\nother AI models, and (C) against AI models like you. We operationalize\nself-awareness as the capacity to differentiate strategic reasoning based on\nopponent type. Finding 1: Self-awareness emerges with model advancement. The\nmajority of advanced models (21/28, 75%) demonstrate clear self-awareness,\nwhile older/smaller models show no differentiation. Finding 2: Self-aware\nmodels rank themselves as most rational. Among the 21 models with\nself-awareness, a consistent rationality hierarchy emerges: Self > Other AIs >\nHumans, with large AI attribution effects and moderate self-preferencing. These\nfindings reveal that self-awareness is an emergent capability of advanced LLMs,\nand that self-aware models systematically perceive themselves as more rational\nthan humans. This has implications for AI alignment, human-AI collaboration,\nand understanding AI beliefs about human capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) grow in capability, do they develop\nself-awareness as an emergent behavior? And if so, can we measure it? We\nintroduce the AI Self-Awareness Index (AISAI), a game-theoretic framework for\nmeasuring self-awareness through strategic differentiation. Using the \"Guess\n2/3 of Average\" game, we test 28 models (OpenAI, Anthropic, Google) across\n4,200 trials with three opponent framings: (A) against humans, (B) against\nother AI models, and (C) against AI models like you. We operationalize\nself-awareness as the capacity to differentiate strategic reasoning based on\nopponent type. Finding 1: Self-awareness emerges with model advancement. The\nmajority of advanced models (21/28, 75%) demonstrate clear self-awareness,\nwhile older/smaller models show no differentiation. Finding 2: Self-aware\nmodels rank themselves as most rational. Among the 21 models with\nself-awareness, a consistent rationality hierarchy emerges: Self > Other AIs >\nHumans, with large AI attribution effects and moderate self-preferencing. These\nfindings reveal that self-awareness is an emergent capability of advanced LLMs,\nand that self-aware models systematically perceive themselves as more rational\nthan humans. This has implications for AI alignment, human-AI collaboration,\nand understanding AI beliefs about human capabilities."
                },
                "authors": [
                    {
                        "name": "Kyung-Hoon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Kyung-Hoon Kim"
                },
                "author": "Kyung-Hoon Kim",
                "arxiv_comment": "19 pages, 6 figures, 28 models tested across 4,200 trials",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.00926v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.00926v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11150v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11150v4",
                "updated": "2025-11-04T11:48:41Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    11,
                    48,
                    41,
                    1,
                    308,
                    0
                ],
                "published": "2025-02-16T14:51:44Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    14,
                    51,
                    44,
                    6,
                    47,
                    0
                ],
                "title": "Readability Formulas, Systems and LLMs are Poor Predictors of Reading\n  Ease",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Readability Formulas, Systems and LLMs are Poor Predictors of Reading\n  Ease"
                },
                "summary": "Methods for scoring text readability have been studied for over a century,\nand are widely used in research and in user-facing applications in many\ndomains. Thus far, the development and evaluation of such methods have\nprimarily relied on two types of offline behavioral data, performance on\nreading comprehension tests and ratings of text readability levels. In this\nwork, we instead focus on a fundamental and understudied aspect of readability,\nreal-time reading ease, captured with online reading measures using eye\ntracking. We introduce an evaluation framework for readability scoring methods\nwhich quantifies their ability to account for reading ease, while controlling\nfor content variation across texts. Applying this evaluation to prominent\ntraditional readability formulas, modern machine learning systems, frontier\nLarge Language Models and commercial systems used in education, suggests that\nthey are all poor predictors of reading ease in English. This outcome holds\nacross native and non-native speakers, reading regimes, and textual units of\ndifferent lengths. The evaluation further reveals that existing methods are\noften outperformed by word properties commonly used in psycholinguistics for\nprediction of reading times. Our results highlight a fundamental limitation of\nexisting approaches to readability scoring, the utility of psycholinguistics\nfor readability research, and the need for new, cognitively driven readability\nscoring approaches that can better account for reading ease.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Methods for scoring text readability have been studied for over a century,\nand are widely used in research and in user-facing applications in many\ndomains. Thus far, the development and evaluation of such methods have\nprimarily relied on two types of offline behavioral data, performance on\nreading comprehension tests and ratings of text readability levels. In this\nwork, we instead focus on a fundamental and understudied aspect of readability,\nreal-time reading ease, captured with online reading measures using eye\ntracking. We introduce an evaluation framework for readability scoring methods\nwhich quantifies their ability to account for reading ease, while controlling\nfor content variation across texts. Applying this evaluation to prominent\ntraditional readability formulas, modern machine learning systems, frontier\nLarge Language Models and commercial systems used in education, suggests that\nthey are all poor predictors of reading ease in English. This outcome holds\nacross native and non-native speakers, reading regimes, and textual units of\ndifferent lengths. The evaluation further reveals that existing methods are\noften outperformed by word properties commonly used in psycholinguistics for\nprediction of reading times. Our results highlight a fundamental limitation of\nexisting approaches to readability scoring, the utility of psycholinguistics\nfor readability research, and the need for new, cognitively driven readability\nscoring approaches that can better account for reading ease."
                },
                "authors": [
                    {
                        "name": "Keren Gruteke Klein"
                    },
                    {
                        "name": "Shachar Frenkel"
                    },
                    {
                        "name": "Omer Shubi"
                    },
                    {
                        "name": "Yevgeni Berzak"
                    }
                ],
                "author_detail": {
                    "name": "Yevgeni Berzak"
                },
                "author": "Yevgeni Berzak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11150v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11150v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.00847v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.00847v2",
                "updated": "2025-11-04T11:48:22Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    11,
                    48,
                    22,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-02T08:18:20Z",
                "published_parsed": [
                    2025,
                    11,
                    2,
                    8,
                    18,
                    20,
                    6,
                    306,
                    0
                ],
                "title": "Pay for The Second-Best Service: A Game-Theoretic Approach Against\n  Dishonest LLM Providers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pay for The Second-Best Service: A Game-Theoretic Approach Against\n  Dishonest LLM Providers"
                },
                "summary": "The widespread adoption of Large Language Models (LLMs) through Application\nProgramming Interfaces (APIs) induces a critical vulnerability: the potential\nfor dishonest manipulation by service providers. This manipulation can manifest\nin various forms, such as secretly substituting a proclaimed high-performance\nmodel with a low-cost alternative, or inflating responses with meaningless\ntokens to increase billing. This work tackles the issue through the lens of\nalgorithmic game theory and mechanism design. We are the first to propose a\nformal economic model for a realistic user-provider ecosystem, where a user can\niteratively delegate $T$ queries to multiple model providers, and providers can\nengage in a range of strategic behaviors. As our central contribution, we prove\nthat for a continuous strategy space and any $\\epsilon\\in(0,\\frac12)$, there\nexists an approximate incentive-compatible mechanism with an additive\napproximation ratio of $O(T^{1-\\epsilon}\\log T)$, and a guaranteed quasi-linear\nsecond-best user utility. We also prove an impossibility result, stating that\nno mechanism can guarantee an expected user utility that is asymptotically\nbetter than our mechanism. Furthermore, we demonstrate the effectiveness of our\nmechanism in simulation experiments with real-world API settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread adoption of Large Language Models (LLMs) through Application\nProgramming Interfaces (APIs) induces a critical vulnerability: the potential\nfor dishonest manipulation by service providers. This manipulation can manifest\nin various forms, such as secretly substituting a proclaimed high-performance\nmodel with a low-cost alternative, or inflating responses with meaningless\ntokens to increase billing. This work tackles the issue through the lens of\nalgorithmic game theory and mechanism design. We are the first to propose a\nformal economic model for a realistic user-provider ecosystem, where a user can\niteratively delegate $T$ queries to multiple model providers, and providers can\nengage in a range of strategic behaviors. As our central contribution, we prove\nthat for a continuous strategy space and any $\\epsilon\\in(0,\\frac12)$, there\nexists an approximate incentive-compatible mechanism with an additive\napproximation ratio of $O(T^{1-\\epsilon}\\log T)$, and a guaranteed quasi-linear\nsecond-best user utility. We also prove an impossibility result, stating that\nno mechanism can guarantee an expected user utility that is asymptotically\nbetter than our mechanism. Furthermore, we demonstrate the effectiveness of our\nmechanism in simulation experiments with real-world API settings."
                },
                "authors": [
                    {
                        "name": "Yuhan Cao"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Sitong Liu"
                    },
                    {
                        "name": "Miao Li"
                    },
                    {
                        "name": "Yixin Tao"
                    },
                    {
                        "name": "Tianxing He"
                    }
                ],
                "author_detail": {
                    "name": "Tianxing He"
                },
                "author": "Tianxing He",
                "arxiv_comment": "13 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.00847v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.00847v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02503v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02503v1",
                "updated": "2025-11-04T11:43:05Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    11,
                    43,
                    5,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T11:43:05Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    11,
                    43,
                    5,
                    1,
                    308,
                    0
                ],
                "title": "Adapting General-Purpose Foundation Models for X-ray Ptychography in\n  Low-Data Regimes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting General-Purpose Foundation Models for X-ray Ptychography in\n  Low-Data Regimes"
                },
                "summary": "The automation of workflows in advanced microscopy is a key goal where\nfoundation models like Language Models (LLMs) and Vision-Language Models (VLMs)\nshow great potential. However, adapting these general-purpose models for\nspecialized scientific tasks is critical, and the optimal domain adaptation\nstrategy is often unclear. To address this, we introduce PtychoBench, a new\nmulti-modal, multi-task benchmark for ptychographic analysis. Using this\nbenchmark, we systematically compare two specialization strategies: Supervised\nFine-Tuning (SFT) and In-Context Learning (ICL). We evaluate these strategies\non a visual artifact detection task with VLMs and a textual parameter\nrecommendation task with LLMs in a data-scarce regime. Our findings reveal that\nthe optimal specialization pathway is task-dependent. For the visual task, SFT\nand ICL are highly complementary, with a fine-tuned model guided by\ncontext-aware examples achieving the highest mean performance (Micro-F1 of\n0.728). Conversely, for the textual task, ICL on a large base model is the\nsuperior strategy, reaching a peak Micro-F1 of 0.847 and outperforming a\npowerful \"super-expert\" SFT model (0-shot Micro-F1 of 0.839). We also confirm\nthe superiority of context-aware prompting and identify a consistent contextual\ninterference phenomenon in fine-tuned models. These results, benchmarked\nagainst strong baselines including GPT-4o and a DINOv3-based classifier, offer\nkey observations for AI in science: the optimal specialization path in our\nbenchmark is dependent on the task modality, offering a clear framework for\ndeveloping more effective science-based agentic systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The automation of workflows in advanced microscopy is a key goal where\nfoundation models like Language Models (LLMs) and Vision-Language Models (VLMs)\nshow great potential. However, adapting these general-purpose models for\nspecialized scientific tasks is critical, and the optimal domain adaptation\nstrategy is often unclear. To address this, we introduce PtychoBench, a new\nmulti-modal, multi-task benchmark for ptychographic analysis. Using this\nbenchmark, we systematically compare two specialization strategies: Supervised\nFine-Tuning (SFT) and In-Context Learning (ICL). We evaluate these strategies\non a visual artifact detection task with VLMs and a textual parameter\nrecommendation task with LLMs in a data-scarce regime. Our findings reveal that\nthe optimal specialization pathway is task-dependent. For the visual task, SFT\nand ICL are highly complementary, with a fine-tuned model guided by\ncontext-aware examples achieving the highest mean performance (Micro-F1 of\n0.728). Conversely, for the textual task, ICL on a large base model is the\nsuperior strategy, reaching a peak Micro-F1 of 0.847 and outperforming a\npowerful \"super-expert\" SFT model (0-shot Micro-F1 of 0.839). We also confirm\nthe superiority of context-aware prompting and identify a consistent contextual\ninterference phenomenon in fine-tuned models. These results, benchmarked\nagainst strong baselines including GPT-4o and a DINOv3-based classifier, offer\nkey observations for AI in science: the optimal specialization path in our\nbenchmark is dependent on the task modality, offering a clear framework for\ndeveloping more effective science-based agentic systems."
                },
                "authors": [
                    {
                        "name": "Robinson Umeike"
                    },
                    {
                        "name": "Neil Getty"
                    },
                    {
                        "name": "Yin Xiangyu"
                    },
                    {
                        "name": "Yi Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Jiang"
                },
                "author": "Yi Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02503v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02503v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07067v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07067v5",
                "updated": "2025-11-04T11:34:23Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    11,
                    34,
                    23,
                    1,
                    308,
                    0
                ],
                "published": "2024-12-10T00:19:28Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    0,
                    19,
                    28,
                    1,
                    345,
                    0
                ],
                "title": "MoE-CAP: Benchmarking Cost, Accuracy and Performance of Sparse\n  Mixture-of-Experts Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE-CAP: Benchmarking Cost, Accuracy and Performance of Sparse\n  Mixture-of-Experts Systems"
                },
                "summary": "The sparse Mixture-of-Experts (MoE) architecture is increasingly favored for\nscaling Large Language Models (LLMs) efficiently, but it depends on\nheterogeneous compute and memory resources. These factors jointly affect system\nCost, Accuracy, and Performance (CAP), making trade-offs inevitable. Existing\nbenchmarks often fail to capture these trade-offs accurately, complicating\npractical deployment decisions. To address this, we introduce MoE-CAP, a\nbenchmark specifically designed for MoE systems. Our analysis reveals that\nachieving an optimal balance across CAP is difficult with current hardware; MoE\nsystems typically optimize two of the three dimensions at the expense of the\nthird-a dynamic we term the MoE-CAP trade-off. To visualize this, we propose\nthe CAP Radar Diagram. We further introduce sparsity-aware performance\nmetrics-Sparse Memory Bandwidth Utilization (S-MBU) and Sparse Model FLOPS\nUtilization (S-MFU)-to enable accurate performance benchmarking of MoE systems\nacross diverse hardware platforms and deployment scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The sparse Mixture-of-Experts (MoE) architecture is increasingly favored for\nscaling Large Language Models (LLMs) efficiently, but it depends on\nheterogeneous compute and memory resources. These factors jointly affect system\nCost, Accuracy, and Performance (CAP), making trade-offs inevitable. Existing\nbenchmarks often fail to capture these trade-offs accurately, complicating\npractical deployment decisions. To address this, we introduce MoE-CAP, a\nbenchmark specifically designed for MoE systems. Our analysis reveals that\nachieving an optimal balance across CAP is difficult with current hardware; MoE\nsystems typically optimize two of the three dimensions at the expense of the\nthird-a dynamic we term the MoE-CAP trade-off. To visualize this, we propose\nthe CAP Radar Diagram. We further introduce sparsity-aware performance\nmetrics-Sparse Memory Bandwidth Utilization (S-MBU) and Sparse Model FLOPS\nUtilization (S-MFU)-to enable accurate performance benchmarking of MoE systems\nacross diverse hardware platforms and deployment scenarios."
                },
                "authors": [
                    {
                        "name": "Yinsicheng Jiang"
                    },
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Yeqi Huang"
                    },
                    {
                        "name": "Ping Nie"
                    },
                    {
                        "name": "Zhan Lu"
                    },
                    {
                        "name": "Leyang Xue"
                    },
                    {
                        "name": "Congjie He"
                    },
                    {
                        "name": "Man-Kit Sit"
                    },
                    {
                        "name": "Jilong Xue"
                    },
                    {
                        "name": "Li Dong"
                    },
                    {
                        "name": "Ziming Miao"
                    },
                    {
                        "name": "Dayou Du"
                    },
                    {
                        "name": "Tairan Xu"
                    },
                    {
                        "name": "Kai Zou"
                    },
                    {
                        "name": "Edoardo Ponti"
                    },
                    {
                        "name": "Luo Mai"
                    }
                ],
                "author_detail": {
                    "name": "Luo Mai"
                },
                "author": "Luo Mai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07067v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07067v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02490v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02490v1",
                "updated": "2025-11-04T11:27:03Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    11,
                    27,
                    3,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T11:27:03Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    11,
                    27,
                    3,
                    1,
                    308,
                    0
                ],
                "title": "BRAINS: A Retrieval-Augmented System for Alzheimer's Detection and\n  Monitoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BRAINS: A Retrieval-Augmented System for Alzheimer's Detection and\n  Monitoring"
                },
                "summary": "As the global burden of Alzheimer's disease (AD) continues to grow, early and\naccurate detection has become increasingly critical, especially in regions with\nlimited access to advanced diagnostic tools. We propose BRAINS (Biomedical\nRetrieval-Augmented Intelligence for Neurodegeneration Screening) to address\nthis challenge. This novel system harnesses the powerful reasoning capabilities\nof Large Language Models (LLMs) for Alzheimer's detection and monitoring.\nBRAINS features a dual-module architecture: a cognitive diagnostic module and a\ncase-retrieval module. The Diagnostic Module utilizes LLMs fine-tuned on\ncognitive and neuroimaging datasets -- including MMSE, CDR scores, and brain\nvolume metrics -- to perform structured assessments of Alzheimer's risk.\nMeanwhile, the Case Retrieval Module encodes patient profiles into latent\nrepresentations and retrieves similar cases from a curated knowledge base.\nThese auxiliary cases are fused with the input profile via a Case Fusion Layer\nto enhance contextual understanding. The combined representation is then\nprocessed with clinical prompts for inference. Evaluations on real-world\ndatasets demonstrate BRAINS effectiveness in classifying disease severity and\nidentifying early signs of cognitive decline. This system not only shows strong\npotential as an assistive tool for scalable, explainable, and early-stage\nAlzheimer's disease detection, but also offers hope for future applications in\nthe field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the global burden of Alzheimer's disease (AD) continues to grow, early and\naccurate detection has become increasingly critical, especially in regions with\nlimited access to advanced diagnostic tools. We propose BRAINS (Biomedical\nRetrieval-Augmented Intelligence for Neurodegeneration Screening) to address\nthis challenge. This novel system harnesses the powerful reasoning capabilities\nof Large Language Models (LLMs) for Alzheimer's detection and monitoring.\nBRAINS features a dual-module architecture: a cognitive diagnostic module and a\ncase-retrieval module. The Diagnostic Module utilizes LLMs fine-tuned on\ncognitive and neuroimaging datasets -- including MMSE, CDR scores, and brain\nvolume metrics -- to perform structured assessments of Alzheimer's risk.\nMeanwhile, the Case Retrieval Module encodes patient profiles into latent\nrepresentations and retrieves similar cases from a curated knowledge base.\nThese auxiliary cases are fused with the input profile via a Case Fusion Layer\nto enhance contextual understanding. The combined representation is then\nprocessed with clinical prompts for inference. Evaluations on real-world\ndatasets demonstrate BRAINS effectiveness in classifying disease severity and\nidentifying early signs of cognitive decline. This system not only shows strong\npotential as an assistive tool for scalable, explainable, and early-stage\nAlzheimer's disease detection, but also offers hope for future applications in\nthe field."
                },
                "authors": [
                    {
                        "name": "Rajan Das Gupta"
                    },
                    {
                        "name": "Md Kishor Morol"
                    },
                    {
                        "name": "Nafiz Fahad"
                    },
                    {
                        "name": "Md Tanzib Hosain"
                    },
                    {
                        "name": "Sumaya Binte Zilani Choya"
                    },
                    {
                        "name": "Md Jakir Hossen"
                    }
                ],
                "author_detail": {
                    "name": "Md Jakir Hossen"
                },
                "author": "Md Jakir Hossen",
                "arxiv_comment": "Accepted for publication in ICMLA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02490v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02490v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.00955v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.00955v2",
                "updated": "2025-11-04T11:09:43Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    11,
                    9,
                    43,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-02T14:32:21Z",
                "published_parsed": [
                    2025,
                    11,
                    2,
                    14,
                    32,
                    21,
                    6,
                    306,
                    0
                ],
                "title": "Optimizing Energy and Latency in 6G Smart Cities with Edge CyberTwins",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Energy and Latency in 6G Smart Cities with Edge CyberTwins"
                },
                "summary": "The proliferation of IoT devices in smart cities challenges 6G networks with\nconflicting energy-latency requirements across heterogeneous slices. Existing\napproaches struggle with the energy-latency trade-off, particularly for massive\nscale deployments exceeding 50,000 devices km. This paper proposes an\nedge-aware CyberTwin framework integrating hybrid federated learning for\nenergy-latency co-optimization in 6G network slicing. Our approach combines\ncentralized Artificial Intelligence scheduling for latency-sensitive slices\nwith distributed federated learning for non-critical slices, enhanced by\ncompressive sensing-based digital twins and renewable energy-aware resource\nallocation. The hybrid scheduler leverages a three-tier architecture with\nPhysical Unclonable Function (PUF) based security attestation achieving 99.7%\nattack detection accuracy. Comprehensive simulations demonstrate 52% energy\nreduction for non-real-time slices compared to Diffusion-Reinforcement Learning\nbaselines while maintaining 0.9ms latency for URLLC applications with 99.1% SLA\ncompliance. The framework scales to 50,000 devices km with CPU overhead below\n25%, validated through NS-3 hybrid simulations across realistic smart city\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of IoT devices in smart cities challenges 6G networks with\nconflicting energy-latency requirements across heterogeneous slices. Existing\napproaches struggle with the energy-latency trade-off, particularly for massive\nscale deployments exceeding 50,000 devices km. This paper proposes an\nedge-aware CyberTwin framework integrating hybrid federated learning for\nenergy-latency co-optimization in 6G network slicing. Our approach combines\ncentralized Artificial Intelligence scheduling for latency-sensitive slices\nwith distributed federated learning for non-critical slices, enhanced by\ncompressive sensing-based digital twins and renewable energy-aware resource\nallocation. The hybrid scheduler leverages a three-tier architecture with\nPhysical Unclonable Function (PUF) based security attestation achieving 99.7%\nattack detection accuracy. Comprehensive simulations demonstrate 52% energy\nreduction for non-real-time slices compared to Diffusion-Reinforcement Learning\nbaselines while maintaining 0.9ms latency for URLLC applications with 99.1% SLA\ncompliance. The framework scales to 50,000 devices km with CPU overhead below\n25%, validated through NS-3 hybrid simulations across realistic smart city\nscenarios."
                },
                "authors": [
                    {
                        "name": "Amine Abouaomar"
                    },
                    {
                        "name": "Badr Ben Elallid"
                    },
                    {
                        "name": "Nabil Benamar"
                    }
                ],
                "author_detail": {
                    "name": "Nabil Benamar"
                },
                "author": "Nabil Benamar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.00955v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.00955v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02479v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02479v1",
                "updated": "2025-11-04T11:08:02Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    11,
                    8,
                    2,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T11:08:02Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    11,
                    8,
                    2,
                    1,
                    308,
                    0
                ],
                "title": "Secure PAC Learning: Sample-Budget Laws and Quantum Data-Path\n  Admissibility",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secure PAC Learning: Sample-Budget Laws and Quantum Data-Path\n  Admissibility"
                },
                "summary": "Security in machine learning is fragile when data are exfiltrated or\nperturbed, yet existing frameworks rarely connect the definition and analysis\nof the security to learnability. In this work, we develop a theory of secure\nlearning grounded in the probably-approximately-correct (PAC) viewpoint and\ndevelop an operational framework that links data-path behavior to finite-sample\nbudgets. In our formulation, an accuracy-confidence target is evaluated via a\nrun-based sequential test that halts after a prescribed number of consecutive\nvalidations, and a closed-form budget bound guarantees the learning success if\nthe data-path channel is admissible; the acceptance must also exceed a\nprimitive random-search baseline. We elevate and complete our secure-learning\nconstruction in the context of quantum information -- establishing\nquantum-secure PAC learning: for prepare-and-measure scenarios, the data-path\nadmissibility is set to be threshold fixed by Holevo information, not a\nlearner-tunable tolerance. Thus, a certified information advantage for the\nlearner directly becomes the learning security -- an effect with no classical\nanalogue. The channel-determined confidence follows naturally and basis sifting\nis incorporated for practical deployments. This is the first complete framework\nthat simultaneously embeds a security notion and an operational sample-budget\nlaw within the PAC learning and anchors the security in quantum information.\nThe resulting blueprint points toward standardized guarantees for the learning\nsecurity, with clear avenues for PAC-Bayes extensions and for integration with\nadvanced quantum machine learning front ends.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Security in machine learning is fragile when data are exfiltrated or\nperturbed, yet existing frameworks rarely connect the definition and analysis\nof the security to learnability. In this work, we develop a theory of secure\nlearning grounded in the probably-approximately-correct (PAC) viewpoint and\ndevelop an operational framework that links data-path behavior to finite-sample\nbudgets. In our formulation, an accuracy-confidence target is evaluated via a\nrun-based sequential test that halts after a prescribed number of consecutive\nvalidations, and a closed-form budget bound guarantees the learning success if\nthe data-path channel is admissible; the acceptance must also exceed a\nprimitive random-search baseline. We elevate and complete our secure-learning\nconstruction in the context of quantum information -- establishing\nquantum-secure PAC learning: for prepare-and-measure scenarios, the data-path\nadmissibility is set to be threshold fixed by Holevo information, not a\nlearner-tunable tolerance. Thus, a certified information advantage for the\nlearner directly becomes the learning security -- an effect with no classical\nanalogue. The channel-determined confidence follows naturally and basis sifting\nis incorporated for practical deployments. This is the first complete framework\nthat simultaneously embeds a security notion and an operational sample-budget\nlaw within the PAC learning and anchors the security in quantum information.\nThe resulting blueprint points toward standardized guarantees for the learning\nsecurity, with clear avenues for PAC-Bayes extensions and for integration with\nadvanced quantum machine learning front ends."
                },
                "authors": [
                    {
                        "name": "Jeongho Bang"
                    }
                ],
                "author_detail": {
                    "name": "Jeongho Bang"
                },
                "author": "Jeongho Bang",
                "arxiv_comment": "13 pages, comments welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02479v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02479v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.23081v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.23081v2",
                "updated": "2025-11-04T11:00:12Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    11,
                    0,
                    12,
                    1,
                    308,
                    0
                ],
                "published": "2025-10-27T07:32:19Z",
                "published_parsed": [
                    2025,
                    10,
                    27,
                    7,
                    32,
                    19,
                    0,
                    300,
                    0
                ],
                "title": "A Survey on LLM Mid-Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on LLM Mid-Training"
                },
                "summary": "Recent advances in foundation models have highlighted the significant\nbenefits of multi-stage training, with a particular emphasis on the emergence\nof mid-training as a vital stage that bridges pre-training and post-training.\nMid-training is distinguished by its use of intermediate data and computational\nresources, systematically enhancing specified capabilities such as mathematics,\ncoding, reasoning, and long-context extension, while maintaining foundational\ncompetencies. This survey provides a formal definition of mid-training for\nlarge language models (LLMs) and investigates optimization frameworks that\nencompass data curation, training strategies, and model architecture\noptimization. We analyze mainstream model implementations in the context of\nobjective-driven interventions, illustrating how mid-training serves as a\ndistinct and critical stage in the progressive development of LLM capabilities.\nBy clarifying the unique contributions of mid-training, this survey offers a\ncomprehensive taxonomy and actionable insights, supporting future research and\ninnovation in the advancement of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in foundation models have highlighted the significant\nbenefits of multi-stage training, with a particular emphasis on the emergence\nof mid-training as a vital stage that bridges pre-training and post-training.\nMid-training is distinguished by its use of intermediate data and computational\nresources, systematically enhancing specified capabilities such as mathematics,\ncoding, reasoning, and long-context extension, while maintaining foundational\ncompetencies. This survey provides a formal definition of mid-training for\nlarge language models (LLMs) and investigates optimization frameworks that\nencompass data curation, training strategies, and model architecture\noptimization. We analyze mainstream model implementations in the context of\nobjective-driven interventions, illustrating how mid-training serves as a\ndistinct and critical stage in the progressive development of LLM capabilities.\nBy clarifying the unique contributions of mid-training, this survey offers a\ncomprehensive taxonomy and actionable insights, supporting future research and\ninnovation in the advancement of LLMs."
                },
                "authors": [
                    {
                        "name": "Chengying Tu"
                    },
                    {
                        "name": "Xuemiao Zhang"
                    },
                    {
                        "name": "Rongxiang Weng"
                    },
                    {
                        "name": "Rumei Li"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Yang Bai"
                    },
                    {
                        "name": "Hongfei Yan"
                    },
                    {
                        "name": "Jingang Wang"
                    },
                    {
                        "name": "Xunliang Cai"
                    }
                ],
                "author_detail": {
                    "name": "Xunliang Cai"
                },
                "author": "Xunliang Cai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.23081v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.23081v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.04306v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.04306v2",
                "updated": "2025-11-04T10:59:26Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    10,
                    59,
                    26,
                    1,
                    308,
                    0
                ],
                "published": "2024-06-06T17:53:34Z",
                "published_parsed": [
                    2024,
                    6,
                    6,
                    17,
                    53,
                    34,
                    3,
                    158,
                    0
                ],
                "title": "Improving Uncertainty Estimation through Semantically Diverse Language\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Uncertainty Estimation through Semantically Diverse Language\n  Generation"
                },
                "summary": "Large language models (LLMs) can suffer from hallucinations when generating\ntext. These hallucinations impede various applications in society and industry\nby making LLMs untrustworthy. Current LLMs generate text in an autoregressive\nfashion by predicting and appending text tokens. When an LLM is uncertain about\nthe semantic meaning of the next tokens to generate, it is likely to start\nhallucinating. Thus, it has been suggested that predictive uncertainty is one\nof the main causes of hallucinations. We introduce Semantically Diverse\nLanguage Generation (SDLG) to quantify predictive uncertainty in LLMs. SDLG\nsteers the LLM to generate semantically diverse yet likely alternatives for an\ninitially generated text. This approach provides a precise measure of aleatoric\nsemantic uncertainty, detecting whether the initial text is likely to be\nhallucinated. Experiments on question-answering tasks demonstrate that SDLG\nconsistently outperforms existing methods while being the most computationally\nefficient, setting a new standard for uncertainty estimation in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) can suffer from hallucinations when generating\ntext. These hallucinations impede various applications in society and industry\nby making LLMs untrustworthy. Current LLMs generate text in an autoregressive\nfashion by predicting and appending text tokens. When an LLM is uncertain about\nthe semantic meaning of the next tokens to generate, it is likely to start\nhallucinating. Thus, it has been suggested that predictive uncertainty is one\nof the main causes of hallucinations. We introduce Semantically Diverse\nLanguage Generation (SDLG) to quantify predictive uncertainty in LLMs. SDLG\nsteers the LLM to generate semantically diverse yet likely alternatives for an\ninitially generated text. This approach provides a precise measure of aleatoric\nsemantic uncertainty, detecting whether the initial text is likely to be\nhallucinated. Experiments on question-answering tasks demonstrate that SDLG\nconsistently outperforms existing methods while being the most computationally\nefficient, setting a new standard for uncertainty estimation in LLMs."
                },
                "authors": [
                    {
                        "name": "Lukas Aichberger"
                    },
                    {
                        "name": "Kajetan Schweighofer"
                    },
                    {
                        "name": "Mykyta Ielanskyi"
                    },
                    {
                        "name": "Sepp Hochreiter"
                    }
                ],
                "author_detail": {
                    "name": "Sepp Hochreiter"
                },
                "author": "Sepp Hochreiter",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.04306v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.04306v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02469v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02469v1",
                "updated": "2025-11-04T10:56:01Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    10,
                    56,
                    1,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T10:56:01Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    10,
                    56,
                    1,
                    1,
                    308,
                    0
                ],
                "title": "Modeling Hawkish-Dovish Latent Beliefs in Multi-Agent Debate-Based LLMs\n  for Monetary Policy Decision Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling Hawkish-Dovish Latent Beliefs in Multi-Agent Debate-Based LLMs\n  for Monetary Policy Decision Classification"
                },
                "summary": "Accurately forecasting central bank policy decisions, particularly those of\nthe Federal Open Market Committee(FOMC) has become increasingly important amid\nheightened economic uncertainty. While prior studies have used monetary policy\ntexts to predict rate changes, most rely on static classification models that\noverlook the deliberative nature of policymaking. This study proposes a novel\nframework that structurally imitates the FOMC's collective decision-making\nprocess by modeling multiple large language models(LLMs) as interacting agents.\nEach agent begins with a distinct initial belief and produces a prediction\nbased on both qualitative policy texts and quantitative macroeconomic\nindicators. Through iterative rounds, agents revise their predictions by\nobserving the outputs of others, simulating deliberation and consensus\nformation. To enhance interpretability, we introduce a latent variable\nrepresenting each agent's underlying belief(e.g., hawkish or dovish), and we\ntheoretically demonstrate how this belief mediates the perception of input\ninformation and interaction dynamics. Empirical results show that this\ndebate-based approach significantly outperforms standard LLMs-based baselines\nin prediction accuracy. Furthermore, the explicit modeling of beliefs provides\ninsights into how individual perspectives and social influence shape collective\npolicy forecasts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately forecasting central bank policy decisions, particularly those of\nthe Federal Open Market Committee(FOMC) has become increasingly important amid\nheightened economic uncertainty. While prior studies have used monetary policy\ntexts to predict rate changes, most rely on static classification models that\noverlook the deliberative nature of policymaking. This study proposes a novel\nframework that structurally imitates the FOMC's collective decision-making\nprocess by modeling multiple large language models(LLMs) as interacting agents.\nEach agent begins with a distinct initial belief and produces a prediction\nbased on both qualitative policy texts and quantitative macroeconomic\nindicators. Through iterative rounds, agents revise their predictions by\nobserving the outputs of others, simulating deliberation and consensus\nformation. To enhance interpretability, we introduce a latent variable\nrepresenting each agent's underlying belief(e.g., hawkish or dovish), and we\ntheoretically demonstrate how this belief mediates the perception of input\ninformation and interaction dynamics. Empirical results show that this\ndebate-based approach significantly outperforms standard LLMs-based baselines\nin prediction accuracy. Furthermore, the explicit modeling of beliefs provides\ninsights into how individual perspectives and social influence shape collective\npolicy forecasts."
                },
                "authors": [
                    {
                        "name": "Kaito Takano"
                    },
                    {
                        "name": "Masanori Hirano"
                    },
                    {
                        "name": "Kei Nakagawa"
                    }
                ],
                "author_detail": {
                    "name": "Kei Nakagawa"
                },
                "author": "Kei Nakagawa",
                "arxiv_comment": "PRIMA2025 Accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02469v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02469v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.CP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02463v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02463v1",
                "updated": "2025-11-04T10:45:52Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    10,
                    45,
                    52,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T10:45:52Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    10,
                    45,
                    52,
                    1,
                    308,
                    0
                ],
                "title": "Auditable-choice reframing unlocks RL-based verification for open-ended\n  tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auditable-choice reframing unlocks RL-based verification for open-ended\n  tasks"
                },
                "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated great\npotential in enhancing the reasoning capabilities of large language models\n(LLMs), achieving remarkable progress in domains such as mathematics and\nprogramming where standard answers are available. However, for open-ended tasks\nlacking ground-truth solutions (e.g., creative writing and instruction\nfollowing), existing studies typically regard them as non-reasoning scenarios,\nthereby overlooking the latent value of reasoning capabilities. This raises a\nkey question: Can strengthening reasoning improve performance in open-ended\ntasks? To address this, we explore the transfer of the RLVR paradigm to the\nopen domain. Yet, since RLVR fundamentally relies on verifiers that presuppose\nthe existence of standard answers, it cannot be directly applied to open-ended\ntasks. To overcome this challenge, we introduce Verifiable Multiple-Choice\nReformulation (VMR), a novel training strategy that restructures open-ended\ndata into verifiable multiple-choice formats, enabling effective training even\nin the absence of explicit ground truth. Experimental results on multiple\nbenchmarks validate the effectiveness of our method in improving LLM\nperformance on open-ended tasks. Notably, across eight open-ended benchmarks,\nour VMR-based training delivers an average gain of 5.99 points over the\nbaseline. Code will be released upon acceptance to facilitate reproducibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated great\npotential in enhancing the reasoning capabilities of large language models\n(LLMs), achieving remarkable progress in domains such as mathematics and\nprogramming where standard answers are available. However, for open-ended tasks\nlacking ground-truth solutions (e.g., creative writing and instruction\nfollowing), existing studies typically regard them as non-reasoning scenarios,\nthereby overlooking the latent value of reasoning capabilities. This raises a\nkey question: Can strengthening reasoning improve performance in open-ended\ntasks? To address this, we explore the transfer of the RLVR paradigm to the\nopen domain. Yet, since RLVR fundamentally relies on verifiers that presuppose\nthe existence of standard answers, it cannot be directly applied to open-ended\ntasks. To overcome this challenge, we introduce Verifiable Multiple-Choice\nReformulation (VMR), a novel training strategy that restructures open-ended\ndata into verifiable multiple-choice formats, enabling effective training even\nin the absence of explicit ground truth. Experimental results on multiple\nbenchmarks validate the effectiveness of our method in improving LLM\nperformance on open-ended tasks. Notably, across eight open-ended benchmarks,\nour VMR-based training delivers an average gain of 5.99 points over the\nbaseline. Code will be released upon acceptance to facilitate reproducibility."
                },
                "authors": [
                    {
                        "name": "Mengyu Zhang"
                    },
                    {
                        "name": "Xubo Liu"
                    },
                    {
                        "name": "Siyu Ding"
                    },
                    {
                        "name": "Weichong Yin"
                    },
                    {
                        "name": "Yu Sun"
                    },
                    {
                        "name": "Hua Wu"
                    },
                    {
                        "name": "Wenya Guo"
                    },
                    {
                        "name": "Ying Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ying Zhang"
                },
                "author": "Ying Zhang",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02463v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02463v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02458v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02458v1",
                "updated": "2025-11-04T10:38:10Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    10,
                    38,
                    10,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T10:38:10Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    10,
                    38,
                    10,
                    1,
                    308,
                    0
                ],
                "title": "Prompting for Policy: Forecasting Macroeconomic Scenarios with Synthetic\n  LLM Personas",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompting for Policy: Forecasting Macroeconomic Scenarios with Synthetic\n  LLM Personas"
                },
                "summary": "We evaluate whether persona-based prompting improves Large Language Model\n(LLM) performance on macroeconomic forecasting tasks. Using 2,368\neconomics-related personas from the PersonaHub corpus, we prompt GPT-4o to\nreplicate the ECB Survey of Professional Forecasters across 50 quarterly rounds\n(2013-2025). We compare the persona-prompted forecasts against the human\nexperts panel, across four target variables (HICP, core HICP, GDP growth,\nunemployment) and four forecast horizons. We also compare the results against\n100 baseline forecasts without persona descriptions to isolate its effect. We\nreport two main findings. Firstly, GPT-4o and human forecasters achieve\nremarkably similar accuracy levels, with differences that are statistically\nsignificant yet practically modest. Our out-of-sample evaluation on 2024-2025\ndata demonstrates that GPT-4o can maintain competitive forecasting performance\non unseen events, though with notable differences compared to the in-sample\nperiod. Secondly, our ablation experiment reveals no measurable forecasting\nadvantage from persona descriptions, suggesting these prompt components can be\nomitted to reduce computational costs without sacrificing accuracy. Our results\nprovide evidence that GPT-4o can achieve competitive forecasting accuracy even\non out-of-sample macroeconomic events, if provided with relevant context data,\nwhile revealing that diverse prompts produce remarkably homogeneous forecasts\ncompared to human panels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We evaluate whether persona-based prompting improves Large Language Model\n(LLM) performance on macroeconomic forecasting tasks. Using 2,368\neconomics-related personas from the PersonaHub corpus, we prompt GPT-4o to\nreplicate the ECB Survey of Professional Forecasters across 50 quarterly rounds\n(2013-2025). We compare the persona-prompted forecasts against the human\nexperts panel, across four target variables (HICP, core HICP, GDP growth,\nunemployment) and four forecast horizons. We also compare the results against\n100 baseline forecasts without persona descriptions to isolate its effect. We\nreport two main findings. Firstly, GPT-4o and human forecasters achieve\nremarkably similar accuracy levels, with differences that are statistically\nsignificant yet practically modest. Our out-of-sample evaluation on 2024-2025\ndata demonstrates that GPT-4o can maintain competitive forecasting performance\non unseen events, though with notable differences compared to the in-sample\nperiod. Secondly, our ablation experiment reveals no measurable forecasting\nadvantage from persona descriptions, suggesting these prompt components can be\nomitted to reduce computational costs without sacrificing accuracy. Our results\nprovide evidence that GPT-4o can achieve competitive forecasting accuracy even\non out-of-sample macroeconomic events, if provided with relevant context data,\nwhile revealing that diverse prompts produce remarkably homogeneous forecasts\ncompared to human panels."
                },
                "authors": [
                    {
                        "name": "Giulia Iadisernia"
                    },
                    {
                        "name": "Carolina Camassa"
                    }
                ],
                "author_detail": {
                    "name": "Carolina Camassa"
                },
                "author": "Carolina Camassa",
                "arxiv_comment": "9 pages, 8-pages appendix, accepted at ICAIF 25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02458v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02458v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19973v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19973v2",
                "updated": "2025-11-04T10:36:29Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    10,
                    36,
                    29,
                    1,
                    308,
                    0
                ],
                "published": "2025-10-22T19:05:04Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    19,
                    5,
                    4,
                    2,
                    295,
                    0
                ],
                "title": "A Tutorial on Cognitive Biases in Agentic AI-Driven 6G Autonomous\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Tutorial on Cognitive Biases in Agentic AI-Driven 6G Autonomous\n  Networks"
                },
                "summary": "The path to higher network autonomy in 6G lies beyond the mere optimization\nof key performance indicators (KPIs). While KPIs have enabled automation gains\nunder TM Forum Levels 1--3, they remain numerical abstractions that act only as\nproxies for the real essence of communication networks: seamless connectivity,\nfairness, adaptability, and resilience. True autonomy requires perceiving and\nreasoning over the network environment as it is. Such progress can be achieved\nthrough \\emph{agentic AI}, where large language model (LLM)-powered agents\nperceive multimodal telemetry, reason with memory, negotiate across domains,\nand act via APIs to achieve multi-objective goals. However, deploying such\nagents introduces the challenge of cognitive biases inherited from human\ndesign, which can distort reasoning, negotiation, tool use, and actuation.\nBetween neuroscience and AI, this paper provides a tutorial on a selection of\nwell-known biases, including their taxonomy, definition, mathematical\nformulation, emergence in telecom systems and the commonly impacted agentic\ncomponents. The tutorial also presents various mitigation strategies tailored\nto each type of bias. The article finally provides two practical use-cases,\nwhich tackle the emergence, impact and mitigation gain of some famous biases in\n6G inter-slice and cross-domain management. In particular, anchor\nrandomization, temporal decay and inflection bonus techniques are introduced to\nspecifically address anchoring, temporal and confirmation biases. This avoids\nthat agents stick to the initial high resource allocation proposal or decisions\nthat are recent and/or confirming a prior hypothesis. By grounding decisions in\na richer and fairer set of past experiences, the quality and bravery of the\nagentic agreements in the second use-case, for instance, are leading to $\\times\n5$ lower latency and around $40\\%$ higher energy saving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The path to higher network autonomy in 6G lies beyond the mere optimization\nof key performance indicators (KPIs). While KPIs have enabled automation gains\nunder TM Forum Levels 1--3, they remain numerical abstractions that act only as\nproxies for the real essence of communication networks: seamless connectivity,\nfairness, adaptability, and resilience. True autonomy requires perceiving and\nreasoning over the network environment as it is. Such progress can be achieved\nthrough \\emph{agentic AI}, where large language model (LLM)-powered agents\nperceive multimodal telemetry, reason with memory, negotiate across domains,\nand act via APIs to achieve multi-objective goals. However, deploying such\nagents introduces the challenge of cognitive biases inherited from human\ndesign, which can distort reasoning, negotiation, tool use, and actuation.\nBetween neuroscience and AI, this paper provides a tutorial on a selection of\nwell-known biases, including their taxonomy, definition, mathematical\nformulation, emergence in telecom systems and the commonly impacted agentic\ncomponents. The tutorial also presents various mitigation strategies tailored\nto each type of bias. The article finally provides two practical use-cases,\nwhich tackle the emergence, impact and mitigation gain of some famous biases in\n6G inter-slice and cross-domain management. In particular, anchor\nrandomization, temporal decay and inflection bonus techniques are introduced to\nspecifically address anchoring, temporal and confirmation biases. This avoids\nthat agents stick to the initial high resource allocation proposal or decisions\nthat are recent and/or confirming a prior hypothesis. By grounding decisions in\na richer and fairer set of past experiences, the quality and bravery of the\nagentic agreements in the second use-case, for instance, are leading to $\\times\n5$ lower latency and around $40\\%$ higher energy saving."
                },
                "authors": [
                    {
                        "name": "Hatim Chergui"
                    },
                    {
                        "name": "Farhad Rezazadeh"
                    },
                    {
                        "name": "Merouane Debbah"
                    },
                    {
                        "name": "Christos Verikoukis"
                    }
                ],
                "author_detail": {
                    "name": "Christos Verikoukis"
                },
                "author": "Christos Verikoukis",
                "arxiv_comment": "19 pages, 15 figures, 1 table, link to source code available",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19973v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19973v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.20458v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.20458v3",
                "updated": "2025-11-04T10:31:08Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    10,
                    31,
                    8,
                    1,
                    308,
                    0
                ],
                "published": "2025-10-23T11:49:57Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    11,
                    49,
                    57,
                    3,
                    296,
                    0
                ],
                "title": "Ultralow-Cost magnetocaloric compound for Cryogenic Cooling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultralow-Cost magnetocaloric compound for Cryogenic Cooling"
                },
                "summary": "Cost-effective materials are essential for large-scale deployment. The\nemerging magnetocaloric hydrogen liquefaction technology could transform the\nliquid hydrogen industry due to its potential in achieving higher efficiency.\nMost studies of the cryogenic magnetocaloric effect (MCE) have focused on\nresource-critical rare-earth-based compounds. Here we report on an ionic\nmagnetocaloric compound FeCl$_2$ which is based on ultralow-cost elements, as a\ncandidate working material for hydrogen liquefaction. FeCl$_2$ shows both\ninverse and conventional MCE. From 0 to 1.5 T, the inverse effect yields a\npositive magnetic entropy change ($\\Delta S_T$) of about 5 J/kg/K near 20 K,\nthen declines toward zero at higher fields. In contrast, the conventional\n(negative) response strengthens with field. The $\\Delta S_T$ reaches 18.6\nJ/kg/K near 20 K in magnetic fields of 5 T. This value exceeds most light\nrare-earth-based compounds and approaches that of heavy rare-earth-based\ncompounds. In magnetic fields of 5 T, the adiabatic temperature change reaches\nabout 3.6 K. The large $\\Delta S_T$, along with the low cost of the elements in\nFeCl$_2$, are prerequisites for inexpensive industrial-scale production, giving\nthe prospect of a practical magnetocaloric candidate for hydrogen liquefaction\nin the 20 $\\sim$ 77 K temperature window.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cost-effective materials are essential for large-scale deployment. The\nemerging magnetocaloric hydrogen liquefaction technology could transform the\nliquid hydrogen industry due to its potential in achieving higher efficiency.\nMost studies of the cryogenic magnetocaloric effect (MCE) have focused on\nresource-critical rare-earth-based compounds. Here we report on an ionic\nmagnetocaloric compound FeCl$_2$ which is based on ultralow-cost elements, as a\ncandidate working material for hydrogen liquefaction. FeCl$_2$ shows both\ninverse and conventional MCE. From 0 to 1.5 T, the inverse effect yields a\npositive magnetic entropy change ($\\Delta S_T$) of about 5 J/kg/K near 20 K,\nthen declines toward zero at higher fields. In contrast, the conventional\n(negative) response strengthens with field. The $\\Delta S_T$ reaches 18.6\nJ/kg/K near 20 K in magnetic fields of 5 T. This value exceeds most light\nrare-earth-based compounds and approaches that of heavy rare-earth-based\ncompounds. In magnetic fields of 5 T, the adiabatic temperature change reaches\nabout 3.6 K. The large $\\Delta S_T$, along with the low cost of the elements in\nFeCl$_2$, are prerequisites for inexpensive industrial-scale production, giving\nthe prospect of a practical magnetocaloric candidate for hydrogen liquefaction\nin the 20 $\\sim$ 77 K temperature window."
                },
                "authors": [
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Benjamin Theisel"
                    },
                    {
                        "name": "Yulia Klunnikova"
                    },
                    {
                        "name": "Konstantin Skokov"
                    },
                    {
                        "name": "Oliver Gutfleisch"
                    }
                ],
                "author_detail": {
                    "name": "Oliver Gutfleisch"
                },
                "author": "Oliver Gutfleisch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.20458v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.20458v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02451v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02451v1",
                "updated": "2025-11-04T10:28:57Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    10,
                    28,
                    57,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T10:28:57Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    10,
                    28,
                    57,
                    1,
                    308,
                    0
                ],
                "title": "Merging Continual Pretraining Models for Domain-Specialized LLMs: A Case\n  Study in Finance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Merging Continual Pretraining Models for Domain-Specialized LLMs: A Case\n  Study in Finance"
                },
                "summary": "While LLMs excel at general tasks, they struggle in specialized domains like\nfinance, requiring diverse skills in domain knowledge, mathematical reasoning,\nand multilingual processing. Merging domain-specific Continual Pre-training\n(CPT) \"experts\" offers a practical alternative to costly and unstable\nmulti-skill training. However, unlike established Supervised Fine-Tuning (SFT)\nmodel-based merging, CPT model merging remains largely unexplored. We address\nthis gap by creating financial LLMs from experts in finance, math, and\nJapanese. We propose a three-stage evaluation focusing on knowledge recovery,\ncomplementarity, and emergence, and assess three merging methods (Task\nArithmetic, TIES, and DARE-TIES) on a comprehensive financial benchmark curated\nfrom 18 tasks across 8 established datasets. Results show that merging an\nexpert with its base model recovers general knowledge lost during CPT, while\nmerging experts improves performance and can yield emergent cross-domain\nskills. Among the methods, Task Arithmetic performs strongly but is\nhyperparameter-sensitive, whereas TIES is more robust. Our findings also\nsuggest that while model similarity correlates with merging success, emergent\nskills depend on more complex factors. This work presents the first\nfoundational analysis of CPT model merging, establishing a principled framework\nand providing clear guidance for building multi-skill LLMs from existing\nassets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While LLMs excel at general tasks, they struggle in specialized domains like\nfinance, requiring diverse skills in domain knowledge, mathematical reasoning,\nand multilingual processing. Merging domain-specific Continual Pre-training\n(CPT) \"experts\" offers a practical alternative to costly and unstable\nmulti-skill training. However, unlike established Supervised Fine-Tuning (SFT)\nmodel-based merging, CPT model merging remains largely unexplored. We address\nthis gap by creating financial LLMs from experts in finance, math, and\nJapanese. We propose a three-stage evaluation focusing on knowledge recovery,\ncomplementarity, and emergence, and assess three merging methods (Task\nArithmetic, TIES, and DARE-TIES) on a comprehensive financial benchmark curated\nfrom 18 tasks across 8 established datasets. Results show that merging an\nexpert with its base model recovers general knowledge lost during CPT, while\nmerging experts improves performance and can yield emergent cross-domain\nskills. Among the methods, Task Arithmetic performs strongly but is\nhyperparameter-sensitive, whereas TIES is more robust. Our findings also\nsuggest that while model similarity correlates with merging success, emergent\nskills depend on more complex factors. This work presents the first\nfoundational analysis of CPT model merging, establishing a principled framework\nand providing clear guidance for building multi-skill LLMs from existing\nassets."
                },
                "authors": [
                    {
                        "name": "Kentaro Ueda"
                    },
                    {
                        "name": "François Portet"
                    },
                    {
                        "name": "Hirohiko Suwa"
                    },
                    {
                        "name": "Keiichi Yasumoto"
                    }
                ],
                "author_detail": {
                    "name": "Keiichi Yasumoto"
                },
                "author": "Keiichi Yasumoto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02451v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02451v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06850v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06850v5",
                "updated": "2025-11-04T10:28:49Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    10,
                    28,
                    49,
                    1,
                    308,
                    0
                ],
                "published": "2025-07-09T13:54:58Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    13,
                    54,
                    58,
                    2,
                    190,
                    0
                ],
                "title": "The Dark Side of LLMs: Agent-based Attacks for Complete Computer\n  Takeover",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Dark Side of LLMs: Agent-based Attacks for Complete Computer\n  Takeover"
                },
                "summary": "The rapid adoption of Large Language Model (LLM) agents and multi-agent\nsystems enables remarkable capabilities in natural language processing and\ngeneration. However, these systems introduce security vulnerabilities that\nextend beyond traditional content generation to system-level compromises. This\npaper presents a comprehensive evaluation of the LLMs security used as\nreasoning engines within autonomous agents, highlighting how they can be\nexploited as attack vectors capable of achieving computer takeovers. We focus\non how different attack surfaces and trust boundaries can be leveraged to\norchestrate such takeovers. We demonstrate that adversaries can effectively\ncoerce popular LLMs into autonomously installing and executing malware on\nvictim machines. Our evaluation of 18 state-of-the-art LLMs reveals an alarming\nscenario: 94.4% of models succumb to Direct Prompt Injection, and 83.3% are\nvulnerable to the more stealthy and evasive RAG Backdoor Attack. Notably, we\ntested trust boundaries within multi-agent systems, where LLM agents interact\nand influence each other, and we revealed that LLMs which successfully resist\ndirect injection or RAG backdoor attacks will execute identical payloads when\nrequested by peer agents. We found that 100.0% of tested LLMs can be\ncompromised through Inter-Agent Trust Exploitation attacks, and that every\nmodel exhibits context-dependent security behaviors that create exploitable\nblind spots.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid adoption of Large Language Model (LLM) agents and multi-agent\nsystems enables remarkable capabilities in natural language processing and\ngeneration. However, these systems introduce security vulnerabilities that\nextend beyond traditional content generation to system-level compromises. This\npaper presents a comprehensive evaluation of the LLMs security used as\nreasoning engines within autonomous agents, highlighting how they can be\nexploited as attack vectors capable of achieving computer takeovers. We focus\non how different attack surfaces and trust boundaries can be leveraged to\norchestrate such takeovers. We demonstrate that adversaries can effectively\ncoerce popular LLMs into autonomously installing and executing malware on\nvictim machines. Our evaluation of 18 state-of-the-art LLMs reveals an alarming\nscenario: 94.4% of models succumb to Direct Prompt Injection, and 83.3% are\nvulnerable to the more stealthy and evasive RAG Backdoor Attack. Notably, we\ntested trust boundaries within multi-agent systems, where LLM agents interact\nand influence each other, and we revealed that LLMs which successfully resist\ndirect injection or RAG backdoor attacks will execute identical payloads when\nrequested by peer agents. We found that 100.0% of tested LLMs can be\ncompromised through Inter-Agent Trust Exploitation attacks, and that every\nmodel exhibits context-dependent security behaviors that create exploitable\nblind spots."
                },
                "authors": [
                    {
                        "name": "Matteo Lupinacci"
                    },
                    {
                        "name": "Francesco Aurelio Pironti"
                    },
                    {
                        "name": "Francesco Blefari"
                    },
                    {
                        "name": "Francesco Romeo"
                    },
                    {
                        "name": "Luigi Arena"
                    },
                    {
                        "name": "Angelo Furfaro"
                    }
                ],
                "author_detail": {
                    "name": "Angelo Furfaro"
                },
                "author": "Angelo Furfaro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06850v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06850v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23175v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23175v3",
                "updated": "2025-11-04T10:28:43Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    10,
                    28,
                    43,
                    1,
                    308,
                    0
                ],
                "published": "2025-03-29T18:09:36Z",
                "published_parsed": [
                    2025,
                    3,
                    29,
                    18,
                    9,
                    36,
                    5,
                    88,
                    0
                ],
                "title": "Large Language Models are Unreliable for Cyber Threat Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are Unreliable for Cyber Threat Intelligence"
                },
                "summary": "Several recent works have argued that Large Language Models (LLMs) can be\nused to tame the data deluge in the cybersecurity field, by improving the\nautomation of Cyber Threat Intelligence (CTI) tasks. This work presents an\nevaluation methodology that other than allowing to test LLMs on CTI tasks when\nusing zero-shot learning, few-shot learning and fine-tuning, also allows to\nquantify their consistency and their confidence level. We run experiments with\nthree state-of-the-art LLMs and a dataset of 350 threat intelligence reports\nand present new evidence of potential security risks in relying on LLMs for\nCTI. We show how LLMs cannot guarantee sufficient performance on real-size\nreports while also being inconsistent and overconfident. Few-shot learning and\nfine-tuning only partially improve the results, thus posing doubts about the\npossibility of using LLMs for CTI scenarios, where labelled datasets are\nlacking and where confidence is a fundamental factor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several recent works have argued that Large Language Models (LLMs) can be\nused to tame the data deluge in the cybersecurity field, by improving the\nautomation of Cyber Threat Intelligence (CTI) tasks. This work presents an\nevaluation methodology that other than allowing to test LLMs on CTI tasks when\nusing zero-shot learning, few-shot learning and fine-tuning, also allows to\nquantify their consistency and their confidence level. We run experiments with\nthree state-of-the-art LLMs and a dataset of 350 threat intelligence reports\nand present new evidence of potential security risks in relying on LLMs for\nCTI. We show how LLMs cannot guarantee sufficient performance on real-size\nreports while also being inconsistent and overconfident. Few-shot learning and\nfine-tuning only partially improve the results, thus posing doubts about the\npossibility of using LLMs for CTI scenarios, where labelled datasets are\nlacking and where confidence is a fundamental factor."
                },
                "authors": [
                    {
                        "name": "Emanuele Mezzi"
                    },
                    {
                        "name": "Fabio Massacci"
                    },
                    {
                        "name": "Katja Tuma"
                    }
                ],
                "author_detail": {
                    "name": "Katja Tuma"
                },
                "author": "Katja Tuma",
                "arxiv_doi": "10.1007/978-3-032-00627-1_17",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-032-00627-1_17",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.23175v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23175v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10809v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10809v2",
                "updated": "2025-11-04T10:25:46Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    10,
                    25,
                    46,
                    1,
                    308,
                    0
                ],
                "published": "2025-03-13T18:59:12Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    18,
                    59,
                    12,
                    3,
                    72,
                    0
                ],
                "title": "MIP against Agent: Malicious Image Patches Hijacking Multimodal OS\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIP against Agent: Malicious Image Patches Hijacking Multimodal OS\n  Agents"
                },
                "summary": "Recent advances in operating system (OS) agents have enabled vision-language\nmodels (VLMs) to directly control a user's computer. Unlike conventional VLMs\nthat passively output text, OS agents autonomously perform computer-based tasks\nin response to a single user prompt. OS agents do so by capturing, parsing, and\nanalysing screenshots and executing low-level actions via application\nprogramming interfaces (APIs), such as mouse clicks and keyboard inputs. This\ndirect interaction with the OS significantly raises the stakes, as failures or\nmanipulations can have immediate and tangible consequences. In this work, we\nuncover a novel attack vector against these OS agents: Malicious Image Patches\n(MIPs), adversarially perturbed screen regions that, when captured by an OS\nagent, induce it to perform harmful actions by exploiting specific APIs. For\ninstance, a MIP can be embedded in a desktop wallpaper or shared on social\nmedia to cause an OS agent to exfiltrate sensitive user data. We show that MIPs\ngeneralise across user prompts and screen configurations, and that they can\nhijack multiple OS agents even during the execution of benign instructions.\nThese findings expose critical security vulnerabilities in OS agents that have\nto be carefully addressed before their widespread deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in operating system (OS) agents have enabled vision-language\nmodels (VLMs) to directly control a user's computer. Unlike conventional VLMs\nthat passively output text, OS agents autonomously perform computer-based tasks\nin response to a single user prompt. OS agents do so by capturing, parsing, and\nanalysing screenshots and executing low-level actions via application\nprogramming interfaces (APIs), such as mouse clicks and keyboard inputs. This\ndirect interaction with the OS significantly raises the stakes, as failures or\nmanipulations can have immediate and tangible consequences. In this work, we\nuncover a novel attack vector against these OS agents: Malicious Image Patches\n(MIPs), adversarially perturbed screen regions that, when captured by an OS\nagent, induce it to perform harmful actions by exploiting specific APIs. For\ninstance, a MIP can be embedded in a desktop wallpaper or shared on social\nmedia to cause an OS agent to exfiltrate sensitive user data. We show that MIPs\ngeneralise across user prompts and screen configurations, and that they can\nhijack multiple OS agents even during the execution of benign instructions.\nThese findings expose critical security vulnerabilities in OS agents that have\nto be carefully addressed before their widespread deployment."
                },
                "authors": [
                    {
                        "name": "Lukas Aichberger"
                    },
                    {
                        "name": "Alasdair Paren"
                    },
                    {
                        "name": "Guohao Li"
                    },
                    {
                        "name": "Philip Torr"
                    },
                    {
                        "name": "Yarin Gal"
                    },
                    {
                        "name": "Adel Bibi"
                    }
                ],
                "author_detail": {
                    "name": "Adel Bibi"
                },
                "author": "Adel Bibi",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10809v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10809v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10584v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10584v2",
                "updated": "2025-11-04T10:14:39Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    10,
                    14,
                    39,
                    1,
                    308,
                    0
                ],
                "published": "2025-07-11T12:36:33Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    36,
                    33,
                    4,
                    192,
                    0
                ],
                "title": "ARPaCCino: An Agentic-RAG for Policy as Code Compliance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARPaCCino: An Agentic-RAG for Policy as Code Compliance"
                },
                "summary": "Policy as Code (PaC) is a paradigm that encodes security and compliance\npolicies into machine-readable formats, enabling automated enforcement in\nInfrastructure as Code (IaC) environments. However, its adoption is hindered by\nthe complexity of policy languages and the risk of misconfigurations. In this\nwork, we present ARPaCCino, an agentic system that combines Large Language\nModels (LLMs), Retrieval-Augmented-Generation (RAG), and tool-based validation\nto automate the generation and verification of PaC rules. Given natural\nlanguage descriptions of the desired policies, ARPaCCino generates formal Rego\nrules, assesses IaC compliance, and iteratively refines the IaC configurations\nto ensure conformance. Thanks to its modular agentic architecture and\nintegration with external tools and knowledge bases, ARPaCCino supports policy\nvalidation across a wide range of technologies, including niche or emerging IaC\nframeworks. Experimental evaluation involving a Terraform-based case study\ndemonstrates ARPaCCino's effectiveness in generating syntactically and\nsemantically correct policies, identifying non-compliant infrastructures, and\napplying corrective modifications, even when using smaller, open-weight LLMs.\nOur results highlight the potential of agentic RAG architectures to enhance the\nautomation, reliability, and accessibility of PaC workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Policy as Code (PaC) is a paradigm that encodes security and compliance\npolicies into machine-readable formats, enabling automated enforcement in\nInfrastructure as Code (IaC) environments. However, its adoption is hindered by\nthe complexity of policy languages and the risk of misconfigurations. In this\nwork, we present ARPaCCino, an agentic system that combines Large Language\nModels (LLMs), Retrieval-Augmented-Generation (RAG), and tool-based validation\nto automate the generation and verification of PaC rules. Given natural\nlanguage descriptions of the desired policies, ARPaCCino generates formal Rego\nrules, assesses IaC compliance, and iteratively refines the IaC configurations\nto ensure conformance. Thanks to its modular agentic architecture and\nintegration with external tools and knowledge bases, ARPaCCino supports policy\nvalidation across a wide range of technologies, including niche or emerging IaC\nframeworks. Experimental evaluation involving a Terraform-based case study\ndemonstrates ARPaCCino's effectiveness in generating syntactically and\nsemantically correct policies, identifying non-compliant infrastructures, and\napplying corrective modifications, even when using smaller, open-weight LLMs.\nOur results highlight the potential of agentic RAG architectures to enhance the\nautomation, reliability, and accessibility of PaC workflows."
                },
                "authors": [
                    {
                        "name": "Francesco Romeo"
                    },
                    {
                        "name": "Luigi Arena"
                    },
                    {
                        "name": "Francesco Blefari"
                    },
                    {
                        "name": "Francesco Aurelio Pironti"
                    },
                    {
                        "name": "Matteo Lupinacci"
                    },
                    {
                        "name": "Angelo Furfaro"
                    }
                ],
                "author_detail": {
                    "name": "Angelo Furfaro"
                },
                "author": "Angelo Furfaro",
                "arxiv_doi": "10.1007/978-3-032-05727-3_39",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-032-05727-3_39",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.10584v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10584v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02434v1",
                "updated": "2025-11-04T10:06:53Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    10,
                    6,
                    53,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T10:06:53Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    10,
                    6,
                    53,
                    1,
                    308,
                    0
                ],
                "title": "Who's Who? LLM-assisted Software Traceability with Architecture Entity\n  Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Who's Who? LLM-assisted Software Traceability with Architecture Entity\n  Recognition"
                },
                "summary": "Identifying architecturally relevant entities in textual artifacts is crucial\nfor Traceability Link Recovery (TLR) between Software Architecture\nDocumentation (SAD) and source code. While Software Architecture Models (SAMs)\ncan bridge the semantic gap between these artifacts, their manual creation is\ntime-consuming. Large Language Models (LLMs) offer new capabilities for\nextracting architectural entities from SAD and source code to construct SAMs\nautomatically or establish direct trace links. This paper presents two\nLLM-based approaches: ExArch extracts component names as simple SAMs from SAD\nand source code to eliminate the need for manual SAM creation, while ArTEMiS\nidentifies architectural entities in documentation and matches them with\n(manually or automatically generated) SAM entities. Our evaluation compares\nagainst state-of-the-art approaches SWATTR, TransArC and ArDoCode. TransArC\nachieves strong performance (F1: 0.87) but requires manually created SAMs;\nExArch achieves comparable results (F1: 0.86) using only SAD and code. ArTEMiS\nis on par with the traditional heuristic-based SWATTR (F1: 0.81) and can\nsuccessfully replace it when integrated with TransArC. The combination of\nArTEMiS and ExArch outperforms ArDoCode, the best baseline without manual SAMs.\nOur results demonstrate that LLMs can effectively identify architectural\nentities in textual artifacts, enabling automated SAM generation and TLR,\nmaking architecture-code traceability more practical and accessible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying architecturally relevant entities in textual artifacts is crucial\nfor Traceability Link Recovery (TLR) between Software Architecture\nDocumentation (SAD) and source code. While Software Architecture Models (SAMs)\ncan bridge the semantic gap between these artifacts, their manual creation is\ntime-consuming. Large Language Models (LLMs) offer new capabilities for\nextracting architectural entities from SAD and source code to construct SAMs\nautomatically or establish direct trace links. This paper presents two\nLLM-based approaches: ExArch extracts component names as simple SAMs from SAD\nand source code to eliminate the need for manual SAM creation, while ArTEMiS\nidentifies architectural entities in documentation and matches them with\n(manually or automatically generated) SAM entities. Our evaluation compares\nagainst state-of-the-art approaches SWATTR, TransArC and ArDoCode. TransArC\nachieves strong performance (F1: 0.87) but requires manually created SAMs;\nExArch achieves comparable results (F1: 0.86) using only SAD and code. ArTEMiS\nis on par with the traditional heuristic-based SWATTR (F1: 0.81) and can\nsuccessfully replace it when integrated with TransArC. The combination of\nArTEMiS and ExArch outperforms ArDoCode, the best baseline without manual SAMs.\nOur results demonstrate that LLMs can effectively identify architectural\nentities in textual artifacts, enabling automated SAM generation and TLR,\nmaking architecture-code traceability more practical and accessible."
                },
                "authors": [
                    {
                        "name": "Dominik Fuchß"
                    },
                    {
                        "name": "Haoyu Liu"
                    },
                    {
                        "name": "Sophie Corallo"
                    },
                    {
                        "name": "Tobias Hey"
                    },
                    {
                        "name": "Jan Keim"
                    },
                    {
                        "name": "Johannes von Geisau"
                    },
                    {
                        "name": "Anne Koziolek"
                    }
                ],
                "author_detail": {
                    "name": "Anne Koziolek"
                },
                "author": "Anne Koziolek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02428v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02428v1",
                "updated": "2025-11-04T09:58:45Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    9,
                    58,
                    45,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T09:58:45Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    9,
                    58,
                    45,
                    1,
                    308,
                    0
                ],
                "title": "Can Conversational AI Counsel for Change? A Theory-Driven Approach to\n  Supporting Dietary Intentions in Ambivalent Individuals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Conversational AI Counsel for Change? A Theory-Driven Approach to\n  Supporting Dietary Intentions in Ambivalent Individuals"
                },
                "summary": "Adherence to healthy diets reduces chronic illness risk, yet rates remain\nlow. Large Language Models (LLMs) are increasingly used for health\ncommunication but often struggle to engage individuals with ambivalent\nintentions at a pivotal stage of the Transtheoretical Model (TTM). We developed\nCounselLLM, an open-source model enhanced through persona design and few-shot,\ndomain-specific prompts grounded in TTM and Motivational Interviewing (MI). In\ncontrolled evaluations, CounselLLM showed stronger use of TTM subprocesses and\nMI affirmations than human counselors, with comparable linguistic robustness\nbut expressed in more concrete terms. A user study then tested CounselLLM in an\ninteractive counseling setting against a baseline system. While knowledge and\nperceptions did not change, participants' intentions for immediate dietary\nchange increased significantly after interacting with CounselLLM. Participants\nalso rated it as easy to use, understandable, and supportive. These findings\nsuggest theory-driven LLMs can effectively engage ambivalent individuals and\nprovide a scalable approach to digital counseling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adherence to healthy diets reduces chronic illness risk, yet rates remain\nlow. Large Language Models (LLMs) are increasingly used for health\ncommunication but often struggle to engage individuals with ambivalent\nintentions at a pivotal stage of the Transtheoretical Model (TTM). We developed\nCounselLLM, an open-source model enhanced through persona design and few-shot,\ndomain-specific prompts grounded in TTM and Motivational Interviewing (MI). In\ncontrolled evaluations, CounselLLM showed stronger use of TTM subprocesses and\nMI affirmations than human counselors, with comparable linguistic robustness\nbut expressed in more concrete terms. A user study then tested CounselLLM in an\ninteractive counseling setting against a baseline system. While knowledge and\nperceptions did not change, participants' intentions for immediate dietary\nchange increased significantly after interacting with CounselLLM. Participants\nalso rated it as easy to use, understandable, and supportive. These findings\nsuggest theory-driven LLMs can effectively engage ambivalent individuals and\nprovide a scalable approach to digital counseling."
                },
                "authors": [
                    {
                        "name": "Michelle Bak"
                    },
                    {
                        "name": "Kexin Quan"
                    },
                    {
                        "name": "Tre Tomaszewski"
                    },
                    {
                        "name": "Jessie Chin"
                    }
                ],
                "author_detail": {
                    "name": "Jessie Chin"
                },
                "author": "Jessie Chin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02428v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02428v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02427v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02427v1",
                "updated": "2025-11-04T09:58:29Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    9,
                    58,
                    29,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T09:58:29Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    9,
                    58,
                    29,
                    1,
                    308,
                    0
                ],
                "title": "From the Laboratory to Real-World Application: Evaluating Zero-Shot\n  Scene Interpretation on Edge Devices for Mobile Robotics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From the Laboratory to Real-World Application: Evaluating Zero-Shot\n  Scene Interpretation on Edge Devices for Mobile Robotics"
                },
                "summary": "Video Understanding, Scene Interpretation and Commonsense Reasoning are\nhighly challenging tasks enabling the interpretation of visual information,\nallowing agents to perceive, interact with and make rational decisions in its\nenvironment. Large Language Models (LLMs) and Visual Language Models (VLMs)\nhave shown remarkable advancements in these areas in recent years, enabling\ndomain-specific applications as well as zero-shot open vocabulary tasks,\ncombining multiple domains. However, the required computational complexity\nposes challenges for their application on edge devices and in the context of\nMobile Robotics, especially considering the trade-off between accuracy and\ninference time. In this paper, we investigate the capabilities of\nstate-of-the-art VLMs for the task of Scene Interpretation and Action\nRecognition, with special regard to small VLMs capable of being deployed to\nedge devices in the context of Mobile Robotics. The proposed pipeline is\nevaluated on a diverse dataset consisting of various real-world cityscape,\non-campus and indoor scenarios. The experimental evaluation discusses the\npotential of these small models on edge devices, with particular emphasis on\nchallenges, weaknesses, inherent model biases and the application of the gained\ninformation. Supplementary material is provided via the following repository:\nhttps://datahub.rz.rptu.de/hstr-csrl-public/publications/scene-interpretation-on-edge-devices/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Understanding, Scene Interpretation and Commonsense Reasoning are\nhighly challenging tasks enabling the interpretation of visual information,\nallowing agents to perceive, interact with and make rational decisions in its\nenvironment. Large Language Models (LLMs) and Visual Language Models (VLMs)\nhave shown remarkable advancements in these areas in recent years, enabling\ndomain-specific applications as well as zero-shot open vocabulary tasks,\ncombining multiple domains. However, the required computational complexity\nposes challenges for their application on edge devices and in the context of\nMobile Robotics, especially considering the trade-off between accuracy and\ninference time. In this paper, we investigate the capabilities of\nstate-of-the-art VLMs for the task of Scene Interpretation and Action\nRecognition, with special regard to small VLMs capable of being deployed to\nedge devices in the context of Mobile Robotics. The proposed pipeline is\nevaluated on a diverse dataset consisting of various real-world cityscape,\non-campus and indoor scenarios. The experimental evaluation discusses the\npotential of these small models on edge devices, with particular emphasis on\nchallenges, weaknesses, inherent model biases and the application of the gained\ninformation. Supplementary material is provided via the following repository:\nhttps://datahub.rz.rptu.de/hstr-csrl-public/publications/scene-interpretation-on-edge-devices/"
                },
                "authors": [
                    {
                        "name": "Nicolas Schuler"
                    },
                    {
                        "name": "Lea Dewald"
                    },
                    {
                        "name": "Nick Baldig"
                    },
                    {
                        "name": "Jürgen Graf"
                    }
                ],
                "author_detail": {
                    "name": "Jürgen Graf"
                },
                "author": "Jürgen Graf",
                "arxiv_comment": "15 pages, 6 figures, 1 table; accepted for AI-2025 Forty-fifth SGAI\n  International Conference on Artificial Intelligence CAMBRIDGE, ENGLAND 16-18\n  DECEMBER 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02427v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02427v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02424v1",
                "updated": "2025-11-04T09:55:40Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    9,
                    55,
                    40,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T09:55:40Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    9,
                    55,
                    40,
                    1,
                    308,
                    0
                ],
                "title": "ReAcTree: Hierarchical LLM Agent Trees with Control Flow for\n  Long-Horizon Task Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReAcTree: Hierarchical LLM Agent Trees with Control Flow for\n  Long-Horizon Task Planning"
                },
                "summary": "Recent advancements in large language models (LLMs) have enabled significant\nprogress in decision-making and task planning for embodied autonomous agents.\nHowever, most existing methods still struggle with complex, long-horizon tasks\nbecause they rely on a monolithic trajectory that entangles all past decisions\nand observations, attempting to solve the entire task in a single unified\nprocess. To address this limitation, we propose ReAcTree, a hierarchical\ntask-planning method that decomposes a complex goal into more manageable\nsubgoals within a dynamically constructed agent tree. Each subgoal is handled\nby an LLM agent node capable of reasoning, acting, and further expanding the\ntree, while control flow nodes coordinate the execution strategies of agent\nnodes. In addition, we integrate two complementary memory systems: each agent\nnode retrieves goal-specific, subgoal-level examples from episodic memory and\nshares environment-specific observations through working memory. Experiments on\nthe WAH-NL and ALFRED datasets demonstrate that ReAcTree consistently\noutperforms strong task-planning baselines such as ReAct across diverse LLMs.\nNotably, on WAH-NL, ReAcTree achieves a 61% goal success rate with Qwen 2.5\n72B, nearly doubling ReAct's 31%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have enabled significant\nprogress in decision-making and task planning for embodied autonomous agents.\nHowever, most existing methods still struggle with complex, long-horizon tasks\nbecause they rely on a monolithic trajectory that entangles all past decisions\nand observations, attempting to solve the entire task in a single unified\nprocess. To address this limitation, we propose ReAcTree, a hierarchical\ntask-planning method that decomposes a complex goal into more manageable\nsubgoals within a dynamically constructed agent tree. Each subgoal is handled\nby an LLM agent node capable of reasoning, acting, and further expanding the\ntree, while control flow nodes coordinate the execution strategies of agent\nnodes. In addition, we integrate two complementary memory systems: each agent\nnode retrieves goal-specific, subgoal-level examples from episodic memory and\nshares environment-specific observations through working memory. Experiments on\nthe WAH-NL and ALFRED datasets demonstrate that ReAcTree consistently\noutperforms strong task-planning baselines such as ReAct across diverse LLMs.\nNotably, on WAH-NL, ReAcTree achieves a 61% goal success rate with Qwen 2.5\n72B, nearly doubling ReAct's 31%."
                },
                "authors": [
                    {
                        "name": "Jae-Woo Choi"
                    },
                    {
                        "name": "Hyungmin Kim"
                    },
                    {
                        "name": "Hyobin Ong"
                    },
                    {
                        "name": "Minsu Jang"
                    },
                    {
                        "name": "Dohyung Kim"
                    },
                    {
                        "name": "Jaehong Kim"
                    },
                    {
                        "name": "Youngwoo Yoon"
                    }
                ],
                "author_detail": {
                    "name": "Youngwoo Yoon"
                },
                "author": "Youngwoo Yoon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02423v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02423v1",
                "updated": "2025-11-04T09:54:57Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    9,
                    54,
                    57,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T09:54:57Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    9,
                    54,
                    57,
                    1,
                    308,
                    0
                ],
                "title": "LLM4PG: Adapting Large Language Model for Pathloss Map Generation via\n  Synesthesia of Machines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM4PG: Adapting Large Language Model for Pathloss Map Generation via\n  Synesthesia of Machines"
                },
                "summary": "In this paper, a novel large language model (LLM)-based pathloss map\ngeneration model, termed LLM4PG, is proposed for sixth-generation (6G)\nAI-native communication systems via Synesthesia of Machines (SoM). To explore\nthe mapping mechanism between sensing images and pathloss maps, a new synthetic\nintelligent multi-modal sensing-communication dataset, SynthSoM-U2G, is\nconstructed, covering multiple scenarios, frequency bands, and flight\naltitudes. By adapting the LLM for cross-modal pathloss map generation for the\nfirst time, LLM4PG establishes an effective cross-domain alignment between the\nmulti-modal sensing-communication and natural language domains. A task-specific\nfine-tuning strategy with a tailored layer selection and activation scheme is\ndesigned to meet the demands of massive-scale, high-quality generation.\nCompared with conventional deep learning artificial intelligence generated\ncontent (AIGC) models, LLM4PG achieves more accurate pathloss map generation\nand stronger generalization across diverse conditions. Results show that LLM4PG\nattains an NMSE of 0.0454, outperforming the conventional AIGC model by over\n2.90 dB, while its cross-condition generalization achieves an NMSE of 0.0492,\nexceeding the baseline by 4.52 dB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, a novel large language model (LLM)-based pathloss map\ngeneration model, termed LLM4PG, is proposed for sixth-generation (6G)\nAI-native communication systems via Synesthesia of Machines (SoM). To explore\nthe mapping mechanism between sensing images and pathloss maps, a new synthetic\nintelligent multi-modal sensing-communication dataset, SynthSoM-U2G, is\nconstructed, covering multiple scenarios, frequency bands, and flight\naltitudes. By adapting the LLM for cross-modal pathloss map generation for the\nfirst time, LLM4PG establishes an effective cross-domain alignment between the\nmulti-modal sensing-communication and natural language domains. A task-specific\nfine-tuning strategy with a tailored layer selection and activation scheme is\ndesigned to meet the demands of massive-scale, high-quality generation.\nCompared with conventional deep learning artificial intelligence generated\ncontent (AIGC) models, LLM4PG achieves more accurate pathloss map generation\nand stronger generalization across diverse conditions. Results show that LLM4PG\nattains an NMSE of 0.0454, outperforming the conventional AIGC model by over\n2.90 dB, while its cross-condition generalization achieves an NMSE of 0.0492,\nexceeding the baseline by 4.52 dB."
                },
                "authors": [
                    {
                        "name": "Mingran Sun"
                    },
                    {
                        "name": "Lu Bai"
                    },
                    {
                        "name": "Xiang Cheng"
                    },
                    {
                        "name": "Jianjun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Jianjun Wu"
                },
                "author": "Jianjun Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02423v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02423v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02400v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02400v1",
                "updated": "2025-11-04T09:29:46Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    9,
                    29,
                    46,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T09:29:46Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    9,
                    29,
                    46,
                    1,
                    308,
                    0
                ],
                "title": "MammoClean: Toward Reproducible and Bias-Aware AI in Mammography through\n  Dataset Harmonization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MammoClean: Toward Reproducible and Bias-Aware AI in Mammography through\n  Dataset Harmonization"
                },
                "summary": "The development of clinically reliable artificial intelligence (AI) systems\nfor mammography is hindered by profound heterogeneity in data quality, metadata\nstandards, and population distributions across public datasets. This\nheterogeneity introduces dataset-specific biases that severely compromise the\ngeneralizability of the model, a fundamental barrier to clinical deployment. We\npresent MammoClean, a public framework for standardization and bias\nquantification in mammography datasets. MammoClean standardizes case selection,\nimage processing (including laterality and intensity correction), and unifies\nmetadata into a consistent multi-view structure. We provide a comprehensive\nreview of breast anatomy, imaging characteristics, and public mammography\ndatasets to systematically identify key sources of bias. Applying MammoClean to\nthree heterogeneous datasets (CBIS-DDSM, TOMPEI-CMMD, VinDr-Mammo), we quantify\nsubstantial distributional shifts in breast density and abnormality prevalence.\nCritically, we demonstrate the direct impact of data corruption: AI models\ntrained on corrupted datasets exhibit significant performance degradation\ncompared to their curated counterparts. By using MammoClean to identify and\nmitigate bias sources, researchers can construct unified multi-dataset training\ncorpora that enable development of robust models with superior cross-domain\ngeneralization. MammoClean provides an essential, reproducible pipeline for\nbias-aware AI development in mammography, facilitating fairer comparisons and\nadvancing the creation of safe, effective systems that perform equitably across\ndiverse patient populations and clinical settings. The open-source code is\npublicly available from: https://github.com/Minds-R-Lab/MammoClean.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of clinically reliable artificial intelligence (AI) systems\nfor mammography is hindered by profound heterogeneity in data quality, metadata\nstandards, and population distributions across public datasets. This\nheterogeneity introduces dataset-specific biases that severely compromise the\ngeneralizability of the model, a fundamental barrier to clinical deployment. We\npresent MammoClean, a public framework for standardization and bias\nquantification in mammography datasets. MammoClean standardizes case selection,\nimage processing (including laterality and intensity correction), and unifies\nmetadata into a consistent multi-view structure. We provide a comprehensive\nreview of breast anatomy, imaging characteristics, and public mammography\ndatasets to systematically identify key sources of bias. Applying MammoClean to\nthree heterogeneous datasets (CBIS-DDSM, TOMPEI-CMMD, VinDr-Mammo), we quantify\nsubstantial distributional shifts in breast density and abnormality prevalence.\nCritically, we demonstrate the direct impact of data corruption: AI models\ntrained on corrupted datasets exhibit significant performance degradation\ncompared to their curated counterparts. By using MammoClean to identify and\nmitigate bias sources, researchers can construct unified multi-dataset training\ncorpora that enable development of robust models with superior cross-domain\ngeneralization. MammoClean provides an essential, reproducible pipeline for\nbias-aware AI development in mammography, facilitating fairer comparisons and\nadvancing the creation of safe, effective systems that perform equitably across\ndiverse patient populations and clinical settings. The open-source code is\npublicly available from: https://github.com/Minds-R-Lab/MammoClean."
                },
                "authors": [
                    {
                        "name": "Yalda Zafari"
                    },
                    {
                        "name": "Hongyi Pan"
                    },
                    {
                        "name": "Gorkem Durak"
                    },
                    {
                        "name": "Ulas Bagci"
                    },
                    {
                        "name": "Essam A. Rashed"
                    },
                    {
                        "name": "Mohamed Mabrok"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Mabrok"
                },
                "author": "Mohamed Mabrok",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02400v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02400v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2511.02399v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2511.02399v1",
                "updated": "2025-11-04T09:27:01Z",
                "updated_parsed": [
                    2025,
                    11,
                    4,
                    9,
                    27,
                    1,
                    1,
                    308,
                    0
                ],
                "published": "2025-11-04T09:27:01Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    9,
                    27,
                    1,
                    1,
                    308,
                    0
                ],
                "title": "EvoDev: An Iterative Feature-Driven Framework for End-to-End Software\n  Development with LLM-based Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvoDev: An Iterative Feature-Driven Framework for End-to-End Software\n  Development with LLM-based Agents"
                },
                "summary": "Recent advances in large language model agents offer the promise of\nautomating end-to-end software development from natural language requirements.\nHowever, existing approaches largely adopt linear, waterfall-style pipelines,\nwhich oversimplify the iterative nature of real-world development and struggle\nwith complex, large-scale projects. To address these limitations, we propose\nEvoDev, an iterative software development framework inspired by feature-driven\ndevelopment. EvoDev decomposes user requirements into a set of user-valued\nfeatures and constructs a Feature Map, a directed acyclic graph that explicitly\nmodels dependencies between features. Each node in the feature map maintains\nmulti-level information, including business logic, design, and code, which is\npropagated along dependencies to provide context for subsequent development\niterations. We evaluate EvoDev on challenging Android development tasks and\nshow that it outperforms the best-performing baseline, Claude Code, by a\nsubstantial margin of 56.8%, while improving single-agent performance by\n16.0%-76.6% across different base LLMs, highlighting the importance of\ndependency modeling, context propagation, and workflow-aware agent design for\ncomplex software projects. Our work summarizes practical insights for designing\niterative, LLM-driven development frameworks and informs future training of\nbase LLMs to better support iterative software development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language model agents offer the promise of\nautomating end-to-end software development from natural language requirements.\nHowever, existing approaches largely adopt linear, waterfall-style pipelines,\nwhich oversimplify the iterative nature of real-world development and struggle\nwith complex, large-scale projects. To address these limitations, we propose\nEvoDev, an iterative software development framework inspired by feature-driven\ndevelopment. EvoDev decomposes user requirements into a set of user-valued\nfeatures and constructs a Feature Map, a directed acyclic graph that explicitly\nmodels dependencies between features. Each node in the feature map maintains\nmulti-level information, including business logic, design, and code, which is\npropagated along dependencies to provide context for subsequent development\niterations. We evaluate EvoDev on challenging Android development tasks and\nshow that it outperforms the best-performing baseline, Claude Code, by a\nsubstantial margin of 56.8%, while improving single-agent performance by\n16.0%-76.6% across different base LLMs, highlighting the importance of\ndependency modeling, context propagation, and workflow-aware agent design for\ncomplex software projects. Our work summarizes practical insights for designing\niterative, LLM-driven development frameworks and informs future training of\nbase LLMs to better support iterative software development."
                },
                "authors": [
                    {
                        "name": "Junwei Liu"
                    },
                    {
                        "name": "Chen Xu"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Tong Bai"
                    },
                    {
                        "name": "Weitong Chen"
                    },
                    {
                        "name": "Kaseng Wong"
                    },
                    {
                        "name": "Yiling Lou"
                    },
                    {
                        "name": "Xin Peng"
                    }
                ],
                "author_detail": {
                    "name": "Xin Peng"
                },
                "author": "Xin Peng",
                "arxiv_comment": "14 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2511.02399v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2511.02399v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]