[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2410.02751v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02751v1",
                "updated": "2024-10-03T17:58:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    58,
                    11,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T17:58:11Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    58,
                    11,
                    3,
                    277,
                    0
                ],
                "title": "ReLIC: A Recipe for 64k Steps of In-Context Reinforcement Learning for\n  Embodied AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReLIC: A Recipe for 64k Steps of In-Context Reinforcement Learning for\n  Embodied AI"
                },
                "summary": "Intelligent embodied agents need to quickly adapt to new scenarios by\nintegrating long histories of experience into decision-making. For instance, a\nrobot in an unfamiliar house initially wouldn't know the locations of objects\nneeded for tasks and might perform inefficiently. However, as it gathers more\nexperience, it should learn the layout of its environment and remember where\nobjects are, allowing it to complete new tasks more efficiently. To enable such\nrapid adaptation to new tasks, we present ReLIC, a new approach for in-context\nreinforcement learning (RL) for embodied agents. With ReLIC, agents are capable\nof adapting to new environments using 64,000 steps of in-context experience\nwith full attention while being trained through self-generated experience via\nRL. We achieve this by proposing a novel policy update scheme for on-policy RL\ncalled \"partial updates'' as well as a Sink-KV mechanism that enables effective\nutilization of a long observation history for embodied agents. Our method\noutperforms a variety of meta-RL baselines in adapting to unseen houses in an\nembodied multi-object navigation task. In addition, we find that ReLIC is\ncapable of few-shot imitation learning despite never being trained with expert\ndemonstrations. We also provide a comprehensive analysis of ReLIC, highlighting\nthat the combination of large-scale RL training, the proposed partial updates\nscheme, and the Sink-KV are essential for effective in-context learning. The\ncode for ReLIC and all our experiments is at https://github.com/aielawady/relic",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent embodied agents need to quickly adapt to new scenarios by\nintegrating long histories of experience into decision-making. For instance, a\nrobot in an unfamiliar house initially wouldn't know the locations of objects\nneeded for tasks and might perform inefficiently. However, as it gathers more\nexperience, it should learn the layout of its environment and remember where\nobjects are, allowing it to complete new tasks more efficiently. To enable such\nrapid adaptation to new tasks, we present ReLIC, a new approach for in-context\nreinforcement learning (RL) for embodied agents. With ReLIC, agents are capable\nof adapting to new environments using 64,000 steps of in-context experience\nwith full attention while being trained through self-generated experience via\nRL. We achieve this by proposing a novel policy update scheme for on-policy RL\ncalled \"partial updates'' as well as a Sink-KV mechanism that enables effective\nutilization of a long observation history for embodied agents. Our method\noutperforms a variety of meta-RL baselines in adapting to unseen houses in an\nembodied multi-object navigation task. In addition, we find that ReLIC is\ncapable of few-shot imitation learning despite never being trained with expert\ndemonstrations. We also provide a comprehensive analysis of ReLIC, highlighting\nthat the combination of large-scale RL training, the proposed partial updates\nscheme, and the Sink-KV are essential for effective in-context learning. The\ncode for ReLIC and all our experiments is at https://github.com/aielawady/relic"
                },
                "authors": [
                    {
                        "name": "Ahmad Elawady"
                    },
                    {
                        "name": "Gunjan Chhablani"
                    },
                    {
                        "name": "Ram Ramrakhya"
                    },
                    {
                        "name": "Karmesh Yadav"
                    },
                    {
                        "name": "Dhruv Batra"
                    },
                    {
                        "name": "Zsolt Kira"
                    },
                    {
                        "name": "Andrew Szot"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Szot"
                },
                "author": "Andrew Szot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02751v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02751v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00023v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00023v2",
                "updated": "2024-10-03T17:50:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    50,
                    33,
                    3,
                    277,
                    0
                ],
                "published": "2024-05-08T06:30:58Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    6,
                    30,
                    58,
                    2,
                    129,
                    0
                ],
                "title": "Preble: Efficient Distributed Prompt Scheduling for LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preble: Efficient Distributed Prompt Scheduling for LLM Serving"
                },
                "summary": "Prompts to large language models (LLMs) have evolved beyond simple user\nquestions. For LLMs to solve complex problems, today's practices are to include\ndomain-specific instructions, illustration of tool usages, and/or long context\nsuch as textbook chapters in prompts. As such, many parts of prompts are\nrepetitive across requests. Recent works propose to cache and reuse KV state of\nprompts. However, they are all confined to a single-GPU optimization, while\nproduction LLM serving systems are distributed by nature.\n  This paper proposes Preble, the first distributed LLM serving platform that\ntargets and optimizes for prompt sharing. We designed a distributed scheduling\nsystem that co-optimizes KV state reuse and computation load-balancing with a\nnew scheduling algorithm and a hierarchical scheduling mechanism. Our\nevaluation of Preble with real workloads and request arrival patterns on two\nopen-source LLMs shows that Preble outperforms the SOTA serving systems by 1.5X\nto 14.5X on average latency and 2X to 10X on p99 latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompts to large language models (LLMs) have evolved beyond simple user\nquestions. For LLMs to solve complex problems, today's practices are to include\ndomain-specific instructions, illustration of tool usages, and/or long context\nsuch as textbook chapters in prompts. As such, many parts of prompts are\nrepetitive across requests. Recent works propose to cache and reuse KV state of\nprompts. However, they are all confined to a single-GPU optimization, while\nproduction LLM serving systems are distributed by nature.\n  This paper proposes Preble, the first distributed LLM serving platform that\ntargets and optimizes for prompt sharing. We designed a distributed scheduling\nsystem that co-optimizes KV state reuse and computation load-balancing with a\nnew scheduling algorithm and a hierarchical scheduling mechanism. Our\nevaluation of Preble with real workloads and request arrival patterns on two\nopen-source LLMs shows that Preble outperforms the SOTA serving systems by 1.5X\nto 14.5X on average latency and 2X to 10X on p99 latency."
                },
                "authors": [
                    {
                        "name": "Vikranth Srivatsa"
                    },
                    {
                        "name": "Zijian He"
                    },
                    {
                        "name": "Reyna Abhyankar"
                    },
                    {
                        "name": "Dongming Li"
                    },
                    {
                        "name": "Yiying Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yiying Zhang"
                },
                "author": "Yiying Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00023v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00023v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02599v1",
                "updated": "2024-10-03T15:41:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    41,
                    31,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T15:41:31Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    41,
                    31,
                    3,
                    277,
                    0
                ],
                "title": "Disaggregated Memory with SmartNIC Offloading: a Case Study on Graph\n  Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated Memory with SmartNIC Offloading: a Case Study on Graph\n  Processing"
                },
                "summary": "Disaggregated memory breaks the boundary of monolithic servers to enable\nmemory provisioning on demand. Using network-attached memory to provide memory\nexpansion for memory-intensive applications on compute nodes can improve the\noverall memory utilization on a cluster and reduce the total cost of ownership.\nHowever, current software solutions for leveraging network-attached memory must\nconsume resources on the compute node for memory management tasks. Emerging\noff-path smartNICs provide general-purpose programmability at low-cost\nlow-power cores. This work provides a general architecture design that enables\nnetwork-attached memory and offloading tasks onto off-path programmable\nSmartNIC. We provide a prototype implementation called SODA on Nvidia BlueField\nDPU. SODA adapts communication paths and data transfer alternatives, pipelines\ndata movement stages, and enables customizable data caching and prefetching\noptimizations. We evaluate SODA in five representative graph applications on\nreal-world graphs. Our results show that SODA can achieve up to 7.9x speedup\ncompared to node-local SSD and reduce network traffic by 42% compared to\ndisaggregated memory without SmartNIC offloading at similar or better\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated memory breaks the boundary of monolithic servers to enable\nmemory provisioning on demand. Using network-attached memory to provide memory\nexpansion for memory-intensive applications on compute nodes can improve the\noverall memory utilization on a cluster and reduce the total cost of ownership.\nHowever, current software solutions for leveraging network-attached memory must\nconsume resources on the compute node for memory management tasks. Emerging\noff-path smartNICs provide general-purpose programmability at low-cost\nlow-power cores. This work provides a general architecture design that enables\nnetwork-attached memory and offloading tasks onto off-path programmable\nSmartNIC. We provide a prototype implementation called SODA on Nvidia BlueField\nDPU. SODA adapts communication paths and data transfer alternatives, pipelines\ndata movement stages, and enables customizable data caching and prefetching\noptimizations. We evaluate SODA in five representative graph applications on\nreal-world graphs. Our results show that SODA can achieve up to 7.9x speedup\ncompared to node-local SSD and reduce network traffic by 42% compared to\ndisaggregated memory without SmartNIC offloading at similar or better\nperformance."
                },
                "authors": [
                    {
                        "name": "Jacob Wahlgren"
                    },
                    {
                        "name": "Gabin Schieffer"
                    },
                    {
                        "name": "Maya Gokhale"
                    },
                    {
                        "name": "Roger Pearce"
                    },
                    {
                        "name": "Ivy Peng"
                    }
                ],
                "author_detail": {
                    "name": "Ivy Peng"
                },
                "author": "Ivy Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02527v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02527v1",
                "updated": "2024-10-03T14:35:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    35,
                    35,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T14:35:35Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    35,
                    35,
                    3,
                    277,
                    0
                ],
                "title": "Learning from Offline Foundation Features with Tensor Augmentations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning from Offline Foundation Features with Tensor Augmentations"
                },
                "summary": "We introduce Learning from Offline Foundation Features with Tensor\nAugmentations (LOFF-TA), an efficient training scheme designed to harness the\ncapabilities of foundation models in limited resource settings where their\ndirect development is not feasible. LOFF-TA involves training a compact\nclassifier on cached feature embeddings from a frozen foundation model,\nresulting in up to $37\\times$ faster training and up to $26\\times$ reduced GPU\nmemory usage. Because the embeddings of augmented images would be too numerous\nto store, yet the augmentation process is essential for training, we propose to\napply tensor augmentations to the cached embeddings of the original\nnon-augmented images. LOFF-TA makes it possible to leverage the power of\nfoundation models, regardless of their size, in settings with limited\ncomputational capacity. Moreover, LOFF-TA can be used to apply foundation\nmodels to high-resolution images without increasing compute. In certain\nscenarios, we find that training with LOFF-TA yields better results than\ndirectly fine-tuning the foundation model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Learning from Offline Foundation Features with Tensor\nAugmentations (LOFF-TA), an efficient training scheme designed to harness the\ncapabilities of foundation models in limited resource settings where their\ndirect development is not feasible. LOFF-TA involves training a compact\nclassifier on cached feature embeddings from a frozen foundation model,\nresulting in up to $37\\times$ faster training and up to $26\\times$ reduced GPU\nmemory usage. Because the embeddings of augmented images would be too numerous\nto store, yet the augmentation process is essential for training, we propose to\napply tensor augmentations to the cached embeddings of the original\nnon-augmented images. LOFF-TA makes it possible to leverage the power of\nfoundation models, regardless of their size, in settings with limited\ncomputational capacity. Moreover, LOFF-TA can be used to apply foundation\nmodels to high-resolution images without increasing compute. In certain\nscenarios, we find that training with LOFF-TA yields better results than\ndirectly fine-tuning the foundation model."
                },
                "authors": [
                    {
                        "name": "Emir Konuk"
                    },
                    {
                        "name": "Christos Matsoukas"
                    },
                    {
                        "name": "Moein Sorkhei"
                    },
                    {
                        "name": "Phitchapha Lertsiravaramet"
                    },
                    {
                        "name": "Kevin Smith"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Smith"
                },
                "author": "Kevin Smith",
                "arxiv_comment": "Accepted to the 38th Conference on Neural Information Processing\n  Systems (NeurIPS 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02527v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02527v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07196v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07196v2",
                "updated": "2024-10-03T11:47:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    11,
                    47,
                    21,
                    3,
                    277,
                    0
                ],
                "published": "2024-09-11T11:40:23Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    11,
                    40,
                    23,
                    2,
                    255,
                    0
                ],
                "title": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses"
                },
                "summary": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed."
                },
                "authors": [
                    {
                        "name": "Benjamin Colmey"
                    },
                    {
                        "name": "Rodrigo T. Paulino"
                    },
                    {
                        "name": "Gaspard Beaufort"
                    },
                    {
                        "name": "David G. Cooke"
                    }
                ],
                "author_detail": {
                    "name": "David G. Cooke"
                },
                "author": "David G. Cooke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07196v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07196v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02369v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02369v1",
                "updated": "2024-10-03T10:33:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    10,
                    33,
                    49,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T10:33:49Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    10,
                    33,
                    49,
                    3,
                    277,
                    0
                ],
                "title": "Unleashing the Potential of the Diffusion Model in Few-shot Semantic\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing the Potential of the Diffusion Model in Few-shot Semantic\n  Segmentation"
                },
                "summary": "The Diffusion Model has not only garnered noteworthy achievements in the\nrealm of image generation but has also demonstrated its potential as an\neffective pretraining method utilizing unlabeled data. Drawing from the\nextensive potential unveiled by the Diffusion Model in both semantic\ncorrespondence and open vocabulary segmentation, our work initiates an\ninvestigation into employing the Latent Diffusion Model for Few-shot Semantic\nSegmentation. Recently, inspired by the in-context learning ability of large\nlanguage models, Few-shot Semantic Segmentation has evolved into In-context\nSegmentation tasks, morphing into a crucial element in assessing generalist\nsegmentation models. In this context, we concentrate on Few-shot Semantic\nSegmentation, establishing a solid foundation for the future development of a\nDiffusion-based generalist model for segmentation. Our initial focus lies in\nunderstanding how to facilitate interaction between the query image and the\nsupport image, resulting in the proposal of a KV fusion method within the\nself-attention framework. Subsequently, we delve deeper into optimizing the\ninfusion of information from the support mask and simultaneously re-evaluating\nhow to provide reasonable supervision from the query mask. Based on our\nanalysis, we establish a simple and effective framework named DiffewS,\nmaximally retaining the original Latent Diffusion Model's generative framework\nand effectively utilizing the pre-training prior. Experimental results\ndemonstrate that our method significantly outperforms the previous SOTA models\nin multiple settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion Model has not only garnered noteworthy achievements in the\nrealm of image generation but has also demonstrated its potential as an\neffective pretraining method utilizing unlabeled data. Drawing from the\nextensive potential unveiled by the Diffusion Model in both semantic\ncorrespondence and open vocabulary segmentation, our work initiates an\ninvestigation into employing the Latent Diffusion Model for Few-shot Semantic\nSegmentation. Recently, inspired by the in-context learning ability of large\nlanguage models, Few-shot Semantic Segmentation has evolved into In-context\nSegmentation tasks, morphing into a crucial element in assessing generalist\nsegmentation models. In this context, we concentrate on Few-shot Semantic\nSegmentation, establishing a solid foundation for the future development of a\nDiffusion-based generalist model for segmentation. Our initial focus lies in\nunderstanding how to facilitate interaction between the query image and the\nsupport image, resulting in the proposal of a KV fusion method within the\nself-attention framework. Subsequently, we delve deeper into optimizing the\ninfusion of information from the support mask and simultaneously re-evaluating\nhow to provide reasonable supervision from the query mask. Based on our\nanalysis, we establish a simple and effective framework named DiffewS,\nmaximally retaining the original Latent Diffusion Model's generative framework\nand effectively utilizing the pre-training prior. Experimental results\ndemonstrate that our method significantly outperforms the previous SOTA models\nin multiple settings."
                },
                "authors": [
                    {
                        "name": "Muzhi Zhu"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Zekai Luo"
                    },
                    {
                        "name": "Chenchen Jing"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Guangkai Xu"
                    },
                    {
                        "name": "Xinlong Wang"
                    },
                    {
                        "name": "Chunhua Shen"
                    }
                ],
                "author_detail": {
                    "name": "Chunhua Shen"
                },
                "author": "Chunhua Shen",
                "arxiv_comment": "Accepted to Proc. Annual Conference on Neural Information Processing\n  Systems (NeurIPS) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02369v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02369v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02069v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02069v3",
                "updated": "2024-10-03T08:46:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    8,
                    46,
                    42,
                    3,
                    277,
                    0
                ],
                "published": "2024-06-04T07:51:30Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    7,
                    51,
                    30,
                    1,
                    156,
                    0
                ],
                "title": "PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information\n  Funneling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information\n  Funneling"
                },
                "summary": "In this study, we investigate whether attention-based information flow inside\nlarge language models (LLMs) is aggregated through noticeable patterns for long\ncontext processing. Our observations reveal that LLMs aggregate information\nthrough Pyramidal Information Funneling where attention is scattering widely in\nlower layers, progressively consolidating within specific contexts, and\nultimately focusing on critical tokens (a.k.a massive activation or attention\nsink) in higher layers. Motivated by these insights, we developed PyramidKV, a\nnovel and effective KV cache compression method. This approach dynamically\nadjusts the KV cache size across different layers, allocating more cache in\nlower layers and less in higher ones, diverging from traditional methods that\nmaintain a uniform KV cache size. Our experimental evaluations, utilizing the\nLongBench benchmark, show that PyramidKV matches the performance of models with\na full KV cache while retaining only 12% of the KV cache, thus significantly\nreducing memory usage. In scenarios emphasizing memory efficiency, where only\n0.7% of the KV cache is maintained, PyramidKV surpasses other KV cache\ncompression techniques, achieving up to a 20.5 absolute accuracy improvement on\nTREC dataset. In the Needle-in-a-Haystack experiment, PyramidKV outperforms\ncompeting methods in maintaining long-context comprehension in LLMs; notably,\nretaining just 128 KV cache entries enables the LLAMA-3-70B model to achieve\n100% Acc. performance, matching that of a full KV cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we investigate whether attention-based information flow inside\nlarge language models (LLMs) is aggregated through noticeable patterns for long\ncontext processing. Our observations reveal that LLMs aggregate information\nthrough Pyramidal Information Funneling where attention is scattering widely in\nlower layers, progressively consolidating within specific contexts, and\nultimately focusing on critical tokens (a.k.a massive activation or attention\nsink) in higher layers. Motivated by these insights, we developed PyramidKV, a\nnovel and effective KV cache compression method. This approach dynamically\nadjusts the KV cache size across different layers, allocating more cache in\nlower layers and less in higher ones, diverging from traditional methods that\nmaintain a uniform KV cache size. Our experimental evaluations, utilizing the\nLongBench benchmark, show that PyramidKV matches the performance of models with\na full KV cache while retaining only 12% of the KV cache, thus significantly\nreducing memory usage. In scenarios emphasizing memory efficiency, where only\n0.7% of the KV cache is maintained, PyramidKV surpasses other KV cache\ncompression techniques, achieving up to a 20.5 absolute accuracy improvement on\nTREC dataset. In the Needle-in-a-Haystack experiment, PyramidKV outperforms\ncompeting methods in maintaining long-context comprehension in LLMs; notably,\nretaining just 128 KV cache entries enables the LLAMA-3-70B model to achieve\n100% Acc. performance, matching that of a full KV cache."
                },
                "authors": [
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Yichi Zhang"
                    },
                    {
                        "name": "Bofei Gao"
                    },
                    {
                        "name": "Yuliang Liu"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Keming Lu"
                    },
                    {
                        "name": "Wayne Xiong"
                    },
                    {
                        "name": "Yue Dong"
                    },
                    {
                        "name": "Baobao Chang"
                    },
                    {
                        "name": "Junjie Hu"
                    },
                    {
                        "name": "Wen Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Xiao"
                },
                "author": "Wen Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02069v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02069v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21018v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21018v2",
                "updated": "2024-10-03T03:03:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    3,
                    3,
                    29,
                    3,
                    277,
                    0
                ],
                "published": "2024-07-30T17:59:08Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    59,
                    8,
                    1,
                    212,
                    0
                ],
                "title": "ThinK: Thinner Key Cache by Query-Driven Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinK: Thinner Key Cache by Query-Driven Pruning"
                },
                "summary": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications. However, their increased computational and memory demands present\nsignificant challenges, especially when handling long sequences. This paper\nfocuses on the long-context scenario, addressing the inefficiencies in KV cache\nmemory consumption during inference. Unlike existing approaches that optimize\nthe memory based on the sequence length, we identify substantial redundancy in\nthe channel dimension of the KV cache, as indicated by an uneven magnitude\ndistribution and a low-rank structure in the attention weights. In response, we\npropose ThinK, a novel query-dependent KV cache pruning method designed to\nminimize attention weight loss while selectively pruning the least significant\nchannels. Our approach not only maintains or enhances model accuracy but also\nachieves a reduction in KV cache memory costs by over 20% compared with vanilla\nKV cache eviction and quantization methods. For instance, ThinK integrated with\nKIVI can achieve a 2.8x reduction in peak memory usage while maintaining nearly\nthe same quality, enabling up to a 5x increase in batch size when using a\nsingle GPU. Extensive evaluations on the LLaMA and Mistral models across\nvarious long-sequence datasets verified the efficiency of ThinK, establishing a\nnew baseline algorithm for efficient LLM deployment without compromising\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications. However, their increased computational and memory demands present\nsignificant challenges, especially when handling long sequences. This paper\nfocuses on the long-context scenario, addressing the inefficiencies in KV cache\nmemory consumption during inference. Unlike existing approaches that optimize\nthe memory based on the sequence length, we identify substantial redundancy in\nthe channel dimension of the KV cache, as indicated by an uneven magnitude\ndistribution and a low-rank structure in the attention weights. In response, we\npropose ThinK, a novel query-dependent KV cache pruning method designed to\nminimize attention weight loss while selectively pruning the least significant\nchannels. Our approach not only maintains or enhances model accuracy but also\nachieves a reduction in KV cache memory costs by over 20% compared with vanilla\nKV cache eviction and quantization methods. For instance, ThinK integrated with\nKIVI can achieve a 2.8x reduction in peak memory usage while maintaining nearly\nthe same quality, enabling up to a 5x increase in batch size when using a\nsingle GPU. Extensive evaluations on the LLaMA and Mistral models across\nvarious long-sequence datasets verified the efficiency of ThinK, establishing a\nnew baseline algorithm for efficient LLM deployment without compromising\nperformance."
                },
                "authors": [
                    {
                        "name": "Yuhui Xu"
                    },
                    {
                        "name": "Zhanming Jie"
                    },
                    {
                        "name": "Hanze Dong"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Aojun Zhou"
                    },
                    {
                        "name": "Amrita Saha"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Doyen Sahoo"
                    }
                ],
                "author_detail": {
                    "name": "Doyen Sahoo"
                },
                "author": "Doyen Sahoo",
                "arxiv_comment": "20 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21018v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21018v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01805v1",
                "updated": "2024-10-02T17:59:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    2,
                    17,
                    59,
                    52,
                    2,
                    276,
                    0
                ],
                "published": "2024-10-02T17:59:52Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    17,
                    59,
                    52,
                    2,
                    276,
                    0
                ],
                "title": "Locret: Enhancing Eviction in Long-Context LLM Inference with Trained\n  Retaining Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Locret: Enhancing Eviction in Long-Context LLM Inference with Trained\n  Retaining Heads"
                },
                "summary": "Large language models (LLMs) have shown remarkable advances in supporting\nlong-context comprehension and processing tasks. However, scaling the\ngeneration inference of LLMs to such long contexts incurs significant\nadditional computation load, and demands a substantial GPU memory footprint to\nmaintain the key-value (KV) cache of transformer-based LLMs. Existing KV cache\ncompression methods, such as quantization, face memory bottlenecks as context\nlength increases, while static-sized caches, such as eviction, suffer from\ninefficient policies. These limitations restrict deployment on consumer-grade\ndevices like a single Nvidia 4090 GPU. To overcome this, we propose Locret, a\nframework for long-context LLM inference that introduces retaining heads to\nevaluate the causal importance of KV cache units, allowing for more accurate\neviction within a fixed cache size. Locret is fine-tuned on top of the frozen\nbackbone LLM using a minimal amount of data from standard long-context SFT\ndatasets. During inference, we evict low-importance cache units along with a\nchunked prefill pattern, significantly reducing peak GPU memory usage. We\nconduct an extensive empirical study to evaluate Locret, where the experimental\nresults show that Locret outperforms the recent competitive approaches,\nincluding InfLLM, Quantization, SirLLM, and MInference, in terms of memory\nefficiency and the quality of generated contents -- Locret achieves over a 20x\nand 8x KV cache compression ratio compared to the full KV cache for\nPhi-3-mini-128K and Llama-3.1-8B-instruct. Additionally, Locret can be combined\nwith other methods, such as quantization and token merging. To our knowledge,\nLocret is the first framework capable of deploying Llama-3.1-8B or similar\nmodels on a single Nvidia 4090 GPU, enabling 128K long-context inference\nwithout compromising generation quality, and requiring little additional system\noptimizations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable advances in supporting\nlong-context comprehension and processing tasks. However, scaling the\ngeneration inference of LLMs to such long contexts incurs significant\nadditional computation load, and demands a substantial GPU memory footprint to\nmaintain the key-value (KV) cache of transformer-based LLMs. Existing KV cache\ncompression methods, such as quantization, face memory bottlenecks as context\nlength increases, while static-sized caches, such as eviction, suffer from\ninefficient policies. These limitations restrict deployment on consumer-grade\ndevices like a single Nvidia 4090 GPU. To overcome this, we propose Locret, a\nframework for long-context LLM inference that introduces retaining heads to\nevaluate the causal importance of KV cache units, allowing for more accurate\neviction within a fixed cache size. Locret is fine-tuned on top of the frozen\nbackbone LLM using a minimal amount of data from standard long-context SFT\ndatasets. During inference, we evict low-importance cache units along with a\nchunked prefill pattern, significantly reducing peak GPU memory usage. We\nconduct an extensive empirical study to evaluate Locret, where the experimental\nresults show that Locret outperforms the recent competitive approaches,\nincluding InfLLM, Quantization, SirLLM, and MInference, in terms of memory\nefficiency and the quality of generated contents -- Locret achieves over a 20x\nand 8x KV cache compression ratio compared to the full KV cache for\nPhi-3-mini-128K and Llama-3.1-8B-instruct. Additionally, Locret can be combined\nwith other methods, such as quantization and token merging. To our knowledge,\nLocret is the first framework capable of deploying Llama-3.1-8B or similar\nmodels on a single Nvidia 4090 GPU, enabling 128K long-context inference\nwithout compromising generation quality, and requiring little additional system\noptimizations."
                },
                "authors": [
                    {
                        "name": "Yuxiang Huang"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Chaojun Xiao"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyuan Liu"
                },
                "author": "Zhiyuan Liu",
                "arxiv_comment": "Preprints",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01760v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01760v1",
                "updated": "2024-10-02T17:14:47Z",
                "updated_parsed": [
                    2024,
                    10,
                    2,
                    17,
                    14,
                    47,
                    2,
                    276,
                    0
                ],
                "published": "2024-10-02T17:14:47Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    17,
                    14,
                    47,
                    2,
                    276,
                    0
                ],
                "title": "Competitive Ratio of Online Caching with Predictions: Lower and Upper\n  Bounds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Competitive Ratio of Online Caching with Predictions: Lower and Upper\n  Bounds"
                },
                "summary": "We address the problem of learning-augmented online caching in the scenario\nwhen each request is accompanied by a prediction of the next occurrence of the\nrequested page. We improve currently known bounds on the competitive ratio of\nthe BlindOracle algorithm, which evicts a page predicted to be requested last.\nWe also prove a lower bound on the competitive ratio of any randomized\nalgorithm and show that a combination of the BlindOracle with the Marker\nalgorithm achieves a competitive ratio that is optimal up to some constant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the problem of learning-augmented online caching in the scenario\nwhen each request is accompanied by a prediction of the next occurrence of the\nrequested page. We improve currently known bounds on the competitive ratio of\nthe BlindOracle algorithm, which evicts a page predicted to be requested last.\nWe also prove a lower bound on the competitive ratio of any randomized\nalgorithm and show that a combination of the BlindOracle with the Marker\nalgorithm achieves a competitive ratio that is optimal up to some constant."
                },
                "authors": [
                    {
                        "name": "Daniel Skachkov"
                    },
                    {
                        "name": "Denis Ponomaryov"
                    },
                    {
                        "name": "Yuri Dorn"
                    },
                    {
                        "name": "Alexander Demin"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Demin"
                },
                "author": "Alexander Demin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01760v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01760v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01723v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01723v1",
                "updated": "2024-10-02T16:34:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    2,
                    16,
                    34,
                    29,
                    2,
                    276,
                    0
                ],
                "published": "2024-10-02T16:34:29Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    16,
                    34,
                    29,
                    2,
                    276,
                    0
                ],
                "title": "HarmoniCa: Harmonizing Training and Inference for Better Feature Cache\n  in Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HarmoniCa: Harmonizing Training and Inference for Better Feature Cache\n  in Diffusion Transformer Acceleration"
                },
                "summary": "Diffusion Transformers (DiTs) have gained prominence for outstanding\nscalability and extraordinary performance in generative tasks. However, their\nconsiderable inference costs impede practical deployment. The feature cache\nmechanism, which involves storing and retrieving redundant computations across\ntimesteps, holds promise for reducing per-step inference time in diffusion\nmodels. Most existing caching methods for DiT are manually designed. Although\nthe learning-based approach attempts to optimize strategies adaptively, it\nsuffers from discrepancies between training and inference, which hampers both\nthe performance and acceleration ratio. Upon detailed analysis, we pinpoint\nthat these discrepancies primarily stem from two aspects: (1) Prior Timestep\nDisregard, where training ignores the effect of cache usage at earlier\ntimesteps, and (2) Objective Mismatch, where the training target (align\npredicted noise in each timestep) deviates from the goal of inference (generate\nthe high-quality image). To alleviate these discrepancies, we propose\nHarmoniCa, a novel method that Harmonizes training and inference with a novel\nlearning-based Caching framework built upon Step-Wise Denoising Training (SDT)\nand Image Error Proxy-Guided Objective (IEPO). Compared to the traditional\ntraining paradigm, the newly proposed SDT maintains the continuity of the\ndenoising process, enabling the model to leverage information from prior\ntimesteps during training, similar to the way it operates during inference.\nFurthermore, we design IEPO, which integrates an efficient proxy mechanism to\napproximate the final image error caused by reusing the cached feature.\nTherefore, IEPO helps balance final image quality and cache utilization,\nresolving the issue of training that only considers the impact of cache usage\non the predicted output at each timestep.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have gained prominence for outstanding\nscalability and extraordinary performance in generative tasks. However, their\nconsiderable inference costs impede practical deployment. The feature cache\nmechanism, which involves storing and retrieving redundant computations across\ntimesteps, holds promise for reducing per-step inference time in diffusion\nmodels. Most existing caching methods for DiT are manually designed. Although\nthe learning-based approach attempts to optimize strategies adaptively, it\nsuffers from discrepancies between training and inference, which hampers both\nthe performance and acceleration ratio. Upon detailed analysis, we pinpoint\nthat these discrepancies primarily stem from two aspects: (1) Prior Timestep\nDisregard, where training ignores the effect of cache usage at earlier\ntimesteps, and (2) Objective Mismatch, where the training target (align\npredicted noise in each timestep) deviates from the goal of inference (generate\nthe high-quality image). To alleviate these discrepancies, we propose\nHarmoniCa, a novel method that Harmonizes training and inference with a novel\nlearning-based Caching framework built upon Step-Wise Denoising Training (SDT)\nand Image Error Proxy-Guided Objective (IEPO). Compared to the traditional\ntraining paradigm, the newly proposed SDT maintains the continuity of the\ndenoising process, enabling the model to leverage information from prior\ntimesteps during training, similar to the way it operates during inference.\nFurthermore, we design IEPO, which integrates an efficient proxy mechanism to\napproximate the final image error caused by reusing the cached feature.\nTherefore, IEPO helps balance final image quality and cache utilization,\nresolving the issue of training that only considers the impact of cache usage\non the predicted output at each timestep."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Zining Wang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Xinjie Zhang"
                    },
                    {
                        "name": "Jinyang Guo"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhang"
                },
                "author": "Jun Zhang",
                "arxiv_comment": "Code will be released soon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01723v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01723v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01518v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01518v1",
                "updated": "2024-10-02T13:09:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    2,
                    13,
                    9,
                    41,
                    2,
                    276,
                    0
                ],
                "published": "2024-10-02T13:09:41Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    13,
                    9,
                    41,
                    2,
                    276,
                    0
                ],
                "title": "InfiniPot: Infinite Context Processing on Memory-Constrained LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniPot: Infinite Context Processing on Memory-Constrained LLMs"
                },
                "summary": "Handling long input contexts remains a significant challenge for Large\nLanguage Models (LLMs), particularly in resource-constrained environments such\nas mobile devices. Our work aims to address this limitation by introducing\nInfiniPot, a novel KV cache control framework designed to enable pre-trained\nLLMs to manage extensive sequences within fixed memory constraints efficiently,\nwithout requiring additional training. InfiniPot leverages Continual Context\nDistillation (CCD), an iterative process that compresses and retains essential\ninformation through novel importance metrics, effectively maintaining critical\ndata even without access to future context. Our comprehensive evaluations\nindicate that InfiniPot significantly outperforms models trained for long\ncontexts in various NLP tasks, establishing its efficacy and versatility. This\nwork represents a substantial advancement toward making LLMs applicable to a\nbroader range of real-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Handling long input contexts remains a significant challenge for Large\nLanguage Models (LLMs), particularly in resource-constrained environments such\nas mobile devices. Our work aims to address this limitation by introducing\nInfiniPot, a novel KV cache control framework designed to enable pre-trained\nLLMs to manage extensive sequences within fixed memory constraints efficiently,\nwithout requiring additional training. InfiniPot leverages Continual Context\nDistillation (CCD), an iterative process that compresses and retains essential\ninformation through novel importance metrics, effectively maintaining critical\ndata even without access to future context. Our comprehensive evaluations\nindicate that InfiniPot significantly outperforms models trained for long\ncontexts in various NLP tasks, establishing its efficacy and versatility. This\nwork represents a substantial advancement toward making LLMs applicable to a\nbroader range of real-world scenarios."
                },
                "authors": [
                    {
                        "name": "Minsoo Kim"
                    },
                    {
                        "name": "Kyuhong Shim"
                    },
                    {
                        "name": "Jungwook Choi"
                    },
                    {
                        "name": "Simyung Chang"
                    }
                ],
                "author_detail": {
                    "name": "Simyung Chang"
                },
                "author": "Simyung Chang",
                "arxiv_comment": "EMNLP 2024 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01518v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01518v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01485v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01485v1",
                "updated": "2024-10-02T12:35:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    2,
                    12,
                    35,
                    53,
                    2,
                    276,
                    0
                ],
                "published": "2024-10-02T12:35:53Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    12,
                    35,
                    53,
                    2,
                    276,
                    0
                ],
                "title": "A Little Goes a Long Way: Efficient Long Context Training and Inference\n  with Partial Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Little Goes a Long Way: Efficient Long Context Training and Inference\n  with Partial Contexts"
                },
                "summary": "Training and serving long-context large language models (LLMs) incurs\nsubstantial overhead. To address this, two critical steps are often required: a\npretrained LLM typically undergoes a separate stage for context length\nextension by training on long-context data, followed by architectural\nmodifications to reduce the overhead of KV cache during serving. This paper\nargues that integrating length extension with a GPU-friendly KV cache reduction\narchitecture not only reduces training overhead during length extension, but\nalso achieves better long-context performance. This leads to our proposed\nLongGen, which finetunes a pretrained LLM into an efficient architecture during\nlength extension. LongGen builds on three key insights: (1) Sparse attention\npatterns, such as window attention (attending to recent tokens), attention sink\n(initial ones), and blockwise sparse attention (strided token blocks) are\nwell-suited for building efficient long-context models, primarily due to their\nGPU-friendly memory access patterns, enabling efficiency gains not just\ntheoretically but in practice as well. (2) It is essential for the model to\nhave direct access to all tokens. A hybrid architecture with 1/3 full attention\nlayers and 2/3 efficient ones achieves a balanced trade-off between efficiency\nand long-context performance. (3) Lightweight training on 5B long-context data\nis sufficient to extend the hybrid model's context length from 4K to 128K.\n  We evaluate LongGen on both Llama-2 7B and Llama-2 70B, demonstrating its\neffectiveness across different scales. During training with 128K-long contexts,\nLongGen achieves 1.55x training speedup and reduces wall-clock time by 36%,\ncompared to a full-attention baseline. During inference, LongGen reduces KV\ncache memory by 62%, achieving 1.67x prefilling speedup and 1.41x decoding\nspeedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training and serving long-context large language models (LLMs) incurs\nsubstantial overhead. To address this, two critical steps are often required: a\npretrained LLM typically undergoes a separate stage for context length\nextension by training on long-context data, followed by architectural\nmodifications to reduce the overhead of KV cache during serving. This paper\nargues that integrating length extension with a GPU-friendly KV cache reduction\narchitecture not only reduces training overhead during length extension, but\nalso achieves better long-context performance. This leads to our proposed\nLongGen, which finetunes a pretrained LLM into an efficient architecture during\nlength extension. LongGen builds on three key insights: (1) Sparse attention\npatterns, such as window attention (attending to recent tokens), attention sink\n(initial ones), and blockwise sparse attention (strided token blocks) are\nwell-suited for building efficient long-context models, primarily due to their\nGPU-friendly memory access patterns, enabling efficiency gains not just\ntheoretically but in practice as well. (2) It is essential for the model to\nhave direct access to all tokens. A hybrid architecture with 1/3 full attention\nlayers and 2/3 efficient ones achieves a balanced trade-off between efficiency\nand long-context performance. (3) Lightweight training on 5B long-context data\nis sufficient to extend the hybrid model's context length from 4K to 128K.\n  We evaluate LongGen on both Llama-2 7B and Llama-2 70B, demonstrating its\neffectiveness across different scales. During training with 128K-long contexts,\nLongGen achieves 1.55x training speedup and reduces wall-clock time by 36%,\ncompared to a full-attention baseline. During inference, LongGen reduces KV\ncache memory by 62%, achieving 1.67x prefilling speedup and 1.41x decoding\nspeedup."
                },
                "authors": [
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Xihui Lin"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Jiawei Han"
                    },
                    {
                        "name": "Hao Peng"
                    }
                ],
                "author_detail": {
                    "name": "Hao Peng"
                },
                "author": "Hao Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01485v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01485v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12335v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12335v2",
                "updated": "2024-10-02T00:19:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    2,
                    0,
                    19,
                    13,
                    2,
                    276,
                    0
                ],
                "published": "2024-06-18T07:01:11Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    7,
                    1,
                    11,
                    1,
                    170,
                    0
                ],
                "title": "Attention Score is not All You Need for Token Importance Indicator in KV\n  Cache Reduction: Value Also Matters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention Score is not All You Need for Token Importance Indicator in KV\n  Cache Reduction: Value Also Matters"
                },
                "summary": "Scaling the context size of large language models (LLMs) enables them to\nperform various new tasks, e.g., book summarization. However, the memory cost\nof the Key and Value (KV) cache in attention significantly limits the practical\napplications of LLMs. Recent works have explored token pruning for KV cache\nreduction in LLMs, relying solely on attention scores as a token importance\nindicator. However, our investigation into value vector norms revealed a\nnotably non-uniform pattern questioning their reliance only on attention\nscores. Inspired by this, we propose a new method: Value-Aware Token Pruning\n(VATP) which uses both attention scores and the $ \\ell_{1} $ norm of value\nvectors to evaluate token importance. Extensive experiments on LLaMA2-7B-chat\nand Vicuna-v1.5-7B across 16 LongBench tasks demonstrate that VATP outperforms\nattention-score-only baselines in over 12 tasks, confirming the effectiveness\nof incorporating value vector norms into token importance evaluation of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling the context size of large language models (LLMs) enables them to\nperform various new tasks, e.g., book summarization. However, the memory cost\nof the Key and Value (KV) cache in attention significantly limits the practical\napplications of LLMs. Recent works have explored token pruning for KV cache\nreduction in LLMs, relying solely on attention scores as a token importance\nindicator. However, our investigation into value vector norms revealed a\nnotably non-uniform pattern questioning their reliance only on attention\nscores. Inspired by this, we propose a new method: Value-Aware Token Pruning\n(VATP) which uses both attention scores and the $ \\ell_{1} $ norm of value\nvectors to evaluate token importance. Extensive experiments on LLaMA2-7B-chat\nand Vicuna-v1.5-7B across 16 LongBench tasks demonstrate that VATP outperforms\nattention-score-only baselines in over 12 tasks, confirming the effectiveness\nof incorporating value vector norms into token importance evaluation of LLMs."
                },
                "authors": [
                    {
                        "name": "Zhiyu Guo"
                    },
                    {
                        "name": "Hidetaka Kamigaito"
                    },
                    {
                        "name": "Taro Watanabe"
                    }
                ],
                "author_detail": {
                    "name": "Taro Watanabe"
                },
                "author": "Taro Watanabe",
                "arxiv_comment": "Accepted at EMNLP 2024 (Main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12335v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12335v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00644v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00644v1",
                "updated": "2024-10-01T12:55:47Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    12,
                    55,
                    47,
                    1,
                    275,
                    0
                ],
                "published": "2024-10-01T12:55:47Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    12,
                    55,
                    47,
                    1,
                    275,
                    0
                ],
                "title": "PARSIR: a Package for Effective Parallel Discrete Event Simulation on\n  Multi-processor Machines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PARSIR: a Package for Effective Parallel Discrete Event Simulation on\n  Multi-processor Machines"
                },
                "summary": "In this article we present PARSIR (PARallel SImulation Runner), a package\nthat enables the effective exploitation of shared-memory multi-processor\nmachines for running discrete event simulation models. PARSIR is a\ncompile/run-time environment for discrete event simulation models developed\nwith the {\\tt C} programming language. The architecture of PARSIR has been\ndesigned in order to keep low the amount of CPU-cycles required for running\nmodels. This is achieved via the combination of a set of techniques like: 1)\ncausally consistent batch-processing of simulation events at an individual\nsimulation object for caching effectiveness; 2) high likelihood of disjoint\naccess parallelism; 3) the favoring of memory accesses on local NUMA\n(Non-Uniform-Memory-Access) nodes in the architecture, while still enabling\nwell balanced workload distribution via work-stealing from remote nodes; 4) the\nuse of RMW (Read-Modify-Write) machine instructions for fast access to\nsimulation engine data required by the worker threads for managing the\nconcurrent simulation objects and distributing the workload. Furthermore, any\narchitectural solution embedded in the PARSIR engine is fully transparent to\nthe application level code implementing the simulation model. We also provide\nexperimental results showing the effectiveness of PARSIR when running the\nreference PHOLD benchmark on a NUMA shared-memory multi-processor machine\nequipped with 40 CPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this article we present PARSIR (PARallel SImulation Runner), a package\nthat enables the effective exploitation of shared-memory multi-processor\nmachines for running discrete event simulation models. PARSIR is a\ncompile/run-time environment for discrete event simulation models developed\nwith the {\\tt C} programming language. The architecture of PARSIR has been\ndesigned in order to keep low the amount of CPU-cycles required for running\nmodels. This is achieved via the combination of a set of techniques like: 1)\ncausally consistent batch-processing of simulation events at an individual\nsimulation object for caching effectiveness; 2) high likelihood of disjoint\naccess parallelism; 3) the favoring of memory accesses on local NUMA\n(Non-Uniform-Memory-Access) nodes in the architecture, while still enabling\nwell balanced workload distribution via work-stealing from remote nodes; 4) the\nuse of RMW (Read-Modify-Write) machine instructions for fast access to\nsimulation engine data required by the worker threads for managing the\nconcurrent simulation objects and distributing the workload. Furthermore, any\narchitectural solution embedded in the PARSIR engine is fully transparent to\nthe application level code implementing the simulation model. We also provide\nexperimental results showing the effectiveness of PARSIR when running the\nreference PHOLD benchmark on a NUMA shared-memory multi-processor machine\nequipped with 40 CPUs."
                },
                "authors": [
                    {
                        "name": "Francesco Quaglia"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Quaglia"
                },
                "author": "Francesco Quaglia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00644v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00644v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00455v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00455v1",
                "updated": "2024-10-01T07:19:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    7,
                    19,
                    21,
                    1,
                    275,
                    0
                ],
                "published": "2024-10-01T07:19:21Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    7,
                    19,
                    21,
                    1,
                    275,
                    0
                ],
                "title": "Fine-Grained Vectorized Merge Sorting on RISC-V: From Register to Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Grained Vectorized Merge Sorting on RISC-V: From Register to Cache"
                },
                "summary": "Merge sort as a divide-sort-merge paradigm has been widely applied in\ncomputer science fields. As modern reduced instruction set computing\narchitectures like the fifth generation (RISC-V) regard multiple registers as a\nvector register group for wide instruction parallelism, optimizing merge sort\nwith this vectorized property is becoming increasingly common. In this paper,\nwe overhaul the divide-sort-merge paradigm, from its register-level sort to the\ncache-aware merge, to develop a fine-grained RISC-V vectorized merge sort\n(RVMS). From the register-level view, the inline vectorized transpose\ninstruction is missed in RISC-V, so implementing it efficiently is non-trivial.\nBesides, the vectorized comparisons do not always work well in the merging\nnetworks. Both issues primarily stem from the expensive data shuffle\ninstruction. To bypass it, RVMS strides to take register data as the proxy of\ndata shuffle to accelerate the transpose operation, and meanwhile replaces\nvectorized comparisons with scalar cousin for more light real value swap. On\nthe other hand, as cache-aware merge makes larger data merge in the cache, most\nmerge schemes have two drawbacks: the in-cache merge usually has low cache\nutilization, while the out-of-cache merging network remains an ineffectively\nsymmetric structure. To this end, we propose the half-merge scheme to employ\nthe auxiliary space of in-place merge to halve the footprint of naive merge\nsort, and meanwhile copy one sequence to this space to avoid the former data\nexchange. Furthermore, an asymmetric merging network is developed to adapt to\ntwo different input sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Merge sort as a divide-sort-merge paradigm has been widely applied in\ncomputer science fields. As modern reduced instruction set computing\narchitectures like the fifth generation (RISC-V) regard multiple registers as a\nvector register group for wide instruction parallelism, optimizing merge sort\nwith this vectorized property is becoming increasingly common. In this paper,\nwe overhaul the divide-sort-merge paradigm, from its register-level sort to the\ncache-aware merge, to develop a fine-grained RISC-V vectorized merge sort\n(RVMS). From the register-level view, the inline vectorized transpose\ninstruction is missed in RISC-V, so implementing it efficiently is non-trivial.\nBesides, the vectorized comparisons do not always work well in the merging\nnetworks. Both issues primarily stem from the expensive data shuffle\ninstruction. To bypass it, RVMS strides to take register data as the proxy of\ndata shuffle to accelerate the transpose operation, and meanwhile replaces\nvectorized comparisons with scalar cousin for more light real value swap. On\nthe other hand, as cache-aware merge makes larger data merge in the cache, most\nmerge schemes have two drawbacks: the in-cache merge usually has low cache\nutilization, while the out-of-cache merging network remains an ineffectively\nsymmetric structure. To this end, we propose the half-merge scheme to employ\nthe auxiliary space of in-place merge to halve the footprint of naive merge\nsort, and meanwhile copy one sequence to this space to avoid the former data\nexchange. Furthermore, an asymmetric merging network is developed to adapt to\ntwo different input sizes."
                },
                "authors": [
                    {
                        "name": "Jin Zhang"
                    },
                    {
                        "name": "Jincheng Zhou"
                    },
                    {
                        "name": "Xiang Zhang"
                    },
                    {
                        "name": "Di Ma"
                    },
                    {
                        "name": "Chunye Gong"
                    }
                ],
                "author_detail": {
                    "name": "Chunye Gong"
                },
                "author": "Chunye Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00455v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00455v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00428v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00428v1",
                "updated": "2024-10-01T06:23:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    6,
                    23,
                    17,
                    1,
                    275,
                    0
                ],
                "published": "2024-10-01T06:23:17Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    6,
                    23,
                    17,
                    1,
                    275,
                    0
                ],
                "title": "LayerKV: Optimizing Large Language Model Serving with Layer-wise KV\n  Cache Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LayerKV: Optimizing Large Language Model Serving with Layer-wise KV\n  Cache Management"
                },
                "summary": "The expanding context windows in large language models (LLMs) have greatly\nenhanced their capabilities in various applications, but they also introduce\nsignificant challenges in maintaining low latency, particularly in Time to\nFirst Token (TTFT). This paper identifies that the sharp rise in TTFT as\ncontext length increases is predominantly driven by queuing delays, which are\ncaused by the growing demands for GPU Key-Value (KV) cache allocation clashing\nwith the limited availability of KV cache blocks. To address this issue, we\npropose LayerKV, a simple yet effective plug-in method that effectively reduces\nTTFT without requiring additional hardware or compromising output performance,\nwhile seamlessly integrating with existing parallelism strategies and\nscheduling techniques. Specifically, LayerKV introduces layer-wise KV block\nallocation, management, and offloading for fine-grained control over system\nmemory, coupled with an SLO-aware scheduler to optimize overall Service Level\nObjectives (SLOs). Comprehensive evaluations on representative models, ranging\nfrom 7B to 70B parameters, across various GPU configurations, demonstrate that\nLayerKV improves TTFT latency up to 11x and reduces SLO violation rates by\n28.7\\%, significantly enhancing the user experience",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expanding context windows in large language models (LLMs) have greatly\nenhanced their capabilities in various applications, but they also introduce\nsignificant challenges in maintaining low latency, particularly in Time to\nFirst Token (TTFT). This paper identifies that the sharp rise in TTFT as\ncontext length increases is predominantly driven by queuing delays, which are\ncaused by the growing demands for GPU Key-Value (KV) cache allocation clashing\nwith the limited availability of KV cache blocks. To address this issue, we\npropose LayerKV, a simple yet effective plug-in method that effectively reduces\nTTFT without requiring additional hardware or compromising output performance,\nwhile seamlessly integrating with existing parallelism strategies and\nscheduling techniques. Specifically, LayerKV introduces layer-wise KV block\nallocation, management, and offloading for fine-grained control over system\nmemory, coupled with an SLO-aware scheduler to optimize overall Service Level\nObjectives (SLOs). Comprehensive evaluations on representative models, ranging\nfrom 7B to 70B parameters, across various GPU configurations, demonstrate that\nLayerKV improves TTFT latency up to 11x and reduces SLO violation rates by\n28.7\\%, significantly enhancing the user experience"
                },
                "authors": [
                    {
                        "name": "Yi Xiong"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Changxu Shao"
                    },
                    {
                        "name": "Ziqing Wang"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Yuhong Guo"
                    },
                    {
                        "name": "Junping Zhao"
                    },
                    {
                        "name": "Ke Zhang"
                    },
                    {
                        "name": "Zhenxuan Pan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenxuan Pan"
                },
                "author": "Zhenxuan Pan",
                "arxiv_comment": "11 pages, 7 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00428v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00428v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11; C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15355v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15355v3",
                "updated": "2024-10-01T03:40:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    3,
                    40,
                    8,
                    1,
                    275,
                    0
                ],
                "published": "2024-09-14T02:34:26Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    2,
                    34,
                    26,
                    5,
                    258,
                    0
                ],
                "title": "Block-Attention for Efficient RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block-Attention for Efficient RAG"
                },
                "summary": "We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context. Instead,\nBlock-Attention divides retrieved documents into discrete blocks, with each\nblock independently calculating key-value (KV) states except for the final\nblock. In RAG scenarios, by defining each passage as a block, Block-Attention\nenables us to reuse the KV states of passages that have been seen before,\nthereby significantly reducing the latency and the computation overhead during\ninference. The implementation of Block-Attention involves block segmentation,\nposition re-encoding, and fine-tuning the LLM to adapt to the Block-Attention\nmechanism. Experiments on four RAG benchmarks demonstrate that after block\nfine-tuning, the Block-Attention model achieves performance comparable to\nself-attention models (68.4\\% vs 67.9\\% on Llama3) or even superior performance\n(62.8\\% vs 59.6\\% on Mistral). Notably, Block-Attention significantly reduces\nthe time to first token (TTFT) and floating point operations (FLOPs) to a very\nlow level. It only takes 45 ms to output the first token for an input sequence\nwith a total length of 32K. Compared to the self-attention models, the time\nconsumption and corresponding FLOPs are reduced by 98.7\\% and 99.8\\%,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context. Instead,\nBlock-Attention divides retrieved documents into discrete blocks, with each\nblock independently calculating key-value (KV) states except for the final\nblock. In RAG scenarios, by defining each passage as a block, Block-Attention\nenables us to reuse the KV states of passages that have been seen before,\nthereby significantly reducing the latency and the computation overhead during\ninference. The implementation of Block-Attention involves block segmentation,\nposition re-encoding, and fine-tuning the LLM to adapt to the Block-Attention\nmechanism. Experiments on four RAG benchmarks demonstrate that after block\nfine-tuning, the Block-Attention model achieves performance comparable to\nself-attention models (68.4\\% vs 67.9\\% on Llama3) or even superior performance\n(62.8\\% vs 59.6\\% on Mistral). Notably, Block-Attention significantly reduces\nthe time to first token (TTFT) and floating point operations (FLOPs) to a very\nlow level. It only takes 45 ms to output the first token for an input sequence\nwith a total length of 32K. Compared to the self-attention models, the time\nconsumption and corresponding FLOPs are reduced by 98.7\\% and 99.8\\%,\nrespectively."
                },
                "authors": [
                    {
                        "name": "East Sun"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Lan Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lan Tian"
                },
                "author": "Lan Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15355v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15355v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00359v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00359v1",
                "updated": "2024-10-01T03:14:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    1,
                    3,
                    14,
                    12,
                    1,
                    275,
                    0
                ],
                "published": "2024-10-01T03:14:12Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    3,
                    14,
                    12,
                    1,
                    275,
                    0
                ],
                "title": "Self-controller: Controlling LLMs with Multi-round Step-by-step\n  Self-awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-controller: Controlling LLMs with Multi-round Step-by-step\n  Self-awareness"
                },
                "summary": "The applications of large language models (LLMs) have been widely spread\nacross all domains. However, the basic abilities such as the controllability of\nLLMs are still limited. To address this, we propose \"Self-controller\", a novel\nagentic framework bringing self-awareness into LLMs' reasoning logic. The core\nidea of this work is to maintain states based on the LLM's response, letting\nthe LLM become self-aware of current status and think step by step in a\nmulti-round chain-of-thought paradigm. Our experiment on the state of textual\nlength has shown the controllability and effectiveness of the Self-controller.\nWe further implement a binary search algorithm to accelerate the generation\nprocess based on the linearity and monotonicity of the textual length state.\nAnother advantage of the Self-controller comes with DeepSeek's Context Caching\ntechnology, which significantly saves computational token consumption when a\ncluster of conversations shares the same prefix of context. Theoretically, we\nprove that in this scenario the extra time complexity is $O(c \\log n)$. Results\nof the back-of-the-envelope estimation suggest that the token consumption of\nour method is no more than twice as much as that of the trivial single-round\ngeneration. Furthermore, our ablation study on word constraints demonstrates\nthe Self-controller's consistent controllability across all foundation models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The applications of large language models (LLMs) have been widely spread\nacross all domains. However, the basic abilities such as the controllability of\nLLMs are still limited. To address this, we propose \"Self-controller\", a novel\nagentic framework bringing self-awareness into LLMs' reasoning logic. The core\nidea of this work is to maintain states based on the LLM's response, letting\nthe LLM become self-aware of current status and think step by step in a\nmulti-round chain-of-thought paradigm. Our experiment on the state of textual\nlength has shown the controllability and effectiveness of the Self-controller.\nWe further implement a binary search algorithm to accelerate the generation\nprocess based on the linearity and monotonicity of the textual length state.\nAnother advantage of the Self-controller comes with DeepSeek's Context Caching\ntechnology, which significantly saves computational token consumption when a\ncluster of conversations shares the same prefix of context. Theoretically, we\nprove that in this scenario the extra time complexity is $O(c \\log n)$. Results\nof the back-of-the-envelope estimation suggest that the token consumption of\nour method is no more than twice as much as that of the trivial single-round\ngeneration. Furthermore, our ablation study on word constraints demonstrates\nthe Self-controller's consistent controllability across all foundation models."
                },
                "authors": [
                    {
                        "name": "Xiao Peng"
                    },
                    {
                        "name": "Xufan Geng"
                    }
                ],
                "author_detail": {
                    "name": "Xufan Geng"
                },
                "author": "Xufan Geng",
                "arxiv_comment": "10 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00359v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00359v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05527v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05527v4",
                "updated": "2024-09-30T22:44:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    22,
                    44,
                    58,
                    0,
                    274,
                    0
                ],
                "published": "2024-03-08T18:48:30Z",
                "published_parsed": [
                    2024,
                    3,
                    8,
                    18,
                    48,
                    30,
                    4,
                    68,
                    0
                ],
                "title": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless\n  Generative Inference of LLM"
                },
                "summary": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-value (KV) caching has become the de-facto to accelerate generation speed\nfor large language models (LLMs) inference. However, the growing cache demand\nwith increasing sequence length has transformed LLM inference to be a memory\nbound problem, significantly constraining the system throughput. Existing\nmethods rely on dropping unimportant tokens or quantizing all entries\nuniformly. Such methods, however, often incur high approximation errors to\nrepresent the compressed matrices. The autoregressive decoding process further\ncompounds the error of each step, resulting in critical deviation in model\ngeneration and deterioration of performance. To tackle this challenge, we\npropose GEAR, an efficient KV cache compression framework that achieves\nnear-lossless high-ratio compression. GEAR first applies quantization to\nmajority of entries of similar magnitudes to ultra-low precision. It then\nemploys a low rank matrix to approximate the quantization error, and a sparse\nmatrix to remedy individual errors from outlier entries. By adeptly integrating\nthree techniques, GEAR is able to fully exploit their synergistic potentials.\nOur experiments demonstrate that compared to alternatives, GEAR achieves\nnear-lossless 4-bit KV cache compression with up to 2.38x throughput\nimprovement, while reducing peak-memory size up to 2.29x. Our code is publicly\navailable at https://github.com/HaoKang-Timmy/GEAR."
                },
                "authors": [
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Qingru Zhang"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Geonhwa Jeong"
                    },
                    {
                        "name": "Zaoxing Liu"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Tuo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Tuo Zhao"
                },
                "author": "Tuo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05527v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05527v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00161v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00161v1",
                "updated": "2024-09-30T19:09:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    19,
                    9,
                    13,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T19:09:13Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    19,
                    9,
                    13,
                    0,
                    274,
                    0
                ],
                "title": "KV-Compress: Paged KV-Cache Compression with Variable Compression Rates\n  per Attention Head",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Compress: Paged KV-Cache Compression with Variable Compression Rates\n  per Attention Head"
                },
                "summary": "Context lengths of Large Language Models (LLMs) have exploded in recent\nyears, with 128k-token context becoming a standard and million-token context\nbecoming a reality. Efficiently supporting long-context inference remains\nchallenging as the memory that must be allocated in key-value (KV) cache for a\ngeneration scales with its context length, limiting the number of long-context\nrequests that can be served concurrently under a given memory budget. KV cache\ncompression can mitigate this issue by removing under-utilized KVs from each\nattention head's cache and reducing its memory footprint. Higher theoretical\ncompression rates can be achieved when the number of removed KVs varies across\nattention heads, but application of such a strategy within existing inference\nframeworks adds fragmentation and cannot realize the theoretical compression\nrates in physical memory. We introduce KV-Compress, a novel compression method\nthat evicts contiguous KV blocks within a PagedAttention framework, reducing\nthe memory footprint of the KV cache proportionally to this theoretical\ncompression rate. Our method achieves state-of-the-art performance on LongBench\nfor both Mistral-7B-Instruct-v0.2 and Llama-3.1-8B-Instruct while lowering the\ntotal number of compressed KVs by 4x compared with prior methods. Evaluations\non Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct-FP8 achieve compression\nrates up to 8x with negligible impact on performance, and up to 64x while\nretaining over 90% of full-cache performance for all but three of the suite's\nsubsets. We benchmark an integration of our method with vLLM that increases\ntotal throughput by up to 5.18x by enabling larger decoding batches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context lengths of Large Language Models (LLMs) have exploded in recent\nyears, with 128k-token context becoming a standard and million-token context\nbecoming a reality. Efficiently supporting long-context inference remains\nchallenging as the memory that must be allocated in key-value (KV) cache for a\ngeneration scales with its context length, limiting the number of long-context\nrequests that can be served concurrently under a given memory budget. KV cache\ncompression can mitigate this issue by removing under-utilized KVs from each\nattention head's cache and reducing its memory footprint. Higher theoretical\ncompression rates can be achieved when the number of removed KVs varies across\nattention heads, but application of such a strategy within existing inference\nframeworks adds fragmentation and cannot realize the theoretical compression\nrates in physical memory. We introduce KV-Compress, a novel compression method\nthat evicts contiguous KV blocks within a PagedAttention framework, reducing\nthe memory footprint of the KV cache proportionally to this theoretical\ncompression rate. Our method achieves state-of-the-art performance on LongBench\nfor both Mistral-7B-Instruct-v0.2 and Llama-3.1-8B-Instruct while lowering the\ntotal number of compressed KVs by 4x compared with prior methods. Evaluations\non Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct-FP8 achieve compression\nrates up to 8x with negligible impact on performance, and up to 64x while\nretaining over 90% of full-cache performance for all but three of the suite's\nsubsets. We benchmark an integration of our method with vLLM that increases\ntotal throughput by up to 5.18x by enabling larger decoding batches."
                },
                "authors": [
                    {
                        "name": "Isaac Rehg"
                    }
                ],
                "author_detail": {
                    "name": "Isaac Rehg"
                },
                "author": "Isaac Rehg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00161v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00161v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2209.09166v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2209.09166v2",
                "updated": "2024-09-30T18:23:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    18,
                    23,
                    7,
                    0,
                    274,
                    0
                ],
                "published": "2022-09-19T16:35:28Z",
                "published_parsed": [
                    2022,
                    9,
                    19,
                    16,
                    35,
                    28,
                    0,
                    262,
                    0
                ],
                "title": "Cache-Oblivious Representation of B-Tree Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Oblivious Representation of B-Tree Structures"
                },
                "summary": "We propose a general data structure CORoBTS for storing B-tree-like search\ntrees dynamically in a cache-oblivious way combining the van Emde Boas memory\nlayout with packed memory array.\n  In the use of the vEB layout mostly search complexity was considered, so far.\nWe show the complexity of depth-first search of a subtree and contiguous memory\narea and provide better insight into the relationship between positions of\nvertices in tree and in memory. We describe how to build an arbitrary tree in\nvEB layout if we can simulate its depth-first search. Similarly, we examine\nbatch updates of packed memory array.\n  In CORoBTS, the stored search tree has to satisfy that all leaves are at the\nsame depth and vertices have arity between the chosen constants $a$ and $b$.\nThe data structure allows searching with an optimal I/O complexity\n$\\mathcal{O}(\\log_B{N})$ and is stored in linear space. It provides operations\nfor inserting and removing a subtree; both have an amortized I/O complexity\n$\\mathcal{O}(S\\cdot(\\log^2 N)/B + \\log_B N\\cdot\\log\\log S + 1)$ and amortized\ntime complexity $\\mathcal{O}(S\\cdot\\log^2 N)$, where $S$ is the size of the\nsubtree and $N$ the size of the whole stored tree. Rebuilding an existing\nsubtree saves the multiplicative $\\mathcal{O}(\\log^2 N)$ in both complexities\nif the number of vertices on individual tree levels is not changed; it is paid\nonly for the inserted/removed vertices otherwise.\n  Modifying cache-oblivious partially persistent array proposed by Davoodi et\nal. [ESA, pages 296-308. Springer, 2014] to use CORoBTS improves its space\ncomplexity from $\\mathcal{O}(U^{\\log_2 3} + V \\log U)$ to $\\mathcal{O}(U + V\n\\log U)$, where $U$ is the maximal size of the array and $V$ is the number of\nversions; the data locality and I/O complexity of both present and persistent\nreads are kept unchanged; I/O complexity of writes is worsened by a\npolylogarithmic factor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a general data structure CORoBTS for storing B-tree-like search\ntrees dynamically in a cache-oblivious way combining the van Emde Boas memory\nlayout with packed memory array.\n  In the use of the vEB layout mostly search complexity was considered, so far.\nWe show the complexity of depth-first search of a subtree and contiguous memory\narea and provide better insight into the relationship between positions of\nvertices in tree and in memory. We describe how to build an arbitrary tree in\nvEB layout if we can simulate its depth-first search. Similarly, we examine\nbatch updates of packed memory array.\n  In CORoBTS, the stored search tree has to satisfy that all leaves are at the\nsame depth and vertices have arity between the chosen constants $a$ and $b$.\nThe data structure allows searching with an optimal I/O complexity\n$\\mathcal{O}(\\log_B{N})$ and is stored in linear space. It provides operations\nfor inserting and removing a subtree; both have an amortized I/O complexity\n$\\mathcal{O}(S\\cdot(\\log^2 N)/B + \\log_B N\\cdot\\log\\log S + 1)$ and amortized\ntime complexity $\\mathcal{O}(S\\cdot\\log^2 N)$, where $S$ is the size of the\nsubtree and $N$ the size of the whole stored tree. Rebuilding an existing\nsubtree saves the multiplicative $\\mathcal{O}(\\log^2 N)$ in both complexities\nif the number of vertices on individual tree levels is not changed; it is paid\nonly for the inserted/removed vertices otherwise.\n  Modifying cache-oblivious partially persistent array proposed by Davoodi et\nal. [ESA, pages 296-308. Springer, 2014] to use CORoBTS improves its space\ncomplexity from $\\mathcal{O}(U^{\\log_2 3} + V \\log U)$ to $\\mathcal{O}(U + V\n\\log U)$, where $U$ is the maximal size of the array and $V$ is the number of\nversions; the data locality and I/O complexity of both present and persistent\nreads are kept unchanged; I/O complexity of writes is worsened by a\npolylogarithmic factor."
                },
                "authors": [
                    {
                        "name": "Luk Ondrek"
                    },
                    {
                        "name": "Ondej Mika"
                    }
                ],
                "author_detail": {
                    "name": "Ondej Mika"
                },
                "author": "Ondej Mika",
                "arxiv_comment": "30 pages + 7 pages of algorithms, 9 figures; changes: paper structure\n  improved, general (sub)tree (re)build added, DFS alg. simplified, build\n  complexity lowered,",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2209.09166v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2209.09166v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "E.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20433v1",
                "updated": "2024-09-30T15:53:36Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    53,
                    36,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T15:53:36Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    53,
                    36,
                    0,
                    274,
                    0
                ],
                "title": "Impact of Device Caching and Handovers on the Performance of 3D UAV\n  Networks with Blockages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Impact of Device Caching and Handovers on the Performance of 3D UAV\n  Networks with Blockages"
                },
                "summary": "We investigate an urban network characterized by blockages, where unmanned\naerial vehicles (UAVs) offer ad-hoc coverage to mobile users with distinct\nservice rate requirements. The UAV-BSs are modeled using a two-dimensional\n(2-D) marked-poisson point process (MPPP), where the marks represent the\naltitude of each UAV-base station (UAV-BS). Initially, we model the network\nblockages and analyze the association probabilities of line-of-sight (LoS) and\nnon-line-of-sight (NLoS) UAV-BSs using stochastic geometry. Subsequently, we\nderive the bth moment of the conditional success probability (CSP) and employ a\nmeta distribution (MD)-based analytical framework of signal-to-interference\nnoise ratio (SINR) taking into account the blockage distribution in the\nnetwork. Furthermore, we proposea cache-based handover management strategy that\ndynamically selects the cell search time and delays the received signal\nstrength (RSS)-based base station (BS) associations. This strategy aims to\nminimize unnecessary handovers (HOs) experienced by users by leveraging caching\ncapabilities at user equipment (UE). We evaluate the HO rate and average\nthroughput experienced by users ensuring their service rate requirements are\nmet. We demonstrate that LoS associations decrease as the network density\nincreases due to the substantial increase of NLoS UAV-BSs in the network.\nAdditionally, we show that the presence of blockages does not necessarily have\na negative impact on network reliability",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate an urban network characterized by blockages, where unmanned\naerial vehicles (UAVs) offer ad-hoc coverage to mobile users with distinct\nservice rate requirements. The UAV-BSs are modeled using a two-dimensional\n(2-D) marked-poisson point process (MPPP), where the marks represent the\naltitude of each UAV-base station (UAV-BS). Initially, we model the network\nblockages and analyze the association probabilities of line-of-sight (LoS) and\nnon-line-of-sight (NLoS) UAV-BSs using stochastic geometry. Subsequently, we\nderive the bth moment of the conditional success probability (CSP) and employ a\nmeta distribution (MD)-based analytical framework of signal-to-interference\nnoise ratio (SINR) taking into account the blockage distribution in the\nnetwork. Furthermore, we proposea cache-based handover management strategy that\ndynamically selects the cell search time and delays the received signal\nstrength (RSS)-based base station (BS) associations. This strategy aims to\nminimize unnecessary handovers (HOs) experienced by users by leveraging caching\ncapabilities at user equipment (UE). We evaluate the HO rate and average\nthroughput experienced by users ensuring their service rate requirements are\nmet. We demonstrate that LoS associations decrease as the network density\nincreases due to the substantial increase of NLoS UAV-BSs in the network.\nAdditionally, we show that the presence of blockages does not necessarily have\na negative impact on network reliability"
                },
                "authors": [
                    {
                        "name": "Neetu R R"
                    },
                    {
                        "name": "Gourab Ghatak"
                    },
                    {
                        "name": "Vivek Ashok Bohara"
                    }
                ],
                "author_detail": {
                    "name": "Vivek Ashok Bohara"
                },
                "author": "Vivek Ashok Bohara",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.08894v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.08894v2",
                "updated": "2024-09-30T14:38:41Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    14,
                    38,
                    41,
                    0,
                    274,
                    0
                ],
                "published": "2023-10-13T06:58:07Z",
                "published_parsed": [
                    2023,
                    10,
                    13,
                    6,
                    58,
                    7,
                    4,
                    286,
                    0
                ],
                "title": "Multi-Antenna Coded Caching for Multi-Access Networks with Cyclic\n  Wrap-Around",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Antenna Coded Caching for Multi-Access Networks with Cyclic\n  Wrap-Around"
                },
                "summary": "This work explores a multiple transmit antenna setting in a multi-access\ncoded caching (MACC) network where each user accesses more than one cache. A\nMACC network has $K$ users and $K$ caches, and each user has access to $r < K$\nconsecutive caches in a cyclic wrap-around manner. There are $L$ antennas at\nthe server, and each cache has a normalized size of $M/N \\leq 1$. The cyclic\nwrap-around MACC network with a single antenna at the server has been a\nwell-investigated topic, and several coded caching schemes and improved lower\nbounds on the performance are known for the same. However, this MACC network\nhas not yet been studied under multi-antenna settings in the coded caching\nliterature. We study the multi-antenna MACC problem and propose a solution for\nthe same by constructing a pair of arrays called caching and delivery arrays.\nWe present three constructions of caching and delivery arrays for different\nscenarios and obtain corresponding multi-antenna MACC schemes for the same. Two\nschemes resulting from the above constructions achieve optimal performance\nunder uncoded placement and one-shot delivery. The optimality is shown by\nmatching the performance of the multi-antenna MACC scheme to that of an optimal\nmulti-antenna scheme for a dedicated cache network having an identical number\nof users, and each user has a normalized cache size of $rM/N$. Further, as a\nspecial case, one of the proposed schemes subsumes an existing optimal MACC\nscheme for the single-antenna setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work explores a multiple transmit antenna setting in a multi-access\ncoded caching (MACC) network where each user accesses more than one cache. A\nMACC network has $K$ users and $K$ caches, and each user has access to $r < K$\nconsecutive caches in a cyclic wrap-around manner. There are $L$ antennas at\nthe server, and each cache has a normalized size of $M/N \\leq 1$. The cyclic\nwrap-around MACC network with a single antenna at the server has been a\nwell-investigated topic, and several coded caching schemes and improved lower\nbounds on the performance are known for the same. However, this MACC network\nhas not yet been studied under multi-antenna settings in the coded caching\nliterature. We study the multi-antenna MACC problem and propose a solution for\nthe same by constructing a pair of arrays called caching and delivery arrays.\nWe present three constructions of caching and delivery arrays for different\nscenarios and obtain corresponding multi-antenna MACC schemes for the same. Two\nschemes resulting from the above constructions achieve optimal performance\nunder uncoded placement and one-shot delivery. The optimality is shown by\nmatching the performance of the multi-antenna MACC scheme to that of an optimal\nmulti-antenna scheme for a dedicated cache network having an identical number\nof users, and each user has a normalized cache size of $rM/N$. Further, as a\nspecial case, one of the proposed schemes subsumes an existing optimal MACC\nscheme for the single-antenna setting."
                },
                "authors": [
                    {
                        "name": "Elizabath Peter"
                    },
                    {
                        "name": "K. K. Krishnan Namboodiri"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "A new construction of caching and delivery arrays is added which is\n  optimal (in Section IV.D). A new section (Section V) is also added which\n  contains performance comparison with existing schemes. 16 pages (double\n  column), 6 Figures and one table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.08894v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.08894v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20133v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20133v1",
                "updated": "2024-09-30T09:33:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    9,
                    33,
                    37,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T09:33:37Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    9,
                    33,
                    37,
                    0,
                    274,
                    0
                ],
                "title": "Improving Achievability of Cache-Aided Private Variable-Length Coding\n  with Zero Leakage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Achievability of Cache-Aided Private Variable-Length Coding\n  with Zero Leakage"
                },
                "summary": "A statistical cache-aided compression problem with a privacy constraint is\nstudied, where a server has access to a database of $N$ files, $(Y_1,...,Y_N)$,\neach of size $F$ bits and is linked through a shared channel to $K$ users,\nwhere each has access to a local cache memory of size $MF$ bits. During the\nplacement phase, the server fills the users' caches without prior knowledge of\ntheir demands, while the delivery phase takes place after the users send their\ndemands to the server. We assume that each file in database $Y_i$ is\narbitrarily correlated with a private attribute $X$, and an adversary is\nassumed to have access to the shared channel. The users and the server have\naccess to a shared key $W$. The goal is to design the cache contents and the\ndelivered message $\\cal C$ such that the average length of $\\mathcal{C}$ is\nminimized, while satisfying: i. The response $\\cal C$ does not reveal any\ninformation about $X$, i.e., $I(X;\\mathcal{C})=0$; ii. User $i$ can decode its\ndemand, $Y_{d_i}$, by using the shared key $W$, $\\cal C$, and its local cache\n$Z_i$. In a previous work, we have proposed a variable-length coding scheme\nthat combines privacy-aware compression with coded caching techniques. In this\npaper, we propose a new achievability scheme using minimum entropy coupling\nconcept and a greedy entropy-based algorithm. We show that the proposed scheme\nimproves the previous results. Moreover, considering two special cases we\nimprove the obtained bounds using the common information concept.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A statistical cache-aided compression problem with a privacy constraint is\nstudied, where a server has access to a database of $N$ files, $(Y_1,...,Y_N)$,\neach of size $F$ bits and is linked through a shared channel to $K$ users,\nwhere each has access to a local cache memory of size $MF$ bits. During the\nplacement phase, the server fills the users' caches without prior knowledge of\ntheir demands, while the delivery phase takes place after the users send their\ndemands to the server. We assume that each file in database $Y_i$ is\narbitrarily correlated with a private attribute $X$, and an adversary is\nassumed to have access to the shared channel. The users and the server have\naccess to a shared key $W$. The goal is to design the cache contents and the\ndelivered message $\\cal C$ such that the average length of $\\mathcal{C}$ is\nminimized, while satisfying: i. The response $\\cal C$ does not reveal any\ninformation about $X$, i.e., $I(X;\\mathcal{C})=0$; ii. User $i$ can decode its\ndemand, $Y_{d_i}$, by using the shared key $W$, $\\cal C$, and its local cache\n$Z_i$. In a previous work, we have proposed a variable-length coding scheme\nthat combines privacy-aware compression with coded caching techniques. In this\npaper, we propose a new achievability scheme using minimum entropy coupling\nconcept and a greedy entropy-based algorithm. We show that the proposed scheme\nimproves the previous results. Moreover, considering two special cases we\nimprove the obtained bounds using the common information concept."
                },
                "authors": [
                    {
                        "name": "Amirreza Zamani"
                    },
                    {
                        "name": "Mikael Skoglund"
                    }
                ],
                "author_detail": {
                    "name": "Mikael Skoglund"
                },
                "author": "Mikael Skoglund",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20133v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20133v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20002v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20002v1",
                "updated": "2024-09-30T06:55:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "published": "2024-09-30T06:55:00Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "title": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems"
                },
                "summary": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats."
                },
                "authors": [
                    {
                        "name": "Linke Song"
                    },
                    {
                        "name": "Zixuan Pang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Yier Jin"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20002v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19720v1",
                "updated": "2024-09-29T14:31:52Z",
                "updated_parsed": [
                    2024,
                    9,
                    29,
                    14,
                    31,
                    52,
                    6,
                    273,
                    0
                ],
                "published": "2024-09-29T14:31:52Z",
                "published_parsed": [
                    2024,
                    9,
                    29,
                    14,
                    31,
                    52,
                    6,
                    273,
                    0
                ],
                "title": "FAST: A Dual-tier Few-Shot Learning Paradigm for Whole Slide Image\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAST: A Dual-tier Few-Shot Learning Paradigm for Whole Slide Image\n  Classification"
                },
                "summary": "The expensive fine-grained annotation and data scarcity have become the\nprimary obstacles for the widespread adoption of deep learning-based Whole\nSlide Images (WSI) classification algorithms in clinical practice. Unlike\nfew-shot learning methods in natural images that can leverage the labels of\neach image, existing few-shot WSI classification methods only utilize a small\nnumber of fine-grained labels or weakly supervised slide labels for training in\norder to avoid expensive fine-grained annotation. They lack sufficient mining\nof available WSIs, severely limiting WSI classification performance. To address\nthe above issues, we propose a novel and efficient dual-tier few-shot learning\nparadigm for WSI classification, named FAST. FAST consists of a dual-level\nannotation strategy and a dual-branch classification framework. Firstly, to\navoid expensive fine-grained annotation, we collect a very small number of WSIs\nat the slide level, and annotate an extremely small number of patches. Then, to\nfully mining the available WSIs, we use all the patches and available patch\nlabels to build a cache branch, which utilizes the labeled patches to learn the\nlabels of unlabeled patches and through knowledge retrieval for patch\nclassification. In addition to the cache branch, we also construct a prior\nbranch that includes learnable prompt vectors, using the text encoder of\nvisual-language models for patch classification. Finally, we integrate the\nresults from both branches to achieve WSI classification. Extensive experiments\non binary and multi-class datasets demonstrate that our proposed method\nsignificantly surpasses existing few-shot classification methods and approaches\nthe accuracy of fully supervised methods with only 0.22$\\%$ annotation costs.\nAll codes and models will be publicly available on\nhttps://github.com/fukexue/FAST.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expensive fine-grained annotation and data scarcity have become the\nprimary obstacles for the widespread adoption of deep learning-based Whole\nSlide Images (WSI) classification algorithms in clinical practice. Unlike\nfew-shot learning methods in natural images that can leverage the labels of\neach image, existing few-shot WSI classification methods only utilize a small\nnumber of fine-grained labels or weakly supervised slide labels for training in\norder to avoid expensive fine-grained annotation. They lack sufficient mining\nof available WSIs, severely limiting WSI classification performance. To address\nthe above issues, we propose a novel and efficient dual-tier few-shot learning\nparadigm for WSI classification, named FAST. FAST consists of a dual-level\nannotation strategy and a dual-branch classification framework. Firstly, to\navoid expensive fine-grained annotation, we collect a very small number of WSIs\nat the slide level, and annotate an extremely small number of patches. Then, to\nfully mining the available WSIs, we use all the patches and available patch\nlabels to build a cache branch, which utilizes the labeled patches to learn the\nlabels of unlabeled patches and through knowledge retrieval for patch\nclassification. In addition to the cache branch, we also construct a prior\nbranch that includes learnable prompt vectors, using the text encoder of\nvisual-language models for patch classification. Finally, we integrate the\nresults from both branches to achieve WSI classification. Extensive experiments\non binary and multi-class datasets demonstrate that our proposed method\nsignificantly surpasses existing few-shot classification methods and approaches\nthe accuracy of fully supervised methods with only 0.22$\\%$ annotation costs.\nAll codes and models will be publicly available on\nhttps://github.com/fukexue/FAST."
                },
                "authors": [
                    {
                        "name": "Kexue Fu"
                    },
                    {
                        "name": "Xiaoyuan Luo"
                    },
                    {
                        "name": "Linhao Qu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Ilias Maglogiannis"
                    },
                    {
                        "name": "Longxiang Gao"
                    },
                    {
                        "name": "Manning Wang"
                    }
                ],
                "author_detail": {
                    "name": "Manning Wang"
                },
                "author": "Manning Wang",
                "arxiv_comment": "Accepted to NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19694v1",
                "updated": "2024-09-29T12:53:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    29,
                    12,
                    53,
                    29,
                    6,
                    273,
                    0
                ],
                "published": "2024-09-29T12:53:29Z",
                "published_parsed": [
                    2024,
                    9,
                    29,
                    12,
                    53,
                    29,
                    6,
                    273,
                    0
                ],
                "title": "Development of a 3D-printed canine head phantom for veterinary\n  radiotherapy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development of a 3D-printed canine head phantom for veterinary\n  radiotherapy"
                },
                "summary": "Purpose: To develop the Ultimate Phantom Dog for Orthovoltage Glioma\nTreatment (UPDOG), an anatomically-correct phantom which mimics a dog's head,\nfor quality assurance (QA) of kilovoltage (kV) radiotherapy treatments.\n  Methods: A computed tomography (CT) scan of a canine glioma patient was\nsegmented into bone and soft tissue using 3DSlicer. The segments were converted\nto stereolithographic (STL) files and smoothed in Fusion360. A slit to\naccommodate a radiochromic film (RCF) was added at the location of the glioma\ntumor. UPDOG was 3D printed on a polyjet printer using VeroUltraWhite ($\\rho$ =\n1.19-1.20 g/cm\\textsuperscript{3}) for the bone and Agilus30 ($\\rho$ =\n1.14-1.15 g/cm\\textsuperscript{3}) for the soft tissue. CT scans of UPDOG were\nacquired on a clinical CT scanner. An LD-V1 RCF was inserted into UPDOG and\nirradiated with a kV x-ray source from two angles. The delivered dose to the\nRCF was compared to Monte Carlo (MC) simulations performed in TOPAS.\n  Results: The bone and soft tissue segments in UPDOG were mimicked the patient\nanatomy well with tube voltage-dependent CT numbers. The contrast in HU was of\n49, 47 and 50 HU for the 80, 100, and 120 kVp scans, respectively, sufficient\nfor anatomy visualization. The irradiations delivered a maximum dose to RCF of\n284 mGy which was compared to the results of MC simulations using a depth dose\ncurve and central-axis (CAX) beam profiles. The mean difference in CAX profiles\nand PDD between RCF and MC results was 15.9\\% and 2.3\\%, respectively.\n  Conclusions: We have demonstrated that UPDOG is a useful QA tool for kV\ncanine radiotherapy. UPDOG successfully anatomically mimicked the dog anatomy,\nwith a reduced but sufficient bone contrast. We showed that dose delivered to a\ncanine glioma with kV x-rays can be successfully measured with an RCF\npositioned at the tumor location.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Purpose: To develop the Ultimate Phantom Dog for Orthovoltage Glioma\nTreatment (UPDOG), an anatomically-correct phantom which mimics a dog's head,\nfor quality assurance (QA) of kilovoltage (kV) radiotherapy treatments.\n  Methods: A computed tomography (CT) scan of a canine glioma patient was\nsegmented into bone and soft tissue using 3DSlicer. The segments were converted\nto stereolithographic (STL) files and smoothed in Fusion360. A slit to\naccommodate a radiochromic film (RCF) was added at the location of the glioma\ntumor. UPDOG was 3D printed on a polyjet printer using VeroUltraWhite ($\\rho$ =\n1.19-1.20 g/cm\\textsuperscript{3}) for the bone and Agilus30 ($\\rho$ =\n1.14-1.15 g/cm\\textsuperscript{3}) for the soft tissue. CT scans of UPDOG were\nacquired on a clinical CT scanner. An LD-V1 RCF was inserted into UPDOG and\nirradiated with a kV x-ray source from two angles. The delivered dose to the\nRCF was compared to Monte Carlo (MC) simulations performed in TOPAS.\n  Results: The bone and soft tissue segments in UPDOG were mimicked the patient\nanatomy well with tube voltage-dependent CT numbers. The contrast in HU was of\n49, 47 and 50 HU for the 80, 100, and 120 kVp scans, respectively, sufficient\nfor anatomy visualization. The irradiations delivered a maximum dose to RCF of\n284 mGy which was compared to the results of MC simulations using a depth dose\ncurve and central-axis (CAX) beam profiles. The mean difference in CAX profiles\nand PDD between RCF and MC results was 15.9\\% and 2.3\\%, respectively.\n  Conclusions: We have demonstrated that UPDOG is a useful QA tool for kV\ncanine radiotherapy. UPDOG successfully anatomically mimicked the dog anatomy,\nwith a reduced but sufficient bone contrast. We showed that dose delivered to a\ncanine glioma with kV x-rays can be successfully measured with an RCF\npositioned at the tumor location."
                },
                "authors": [
                    {
                        "name": "Sandhya Rottoo"
                    },
                    {
                        "name": "Luke Frangella"
                    },
                    {
                        "name": "Magdalena Bazalova-Carter"
                    },
                    {
                        "name": "Olivia Masella"
                    }
                ],
                "author_detail": {
                    "name": "Olivia Masella"
                },
                "author": "Olivia Masella",
                "arxiv_comment": "9 pages, 6 figures. Submitted to Biomedical Physics & Engineering\n  Express",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19478v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19478v1",
                "updated": "2024-09-28T23:01:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    28,
                    23,
                    1,
                    48,
                    5,
                    272,
                    0
                ],
                "published": "2024-09-28T23:01:48Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    23,
                    1,
                    48,
                    5,
                    272,
                    0
                ],
                "title": "RTL2M$$PATH: Multi-$$PATH Synthesis with Applications to Hardware\n  Security Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RTL2M$$PATH: Multi-$$PATH Synthesis with Applications to Hardware\n  Security Verification"
                },
                "summary": "The Check tools automate formal memory consistency model and security\nverification of processors by analyzing abstract models of microarchitectures,\ncalled $\\mu$SPEC models. Despite the efficacy of this approach, a verification\ngap between $\\mu$SPEC models, which must be manually written, and RTL limits\nthe Check tools' broad adoption. Our prior work, called RTL2$\\mu$SPEC, narrows\nthis gap by automatically synthesizing formally verified $\\mu$SPEC models from\nSystemVerilog implementations of simple processors. But, RTL2$\\mu$SPEC assumes\ninput designs where an instruction (e.g., a load) cannot exhibit more than one\nmicroarchitectural execution path ($\\mu$PATH, e.g., a cache hit or miss path)\n-- its single-execution-path assumption.\n  In this paper, we first propose an automated approach and tool, called\nRTL2M$\\mu$PATH, that resolves RTL2$\\mu$SPEC's single-execution-path assumption.\nGiven a SystemVerilog processor design, instruction encodings, and modest\ndesign metadata, RTL2M$\\mu$PATH finds a complete set of formally verified\n$\\mu$PATHs for each instruction. Next, we make an important observation: an\ninstruction that can exhibit more than one $\\mu$PATH strongly indicates the\npresence of a microarchitectural side channel in the input design. Based on\nthis observation, we then propose an automated approach and tool, called\nSynthLC, that extends RTL2M$\\mu$PATH with a symbolic information flow analysis\nto support synthesizing a variety of formally verified leakage contracts from\nSystemVerilog processor designs. Leakage contracts are foundational to\nstate-of-the-art defenses against hardware side-channel attacks. SynthLC is the\nfirst automated methodology for formally verifying hardware adherence to them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Check tools automate formal memory consistency model and security\nverification of processors by analyzing abstract models of microarchitectures,\ncalled $\\mu$SPEC models. Despite the efficacy of this approach, a verification\ngap between $\\mu$SPEC models, which must be manually written, and RTL limits\nthe Check tools' broad adoption. Our prior work, called RTL2$\\mu$SPEC, narrows\nthis gap by automatically synthesizing formally verified $\\mu$SPEC models from\nSystemVerilog implementations of simple processors. But, RTL2$\\mu$SPEC assumes\ninput designs where an instruction (e.g., a load) cannot exhibit more than one\nmicroarchitectural execution path ($\\mu$PATH, e.g., a cache hit or miss path)\n-- its single-execution-path assumption.\n  In this paper, we first propose an automated approach and tool, called\nRTL2M$\\mu$PATH, that resolves RTL2$\\mu$SPEC's single-execution-path assumption.\nGiven a SystemVerilog processor design, instruction encodings, and modest\ndesign metadata, RTL2M$\\mu$PATH finds a complete set of formally verified\n$\\mu$PATHs for each instruction. Next, we make an important observation: an\ninstruction that can exhibit more than one $\\mu$PATH strongly indicates the\npresence of a microarchitectural side channel in the input design. Based on\nthis observation, we then propose an automated approach and tool, called\nSynthLC, that extends RTL2M$\\mu$PATH with a symbolic information flow analysis\nto support synthesizing a variety of formally verified leakage contracts from\nSystemVerilog processor designs. Leakage contracts are foundational to\nstate-of-the-art defenses against hardware side-channel attacks. SynthLC is the\nfirst automated methodology for formally verifying hardware adherence to them."
                },
                "authors": [
                    {
                        "name": "Yao Hsiao"
                    },
                    {
                        "name": "Nikos Nikoleris"
                    },
                    {
                        "name": "Artem Khyzha"
                    },
                    {
                        "name": "Dominic P. Mulligan"
                    },
                    {
                        "name": "Gustavo Petri"
                    },
                    {
                        "name": "Christopher W. Fletcher"
                    },
                    {
                        "name": "Caroline Trippel"
                    }
                ],
                "author_detail": {
                    "name": "Caroline Trippel"
                },
                "author": "Caroline Trippel",
                "arxiv_comment": "Authors' version; to appear in the Proceedings of the 57th Annual\n  IEEE/ACM International Symposium on Microarchitecture 57th (MICRO 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19478v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19478v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19375v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19375v1",
                "updated": "2024-09-28T15:03:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    28,
                    15,
                    3,
                    28,
                    5,
                    272,
                    0
                ],
                "published": "2024-09-28T15:03:28Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    15,
                    3,
                    28,
                    5,
                    272,
                    0
                ],
                "title": "DOTA: Distributional Test-Time Adaptation of Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DOTA: Distributional Test-Time Adaptation of Vision-Language Models"
                },
                "summary": "Vision-language foundation models (e.g., CLIP) have shown remarkable\nperformance across a wide range of tasks. However, deploying these models may\nbe unreliable when significant distribution gaps exist between the training and\ntest data. The training-free test-time dynamic adapter (TDA) is a promising\napproach to address this issue by storing representative test samples to guide\nthe classification of subsequent ones. However, TDA only naively maintains a\nlimited number of reference samples in the cache, leading to severe test-time\ncatastrophic forgetting when the cache is updated by dropping samples. In this\npaper, we propose a simple yet effective method for DistributiOnal Test-time\nAdaptation (Dota). Instead of naively memorizing representative test samples,\nDota continually estimates the distributions of test samples, allowing the\nmodel to continually adapt to the deployment environment. The test-time\nposterior probabilities are then computed using the estimated distributions\nbased on Bayes' theorem for adaptation purposes. To further enhance the\nadaptability on the uncertain samples, we introduce a new human-in-the-loop\nparadigm which identifies uncertain samples, collects human-feedback, and\nincorporates it into the Dota framework. Extensive experiments validate that\nDota enables CLIP to continually learn, resulting in a significant improvement\ncompared to current state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language foundation models (e.g., CLIP) have shown remarkable\nperformance across a wide range of tasks. However, deploying these models may\nbe unreliable when significant distribution gaps exist between the training and\ntest data. The training-free test-time dynamic adapter (TDA) is a promising\napproach to address this issue by storing representative test samples to guide\nthe classification of subsequent ones. However, TDA only naively maintains a\nlimited number of reference samples in the cache, leading to severe test-time\ncatastrophic forgetting when the cache is updated by dropping samples. In this\npaper, we propose a simple yet effective method for DistributiOnal Test-time\nAdaptation (Dota). Instead of naively memorizing representative test samples,\nDota continually estimates the distributions of test samples, allowing the\nmodel to continually adapt to the deployment environment. The test-time\nposterior probabilities are then computed using the estimated distributions\nbased on Bayes' theorem for adaptation purposes. To further enhance the\nadaptability on the uncertain samples, we introduce a new human-in-the-loop\nparadigm which identifies uncertain samples, collects human-feedback, and\nincorporates it into the Dota framework. Extensive experiments validate that\nDota enables CLIP to continually learn, resulting in a significant improvement\ncompared to current state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Zongbo Han"
                    },
                    {
                        "name": "Jialong Yang"
                    },
                    {
                        "name": "Junfan Li"
                    },
                    {
                        "name": "Qinghua Hu"
                    },
                    {
                        "name": "Qianli Xu"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    },
                    {
                        "name": "Changqing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Changqing Zhang"
                },
                "author": "Changqing Zhang",
                "arxiv_comment": "In submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19375v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19315v1",
                "updated": "2024-09-28T11:00:11Z",
                "updated_parsed": [
                    2024,
                    9,
                    28,
                    11,
                    0,
                    11,
                    5,
                    272,
                    0
                ],
                "published": "2024-09-28T11:00:11Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    11,
                    0,
                    11,
                    5,
                    272,
                    0
                ],
                "title": "Analog In-Memory Computing Attention Mechanism for Fast and\n  Energy-Efficient Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analog In-Memory Computing Attention Mechanism for Fast and\n  Energy-Efficient Large Language Models"
                },
                "summary": "Transformer neural networks, driven by self-attention mechanisms, are core\ncomponents of foundational and Large Language Models. In generative\ntransformers, self-attention uses cache memory to store token projections,\navoiding recomputation at each time step. However, GPU-stored projections must\nbe loaded into SRAM for each new generation step, causing latency and energy\nbottlenecks for long sequences. In this work, we propose a fast and\nenergy-efficient hardware implementation of self-attention using analog\nin-memory computing based on gain cell memories. Volatile gain cell memories\ncan be efficiently written to store new tokens during sequence generation,\nwhile performing analog signed weight multiplications to compute the\ndot-products required for self-attention. We implement Sliding Window\nAttention, which keeps memory of a finite set of past steps. A charge-to-pulse\nconverter for array readout eliminates the need for analog-to-digital\nconversion between self-attention stages. Using a co-designed initialization\nalgorithm to adapt pre-trained weights to gain cell non-idealities, we achieve\nNLP performance comparable to ChatGPT-2 with minimal training iterations,\ndespite hardware constraints. Our end-to-end hardware design includes digital\ncontrols, estimating area, latency, and energy. The system reduces attention\nlatency by up to two orders of magnitude and energy consumption by up to five\norders compared to GPUs, marking a significant step toward ultra-fast,\nlow-power sequence generation in Large Language Models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer neural networks, driven by self-attention mechanisms, are core\ncomponents of foundational and Large Language Models. In generative\ntransformers, self-attention uses cache memory to store token projections,\navoiding recomputation at each time step. However, GPU-stored projections must\nbe loaded into SRAM for each new generation step, causing latency and energy\nbottlenecks for long sequences. In this work, we propose a fast and\nenergy-efficient hardware implementation of self-attention using analog\nin-memory computing based on gain cell memories. Volatile gain cell memories\ncan be efficiently written to store new tokens during sequence generation,\nwhile performing analog signed weight multiplications to compute the\ndot-products required for self-attention. We implement Sliding Window\nAttention, which keeps memory of a finite set of past steps. A charge-to-pulse\nconverter for array readout eliminates the need for analog-to-digital\nconversion between self-attention stages. Using a co-designed initialization\nalgorithm to adapt pre-trained weights to gain cell non-idealities, we achieve\nNLP performance comparable to ChatGPT-2 with minimal training iterations,\ndespite hardware constraints. Our end-to-end hardware design includes digital\ncontrols, estimating area, latency, and energy. The system reduces attention\nlatency by up to two orders of magnitude and energy consumption by up to five\norders compared to GPUs, marking a significant step toward ultra-fast,\nlow-power sequence generation in Large Language Models."
                },
                "authors": [
                    {
                        "name": "Nathan Leroux"
                    },
                    {
                        "name": "Paul-Philipp Manea"
                    },
                    {
                        "name": "Chirag Sudarshan"
                    },
                    {
                        "name": "Jan Finkbeiner"
                    },
                    {
                        "name": "Sebastian Siegel"
                    },
                    {
                        "name": "John Paul Strachan"
                    },
                    {
                        "name": "Emre Neftci"
                    }
                ],
                "author_detail": {
                    "name": "Emre Neftci"
                },
                "author": "Emre Neftci",
                "arxiv_comment": "25 pages, 6 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18523v1",
                "updated": "2024-09-27T08:05:34Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    8,
                    5,
                    34,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-27T08:05:34Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    8,
                    5,
                    34,
                    4,
                    271,
                    0
                ],
                "title": "Token Caching for Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token Caching for Diffusion Transformer Acceleration"
                },
                "summary": "Diffusion transformers have gained substantial interest in diffusion\ngenerative modeling due to their outstanding performance. However, their high\ncomputational cost, arising from the quadratic computational complexity of\nattention mechanisms and multi-step inference, presents a significant\nbottleneck. To address this challenge, we propose TokenCache, a novel\npost-training acceleration method that leverages the token-based multi-block\narchitecture of transformers to reduce redundant computations among tokens\nacross inference steps. TokenCache specifically addresses three critical\nquestions in the context of diffusion transformers: (1) which tokens should be\npruned to eliminate redundancy, (2) which blocks should be targeted for\nefficient pruning, and (3) at which time steps caching should be applied to\nbalance speed and quality. In response to these challenges, TokenCache\nintroduces a Cache Predictor that assigns importance scores to tokens, enabling\nselective pruning without compromising model performance. Furthermore, we\npropose an adaptive block selection strategy to focus on blocks with minimal\nimpact on the network's output, along with a Two-Phase Round-Robin (TPRR)\nscheduling policy to optimize caching intervals throughout the denoising\nprocess. Experimental results across various models demonstrate that TokenCache\nachieves an effective trade-off between generation quality and inference speed\nfor diffusion transformers. Our code will be publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have gained substantial interest in diffusion\ngenerative modeling due to their outstanding performance. However, their high\ncomputational cost, arising from the quadratic computational complexity of\nattention mechanisms and multi-step inference, presents a significant\nbottleneck. To address this challenge, we propose TokenCache, a novel\npost-training acceleration method that leverages the token-based multi-block\narchitecture of transformers to reduce redundant computations among tokens\nacross inference steps. TokenCache specifically addresses three critical\nquestions in the context of diffusion transformers: (1) which tokens should be\npruned to eliminate redundancy, (2) which blocks should be targeted for\nefficient pruning, and (3) at which time steps caching should be applied to\nbalance speed and quality. In response to these challenges, TokenCache\nintroduces a Cache Predictor that assigns importance scores to tokens, enabling\nselective pruning without compromising model performance. Furthermore, we\npropose an adaptive block selection strategy to focus on blocks with minimal\nimpact on the network's output, along with a Two-Phase Round-Robin (TPRR)\nscheduling policy to optimize caching intervals throughout the denoising\nprocess. Experimental results across various models demonstrate that TokenCache\nachieves an effective trade-off between generation quality and inference speed\nfor diffusion transformers. Our code will be publicly available."
                },
                "authors": [
                    {
                        "name": "Jinming Lou"
                    },
                    {
                        "name": "Wenyang Luo"
                    },
                    {
                        "name": "Yufan Liu"
                    },
                    {
                        "name": "Bing Li"
                    },
                    {
                        "name": "Xinmiao Ding"
                    },
                    {
                        "name": "Weiming Hu"
                    },
                    {
                        "name": "Jiajiong Cao"
                    },
                    {
                        "name": "Yuming Li"
                    },
                    {
                        "name": "Chenguang Ma"
                    }
                ],
                "author_detail": {
                    "name": "Chenguang Ma"
                },
                "author": "Chenguang Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14360v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14360v2",
                "updated": "2024-09-27T03:31:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    27,
                    3,
                    31,
                    39,
                    4,
                    271,
                    0
                ],
                "published": "2024-09-22T08:30:43Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    8,
                    30,
                    43,
                    6,
                    266,
                    0
                ],
                "title": "In-place Switch: Reprogramming based SLC Cache Design for Hybrid 3D SSDs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-place Switch: Reprogramming based SLC Cache Design for Hybrid 3D SSDs"
                },
                "summary": "Recently, 3D SSDs are widely adopted in PCs, data centers, and cloud storage\nsystems. To increase capacity, high bit-density cells, such as Triple-Level\nCell (TLC), are utilized within 3D SSDs. However, due to the inferior\nperformance of TLC, a portion of TLCs is configured to operate as Single-Level\nCell (SLC) to provide high performance, with host data initially directed to\nthe SLCs. In SLC/TLC hybrid 3D SSDs, a portion of the TLC space is designated\nas an SLC cache to achieve high SSD performance by writing host data at the SLC\nspeed. Given the limited size of the SLC cache, block reclamation is necessary\nto free up the SLC cache during idle periods. However, our preliminary studies\nindicate that the SLC cache can lead to a performance cliff if filled rapidly\nand cause significant write amplification when data migration occurs during\nidle times.\n  In this work, we propose leveraging a reprogram operation to address these\nchallenges. Specifically, when the SLC cache is full or during idle periods, a\nreprogram operation is performed to switch used SLC pages to TLC pages in place\n(termed In-place Switch, IPS). Subsequently, other free TLC space is allocated\nas the new SLC cache. IPS can continuously provide sufficient SLC cache within\nSSDs, significantly improving write performance and reducing write\namplification. Experimental results demonstrate that IPS can reduce write\nlatency and write amplification by up to 0.75 times and 0.53 times,\nrespectively, compared to state-of-the-art SLC cache technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, 3D SSDs are widely adopted in PCs, data centers, and cloud storage\nsystems. To increase capacity, high bit-density cells, such as Triple-Level\nCell (TLC), are utilized within 3D SSDs. However, due to the inferior\nperformance of TLC, a portion of TLCs is configured to operate as Single-Level\nCell (SLC) to provide high performance, with host data initially directed to\nthe SLCs. In SLC/TLC hybrid 3D SSDs, a portion of the TLC space is designated\nas an SLC cache to achieve high SSD performance by writing host data at the SLC\nspeed. Given the limited size of the SLC cache, block reclamation is necessary\nto free up the SLC cache during idle periods. However, our preliminary studies\nindicate that the SLC cache can lead to a performance cliff if filled rapidly\nand cause significant write amplification when data migration occurs during\nidle times.\n  In this work, we propose leveraging a reprogram operation to address these\nchallenges. Specifically, when the SLC cache is full or during idle periods, a\nreprogram operation is performed to switch used SLC pages to TLC pages in place\n(termed In-place Switch, IPS). Subsequently, other free TLC space is allocated\nas the new SLC cache. IPS can continuously provide sufficient SLC cache within\nSSDs, significantly improving write performance and reducing write\namplification. Experimental results demonstrate that IPS can reduce write\nlatency and write amplification by up to 0.75 times and 0.53 times,\nrespectively, compared to state-of-the-art SLC cache technologies."
                },
                "authors": [
                    {
                        "name": "Xufeng Yang"
                    },
                    {
                        "name": "Zhengjian Cong"
                    },
                    {
                        "name": "Congming Gao"
                    }
                ],
                "author_detail": {
                    "name": "Congming Gao"
                },
                "author": "Congming Gao",
                "arxiv_comment": "This paper has been submitted to NAS'24 (The 17th International\n  Conference on Networking, Architecture and Storage)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14360v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14360v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17606v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17606v1",
                "updated": "2024-09-26T07:44:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    26,
                    7,
                    44,
                    47,
                    3,
                    270,
                    0
                ],
                "published": "2024-09-26T07:44:47Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    7,
                    44,
                    47,
                    3,
                    270,
                    0
                ],
                "title": "FlooNoC: A 645 Gbps/link 0.15 pJ/B/hop Open-Source NoC with Wide\n  Physical Links and End-to-End AXI4 Parallel Multi-Stream Support",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlooNoC: A 645 Gbps/link 0.15 pJ/B/hop Open-Source NoC with Wide\n  Physical Links and End-to-End AXI4 Parallel Multi-Stream Support"
                },
                "summary": "The new generation of domain-specific AI accelerators is characterized by\nrapidly increasing demands for bulk data transfers, as opposed to small,\nlatency-critical cache line transfers typical of traditional cache-coherent\nsystems. In this paper, we address this critical need by introducing the\nFlooNoC Network-on-Chip (NoC), featuring very wide, fully Advanced eXtensible\nInterface (AXI4) compliant links designed to meet the massive bandwidth needs\nat high energy efficiency. At the transport level, non-blocking transactions\nare supported for latency tolerance. Additionally, a novel end-to-end ordering\napproach for AXI4, enabled by a multi-stream capable Direct Memory Access (DMA)\nengine simplifies network interfaces and eliminates inter-stream dependencies.\nFurthermore, dedicated physical links are instantiated for short,\nlatency-critical messages. A complete end-to-end reference implementation in\n12nm FinFET technology demonstrates the physical feasibility and power\nperformance area (PPA) benefits of our approach. Utilizing wide links on high\nlevels of metal, we achieve a bandwidth of 645 Gbps per link and a total\naggregate bandwidth of 103 Tbps for an 8x4 mesh of processors cluster tiles,\nwith a total of 288 RISC-V cores. The NoC imposes a minimal area overhead of\nonly 3.5% per compute tile and achieves a leading-edge energy efficiency of\n0.15 pJ/B/hop at 0.8 V. Compared to state-of-the-art NoCs, our system offers\nthree times the energy efficiency and more than double the link bandwidth.\nFurthermore, compared to a traditional AXI4-based multi-layer interconnect, our\nNoC achieves a 30% reduction in area, corresponding to a 47% increase in\nGFLOPSDP within the same floorplan.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The new generation of domain-specific AI accelerators is characterized by\nrapidly increasing demands for bulk data transfers, as opposed to small,\nlatency-critical cache line transfers typical of traditional cache-coherent\nsystems. In this paper, we address this critical need by introducing the\nFlooNoC Network-on-Chip (NoC), featuring very wide, fully Advanced eXtensible\nInterface (AXI4) compliant links designed to meet the massive bandwidth needs\nat high energy efficiency. At the transport level, non-blocking transactions\nare supported for latency tolerance. Additionally, a novel end-to-end ordering\napproach for AXI4, enabled by a multi-stream capable Direct Memory Access (DMA)\nengine simplifies network interfaces and eliminates inter-stream dependencies.\nFurthermore, dedicated physical links are instantiated for short,\nlatency-critical messages. A complete end-to-end reference implementation in\n12nm FinFET technology demonstrates the physical feasibility and power\nperformance area (PPA) benefits of our approach. Utilizing wide links on high\nlevels of metal, we achieve a bandwidth of 645 Gbps per link and a total\naggregate bandwidth of 103 Tbps for an 8x4 mesh of processors cluster tiles,\nwith a total of 288 RISC-V cores. The NoC imposes a minimal area overhead of\nonly 3.5% per compute tile and achieves a leading-edge energy efficiency of\n0.15 pJ/B/hop at 0.8 V. Compared to state-of-the-art NoCs, our system offers\nthree times the energy efficiency and more than double the link bandwidth.\nFurthermore, compared to a traditional AXI4-based multi-layer interconnect, our\nNoC achieves a 30% reduction in area, corresponding to a 47% increase in\nGFLOPSDP within the same floorplan."
                },
                "authors": [
                    {
                        "name": "Tim Fischer"
                    },
                    {
                        "name": "Michael Rogenmoser"
                    },
                    {
                        "name": "Thomas Benz"
                    },
                    {
                        "name": "Frank K. Grkaynak"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17606v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17606v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17374v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17374v1",
                "updated": "2024-09-25T21:37:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    21,
                    37,
                    1,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T21:37:01Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    21,
                    37,
                    1,
                    2,
                    269,
                    0
                ],
                "title": "NiOx/\\b{eta}-Ga2O3 Heterojunction Diode Achieving Breakdown Voltage >3\n  kV with Plasma Etch Field-Termination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NiOx/\\b{eta}-Ga2O3 Heterojunction Diode Achieving Breakdown Voltage >3\n  kV with Plasma Etch Field-Termination"
                },
                "summary": "This work reports the fabrication and characterization of a\nNiOx/\\b{eta}-Ga2O3 heterojunction diode (HJD) that uses a metallic nickel (Ni)\ntarget to deposit NiOx layers via reactive RF magnetron sputtering and lift-off\nprocessing with >3 kV breakdown voltage, record-low reverse current leakage\nunder high reverse bias, and high junction electric fields (>3.34 MV/cm). The\nheterojunction diodes are fabricated via bilayer NiOx sputtering followed by\nself-aligned mesa-etching for field-termination on both large (1-mm2) and small\narea (100-{\\mu}m diameter) devices. The HJD exhibits a ~135 A/cm2 forward\ncurrent density at 5 V with a rectifying ratio of ~1010. The minimum\ndifferential specific on-resistance is measured to be 17.26 m{\\Omega} cm2. The\nbreakdown voltage on 100-{\\mu}m diameter pads was measured to be greater than 3\nkV with a noise floor-level reverse leakage current density (10-8~10-6 A/cm2)\nuntil 3 kV, accomplishing a parallel-plane junction electric field to be at\nleast 3.34 MV/cm at 3 kV with a power figure of merit (PFOM) >0.52 GW/cm2.\nTemperature-dependent forward current density-voltage (J-V) measurements are\nperformed from room temperature (25 C) to 200 C which showed a temperature\ncoefficient of resistance ({\\alpha}) equaling 1.56, higher than that of\n\\b{eta}-Ga2O3 Schottky barrier diodes (SBDs), indicating potential conductivity\ndegradation within NiOx at elevated temperatures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work reports the fabrication and characterization of a\nNiOx/\\b{eta}-Ga2O3 heterojunction diode (HJD) that uses a metallic nickel (Ni)\ntarget to deposit NiOx layers via reactive RF magnetron sputtering and lift-off\nprocessing with >3 kV breakdown voltage, record-low reverse current leakage\nunder high reverse bias, and high junction electric fields (>3.34 MV/cm). The\nheterojunction diodes are fabricated via bilayer NiOx sputtering followed by\nself-aligned mesa-etching for field-termination on both large (1-mm2) and small\narea (100-{\\mu}m diameter) devices. The HJD exhibits a ~135 A/cm2 forward\ncurrent density at 5 V with a rectifying ratio of ~1010. The minimum\ndifferential specific on-resistance is measured to be 17.26 m{\\Omega} cm2. The\nbreakdown voltage on 100-{\\mu}m diameter pads was measured to be greater than 3\nkV with a noise floor-level reverse leakage current density (10-8~10-6 A/cm2)\nuntil 3 kV, accomplishing a parallel-plane junction electric field to be at\nleast 3.34 MV/cm at 3 kV with a power figure of merit (PFOM) >0.52 GW/cm2.\nTemperature-dependent forward current density-voltage (J-V) measurements are\nperformed from room temperature (25 C) to 200 C which showed a temperature\ncoefficient of resistance ({\\alpha}) equaling 1.56, higher than that of\n\\b{eta}-Ga2O3 Schottky barrier diodes (SBDs), indicating potential conductivity\ndegradation within NiOx at elevated temperatures."
                },
                "authors": [
                    {
                        "name": "Yizheng Liu"
                    },
                    {
                        "name": "Saurav Roy"
                    },
                    {
                        "name": "Carl Peterson"
                    },
                    {
                        "name": "Arkka Bhattacharyya"
                    },
                    {
                        "name": "Sriram Krishnamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Sriram Krishnamoorthy"
                },
                "author": "Sriram Krishnamoorthy",
                "arxiv_comment": "6 pages, 5 figures, APL Journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17374v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17374v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17264v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17264v1",
                "updated": "2024-09-25T18:21:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    18,
                    21,
                    5,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T18:21:05Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    18,
                    21,
                    5,
                    2,
                    269,
                    0
                ],
                "title": "Mnemosyne: Parallelization Strategies for Efficiently Serving\n  Multi-Million Context Length LLM Inference Requests Without Approximations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mnemosyne: Parallelization Strategies for Efficiently Serving\n  Multi-Million Context Length LLM Inference Requests Without Approximations"
                },
                "summary": "As large language models (LLMs) evolve to handle increasingly longer\ncontexts, serving inference requests for context lengths in the range of\nmillions of tokens presents unique challenges. While existing techniques are\neffective for training, they fail to address the unique challenges of\ninference, such as varying prefill and decode phases and their associated\nlatency constraints - like Time to First Token (TTFT) and Time Between Tokens\n(TBT). Furthermore, there are no long context inference solutions that allow\nbatching requests to increase the hardware utilization today.\n  In this paper, we propose three key innovations for efficient interactive\nlong context LLM inference, without resorting to any approximation: adaptive\nchunking to reduce prefill overheads in mixed batching, Sequence Pipeline\nParallelism (SPP) to lower TTFT, and KV Cache Parallelism (KVP) to minimize\nTBT. These contributions are combined into a 3D parallelism strategy, enabling\nMnemosyne to scale interactive inference to context lengths at least up to 10\nmillion tokens with high throughput enabled with batching. To our knowledge,\nMnemosyne is the first to be able to achieve support for 10 million long\ncontext inference efficiently, while satisfying production-grade SLOs on TBT\n(30ms) on contexts up to and including 10 million.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) evolve to handle increasingly longer\ncontexts, serving inference requests for context lengths in the range of\nmillions of tokens presents unique challenges. While existing techniques are\neffective for training, they fail to address the unique challenges of\ninference, such as varying prefill and decode phases and their associated\nlatency constraints - like Time to First Token (TTFT) and Time Between Tokens\n(TBT). Furthermore, there are no long context inference solutions that allow\nbatching requests to increase the hardware utilization today.\n  In this paper, we propose three key innovations for efficient interactive\nlong context LLM inference, without resorting to any approximation: adaptive\nchunking to reduce prefill overheads in mixed batching, Sequence Pipeline\nParallelism (SPP) to lower TTFT, and KV Cache Parallelism (KVP) to minimize\nTBT. These contributions are combined into a 3D parallelism strategy, enabling\nMnemosyne to scale interactive inference to context lengths at least up to 10\nmillion tokens with high throughput enabled with batching. To our knowledge,\nMnemosyne is the first to be able to achieve support for 10 million long\ncontext inference efficiently, while satisfying production-grade SLOs on TBT\n(30ms) on contexts up to and including 10 million."
                },
                "authors": [
                    {
                        "name": "Amey Agrawal"
                    },
                    {
                        "name": "Junda Chen"
                    },
                    {
                        "name": "igo Goiri"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    },
                    {
                        "name": "Chaojie Zhang"
                    },
                    {
                        "name": "Alexey Tumanov"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17264v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17264v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17136v1",
                "updated": "2024-09-25T17:55:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    55,
                    7,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T17:55:07Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    17,
                    55,
                    7,
                    2,
                    269,
                    0
                ],
                "title": "Adaptive Cost Model for Query Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Cost Model for Query Optimization"
                },
                "summary": "The principal component of conventional database query optimizers is a cost\nmodel that is used to estimate expected performance of query plans. The\naccuracy of the cost model has direct impact on the optimality of execution\nplans selected by the optimizer and thus, on the resulting query latency.\nSeveral common parameters of cost models in modern DBMS are related to the\nperformance of CPU and I/O and are typically set by a database administrator\nupon system tuning. However these performance characteristics are not stable\nand therefore, a single point estimation may not suffice for all DB load\nregimes. In this paper, we propose an Adaptive Cost Model (ACM) which\ndynamically optimizes CPU- and I/O-related plan cost parameters at DB runtime.\nBy continuously monitoring query execution statistics and the state of DB\nbuffer cache ACM adjusts cost parameters without the need for manual\nintervention from a database administrator. This allows for responding to\nchanges in the workload and system performance ensuring more optimal query\nexecution plans. We describe the main ideas in the implementation of ACM and\nreport on a preliminary experimental evaluation showing 20\\% end-to-end latency\nimprovement on TPC-H benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The principal component of conventional database query optimizers is a cost\nmodel that is used to estimate expected performance of query plans. The\naccuracy of the cost model has direct impact on the optimality of execution\nplans selected by the optimizer and thus, on the resulting query latency.\nSeveral common parameters of cost models in modern DBMS are related to the\nperformance of CPU and I/O and are typically set by a database administrator\nupon system tuning. However these performance characteristics are not stable\nand therefore, a single point estimation may not suffice for all DB load\nregimes. In this paper, we propose an Adaptive Cost Model (ACM) which\ndynamically optimizes CPU- and I/O-related plan cost parameters at DB runtime.\nBy continuously monitoring query execution statistics and the state of DB\nbuffer cache ACM adjusts cost parameters without the need for manual\nintervention from a database administrator. This allows for responding to\nchanges in the workload and system performance ensuring more optimal query\nexecution plans. We describe the main ideas in the implementation of ACM and\nreport on a preliminary experimental evaluation showing 20\\% end-to-end latency\nimprovement on TPC-H benchmark."
                },
                "authors": [
                    {
                        "name": "Nikita Vasilenko"
                    },
                    {
                        "name": "Alexander Demin"
                    },
                    {
                        "name": "Denis Ponomaryov"
                    }
                ],
                "author_detail": {
                    "name": "Denis Ponomaryov"
                },
                "author": "Denis Ponomaryov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T05, 68P15",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16743v1",
                "updated": "2024-09-25T08:52:07Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    8,
                    52,
                    7,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T08:52:07Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    8,
                    52,
                    7,
                    2,
                    269,
                    0
                ],
                "title": "Event-Triggered Non-Linear Control of Offshore MMC Grids for\n  Asymmetrical AC Faults",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Event-Triggered Non-Linear Control of Offshore MMC Grids for\n  Asymmetrical AC Faults"
                },
                "summary": "Fault ride-through capability studies of MMC-HVDC connected wind power plants\nhave focused primarily on the DC link and onshore AC grid faults. Offshore AC\nfaults, mainly asymmetrical faults have not gained much attention in the\nliterature despite being included in the future development at national levels\nin the ENTSO-E HVDC code. The proposed work gives an event-triggered control to\nstabilize the system once the offshore AC fault has occurred, identified, and\nisolated. Different types of control actions such as proportional-integral (PI)\ncontroller and super-twisted sliding mode control (STSMC) are used to smoothly\ntransition the post-fault system to a new steady state operating point by\nsuppressing the negative sequence control. Initially, the effect of a negative\nsequence current control scheme on the transient behavior of the power system\nwith a PI controller is discussed in this paper. Further, a non-linear control\nstrategy (STSMC) is proposed which gives quicker convergence of the system\npost-fault in comparison to PI control action. These post-fault control\noperations are only triggered in the presence of a fault in the system, i.e.,\nthey are event-triggered. The validity of the proposed strategy is demonstrated\nby simulation on a $\\pm$525 kV, three-terminal meshed MMC-HVDC system model in\nReal Time Digital Simulator (RTDS).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fault ride-through capability studies of MMC-HVDC connected wind power plants\nhave focused primarily on the DC link and onshore AC grid faults. Offshore AC\nfaults, mainly asymmetrical faults have not gained much attention in the\nliterature despite being included in the future development at national levels\nin the ENTSO-E HVDC code. The proposed work gives an event-triggered control to\nstabilize the system once the offshore AC fault has occurred, identified, and\nisolated. Different types of control actions such as proportional-integral (PI)\ncontroller and super-twisted sliding mode control (STSMC) are used to smoothly\ntransition the post-fault system to a new steady state operating point by\nsuppressing the negative sequence control. Initially, the effect of a negative\nsequence current control scheme on the transient behavior of the power system\nwith a PI controller is discussed in this paper. Further, a non-linear control\nstrategy (STSMC) is proposed which gives quicker convergence of the system\npost-fault in comparison to PI control action. These post-fault control\noperations are only triggered in the presence of a fault in the system, i.e.,\nthey are event-triggered. The validity of the proposed strategy is demonstrated\nby simulation on a $\\pm$525 kV, three-terminal meshed MMC-HVDC system model in\nReal Time Digital Simulator (RTDS)."
                },
                "authors": [
                    {
                        "name": "Naajein Cherat"
                    },
                    {
                        "name": "Vaibhav Nougain"
                    },
                    {
                        "name": "Milovan Majstorovi"
                    },
                    {
                        "name": "Peter Palensky"
                    },
                    {
                        "name": "Aleksandra Leki"
                    }
                ],
                "author_detail": {
                    "name": "Aleksandra Leki"
                },
                "author": "Aleksandra Leki",
                "arxiv_journal_ref": "ISGT 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16546v1",
                "updated": "2024-09-25T01:39:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    25,
                    1,
                    39,
                    2,
                    2,
                    269,
                    0
                ],
                "published": "2024-09-25T01:39:02Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    1,
                    39,
                    2,
                    2,
                    269,
                    0
                ],
                "title": "AlignedKV: Reducing Memory Access of KV-Cache with Precision-Aligned\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlignedKV: Reducing Memory Access of KV-Cache with Precision-Aligned\n  Quantization"
                },
                "summary": "Model quantization has become a crucial technique to address the issues of\nlarge memory consumption and long inference times associated with LLMs.\nMixed-precision quantization, which distinguishes between important and\nunimportant parameters, stands out among numerous quantization schemes as it\nachieves a balance between precision and compression rate. However, existing\napproaches can only identify important parameters through qualitative analysis\nand manual experiments without quantitatively analyzing how their importance is\ndetermined. We propose a new criterion, so-called 'precision alignment', to\nbuild a quantitative framework to holistically evaluate the importance of\nparameters in mixed-precision quantization. Our observations on floating point\naddition under various real-world scenarios suggest that two addends should\nhave identical precision, otherwise the information in the higher-precision\nnumber will be wasted. Such an observation offers an essential principle to\ndetermine the precision of each parameter in matrix multiplication operation.\nAs the first step towards applying the above discovery to large model\ninference, we develop a dynamic KV-Cache quantization technique to effectively\nreduce memory access latency. Different from existing quantization approaches\nthat focus on memory saving, this work directly aims to accelerate LLM\ninference through quantifying floating numbers. The proposed technique attains\na 25% saving of memory access and delivers up to 1.3x speedup in the\ncomputation of attention in the decoding phase of LLM, with almost no loss of\nprecision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model quantization has become a crucial technique to address the issues of\nlarge memory consumption and long inference times associated with LLMs.\nMixed-precision quantization, which distinguishes between important and\nunimportant parameters, stands out among numerous quantization schemes as it\nachieves a balance between precision and compression rate. However, existing\napproaches can only identify important parameters through qualitative analysis\nand manual experiments without quantitatively analyzing how their importance is\ndetermined. We propose a new criterion, so-called 'precision alignment', to\nbuild a quantitative framework to holistically evaluate the importance of\nparameters in mixed-precision quantization. Our observations on floating point\naddition under various real-world scenarios suggest that two addends should\nhave identical precision, otherwise the information in the higher-precision\nnumber will be wasted. Such an observation offers an essential principle to\ndetermine the precision of each parameter in matrix multiplication operation.\nAs the first step towards applying the above discovery to large model\ninference, we develop a dynamic KV-Cache quantization technique to effectively\nreduce memory access latency. Different from existing quantization approaches\nthat focus on memory saving, this work directly aims to accelerate LLM\ninference through quantifying floating numbers. The proposed technique attains\na 25% saving of memory access and delivers up to 1.3x speedup in the\ncomputation of attention in the decoding phase of LLM, with almost no loss of\nprecision."
                },
                "authors": [
                    {
                        "name": "Yifan Tan"
                    },
                    {
                        "name": "Haoze Wang"
                    },
                    {
                        "name": "Chao Yan"
                    },
                    {
                        "name": "Yangdong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yangdong Deng"
                },
                "author": "Yangdong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16258v1",
                "updated": "2024-09-24T17:28:47Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    17,
                    28,
                    47,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T17:28:47Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    17,
                    28,
                    47,
                    1,
                    268,
                    0
                ],
                "title": "SWARM: Replicating Shared Disaggregated-Memory Data in No Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWARM: Replicating Shared Disaggregated-Memory Data in No Time"
                },
                "summary": "Memory disaggregation is an emerging data center architecture that improves\nresource utilization and scalability. Replication is key to ensure the fault\ntolerance of applications, but replicating shared data in disaggregated memory\nis hard. We propose SWARM (Swift WAit-free Replication in disaggregated\nMemory), the first replication scheme for in-disaggregated-memory shared\nobjects to provide (1) single-roundtrip reads and writes in the common case,\n(2) strong consistency (linearizability), and (3) strong liveness\n(wait-freedom). SWARM makes two independent contributions. The first is\nSafe-Guess, a novel wait-free replication protocol with single-roundtrip\noperations. The second is In-n-Out, a novel technique to provide conditional\natomic update and atomic retrieval of large buffers in disaggregated memory in\none roundtrip. Using SWARM, we build SWARM-KV, a low-latency, strongly\nconsistent and highly available disaggregated key-value store. We evaluate\nSWARM-KV and find that it has marginal latency overhead compared to an\nunreplicated key-value store, and that it offers much lower latency and better\navailability than FUSEE, a state-of-the-art replicated disaggregated key-value\nstore.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory disaggregation is an emerging data center architecture that improves\nresource utilization and scalability. Replication is key to ensure the fault\ntolerance of applications, but replicating shared data in disaggregated memory\nis hard. We propose SWARM (Swift WAit-free Replication in disaggregated\nMemory), the first replication scheme for in-disaggregated-memory shared\nobjects to provide (1) single-roundtrip reads and writes in the common case,\n(2) strong consistency (linearizability), and (3) strong liveness\n(wait-freedom). SWARM makes two independent contributions. The first is\nSafe-Guess, a novel wait-free replication protocol with single-roundtrip\noperations. The second is In-n-Out, a novel technique to provide conditional\natomic update and atomic retrieval of large buffers in disaggregated memory in\none roundtrip. Using SWARM, we build SWARM-KV, a low-latency, strongly\nconsistent and highly available disaggregated key-value store. We evaluate\nSWARM-KV and find that it has marginal latency overhead compared to an\nunreplicated key-value store, and that it offers much lower latency and better\navailability than FUSEE, a state-of-the-art replicated disaggregated key-value\nstore."
                },
                "authors": [
                    {
                        "name": "Antoine Murat"
                    },
                    {
                        "name": "Clment Burgelin"
                    },
                    {
                        "name": "Athanasios Xygkis"
                    },
                    {
                        "name": "Igor Zablotchi"
                    },
                    {
                        "name": "Marcos K. Aguilera"
                    },
                    {
                        "name": "Rachid Guerraoui"
                    }
                ],
                "author_detail": {
                    "name": "Rachid Guerraoui"
                },
                "author": "Rachid Guerraoui",
                "arxiv_doi": "10.1145/3694715.3695945",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3694715.3695945",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.16258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "To appear in the proceedings of SOSP '24",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16110v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16110v1",
                "updated": "2024-09-24T14:16:26Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    14,
                    16,
                    26,
                    1,
                    268,
                    0
                ],
                "published": "2024-09-24T14:16:26Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    14,
                    16,
                    26,
                    1,
                    268,
                    0
                ],
                "title": "Wind lulls and slews; consequences for the stability of future UK\n  electricity systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wind lulls and slews; consequences for the stability of future UK\n  electricity systems"
                },
                "summary": "As the United Kingdom wind fleet increases in size, wind lulls and slews will\nincreasingly challenge the stability of its electricity system. The paper\ndescribes the use of models based on real time records and including solar\nslews, to investigate the most extreme wind variations likely to be encountered\nin future, enabling strategies to be devised to mitigate them. Wind lulls are\nsurprisingly frequent, occasionally lasting a week or more, and are always\nlikely to be beyond the capabilities of stored or imported electrical energy to\nmitigate them. The models indicate that there will be a continuing need for gas\npowered generation to mitigate wind lulls. Currently, Combined Cycle Gas\nTurbines (CCGTs) provide most of the dispatchable generation. However, CCGTs\nare not sufficiently fast acting to cope with the wind and solar slews\nanticipated in future. The paper suggests that a range of already proven\nfast-acting sources of dispatchable generation, including Open Cycle Gas\nTurbines (OCGTs), Internal Combustion Gas-Fired Reciprocating engines (ICGRs)\nand stored electrical energy systems, should be capable of coping with the\nlargest wind and solar slews likely to be encountered up to the year 2035.\nExamples are given of the recent introduction of these fast-acting sources of\ngeneration which, it is suggested, will progressively replace CCGTs as the wind\nand solar fleets increase in size. Moreover, we see the pattern of recent\ninvestments, summarised in the paper, as a good indication of likely future\ninvestments, with OCGT investments mainly serving the 440 kV grid, and ICGRs\nand stored electrical energy more local networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the United Kingdom wind fleet increases in size, wind lulls and slews will\nincreasingly challenge the stability of its electricity system. The paper\ndescribes the use of models based on real time records and including solar\nslews, to investigate the most extreme wind variations likely to be encountered\nin future, enabling strategies to be devised to mitigate them. Wind lulls are\nsurprisingly frequent, occasionally lasting a week or more, and are always\nlikely to be beyond the capabilities of stored or imported electrical energy to\nmitigate them. The models indicate that there will be a continuing need for gas\npowered generation to mitigate wind lulls. Currently, Combined Cycle Gas\nTurbines (CCGTs) provide most of the dispatchable generation. However, CCGTs\nare not sufficiently fast acting to cope with the wind and solar slews\nanticipated in future. The paper suggests that a range of already proven\nfast-acting sources of dispatchable generation, including Open Cycle Gas\nTurbines (OCGTs), Internal Combustion Gas-Fired Reciprocating engines (ICGRs)\nand stored electrical energy systems, should be capable of coping with the\nlargest wind and solar slews likely to be encountered up to the year 2035.\nExamples are given of the recent introduction of these fast-acting sources of\ngeneration which, it is suggested, will progressively replace CCGTs as the wind\nand solar fleets increase in size. Moreover, we see the pattern of recent\ninvestments, summarised in the paper, as a good indication of likely future\ninvestments, with OCGT investments mainly serving the 440 kV grid, and ICGRs\nand stored electrical energy more local networks."
                },
                "authors": [
                    {
                        "name": "Anthony D Stephens"
                    },
                    {
                        "name": "David R Walwyn"
                    }
                ],
                "author_detail": {
                    "name": "David R Walwyn"
                },
                "author": "David R Walwyn",
                "arxiv_comment": "13 pages, 8 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16110v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16110v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15440v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15440v3",
                "updated": "2024-09-24T11:37:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    24,
                    11,
                    37,
                    43,
                    1,
                    268,
                    0
                ],
                "published": "2024-07-22T07:42:57Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    7,
                    42,
                    57,
                    0,
                    204,
                    0
                ],
                "title": "The Bicameral Cache: a split cache for vector architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache: a split cache for vector architectures"
                },
                "summary": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value."
                },
                "authors": [
                    {
                        "name": "Susana Rebolledo"
                    },
                    {
                        "name": "Borja Perez"
                    },
                    {
                        "name": "Jose Luis Bosque"
                    },
                    {
                        "name": "Peter Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Peter Hsu"
                },
                "author": "Peter Hsu",
                "arxiv_comment": "9 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15440v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15440v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15523v1",
                "updated": "2024-09-23T20:16:49Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    20,
                    16,
                    49,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T20:16:49Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    20,
                    16,
                    49,
                    0,
                    267,
                    0
                ],
                "title": "SEAL: Suite for Evaluating API-use of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEAL: Suite for Evaluating API-use of LLMs"
                },
                "summary": "Large language models (LLMs) have limitations in handling tasks that require\nreal-time access to external APIs. While several benchmarks like ToolBench and\nAPIGen have been developed to assess LLMs' API-use capabilities, they often\nsuffer from issues such as lack of generalizability, limited multi-step\nreasoning coverage, and instability due to real-time API fluctuations. In this\npaper, we introduce SEAL, an end-to-end testbed designed to evaluate LLMs in\nreal-world API usage. SEAL standardizes existing benchmarks, integrates an\nagent system for testing API retrieval and planning, and addresses the\ninstability of real-time APIs by introducing a GPT-4-powered API simulator with\ncaching for deterministic evaluations. Our testbed provides a comprehensive\nevaluation pipeline that covers API retrieval, API calls, and final responses,\noffering a reliable framework for structured performance comparison in diverse\nreal-world scenarios. SEAL is publicly available, with ongoing updates for new\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have limitations in handling tasks that require\nreal-time access to external APIs. While several benchmarks like ToolBench and\nAPIGen have been developed to assess LLMs' API-use capabilities, they often\nsuffer from issues such as lack of generalizability, limited multi-step\nreasoning coverage, and instability due to real-time API fluctuations. In this\npaper, we introduce SEAL, an end-to-end testbed designed to evaluate LLMs in\nreal-world API usage. SEAL standardizes existing benchmarks, integrates an\nagent system for testing API retrieval and planning, and addresses the\ninstability of real-time APIs by introducing a GPT-4-powered API simulator with\ncaching for deterministic evaluations. Our testbed provides a comprehensive\nevaluation pipeline that covers API retrieval, API calls, and final responses,\noffering a reliable framework for structured performance comparison in diverse\nreal-world scenarios. SEAL is publicly available, with ongoing updates for new\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Woojeong Kim"
                    },
                    {
                        "name": "Ashish Jagmohan"
                    },
                    {
                        "name": "Aditya Vempaty"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Vempaty"
                },
                "author": "Aditya Vempaty",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.18322v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.18322v2",
                "updated": "2024-09-23T20:09:28Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    20,
                    9,
                    28,
                    0,
                    267,
                    0
                ],
                "published": "2024-04-28T21:23:40Z",
                "published_parsed": [
                    2024,
                    4,
                    28,
                    21,
                    23,
                    40,
                    6,
                    119,
                    0
                ],
                "title": "BlockLLM: Multi-tenant Finer-grained Serving for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BlockLLM: Multi-tenant Finer-grained Serving for Large Language Models"
                },
                "summary": "The increasing demand for Large Language Models (LLMs) across various\napplications has led to a significant shift in the design of deep learning\nserving systems. Deploying LLMs, particularly in multi-tenant environments,\nposes substantial challenges due to their high computational and memory\ndemands. We introduce BlockLLM, a serving system that leverages component\nsharing among fine-tuned LLM models to provide an efficient and flexible\nsolution for LLM workloads. BlockLLM partitions models into finer-grained\nblocks, enabling the reuse of model components and independent provisioning to\nimprove computation efficiency. BlockLLM comprises an offline block zoo for\nstoring blocks and an online system to serve requests through chains of blocks.\nIt offers multi-fold flexibilities: (1) Adaptive assembly of blocks on-the-fly\nthrough equivalence evaluation among blocks in the zoo; (2) Per-block batch\nsize configuration and best-effort KV cache coordination at the individual\nblock level; (3) Speculative execution and locality-aware block placement to\nreduce communication costs from dynamic block resource allocation. Our\nevaluation shows that BlockLLM reduces memory and storage footprints and\nimproves computational efficiency, outperforming existing serving approach in\n95%ile latency and GPU utilization by 33.5% and 20.1%, respectively, with\nminimal impact on accuracy",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing demand for Large Language Models (LLMs) across various\napplications has led to a significant shift in the design of deep learning\nserving systems. Deploying LLMs, particularly in multi-tenant environments,\nposes substantial challenges due to their high computational and memory\ndemands. We introduce BlockLLM, a serving system that leverages component\nsharing among fine-tuned LLM models to provide an efficient and flexible\nsolution for LLM workloads. BlockLLM partitions models into finer-grained\nblocks, enabling the reuse of model components and independent provisioning to\nimprove computation efficiency. BlockLLM comprises an offline block zoo for\nstoring blocks and an online system to serve requests through chains of blocks.\nIt offers multi-fold flexibilities: (1) Adaptive assembly of blocks on-the-fly\nthrough equivalence evaluation among blocks in the zoo; (2) Per-block batch\nsize configuration and best-effort KV cache coordination at the individual\nblock level; (3) Speculative execution and locality-aware block placement to\nreduce communication costs from dynamic block resource allocation. Our\nevaluation shows that BlockLLM reduces memory and storage footprints and\nimproves computational efficiency, outperforming existing serving approach in\n95%ile latency and GPU utilization by 33.5% and 20.1%, respectively, with\nminimal impact on accuracy"
                },
                "authors": [
                    {
                        "name": "Bodun Hu"
                    },
                    {
                        "name": "Jiamin Li"
                    },
                    {
                        "name": "Le Xu"
                    },
                    {
                        "name": "Myungjin Lee"
                    },
                    {
                        "name": "Akshay Jajoo"
                    },
                    {
                        "name": "Geon-Woo Kim"
                    },
                    {
                        "name": "Hong Xu"
                    },
                    {
                        "name": "Aditya Akella"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Akella"
                },
                "author": "Aditya Akella",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.18322v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.18322v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13122v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13122v2",
                "updated": "2024-09-23T19:53:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    19,
                    53,
                    37,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-19T23:38:59Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    23,
                    38,
                    59,
                    3,
                    263,
                    0
                ],
                "title": "RepoGenReflex: Enhancing Repository-Level Code Completion with Verbal\n  Reinforcement and Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RepoGenReflex: Enhancing Repository-Level Code Completion with Verbal\n  Reinforcement and Retrieval-Augmented Generation"
                },
                "summary": "In real-world software engineering tasks, solving a problem often requires\nunderstanding and modifying multiple functions, classes, and files across a\nlarge codebase. Therefore, on the repository level, it is crucial to extract\nthe relevant information to achieve accurate code completion effectively.\nExisting code completion tools have achieved some success, but they struggle to\noptimize the retrieval and generation process dynamically. In this paper, we\npropose RepoGenReflex, a generic, dynamic, effective framework to address this\nchallenge. By leveraging the Retrieval-Augmented Generation (RAG) enhanced with\nVerbal Reinforcement Learning (VRL), it can dynamically choose the optimal\nresults for repository-level code completion. RepoGenReflex uses Reflector to\ngive directional feedback to the next loop. RepoGenReflex chooses the optimal\nresults stored in the Experience cache based on the RAG-VRL loop. To validate\nthe framework's generalization ability, we propose a new benchmark RepoGenEval,\nwhich consists of the latest, high-quality real-world repositories in line\ncompletion scenarios. Our experiments demonstrate that RepoGenReflex achieves\nsignificant improvements after optimizing the Reflector component, resulting in\nenhanced accuracy and relevance of code completions. Additionally,\nRepoGenReflex consistently demonstrates superior performance and effectiveness\nacross standard code completion tasks, highlighting the robustness and\nadaptability of our framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In real-world software engineering tasks, solving a problem often requires\nunderstanding and modifying multiple functions, classes, and files across a\nlarge codebase. Therefore, on the repository level, it is crucial to extract\nthe relevant information to achieve accurate code completion effectively.\nExisting code completion tools have achieved some success, but they struggle to\noptimize the retrieval and generation process dynamically. In this paper, we\npropose RepoGenReflex, a generic, dynamic, effective framework to address this\nchallenge. By leveraging the Retrieval-Augmented Generation (RAG) enhanced with\nVerbal Reinforcement Learning (VRL), it can dynamically choose the optimal\nresults for repository-level code completion. RepoGenReflex uses Reflector to\ngive directional feedback to the next loop. RepoGenReflex chooses the optimal\nresults stored in the Experience cache based on the RAG-VRL loop. To validate\nthe framework's generalization ability, we propose a new benchmark RepoGenEval,\nwhich consists of the latest, high-quality real-world repositories in line\ncompletion scenarios. Our experiments demonstrate that RepoGenReflex achieves\nsignificant improvements after optimizing the Reflector component, resulting in\nenhanced accuracy and relevance of code completions. Additionally,\nRepoGenReflex consistently demonstrates superior performance and effectiveness\nacross standard code completion tasks, highlighting the robustness and\nadaptability of our framework."
                },
                "authors": [
                    {
                        "name": "Jicheng Wang"
                    },
                    {
                        "name": "Yifeng He"
                    },
                    {
                        "name": "Hao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hao Chen"
                },
                "author": "Hao Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13122v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13122v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15441v1",
                "updated": "2024-09-23T18:06:32Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    18,
                    6,
                    32,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T18:06:32Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    18,
                    6,
                    32,
                    0,
                    267,
                    0
                ],
                "title": "Steward: Natural Language Web Automation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steward: Natural Language Web Automation"
                },
                "summary": "Recently, large language models (LLMs) have demonstrated exceptional\ncapabilities in serving as the foundation for AI assistants. One emerging\napplication of LLMs, navigating through websites and interacting with UI\nelements across various web pages, remains somewhat underexplored. We introduce\nSteward, a novel LLM-powered web automation tool designed to serve as a\ncost-effective, scalable, end-to-end solution for automating web interactions.\nTraditional browser automation frameworks like Selenium, Puppeteer, and\nPlaywright are not scalable for extensive web interaction tasks, such as\nstudying recommendation algorithms on platforms like YouTube and Twitter. These\nframeworks require manual coding of interactions, limiting their utility in\nlarge-scale or dynamic contexts. Steward addresses these limitations by\nintegrating LLM capabilities with browser automation, allowing for natural\nlanguage-driven interaction with websites. Steward operates by receiving\nnatural language instructions and reactively planning and executing a sequence\nof actions on websites, looping until completion, making it a practical tool\nfor developers and researchers to use. It achieves high efficiency, completing\nactions in 8.52 to 10.14 seconds at a cost of $0.028 per action or an average\nof $0.18 per task, which is further reduced to 4.8 seconds and $0.022 through a\ncaching mechanism. It runs tasks on real websites with a 40% completion success\nrate. We discuss various design and implementation challenges, including state\nrepresentation, action sequence selection, system responsiveness, detecting\ntask completion, and caching implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) have demonstrated exceptional\ncapabilities in serving as the foundation for AI assistants. One emerging\napplication of LLMs, navigating through websites and interacting with UI\nelements across various web pages, remains somewhat underexplored. We introduce\nSteward, a novel LLM-powered web automation tool designed to serve as a\ncost-effective, scalable, end-to-end solution for automating web interactions.\nTraditional browser automation frameworks like Selenium, Puppeteer, and\nPlaywright are not scalable for extensive web interaction tasks, such as\nstudying recommendation algorithms on platforms like YouTube and Twitter. These\nframeworks require manual coding of interactions, limiting their utility in\nlarge-scale or dynamic contexts. Steward addresses these limitations by\nintegrating LLM capabilities with browser automation, allowing for natural\nlanguage-driven interaction with websites. Steward operates by receiving\nnatural language instructions and reactively planning and executing a sequence\nof actions on websites, looping until completion, making it a practical tool\nfor developers and researchers to use. It achieves high efficiency, completing\nactions in 8.52 to 10.14 seconds at a cost of $0.028 per action or an average\nof $0.18 per task, which is further reduced to 4.8 seconds and $0.022 through a\ncaching mechanism. It runs tasks on real websites with a 40% completion success\nrate. We discuss various design and implementation challenges, including state\nrepresentation, action sequence selection, system responsiveness, detecting\ntask completion, and caching implementation."
                },
                "authors": [
                    {
                        "name": "Brian Tang"
                    },
                    {
                        "name": "Kang G. Shin"
                    }
                ],
                "author_detail": {
                    "name": "Kang G. Shin"
                },
                "author": "Kang G. Shin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15104v1",
                "updated": "2024-09-23T15:16:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    15,
                    16,
                    29,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T15:16:29Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    15,
                    16,
                    29,
                    0,
                    267,
                    0
                ],
                "title": "CSPS: A Communication-Efficient Sequence-Parallelism based Serving\n  System for Transformer based Models with Long Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSPS: A Communication-Efficient Sequence-Parallelism based Serving\n  System for Transformer based Models with Long Prompts"
                },
                "summary": "Long-sequence generative large-language model (LLM) applications have become\nincreasingly popular. In this paper, through trace-based experiments, we found\nthat the existing method for long sequences results in a high\nTime-To-First-Token (TTFT) due to sequential chunk processing, long\nTime-Between-Tokens (TBT) from batching long-sequence prefills and decodes, and\nlow throughput due to constrained key-value cache (KVC) for long sequences. To\naddress these issues, we propose two Sequence-Parallelism (SP) architectures\nfor both tensor parallelism (TP) and non-TP. However, SP introduces two\nchallenges: 1) network communication and computation become performance\nbottlenecks; 2) the latter two issues above are mitigated but not resolved, and\nSP's resultant KV value distribution across GPUs still requires communication\nfor decode, increasing TBT. Hence, we propose a Communication-efficient Sparse\nAttention (CSA) and communication-computation-communication three-phase\npipelining. We also propose SP-based decode that processes decode separately\nfrom prefill, distributes KV values of a request across different GPUs, and\nnovelly moves Query (Q) values instead of KV values to reduce communication\noverhead. These methods constitute a communication-efficient\nSequence-Parallelism based LLM Serving System (SPS2). Our trace-driven\nevaluation demonstrates that SPS2 improves the average TTFT, TBT, and response\ntime by up to 7.5x, 1.92x, and 9.8x and improves the prefill and decode\nthroughput by 8.2x and 5.2x while maintaining the accuracy compared to\nSarathi-Serve. We distributed our source code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-sequence generative large-language model (LLM) applications have become\nincreasingly popular. In this paper, through trace-based experiments, we found\nthat the existing method for long sequences results in a high\nTime-To-First-Token (TTFT) due to sequential chunk processing, long\nTime-Between-Tokens (TBT) from batching long-sequence prefills and decodes, and\nlow throughput due to constrained key-value cache (KVC) for long sequences. To\naddress these issues, we propose two Sequence-Parallelism (SP) architectures\nfor both tensor parallelism (TP) and non-TP. However, SP introduces two\nchallenges: 1) network communication and computation become performance\nbottlenecks; 2) the latter two issues above are mitigated but not resolved, and\nSP's resultant KV value distribution across GPUs still requires communication\nfor decode, increasing TBT. Hence, we propose a Communication-efficient Sparse\nAttention (CSA) and communication-computation-communication three-phase\npipelining. We also propose SP-based decode that processes decode separately\nfrom prefill, distributes KV values of a request across different GPUs, and\nnovelly moves Query (Q) values instead of KV values to reduce communication\noverhead. These methods constitute a communication-efficient\nSequence-Parallelism based LLM Serving System (SPS2). Our trace-driven\nevaluation demonstrates that SPS2 improves the average TTFT, TBT, and response\ntime by up to 7.5x, 1.92x, and 9.8x and improves the prefill and decode\nthroughput by 8.2x and 5.2x while maintaining the accuracy compared to\nSarathi-Serve. We distributed our source code."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Haiying Shen"
                    }
                ],
                "author_detail": {
                    "name": "Haiying Shen"
                },
                "author": "Haiying Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15012v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15012v1",
                "updated": "2024-09-23T13:37:25Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    13,
                    37,
                    25,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T13:37:25Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    13,
                    37,
                    25,
                    0,
                    267,
                    0
                ],
                "title": "Inference-Friendly Models With MixAttention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-Friendly Models With MixAttention"
                },
                "summary": "The size of the key-value (KV) cache plays a critical role in determining\nboth the maximum context length and the number of concurrent requests supported\nduring inference in modern language models. The KV cache size grows\nproportionally with the number of attention heads and the tokens processed,\nleading to increased memory consumption and slower inference for long inputs.\nIn this work, we explore the use of MixAttention, a model architecture\nmodification closely related to a blog published by Character.AI. MixAttention\ncombines sliding window attention, where only a small subset of recent tokens\nis stored in the KV cache, with KV cache sharing across layers. Our experiments\ndemonstrate that MixAttention significantly reduces memory usage and improves\ninference speed without sacrificing model performance in both short and\nlong-context tasks. We also explore various configurations of this\narchitecture, identifying those that maintain quality across evaluation metrics\nwhile optimizing resource efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The size of the key-value (KV) cache plays a critical role in determining\nboth the maximum context length and the number of concurrent requests supported\nduring inference in modern language models. The KV cache size grows\nproportionally with the number of attention heads and the tokens processed,\nleading to increased memory consumption and slower inference for long inputs.\nIn this work, we explore the use of MixAttention, a model architecture\nmodification closely related to a blog published by Character.AI. MixAttention\ncombines sliding window attention, where only a small subset of recent tokens\nis stored in the KV cache, with KV cache sharing across layers. Our experiments\ndemonstrate that MixAttention significantly reduces memory usage and improves\ninference speed without sacrificing model performance in both short and\nlong-context tasks. We also explore various configurations of this\narchitecture, identifying those that maintain quality across evaluation metrics\nwhile optimizing resource efficiency."
                },
                "authors": [
                    {
                        "name": "Shashank Rajput"
                    },
                    {
                        "name": "Ying Sheng"
                    },
                    {
                        "name": "Sean Owen"
                    },
                    {
                        "name": "Vitaliy Chiley"
                    }
                ],
                "author_detail": {
                    "name": "Vitaliy Chiley"
                },
                "author": "Vitaliy Chiley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15012v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15012v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14968v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14968v1",
                "updated": "2024-09-23T12:37:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    12,
                    37,
                    56,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T12:37:56Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    12,
                    37,
                    56,
                    0,
                    267,
                    0
                ],
                "title": "Mutation-Based Deep Learning Framework Testing Method in JavaScript\n  Environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mutation-Based Deep Learning Framework Testing Method in JavaScript\n  Environment"
                },
                "summary": "In recent years, Deep Learning (DL) applications in JavaScript environment\nhave become increasingly popular. As the infrastructure for DL applications,\nJavaScript DL frameworks play a crucial role in the development and deployment.\nIt is essential to ensure the quality of JavaScript DL frameworks. However, the\nbottleneck of limited computational resources in the JavaScript environment\nbrings new challenges to framework testing. Specifically, JavaScript DL\nframeworks are equipped with various optimization mechanisms (e.g., cache\nreuse, inference acceleration) to overcome the bottleneck of limited\ncomputational resources. These optimization mechanisms are overlooked by\nexisting methods, resulting in many bugs in JavaScript DL frameworks being\nmissed. To address the above challenges, we propose a mutation-based JavaScript\nDL framework testing method named DLJSFuzzer. DLJSFuzzer designs 13 tensor\nmutation rules targeting the cache reuse mechanism to generate test input\ntensors. Besides, DLJSFuzzer designs eight model mutation rules targeting the\ninference acceleration mechanism to generate test input models. To evaluate the\neffectiveness of DLJSFuzzer, we conduct experiments on the most widely-used\nJavaScript DL framework, TensorFlow.js. The experimental results show that\nDLJSFuzzer outperforms state-of-the-art methods in both effectiveness and\nefficiency. DLJSFuzzer successfully detects 21 unique crashes and 126 unique\nNaN & Inconsistency bugs. All detected crashes have been reported to the\nopen-source community, with 12 of them already confirmed by developers.\nAdditionally, DLJSFuzzer has improved by over 47% in model generation\nefficiency and over 91% in bug detection efficiency compared to all baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Deep Learning (DL) applications in JavaScript environment\nhave become increasingly popular. As the infrastructure for DL applications,\nJavaScript DL frameworks play a crucial role in the development and deployment.\nIt is essential to ensure the quality of JavaScript DL frameworks. However, the\nbottleneck of limited computational resources in the JavaScript environment\nbrings new challenges to framework testing. Specifically, JavaScript DL\nframeworks are equipped with various optimization mechanisms (e.g., cache\nreuse, inference acceleration) to overcome the bottleneck of limited\ncomputational resources. These optimization mechanisms are overlooked by\nexisting methods, resulting in many bugs in JavaScript DL frameworks being\nmissed. To address the above challenges, we propose a mutation-based JavaScript\nDL framework testing method named DLJSFuzzer. DLJSFuzzer designs 13 tensor\nmutation rules targeting the cache reuse mechanism to generate test input\ntensors. Besides, DLJSFuzzer designs eight model mutation rules targeting the\ninference acceleration mechanism to generate test input models. To evaluate the\neffectiveness of DLJSFuzzer, we conduct experiments on the most widely-used\nJavaScript DL framework, TensorFlow.js. The experimental results show that\nDLJSFuzzer outperforms state-of-the-art methods in both effectiveness and\nefficiency. DLJSFuzzer successfully detects 21 unique crashes and 126 unique\nNaN & Inconsistency bugs. All detected crashes have been reported to the\nopen-source community, with 12 of them already confirmed by developers.\nAdditionally, DLJSFuzzer has improved by over 47% in model generation\nefficiency and over 91% in bug detection efficiency compared to all baselines."
                },
                "authors": [
                    {
                        "name": "Yinglong Zou"
                    },
                    {
                        "name": "Juan Zhai"
                    },
                    {
                        "name": "Chunrong Fang"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Tao Zheng"
                    },
                    {
                        "name": "Zhenyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhenyu Chen"
                },
                "author": "Zhenyu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14968v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14968v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14846v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14846v1",
                "updated": "2024-09-23T09:22:59Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    9,
                    22,
                    59,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-23T09:22:59Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    9,
                    22,
                    59,
                    0,
                    267,
                    0
                ],
                "title": "A-VL: Adaptive Attention for Large Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A-VL: Adaptive Attention for Large Vision-Language Models"
                },
                "summary": "The Large Vision-Language Model (LVLM) integrates computer vision and natural\nlanguage processing techniques, offering substantial application potential.\nHowever, these models demand extensive resources during inference. Adaptive\nattention techniques can dynamically reduce computational redundancy and thus\nimprove efficiency. Although current adaptive attention methods significantly\nreduce the memory requirements of Transformer-based language models, they are\nnot tailored for LVLMs. We observe that LVLMs generate responses from both\nremote image tokens and local text tokens, and different modalities have\ndifferent attention patterns. This observation inspires us to manage the\nattention for each modality separately. Specifically, for visual input, we\nstore the cache of potentially useful information but only compute the most\ncritical parts. For language input, we care more about local information. Based\non our observation and analysis of vision-language attention patterns, we\ndevelop A-VL, a plug-and-play adaptive attention tailored for LVLM inference.\nExtensive evaluations on three vision-language tasks and five datasets show the\neffectiveness of our designs. Our approach A-VL outperforms existing adaptive\nattention methods in reducing memory usage and computational load without\ncompromising performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Large Vision-Language Model (LVLM) integrates computer vision and natural\nlanguage processing techniques, offering substantial application potential.\nHowever, these models demand extensive resources during inference. Adaptive\nattention techniques can dynamically reduce computational redundancy and thus\nimprove efficiency. Although current adaptive attention methods significantly\nreduce the memory requirements of Transformer-based language models, they are\nnot tailored for LVLMs. We observe that LVLMs generate responses from both\nremote image tokens and local text tokens, and different modalities have\ndifferent attention patterns. This observation inspires us to manage the\nattention for each modality separately. Specifically, for visual input, we\nstore the cache of potentially useful information but only compute the most\ncritical parts. For language input, we care more about local information. Based\non our observation and analysis of vision-language attention patterns, we\ndevelop A-VL, a plug-and-play adaptive attention tailored for LVLM inference.\nExtensive evaluations on three vision-language tasks and five datasets show the\neffectiveness of our designs. Our approach A-VL outperforms existing adaptive\nattention methods in reducing memory usage and computational load without\ncompromising performance."
                },
                "authors": [
                    {
                        "name": "Junyang Zhang"
                    },
                    {
                        "name": "Mu Yuan"
                    },
                    {
                        "name": "Ruiguang Zhong"
                    },
                    {
                        "name": "Puhan Luo"
                    },
                    {
                        "name": "Huiyou Zhan"
                    },
                    {
                        "name": "Ningkang Zhang"
                    },
                    {
                        "name": "Chengchen Hu"
                    },
                    {
                        "name": "Xiangyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyang Li"
                },
                "author": "Xiangyang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14846v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14846v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12490v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12490v2",
                "updated": "2024-09-23T02:24:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    23,
                    2,
                    24,
                    33,
                    0,
                    267,
                    0
                ],
                "published": "2024-09-19T06:09:56Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    6,
                    9,
                    56,
                    3,
                    263,
                    0
                ],
                "title": "CritiPrefill: A Segment-wise Criticality-based Approach for Prefilling\n  Acceleration in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CritiPrefill: A Segment-wise Criticality-based Approach for Prefilling\n  Acceleration in LLMs"
                },
                "summary": "Large language models have achieved notable success across various domains,\nyet efficient inference is still limited by the quadratic computation\ncomplexity of the attention mechanism. The inference consists of prefilling and\ndecoding phases. Although several attempts have been made to accelerate\ndecoding, the inefficiency of the prefilling phase, especially for long-context\ntasks, remains a challenge. In this paper, we observe a locality in query\ncriticality during the prefilling phase of long-context processing: adjacent\nquery tokens tend to focus on similar subsets of the past Key-Value (KV) cache.\nBased on this observation, we propose CritiPrefill, a criticality-based\nsegment-wise prefilling method. This method partitions the input sequence's\nqueries and KV cache into segments and blocks, utilizing a segment-wise\nalgorithm to estimate the query criticality. By pruning non-critical\ncomputations between query segments and cache blocks in the self-attention\nmechanism, the prefilling process can be significantly accelerated. Extensive\nevaluations on multiple long-context datasets show up to 2.7x speedup on\nLlama3-8B and 3.0x speedup on Yi-9B for 128K context length on a single A100\nGPU, with minimal quality degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have achieved notable success across various domains,\nyet efficient inference is still limited by the quadratic computation\ncomplexity of the attention mechanism. The inference consists of prefilling and\ndecoding phases. Although several attempts have been made to accelerate\ndecoding, the inefficiency of the prefilling phase, especially for long-context\ntasks, remains a challenge. In this paper, we observe a locality in query\ncriticality during the prefilling phase of long-context processing: adjacent\nquery tokens tend to focus on similar subsets of the past Key-Value (KV) cache.\nBased on this observation, we propose CritiPrefill, a criticality-based\nsegment-wise prefilling method. This method partitions the input sequence's\nqueries and KV cache into segments and blocks, utilizing a segment-wise\nalgorithm to estimate the query criticality. By pruning non-critical\ncomputations between query segments and cache blocks in the self-attention\nmechanism, the prefilling process can be significantly accelerated. Extensive\nevaluations on multiple long-context datasets show up to 2.7x speedup on\nLlama3-8B and 3.0x speedup on Yi-9B for 128K context length on a single A100\nGPU, with minimal quality degradation."
                },
                "authors": [
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "Xin Jia"
                    },
                    {
                        "name": "Qirong Peng"
                    },
                    {
                        "name": "Guiming Xie"
                    }
                ],
                "author_detail": {
                    "name": "Guiming Xie"
                },
                "author": "Guiming Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12490v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12490v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14350v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14350v1",
                "updated": "2024-09-22T07:24:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    22,
                    7,
                    24,
                    2,
                    6,
                    266,
                    0
                ],
                "published": "2024-09-22T07:24:02Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    7,
                    24,
                    2,
                    6,
                    266,
                    0
                ],
                "title": "D2D Coded Caching from Two Classes of Optimal DPDAs using Cross\n  Resolvable Designs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "D2D Coded Caching from Two Classes of Optimal DPDAs using Cross\n  Resolvable Designs"
                },
                "summary": "Coded caching in a wireless device-to-device (D2D) network was first studied\nby Ji \\textit{et al.} in [4] (referred to as the JCM scheme). In a D2D network,\na central server first places the data in the user cache memories and all the\nuser's demands are served by inter-user coded multicast transmissions. Low\nsubpacketization level D2D coded caching schemes are desirable for practical\nimplementations. Wang \\textit{et al.} in [7] proposed an array called D2D\nplacement delivery array (DPDA) which characterizes the placement phase and the\ndelivery phase in a D2D network. A lower bound on the transmission load of a\nDPDA is derived and only the JCM scheme achieves this lower bound, but requires\na subpacketization level that grows exponentially with the number of users. Low\nsubpacketization level D2D schemes can be obtained by constructing appropriate\nDPDAs. In this paper, we propose two new classes of DPDA constructions that\ngive low subpacketization level D2D schemes using cross resolvable designs. The\nfirst class of constructed DPDA achieves the known lower bound on the\ntransmission load of DPDA while requiring a subpacketization level lesser than\nthat of the JCM scheme. We propose another lower bound on the transmission load\nof a DPDA and show that the second class of constructed DPDA achieves this\nlower bound.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching in a wireless device-to-device (D2D) network was first studied\nby Ji \\textit{et al.} in [4] (referred to as the JCM scheme). In a D2D network,\na central server first places the data in the user cache memories and all the\nuser's demands are served by inter-user coded multicast transmissions. Low\nsubpacketization level D2D coded caching schemes are desirable for practical\nimplementations. Wang \\textit{et al.} in [7] proposed an array called D2D\nplacement delivery array (DPDA) which characterizes the placement phase and the\ndelivery phase in a D2D network. A lower bound on the transmission load of a\nDPDA is derived and only the JCM scheme achieves this lower bound, but requires\na subpacketization level that grows exponentially with the number of users. Low\nsubpacketization level D2D schemes can be obtained by constructing appropriate\nDPDAs. In this paper, we propose two new classes of DPDA constructions that\ngive low subpacketization level D2D schemes using cross resolvable designs. The\nfirst class of constructed DPDA achieves the known lower bound on the\ntransmission load of DPDA while requiring a subpacketization level lesser than\nthat of the JCM scheme. We propose another lower bound on the transmission load\nof a DPDA and show that the second class of constructed DPDA achieves this\nlower bound."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "9 pages, 3 tables and 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14350v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14350v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02000v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02000v2",
                "updated": "2024-09-21T20:45:41Z",
                "updated_parsed": [
                    2024,
                    9,
                    21,
                    20,
                    45,
                    41,
                    5,
                    265,
                    0
                ],
                "published": "2024-07-02T07:15:40Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    7,
                    15,
                    40,
                    1,
                    184,
                    0
                ],
                "title": "Sub-millisecond electric field sensing with an individual rare-earth\n  doped ferroelectric nanocrystal",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sub-millisecond electric field sensing with an individual rare-earth\n  doped ferroelectric nanocrystal"
                },
                "summary": "Understanding the dynamics of electrical signals within neuronal assemblies\nis crucial to unraveling complex brain function. Despite recent advances in\nemploying optically active nanostructures in transmembrane potential sensing,\nthere remains room for improvement in terms of response time and sensitivity.\nHere, we report the development of such a nanosensor capable of detecting\nelectric fields with a submillisecond response time at the single particle\nlevel. We achieve this by using ferroelectric nanocrystals doped with rare\nearth ions producing upconversion (UC). When such a nanocrystal experiences a\nvariation of surrounding electric potential, its surface charge density\nchanges, inducing electric polarization modifications that vary, via converse\npiezoelectric effect, the crystal field around the ions. The latter variation\nis finally converted into UC spectral changes, enabling optical detection of\nelectric potential. To develop such a sensor, we synthesized erbium and\nytterbium-doped barium titanate crystals of size $\\approx160$~nm. We observed\ndistinct changes in the UC spectrum when individual nanocrystals were subjected\nto an external field via a conductive AFM tip, with a response time of\n100~$\\mu$s. Furthermore, our sensor exhibits a remarkable sensitivity of\n4.8~kV/cm/$\\sqrt{\\rm Hz}$, enabling time-resolved detection of fast changing\nelectric field of amplitude comparable to that generated during a neuron action\npotential.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the dynamics of electrical signals within neuronal assemblies\nis crucial to unraveling complex brain function. Despite recent advances in\nemploying optically active nanostructures in transmembrane potential sensing,\nthere remains room for improvement in terms of response time and sensitivity.\nHere, we report the development of such a nanosensor capable of detecting\nelectric fields with a submillisecond response time at the single particle\nlevel. We achieve this by using ferroelectric nanocrystals doped with rare\nearth ions producing upconversion (UC). When such a nanocrystal experiences a\nvariation of surrounding electric potential, its surface charge density\nchanges, inducing electric polarization modifications that vary, via converse\npiezoelectric effect, the crystal field around the ions. The latter variation\nis finally converted into UC spectral changes, enabling optical detection of\nelectric potential. To develop such a sensor, we synthesized erbium and\nytterbium-doped barium titanate crystals of size $\\approx160$~nm. We observed\ndistinct changes in the UC spectrum when individual nanocrystals were subjected\nto an external field via a conductive AFM tip, with a response time of\n100~$\\mu$s. Furthermore, our sensor exhibits a remarkable sensitivity of\n4.8~kV/cm/$\\sqrt{\\rm Hz}$, enabling time-resolved detection of fast changing\nelectric field of amplitude comparable to that generated during a neuron action\npotential."
                },
                "authors": [
                    {
                        "name": "Athulya Muraleedharan"
                    },
                    {
                        "name": "Jingye Zou"
                    },
                    {
                        "name": "Maxime Vallet"
                    },
                    {
                        "name": "Abdelali Zaki"
                    },
                    {
                        "name": "Christine Bogicevic"
                    },
                    {
                        "name": "Charles Paillard"
                    },
                    {
                        "name": "Karen Perronet"
                    },
                    {
                        "name": "Franois Treussart"
                    }
                ],
                "author_detail": {
                    "name": "Franois Treussart"
                },
                "author": "Franois Treussart",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02000v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02000v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.other",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10593v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10593v2",
                "updated": "2024-09-21T13:01:43Z",
                "updated_parsed": [
                    2024,
                    9,
                    21,
                    13,
                    1,
                    43,
                    5,
                    265,
                    0
                ],
                "published": "2024-09-16T17:36:50Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    36,
                    50,
                    0,
                    260,
                    0
                ],
                "title": "CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context\n  Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context\n  Scenarios"
                },
                "summary": "Large Language Models (LLMs) have been widely adopted to process long-context\ntasks. However, the large memory overhead of the key-value (KV) cache poses\nsignificant challenges in long-context scenarios. Existing training-free KV\ncache compression methods typically focus on quantization and token pruning,\nwhich have compression limits, and excessive sparsity can lead to severe\nperformance degradation. Other methods design new architectures with less KV\noverhead but require significant training overhead. To address the above two\ndrawbacks, we further explore the redundancy in the channel dimension and apply\nan architecture-level design with minor training costs. Therefore, we introduce\nCSKV, a training-efficient Channel Shrinking technique for KV cache\ncompression: (1) We first analyze the singular value distribution of the KV\ncache, revealing significant redundancy and compression potential along the\nchannel dimension. Based on this observation, we propose using low-rank\ndecomposition for key and value layers and storing the low-dimension features.\n(2) To preserve model performance, we introduce a bi-branch KV cache, including\na window-based full-precision KV cache and a low-precision compressed KV cache.\n(3) To reduce the training costs, we minimize the layer-wise reconstruction\nloss for the compressed KV cache instead of retraining the entire LLMs.\nExtensive experiments show that CSKV can reduce the memory overhead of the KV\ncache by 80% while maintaining the model's long-context capability. Moreover,\nwe show that our method can be seamlessly combined with quantization to further\nreduce the memory overhead, achieving a compression ratio of up to 95%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely adopted to process long-context\ntasks. However, the large memory overhead of the key-value (KV) cache poses\nsignificant challenges in long-context scenarios. Existing training-free KV\ncache compression methods typically focus on quantization and token pruning,\nwhich have compression limits, and excessive sparsity can lead to severe\nperformance degradation. Other methods design new architectures with less KV\noverhead but require significant training overhead. To address the above two\ndrawbacks, we further explore the redundancy in the channel dimension and apply\nan architecture-level design with minor training costs. Therefore, we introduce\nCSKV, a training-efficient Channel Shrinking technique for KV cache\ncompression: (1) We first analyze the singular value distribution of the KV\ncache, revealing significant redundancy and compression potential along the\nchannel dimension. Based on this observation, we propose using low-rank\ndecomposition for key and value layers and storing the low-dimension features.\n(2) To preserve model performance, we introduce a bi-branch KV cache, including\na window-based full-precision KV cache and a low-precision compressed KV cache.\n(3) To reduce the training costs, we minimize the layer-wise reconstruction\nloss for the compressed KV cache instead of retraining the entire LLMs.\nExtensive experiments show that CSKV can reduce the memory overhead of the KV\ncache by 80% while maintaining the model's long-context capability. Moreover,\nwe show that our method can be seamlessly combined with quantization to further\nreduce the memory overhead, achieving a compression ratio of up to 95%."
                },
                "authors": [
                    {
                        "name": "Luning Wang"
                    },
                    {
                        "name": "Shiyao Li"
                    },
                    {
                        "name": "Xuefei Ning"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Shengen Yan"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10593v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10593v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11430v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11430v2",
                "updated": "2024-09-21T12:33:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    21,
                    12,
                    33,
                    0,
                    5,
                    265,
                    0
                ],
                "published": "2024-06-17T11:35:16Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    11,
                    35,
                    16,
                    0,
                    169,
                    0
                ],
                "title": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression"
                },
                "summary": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability."
                },
                "authors": [
                    {
                        "name": "Alessio Devoto"
                    },
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Simone Scardapane"
                    },
                    {
                        "name": "Pasquale Minervini"
                    }
                ],
                "author_detail": {
                    "name": "Pasquale Minervini"
                },
                "author": "Pasquale Minervini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11430v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11430v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06799v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06799v2",
                "updated": "2024-09-21T09:10:02Z",
                "updated_parsed": [
                    2024,
                    9,
                    21,
                    9,
                    10,
                    2,
                    5,
                    265,
                    0
                ],
                "published": "2024-06-10T21:08:39Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    21,
                    8,
                    39,
                    0,
                    162,
                    0
                ],
                "title": "LLM-dCache: Improving Tool-Augmented LLMs with GPT-Driven Localized Data\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-dCache: Improving Tool-Augmented LLMs with GPT-Driven Localized Data\n  Caching"
                },
                "summary": "As Large Language Models (LLMs) broaden their capabilities to manage\nthousands of API calls, they are confronted with complex data operations across\nvast datasets with significant overhead to the underlying system. In this work,\nwe introduce LLM-dCache to optimize data accesses by treating cache operations\nas callable API functions exposed to the tool-augmented agent. We grant LLMs\nthe autonomy to manage cache decisions via prompting, seamlessly integrating\nwith existing function-calling mechanisms. Tested on an industry-scale\nmassively parallel platform that spans hundreds of GPT endpoints and terabytes\nof imagery, our method improves Copilot times by an average of 1.24x across\nvarious LLMs and prompting techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) broaden their capabilities to manage\nthousands of API calls, they are confronted with complex data operations across\nvast datasets with significant overhead to the underlying system. In this work,\nwe introduce LLM-dCache to optimize data accesses by treating cache operations\nas callable API functions exposed to the tool-augmented agent. We grant LLMs\nthe autonomy to manage cache decisions via prompting, seamlessly integrating\nwith existing function-calling mechanisms. Tested on an industry-scale\nmassively parallel platform that spans hundreds of GPT endpoints and terabytes\nof imagery, our method improves Copilot times by an average of 1.24x across\nvarious LLMs and prompting techniques."
                },
                "authors": [
                    {
                        "name": "Simranjit Singh"
                    },
                    {
                        "name": "Michael Fore"
                    },
                    {
                        "name": "Andreas Karatzas"
                    },
                    {
                        "name": "Chaehong Lee"
                    },
                    {
                        "name": "Yanan Jian"
                    },
                    {
                        "name": "Longfei Shangguan"
                    },
                    {
                        "name": "Fuxun Yu"
                    },
                    {
                        "name": "Iraklis Anagnostopoulos"
                    },
                    {
                        "name": "Dimitrios Stamoulis"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Stamoulis"
                },
                "author": "Dimitrios Stamoulis",
                "arxiv_comment": "ICECS 2024 Camera-Ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06799v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06799v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21625v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21625v2",
                "updated": "2024-09-20T16:59:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    20,
                    16,
                    59,
                    29,
                    4,
                    264,
                    0
                ],
                "published": "2024-07-31T14:17:49Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    14,
                    17,
                    49,
                    2,
                    213,
                    0
                ],
                "title": "ARCANE: Adaptive Routing with Caching and Network Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARCANE: Adaptive Routing with Caching and Network Exploration"
                },
                "summary": "Most datacenter transport protocols traditionally depend on in-order packet\ndelivery, a legacy design choice that prioritizes simplicity. However,\ntechnological advancements, such as RDMA, now enable the relaxation of this\nrequirement, allowing for more efficient utilization of modern datacenter\ntopologies like FatTree and Dragonfly. With the growing prevalence of AI/ML\nworkloads, the demand for improved link utilization has intensified, creating\nchallenges for single-path load balancers due to problems like ECMP collisions.\nIn this paper, we present ARCANE, a novel, adaptive per-packet traffic\nload-balancing algorithm designed to work seamlessly with existing congestion\ncontrol mechanisms. ARCANE dynamically routes packets to bypass congested areas\nand network failures, all while maintaining a lightweight footprint with\nminimal state requirements. Our evaluation shows that ARCANE delivers\nsignificant performance gains over traditional load-balancing methods,\nincluding packet spraying and other advanced solutions, substantially enhancing\nboth performance and link utilization in modern datacenter networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most datacenter transport protocols traditionally depend on in-order packet\ndelivery, a legacy design choice that prioritizes simplicity. However,\ntechnological advancements, such as RDMA, now enable the relaxation of this\nrequirement, allowing for more efficient utilization of modern datacenter\ntopologies like FatTree and Dragonfly. With the growing prevalence of AI/ML\nworkloads, the demand for improved link utilization has intensified, creating\nchallenges for single-path load balancers due to problems like ECMP collisions.\nIn this paper, we present ARCANE, a novel, adaptive per-packet traffic\nload-balancing algorithm designed to work seamlessly with existing congestion\ncontrol mechanisms. ARCANE dynamically routes packets to bypass congested areas\nand network failures, all while maintaining a lightweight footprint with\nminimal state requirements. Our evaluation shows that ARCANE delivers\nsignificant performance gains over traditional load-balancing methods,\nincluding packet spraying and other advanced solutions, substantially enhancing\nboth performance and link utilization in modern datacenter networks."
                },
                "authors": [
                    {
                        "name": "Tommaso Bonato"
                    },
                    {
                        "name": "Abdul Kabbani"
                    },
                    {
                        "name": "Ahmad Ghalayini"
                    },
                    {
                        "name": "Mohammad Dohadwala"
                    },
                    {
                        "name": "Michael Papamichael"
                    },
                    {
                        "name": "Daniele De Sensi"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21625v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21625v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04870v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04870v4",
                "updated": "2024-09-20T15:51:17Z",
                "updated_parsed": [
                    2024,
                    9,
                    20,
                    15,
                    51,
                    17,
                    4,
                    264,
                    0
                ],
                "published": "2024-08-09T05:20:05Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    20,
                    5,
                    4,
                    222,
                    0
                ],
                "title": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs"
                },
                "summary": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Ayush RoyChowdhury"
                    },
                    {
                        "name": "Mulong Luo"
                    },
                    {
                        "name": "Prateek Sahu"
                    },
                    {
                        "name": "Sarbartha Banerjee"
                    },
                    {
                        "name": "Mohit Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Tiwari"
                },
                "author": "Mohit Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04870v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04870v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13175v1",
                "updated": "2024-09-20T03:02:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    20,
                    3,
                    2,
                    42,
                    4,
                    264,
                    0
                ],
                "published": "2024-09-20T03:02:42Z",
                "published_parsed": [
                    2024,
                    9,
                    20,
                    3,
                    2,
                    42,
                    4,
                    264,
                    0
                ],
                "title": "RPAF: A Reinforcement Prediction-Allocation Framework for Cache\n  Allocation in Large-Scale Recommender Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RPAF: A Reinforcement Prediction-Allocation Framework for Cache\n  Allocation in Large-Scale Recommender Systems"
                },
                "summary": "Modern recommender systems are built upon computation-intensive\ninfrastructure, and it is challenging to perform real-time computation for each\nrequest, especially in peak periods, due to the limited computational\nresources. Recommending by user-wise result caches is widely used when the\nsystem cannot afford a real-time recommendation. However, it is challenging to\nallocate real-time and cached recommendations to maximize the users' overall\nengagement. This paper shows two key challenges to cache allocation, i.e., the\nvalue-strategy dependency and the streaming allocation. Then, we propose a\nreinforcement prediction-allocation framework (RPAF) to address these issues.\nRPAF is a reinforcement-learning-based two-stage framework containing\nprediction and allocation stages. The prediction stage estimates the values of\nthe cache choices considering the value-strategy dependency, and the allocation\nstage determines the cache choices for each individual request while satisfying\nthe global budget constraint. We show that the challenge of training RPAF\nincludes globality and the strictness of budget constraints, and a relaxed\nlocal allocator (RLA) is proposed to address this issue. Moreover, a PoolRank\nalgorithm is used in the allocation stage to deal with the streaming allocation\nproblem. Experiments show that RPAF significantly improves users' engagement\nunder computational budget constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern recommender systems are built upon computation-intensive\ninfrastructure, and it is challenging to perform real-time computation for each\nrequest, especially in peak periods, due to the limited computational\nresources. Recommending by user-wise result caches is widely used when the\nsystem cannot afford a real-time recommendation. However, it is challenging to\nallocate real-time and cached recommendations to maximize the users' overall\nengagement. This paper shows two key challenges to cache allocation, i.e., the\nvalue-strategy dependency and the streaming allocation. Then, we propose a\nreinforcement prediction-allocation framework (RPAF) to address these issues.\nRPAF is a reinforcement-learning-based two-stage framework containing\nprediction and allocation stages. The prediction stage estimates the values of\nthe cache choices considering the value-strategy dependency, and the allocation\nstage determines the cache choices for each individual request while satisfying\nthe global budget constraint. We show that the challenge of training RPAF\nincludes globality and the strictness of budget constraints, and a relaxed\nlocal allocator (RLA) is proposed to address this issue. Moreover, a PoolRank\nalgorithm is used in the allocation stage to deal with the streaming allocation\nproblem. Experiments show that RPAF significantly improves users' engagement\nunder computational budget constraints."
                },
                "authors": [
                    {
                        "name": "Shuo Su"
                    },
                    {
                        "name": "Xiaoshuang Chen"
                    },
                    {
                        "name": "Yao Wang"
                    },
                    {
                        "name": "Yulin Wu"
                    },
                    {
                        "name": "Ziqiang Zhang"
                    },
                    {
                        "name": "Kaiqiao Zhan"
                    },
                    {
                        "name": "Ben Wang"
                    },
                    {
                        "name": "Kun Gai"
                    }
                ],
                "author_detail": {
                    "name": "Kun Gai"
                },
                "author": "Kun Gai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12892v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12892v1",
                "updated": "2024-09-19T16:31:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    19,
                    16,
                    31,
                    44,
                    3,
                    263,
                    0
                ],
                "published": "2024-09-19T16:31:44Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    16,
                    31,
                    44,
                    3,
                    263,
                    0
                ],
                "title": "3DGS-LM: Faster Gaussian-Splatting Optimization with Levenberg-Marquardt",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3DGS-LM: Faster Gaussian-Splatting Optimization with Levenberg-Marquardt"
                },
                "summary": "We present 3DGS-LM, a new method that accelerates the reconstruction of 3D\nGaussian Splatting (3DGS) by replacing its ADAM optimizer with a tailored\nLevenberg-Marquardt (LM). Existing methods reduce the optimization time by\ndecreasing the number of Gaussians or by improving the implementation of the\ndifferentiable rasterizer. However, they still rely on the ADAM optimizer to\nfit Gaussian parameters of a scene in thousands of iterations, which can take\nup to an hour. To this end, we change the optimizer to LM that runs in\nconjunction with the 3DGS differentiable rasterizer. For efficient GPU\nparallization, we propose a caching data structure for intermediate gradients\nthat allows us to efficiently calculate Jacobian-vector products in custom CUDA\nkernels. In every LM iteration, we calculate update directions from multiple\nimage subsets using these kernels and combine them in a weighted mean. Overall,\nour method is 30% faster than the original 3DGS while obtaining the same\nreconstruction quality. Our optimization is also agnostic to other methods that\nacclerate 3DGS, thus enabling even faster speedups compared to vanilla 3DGS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present 3DGS-LM, a new method that accelerates the reconstruction of 3D\nGaussian Splatting (3DGS) by replacing its ADAM optimizer with a tailored\nLevenberg-Marquardt (LM). Existing methods reduce the optimization time by\ndecreasing the number of Gaussians or by improving the implementation of the\ndifferentiable rasterizer. However, they still rely on the ADAM optimizer to\nfit Gaussian parameters of a scene in thousands of iterations, which can take\nup to an hour. To this end, we change the optimizer to LM that runs in\nconjunction with the 3DGS differentiable rasterizer. For efficient GPU\nparallization, we propose a caching data structure for intermediate gradients\nthat allows us to efficiently calculate Jacobian-vector products in custom CUDA\nkernels. In every LM iteration, we calculate update directions from multiple\nimage subsets using these kernels and combine them in a weighted mean. Overall,\nour method is 30% faster than the original 3DGS while obtaining the same\nreconstruction quality. Our optimization is also agnostic to other methods that\nacclerate 3DGS, thus enabling even faster speedups compared to vanilla 3DGS."
                },
                "authors": [
                    {
                        "name": "Lukas Hllein"
                    },
                    {
                        "name": "Alja Boi"
                    },
                    {
                        "name": "Michael Zollhfer"
                    },
                    {
                        "name": "Matthias Niener"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Niener"
                },
                "author": "Matthias Niener",
                "arxiv_comment": "project page: https://lukashoel.github.io/3DGS-LM, video:\n  https://www.youtube.com/watch?v=tDiGuGMssg8, code:\n  https://github.com/lukasHoel/3DGS-LM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12892v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12892v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15766v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15766v2",
                "updated": "2024-09-19T15:46:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    19,
                    15,
                    46,
                    57,
                    3,
                    263,
                    0
                ],
                "published": "2024-08-28T12:59:12Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    12,
                    59,
                    12,
                    2,
                    241,
                    0
                ],
                "title": "Learning Harmonized Representations for Speculative Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Harmonized Representations for Speculative Sampling"
                },
                "summary": "Speculative sampling is a promising approach to accelerate the decoding stage\nfor Large Language Models (LLMs). Recent advancements that leverage target\nLLM's contextual information, such as hidden states and KV cache, have shown\nsignificant practical improvements. However, these approaches suffer from\ninconsistent context between training and decoding. We also observe another\ndiscrepancy between the training and decoding objectives in existing\nspeculative sampling methods. In this work, we propose a solution named\nHArmonized Speculative Sampling (HASS) that learns harmonized representations\nto address these issues. HASS accelerates the decoding stage without adding\ninference overhead through harmonized objective distillation and harmonized\ncontext alignment. Experiments on four LLaMA models demonstrate that HASS\nachieves 2.81x-4.05x wall-clock time speedup ratio averaging across three\ndatasets, surpassing EAGLE-2 by 8%-20%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative sampling is a promising approach to accelerate the decoding stage\nfor Large Language Models (LLMs). Recent advancements that leverage target\nLLM's contextual information, such as hidden states and KV cache, have shown\nsignificant practical improvements. However, these approaches suffer from\ninconsistent context between training and decoding. We also observe another\ndiscrepancy between the training and decoding objectives in existing\nspeculative sampling methods. In this work, we propose a solution named\nHArmonized Speculative Sampling (HASS) that learns harmonized representations\nto address these issues. HASS accelerates the decoding stage without adding\ninference overhead through harmonized objective distillation and harmonized\ncontext alignment. Experiments on four LLaMA models demonstrate that HASS\nachieves 2.81x-4.05x wall-clock time speedup ratio averaging across three\ndatasets, surpassing EAGLE-2 by 8%-20%."
                },
                "authors": [
                    {
                        "name": "Lefan Zhang"
                    },
                    {
                        "name": "Xiaodan Wang"
                    },
                    {
                        "name": "Yanhua Huang"
                    },
                    {
                        "name": "Ruiwen Xu"
                    }
                ],
                "author_detail": {
                    "name": "Ruiwen Xu"
                },
                "author": "Ruiwen Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15766v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15766v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12387v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12387v1",
                "updated": "2024-09-19T01:13:03Z",
                "updated_parsed": [
                    2024,
                    9,
                    19,
                    1,
                    13,
                    3,
                    3,
                    263,
                    0
                ],
                "published": "2024-09-19T01:13:03Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    1,
                    13,
                    3,
                    3,
                    263,
                    0
                ],
                "title": "On the Regret of Coded Caching with Adversarial Requests",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Regret of Coded Caching with Adversarial Requests"
                },
                "summary": "We study the well-known coded caching problem in an online learning\nframework, wherein requests arrive sequentially, and an online policy can\nupdate the cache contents based on the history of requests seen thus far. We\nintroduce a caching policy based on the Follow-The-Perturbed-Leader principle\nand show that for any time horizon T and any request sequence, it achieves a\nsub-linear regret of \\mathcal{O}(\\sqrt(T) ) with respect to an oracle that\nknows the request sequence beforehand. Our study marks the first examination of\nadversarial regret in the coded caching setup. Furthermore, we also address the\nissue of switching cost by establishing an upper bound on the expected number\nof cache updates made by our algorithm under unrestricted switching and also\nprovide an upper bound on the regret under restricted switching when cache\nupdates can only happen in a pre-specified subset of timeslots. Finally, we\nvalidate our theoretical insights with numerical results using a real-world\ndataset",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the well-known coded caching problem in an online learning\nframework, wherein requests arrive sequentially, and an online policy can\nupdate the cache contents based on the history of requests seen thus far. We\nintroduce a caching policy based on the Follow-The-Perturbed-Leader principle\nand show that for any time horizon T and any request sequence, it achieves a\nsub-linear regret of \\mathcal{O}(\\sqrt(T) ) with respect to an oracle that\nknows the request sequence beforehand. Our study marks the first examination of\nadversarial regret in the coded caching setup. Furthermore, we also address the\nissue of switching cost by establishing an upper bound on the expected number\nof cache updates made by our algorithm under unrestricted switching and also\nprovide an upper bound on the regret under restricted switching when cache\nupdates can only happen in a pre-specified subset of timeslots. Finally, we\nvalidate our theoretical insights with numerical results using a real-world\ndataset"
                },
                "authors": [
                    {
                        "name": "Anupam Nayak"
                    },
                    {
                        "name": "Kota Srinivas Reddy"
                    },
                    {
                        "name": "Nikhil Karamchandani"
                    }
                ],
                "author_detail": {
                    "name": "Nikhil Karamchandani"
                },
                "author": "Nikhil Karamchandani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12387v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12387v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15366v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15366v1",
                "updated": "2024-09-18T17:33:31Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    33,
                    31,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T17:33:31Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    33,
                    31,
                    2,
                    262,
                    0
                ],
                "title": "Trajectory Anomaly Detection with Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trajectory Anomaly Detection with Language Models"
                },
                "summary": "This paper presents a novel approach for trajectory anomaly detection using\nan autoregressive causal-attention model, termed LM-TAD. This method leverages\nthe similarities between language statements and trajectories, both of which\nconsist of ordered elements requiring coherence through external rules and\ncontextual variations. By treating trajectories as sequences of tokens, our\nmodel learns the probability distributions over trajectories, enabling the\nidentification of anomalous locations with high precision. We incorporate\nuser-specific tokens to account for individual behavior patterns, enhancing\nanomaly detection tailored to user context. Our experiments demonstrate the\neffectiveness of LM-TAD on both synthetic and real-world datasets. In\nparticular, the model outperforms existing methods on the Pattern of Life (PoL)\ndataset by detecting user-contextual anomalies and achieves competitive results\non the Porto taxi dataset, highlighting its adaptability and robustness.\nAdditionally, we introduce the use of perplexity and surprisal rate metrics for\ndetecting outliers and pinpointing specific anomalous locations within\ntrajectories. The LM-TAD framework supports various trajectory representations,\nincluding GPS coordinates, staypoints, and activity types, proving its\nversatility in handling diverse trajectory data. Moreover, our approach is\nwell-suited for online trajectory anomaly detection, significantly reducing\ncomputational latency by caching key-value states of the attention mechanism,\nthereby avoiding repeated computations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a novel approach for trajectory anomaly detection using\nan autoregressive causal-attention model, termed LM-TAD. This method leverages\nthe similarities between language statements and trajectories, both of which\nconsist of ordered elements requiring coherence through external rules and\ncontextual variations. By treating trajectories as sequences of tokens, our\nmodel learns the probability distributions over trajectories, enabling the\nidentification of anomalous locations with high precision. We incorporate\nuser-specific tokens to account for individual behavior patterns, enhancing\nanomaly detection tailored to user context. Our experiments demonstrate the\neffectiveness of LM-TAD on both synthetic and real-world datasets. In\nparticular, the model outperforms existing methods on the Pattern of Life (PoL)\ndataset by detecting user-contextual anomalies and achieves competitive results\non the Porto taxi dataset, highlighting its adaptability and robustness.\nAdditionally, we introduce the use of perplexity and surprisal rate metrics for\ndetecting outliers and pinpointing specific anomalous locations within\ntrajectories. The LM-TAD framework supports various trajectory representations,\nincluding GPS coordinates, staypoints, and activity types, proving its\nversatility in handling diverse trajectory data. Moreover, our approach is\nwell-suited for online trajectory anomaly detection, significantly reducing\ncomputational latency by caching key-value states of the attention mechanism,\nthereby avoiding repeated computations."
                },
                "authors": [
                    {
                        "name": "Jonathan Mbuya"
                    },
                    {
                        "name": "Dieter Pfoser"
                    },
                    {
                        "name": "Antonios Anastasopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Antonios Anastasopoulos"
                },
                "author": "Antonios Anastasopoulos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15366v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15366v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11326v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11326v2",
                "updated": "2024-09-18T17:09:42Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    17,
                    9,
                    42,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-17T16:22:49Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    16,
                    22,
                    49,
                    1,
                    261,
                    0
                ],
                "title": "Autonomous Navigation in Ice-Covered Waters with Learned Predictions on\n  Ship-Ice Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous Navigation in Ice-Covered Waters with Learned Predictions on\n  Ship-Ice Interactions"
                },
                "summary": "Autonomous navigation in ice-covered waters poses significant challenges due\nto the frequent lack of viable collision-free trajectories. When complete\nobstacle avoidance is infeasible, it becomes imperative for the navigation\nstrategy to minimize collisions. Additionally, the dynamic nature of ice, which\nmoves in response to ship maneuvers, complicates the path planning process. To\naddress these challenges, we propose a novel deep learning model to estimate\nthe coarse dynamics of ice movements triggered by ship actions through\noccupancy estimation. To ensure real-time applicability, we propose a novel\napproach that caches intermediate prediction results and seamlessly integrates\nthe predictive model into a graph search planner. We evaluate the proposed\nplanner both in simulation and in a physical testbed against existing\napproaches and show that our planner significantly reduces collisions with ice\nwhen compared to the state-of-the-art. Codes and demos of this work are\navailable at https://github.com/IvanIZ/predictive-asv-planner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous navigation in ice-covered waters poses significant challenges due\nto the frequent lack of viable collision-free trajectories. When complete\nobstacle avoidance is infeasible, it becomes imperative for the navigation\nstrategy to minimize collisions. Additionally, the dynamic nature of ice, which\nmoves in response to ship maneuvers, complicates the path planning process. To\naddress these challenges, we propose a novel deep learning model to estimate\nthe coarse dynamics of ice movements triggered by ship actions through\noccupancy estimation. To ensure real-time applicability, we propose a novel\napproach that caches intermediate prediction results and seamlessly integrates\nthe predictive model into a graph search planner. We evaluate the proposed\nplanner both in simulation and in a physical testbed against existing\napproaches and show that our planner significantly reduces collisions with ice\nwhen compared to the state-of-the-art. Codes and demos of this work are\navailable at https://github.com/IvanIZ/predictive-asv-planner."
                },
                "authors": [
                    {
                        "name": "Ninghan Zhong"
                    },
                    {
                        "name": "Alessandro Potenza"
                    },
                    {
                        "name": "Stephen L. Smith"
                    }
                ],
                "author_detail": {
                    "name": "Stephen L. Smith"
                },
                "author": "Stephen L. Smith",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11326v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11326v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12021v1",
                "updated": "2024-09-18T14:31:33Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    14,
                    31,
                    33,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-18T14:31:33Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    14,
                    31,
                    33,
                    2,
                    262,
                    0
                ],
                "title": "Optimal Offline ORAM with Perfect Security via Simple Oblivious Priority\n  Queues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Offline ORAM with Perfect Security via Simple Oblivious Priority\n  Queues"
                },
                "summary": "Oblivious RAM (ORAM) is a well-researched primitive to hide the memory access\npattern of a RAM computation; it has a variety of applications in trusted\ncomputing, outsourced storage, and multiparty computation. In this paper, we\nstudy the so-called offline ORAM in which the sequence of memory access\nlocations to be hidden is known in advance. Apart from their theoretical\nsignificance, offline ORAMs can be used to construct efficient oblivious\nalgorithms.\n  We obtain the first optimal offline ORAM with perfect security from oblivious\npriority queues via time-forward processing. For this, we present a simple\nconstruction of an oblivious priority queue with perfect security. Our\nconstruction achieves an asymptotically optimal (amortized) runtime of\n$\\Theta(\\log N)$ per operation for a capacity of $N$ elements and is of\nindependent interest.\n  Building on our construction, we additionally present efficient\nexternal-memory instantiations of our oblivious, perfectly-secure construction:\nFor the cache-aware setting, we match the optimal I/O complexity of\n$\\Theta(\\frac{1}{B} \\log \\frac{N}{M})$ per operation (amortized), and for the\ncache-oblivious setting we achieve a near-optimal I/O complexity of\n$O(\\frac{1}{B} \\log \\frac{N}{M} \\log\\log_M N)$ per operation (amortized).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oblivious RAM (ORAM) is a well-researched primitive to hide the memory access\npattern of a RAM computation; it has a variety of applications in trusted\ncomputing, outsourced storage, and multiparty computation. In this paper, we\nstudy the so-called offline ORAM in which the sequence of memory access\nlocations to be hidden is known in advance. Apart from their theoretical\nsignificance, offline ORAMs can be used to construct efficient oblivious\nalgorithms.\n  We obtain the first optimal offline ORAM with perfect security from oblivious\npriority queues via time-forward processing. For this, we present a simple\nconstruction of an oblivious priority queue with perfect security. Our\nconstruction achieves an asymptotically optimal (amortized) runtime of\n$\\Theta(\\log N)$ per operation for a capacity of $N$ elements and is of\nindependent interest.\n  Building on our construction, we additionally present efficient\nexternal-memory instantiations of our oblivious, perfectly-secure construction:\nFor the cache-aware setting, we match the optimal I/O complexity of\n$\\Theta(\\frac{1}{B} \\log \\frac{N}{M})$ per operation (amortized), and for the\ncache-oblivious setting we achieve a near-optimal I/O complexity of\n$O(\\frac{1}{B} \\log \\frac{N}{M} \\log\\log_M N)$ per operation (amortized)."
                },
                "authors": [
                    {
                        "name": "Thore Thieen"
                    },
                    {
                        "name": "Jan Vahrenhold"
                    }
                ],
                "author_detail": {
                    "name": "Jan Vahrenhold"
                },
                "author": "Jan Vahrenhold",
                "arxiv_doi": "10.4230/LIPIcs.ISAAC.2024.36",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.4230/LIPIcs.ISAAC.2024.36",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.12021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "23 pages, full version of the paper to appear in ISAAC 2024",
                "arxiv_journal_ref": "Thore Thie{\\ss}en and Jan Vahrenhold. Optimal offline ORAM with\n  perfect security via simple oblivious priority queues. In 35th International\n  Symposium on Algorithms and Computation (ISAAC 2024), 19 pages. 2024",
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10516v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10516v2",
                "updated": "2024-09-18T13:11:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    13,
                    11,
                    13,
                    2,
                    262,
                    0
                ],
                "published": "2024-09-16T17:59:52Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    59,
                    52,
                    0,
                    260,
                    0
                ],
                "title": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval"
                },
                "summary": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nlatency and high GPU memory consumption for caching key-value (KV) vectors.\nThis paper proposes RetrievalAttention, a training-free approach to both\naccelerate attention computation and reduce GPU memory consumption. By\nleveraging the dynamic sparsity of attention mechanism, RetrievalAttention\nproposes to use approximate nearest neighbor search (ANNS) indexes for KV\nvectors in CPU memory and retrieves the most relevant ones with vector search\nduring generation. Unfortunately, we observe that the off-the-shelf ANNS\nindexes are often ineffective for such retrieval tasks due to the\nout-of-distribution (OOD) between query vectors and key vectors in attention\nmechanism. RetrievalAttention addresses the OOD challenge by designing an\nattention-aware vector search algorithm that can adapt to the distribution of\nquery vectors. Our evaluation shows that RetrievalAttention only needs to\naccess 1--3% of data while maintaining high model accuracy. This leads to\nsignificant reduction in the inference cost of long-context LLMs with much\nlower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) for serving 128K tokens in LLMs with 8B\nparameters, which is capable of generating one token in 0.188 seconds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nlatency and high GPU memory consumption for caching key-value (KV) vectors.\nThis paper proposes RetrievalAttention, a training-free approach to both\naccelerate attention computation and reduce GPU memory consumption. By\nleveraging the dynamic sparsity of attention mechanism, RetrievalAttention\nproposes to use approximate nearest neighbor search (ANNS) indexes for KV\nvectors in CPU memory and retrieves the most relevant ones with vector search\nduring generation. Unfortunately, we observe that the off-the-shelf ANNS\nindexes are often ineffective for such retrieval tasks due to the\nout-of-distribution (OOD) between query vectors and key vectors in attention\nmechanism. RetrievalAttention addresses the OOD challenge by designing an\nattention-aware vector search algorithm that can adapt to the distribution of\nquery vectors. Our evaluation shows that RetrievalAttention only needs to\naccess 1--3% of data while maintaining high model accuracy. This leads to\nsignificant reduction in the inference cost of long-context LLMs with much\nlower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) for serving 128K tokens in LLMs with 8B\nparameters, which is capable of generating one token in 0.188 seconds."
                },
                "authors": [
                    {
                        "name": "Di Liu"
                    },
                    {
                        "name": "Meng Chen"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Zhenhua Han"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Bailu Ding"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10516v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10516v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10687v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10687v2",
                "updated": "2024-09-18T08:22:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    8,
                    22,
                    23,
                    2,
                    262,
                    0
                ],
                "published": "2024-05-17T10:40:33Z",
                "published_parsed": [
                    2024,
                    5,
                    17,
                    10,
                    40,
                    33,
                    4,
                    138,
                    0
                ],
                "title": "Proportional scintillation in liquid xenon: demonstration in a\n  single-phase liquid-only time projection chamber",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proportional scintillation in liquid xenon: demonstration in a\n  single-phase liquid-only time projection chamber"
                },
                "summary": "The largest direct dark matter search experiments to date employ dual-phase\ntime projection chambers (TPCs) with liquid noble gas targets. These detect\nboth the primary photons generated by particle interactions in the liquid\ntarget, as well as proportional secondary scintillation light created by the\nionization electrons in a strong electric field in the gas phase between the\nliquid-gas interface and the anode. In this work, we describe the detection of\ncharge signals in a small-scale single-phase liquid-xenon-only TPC, that\nfeatures the well-established TPC geometry with light readout above and below a\ncylindrical target. In the single-phase TPC, the proportional scintillation\nlight (S2) is generated in liquid xenon in close proximity to 10 {\\mu}m\ndiameter anode wires. The detector was characterized and the proportional\nscintillation process was studied using the 32.1 keV and 9.4 keV signals from\n83mKr decays. A charge gain factor g2 of up to (1.9 $\\pm$ 0.3) PE/electron was\nreached at an anode voltage 4.4 kV higher than the gate electrode 5 mm below\nit, corresponding to (29 $\\pm$ 6) photons emitted per ionization electron. The\nduration of S2 signals is dominated by electron diffusion and approaches the\nxenon de-excitation timescale for very short electron drift times. The electron\ndrift velocity and the longitudinal diffusion constant were measured at a drift\nfield of 470 V/cm. The results agree with the literature and demonstrate that a\nsingle-phase TPC can be operated successfully.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The largest direct dark matter search experiments to date employ dual-phase\ntime projection chambers (TPCs) with liquid noble gas targets. These detect\nboth the primary photons generated by particle interactions in the liquid\ntarget, as well as proportional secondary scintillation light created by the\nionization electrons in a strong electric field in the gas phase between the\nliquid-gas interface and the anode. In this work, we describe the detection of\ncharge signals in a small-scale single-phase liquid-xenon-only TPC, that\nfeatures the well-established TPC geometry with light readout above and below a\ncylindrical target. In the single-phase TPC, the proportional scintillation\nlight (S2) is generated in liquid xenon in close proximity to 10 {\\mu}m\ndiameter anode wires. The detector was characterized and the proportional\nscintillation process was studied using the 32.1 keV and 9.4 keV signals from\n83mKr decays. A charge gain factor g2 of up to (1.9 $\\pm$ 0.3) PE/electron was\nreached at an anode voltage 4.4 kV higher than the gate electrode 5 mm below\nit, corresponding to (29 $\\pm$ 6) photons emitted per ionization electron. The\nduration of S2 signals is dominated by electron diffusion and approaches the\nxenon de-excitation timescale for very short electron drift times. The electron\ndrift velocity and the longitudinal diffusion constant were measured at a drift\nfield of 470 V/cm. The results agree with the literature and demonstrate that a\nsingle-phase TPC can be operated successfully."
                },
                "authors": [
                    {
                        "name": "Florian Tnnies"
                    },
                    {
                        "name": "Adam Brown"
                    },
                    {
                        "name": "Baris Kiyim"
                    },
                    {
                        "name": "Fabian Kuger"
                    },
                    {
                        "name": "Sebastian Lindemann"
                    },
                    {
                        "name": "Patrick Meinhardt"
                    },
                    {
                        "name": "Marc Schumann"
                    },
                    {
                        "name": "Andrew Stevens"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Stevens"
                },
                "author": "Andrew Stevens",
                "arxiv_comment": "20 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10687v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10687v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05821v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05821v3",
                "updated": "2024-09-18T04:53:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    18,
                    4,
                    53,
                    46,
                    2,
                    262,
                    0
                ],
                "published": "2023-12-10T08:41:24Z",
                "published_parsed": [
                    2023,
                    12,
                    10,
                    8,
                    41,
                    24,
                    6,
                    344,
                    0
                ],
                "title": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models"
                },
                "summary": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank factorization, and find that the challenges of this task\nstem from the outlier phenomenon in the LLM activations and the sensitivity\ndifference among various kinds of layers. To address these issues, we propose a\ntraining-free approach called Activation-aware Singular Value Decomposition\n(ASVD). Specifically, ASVD manages activation outliers by scaling the weight\nmatrix based on the activation distribution, thereby enhancing decomposition\naccuracy. Additionally, we propose an efficient iterative calibration process\nto optimize layer-specific decomposition by addressing the varying sensitivity\nof different LLM layers. ASVD can compress a network by 10-20%, without\ncompromising the performance of LLMs. Based on the success of the low-rank\ndecomposition of projection matrices in the self-attention module, we further\nintroduce ASVD to compress the KV cache. By reducing the channel dimension of\nKV activations, memory requirements for KV cache can be largely reduced. Thanks\nto the 50-75% reduction in the rank of the KV projection matrices, ASVD can\nfurther achieve 50% KV cache reductions without performance drop in a\ntraining-free manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank factorization, and find that the challenges of this task\nstem from the outlier phenomenon in the LLM activations and the sensitivity\ndifference among various kinds of layers. To address these issues, we propose a\ntraining-free approach called Activation-aware Singular Value Decomposition\n(ASVD). Specifically, ASVD manages activation outliers by scaling the weight\nmatrix based on the activation distribution, thereby enhancing decomposition\naccuracy. Additionally, we propose an efficient iterative calibration process\nto optimize layer-specific decomposition by addressing the varying sensitivity\nof different LLM layers. ASVD can compress a network by 10-20%, without\ncompromising the performance of LLMs. Based on the success of the low-rank\ndecomposition of projection matrices in the self-attention module, we further\nintroduce ASVD to compress the KV cache. By reducing the channel dimension of\nKV activations, memory requirements for KV cache can be largely reduced. Thanks\nto the 50-75% reduction in the rank of the KV projection matrices, ASVD can\nfurther achieve 50% KV cache reductions without performance drop in a\ntraining-free manner."
                },
                "authors": [
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Yuzhang Shang"
                    },
                    {
                        "name": "Yue Song"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Yan Yan"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.05821v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05821v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11600v1",
                "updated": "2024-09-17T23:15:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    23,
                    15,
                    39,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T23:15:39Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    23,
                    15,
                    39,
                    1,
                    261,
                    0
                ],
                "title": "No Saved Kaleidosope: an 100% Jitted Neural Network Coding Language with\n  Pythonic Syntax",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "No Saved Kaleidosope: an 100% Jitted Neural Network Coding Language with\n  Pythonic Syntax"
                },
                "summary": "We developed a jitted compiler for training Artificial Neural Networks using\nC++, LLVM and Cuda. It features object-oriented characteristics, strong typing,\nparallel workers for data pre-processing, pythonic syntax for expressions,\nPyTorch like model declaration and Automatic Differentiation. We implement the\nmechanisms of cache and pooling in order to manage VRAM, cuBLAS for high\nperformance matrix multiplication and cuDNN for convolutional layers. Our\nexperiments with Residual Convolutional Neural Networks on ImageNet, we reach\nsimilar speed but degraded performance. Also, the GRU network experiments show\nsimilar accuracy, but our compiler have degraded speed in that task. However,\nour compiler demonstrates promising results at the CIFAR-10 benchmark, in which\nwe reach the same performance and about the same speed as PyTorch. We make the\ncode publicly available at: https://github.com/NoSavedDATA/NoSavedKaleidoscope",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We developed a jitted compiler for training Artificial Neural Networks using\nC++, LLVM and Cuda. It features object-oriented characteristics, strong typing,\nparallel workers for data pre-processing, pythonic syntax for expressions,\nPyTorch like model declaration and Automatic Differentiation. We implement the\nmechanisms of cache and pooling in order to manage VRAM, cuBLAS for high\nperformance matrix multiplication and cuDNN for convolutional layers. Our\nexperiments with Residual Convolutional Neural Networks on ImageNet, we reach\nsimilar speed but degraded performance. Also, the GRU network experiments show\nsimilar accuracy, but our compiler have degraded speed in that task. However,\nour compiler demonstrates promising results at the CIFAR-10 benchmark, in which\nwe reach the same performance and about the same speed as PyTorch. We make the\ncode publicly available at: https://github.com/NoSavedDATA/NoSavedKaleidoscope"
                },
                "authors": [
                    {
                        "name": "Augusto Seben da Rosa"
                    },
                    {
                        "name": "Marlon Daniel Angeli"
                    },
                    {
                        "name": "Jorge Aikes Junior"
                    },
                    {
                        "name": "Alef Iury Ferreira"
                    },
                    {
                        "name": "Lucas Rafael Gris"
                    },
                    {
                        "name": "Anderson da Silva Soares"
                    },
                    {
                        "name": "Arnaldo Candido Junior"
                    },
                    {
                        "name": "Frederico Santos de Oliveira"
                    },
                    {
                        "name": "Gabriel Trevisan Damke"
                    },
                    {
                        "name": "Rafael Teixeira Sousa"
                    }
                ],
                "author_detail": {
                    "name": "Rafael Teixeira Sousa"
                },
                "author": "Rafael Teixeira Sousa",
                "arxiv_comment": "12 pages, 3 figures and 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.3; I.2; I.4; I.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11258v1",
                "updated": "2024-09-17T15:07:05Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    15,
                    7,
                    5,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T15:07:05Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    15,
                    7,
                    5,
                    1,
                    261,
                    0
                ],
                "title": "Attacking Slicing Network via Side-channel Reinforcement Learning Attack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attacking Slicing Network via Side-channel Reinforcement Learning Attack"
                },
                "summary": "Network slicing in 5G and the future 6G networks will enable the creation of\nmultiple virtualized networks on a shared physical infrastructure. This\ninnovative approach enables the provision of tailored networks to accommodate\nspecific business types or industry users, thus delivering more customized and\nefficient services. However, the shared memory and cache in network slicing\nintroduce security vulnerabilities that have yet to be fully addressed. In this\npaper, we introduce a reinforcement learning-based side-channel cache attack\nframework specifically designed for network slicing environments. Unlike\ntraditional cache attack methods, our framework leverages reinforcement\nlearning to dynamically identify and exploit cache locations storing sensitive\ninformation, such as authentication keys and user registration data. We assume\nthat one slice network is compromised and demonstrate how the attacker can\ninduce another shared slice to send registration requests, thereby estimating\nthe cache locations of critical data. By formulating the cache timing channel\nattack as a reinforcement learning-driven guessing game between the attack\nslice and the victim slice, our model efficiently explores possible actions to\npinpoint memory blocks containing sensitive information. Experimental results\nshowcase the superiority of our approach, achieving a success rate of\napproximately 95\\% to 98\\% in accurately identifying the storage locations of\nsensitive data. This high level of accuracy underscores the potential risks in\nshared network slicing environments and highlights the need for robust security\nmeasures to safeguard against such advanced side-channel attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network slicing in 5G and the future 6G networks will enable the creation of\nmultiple virtualized networks on a shared physical infrastructure. This\ninnovative approach enables the provision of tailored networks to accommodate\nspecific business types or industry users, thus delivering more customized and\nefficient services. However, the shared memory and cache in network slicing\nintroduce security vulnerabilities that have yet to be fully addressed. In this\npaper, we introduce a reinforcement learning-based side-channel cache attack\nframework specifically designed for network slicing environments. Unlike\ntraditional cache attack methods, our framework leverages reinforcement\nlearning to dynamically identify and exploit cache locations storing sensitive\ninformation, such as authentication keys and user registration data. We assume\nthat one slice network is compromised and demonstrate how the attacker can\ninduce another shared slice to send registration requests, thereby estimating\nthe cache locations of critical data. By formulating the cache timing channel\nattack as a reinforcement learning-driven guessing game between the attack\nslice and the victim slice, our model efficiently explores possible actions to\npinpoint memory blocks containing sensitive information. Experimental results\nshowcase the superiority of our approach, achieving a success rate of\napproximately 95\\% to 98\\% in accurately identifying the storage locations of\nsensitive data. This high level of accuracy underscores the potential risks in\nshared network slicing environments and highlights the need for robust security\nmeasures to safeguard against such advanced side-channel attacks."
                },
                "authors": [
                    {
                        "name": "Wei Shao"
                    },
                    {
                        "name": "Chandra Thapa"
                    },
                    {
                        "name": "Rayne Holland"
                    },
                    {
                        "name": "Sarah Ali Siddiqui"
                    },
                    {
                        "name": "Seyit Camtepe"
                    }
                ],
                "author_detail": {
                    "name": "Seyit Camtepe"
                },
                "author": "Seyit Camtepe",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11102v1",
                "updated": "2024-09-17T11:54:24Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    11,
                    54,
                    24,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T11:54:24Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    11,
                    54,
                    24,
                    1,
                    261,
                    0
                ],
                "title": "Electron-beam-induced adatom-vacancy-complexes in mono- and bilayer\n  phosphorene",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron-beam-induced adatom-vacancy-complexes in mono- and bilayer\n  phosphorene"
                },
                "summary": "Phosphorene, a puckered two-dimensional allotrope of phosphorus, has sparked\nconsiderable interest in recent years due to its potential especially for\noptoelectronic applications with its layer-number-dependant direct band gap and\nstrongly bound excitons. However, detailed experimental characterization of its\nintrinsic defects as well as its defect creation characteristics under electron\nirradiation are scarce. Here, we report on the creation and stability of a\nvariety of defect configurations under 60 kV electron irradiation in mono- and\nbilayer phosphorene including the first experimental reports of stable\nadatom-vacancy-complexes. Displacement cross section measurements in bilayer\nphosphorene yield a value of 7.7 +- 1.4 barn with an estimated lifetime of\nadatom-vacancy-complexes of 19.9 +- 0.7 s, while some are stable for up to 68 s\nunder continuous electron irradiation. Surprisingly, ab initio-based\nsimulations indicate that the complexes should readily recombine, even in\nstructures strained by up to 3 %. The presented results will help to improve\nthe understanding of the wide variety of defects in phosphorene, their\ncreation, and their stability, which may enable new pathways for defect\nengineered phosphorene devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phosphorene, a puckered two-dimensional allotrope of phosphorus, has sparked\nconsiderable interest in recent years due to its potential especially for\noptoelectronic applications with its layer-number-dependant direct band gap and\nstrongly bound excitons. However, detailed experimental characterization of its\nintrinsic defects as well as its defect creation characteristics under electron\nirradiation are scarce. Here, we report on the creation and stability of a\nvariety of defect configurations under 60 kV electron irradiation in mono- and\nbilayer phosphorene including the first experimental reports of stable\nadatom-vacancy-complexes. Displacement cross section measurements in bilayer\nphosphorene yield a value of 7.7 +- 1.4 barn with an estimated lifetime of\nadatom-vacancy-complexes of 19.9 +- 0.7 s, while some are stable for up to 68 s\nunder continuous electron irradiation. Surprisingly, ab initio-based\nsimulations indicate that the complexes should readily recombine, even in\nstructures strained by up to 3 %. The presented results will help to improve\nthe understanding of the wide variety of defects in phosphorene, their\ncreation, and their stability, which may enable new pathways for defect\nengineered phosphorene devices."
                },
                "authors": [
                    {
                        "name": "Carsten Speckmann"
                    },
                    {
                        "name": "Andrea Angeletti"
                    },
                    {
                        "name": "Luk Kvala"
                    },
                    {
                        "name": "David Lamprecht"
                    },
                    {
                        "name": "Felix Herterich"
                    },
                    {
                        "name": "Clemens Mangler"
                    },
                    {
                        "name": "Lado Filipovic"
                    },
                    {
                        "name": "Christoph Dellago"
                    },
                    {
                        "name": "Cesare Franchini"
                    },
                    {
                        "name": "Jani Kotakoski"
                    }
                ],
                "author_detail": {
                    "name": "Jani Kotakoski"
                },
                "author": "Jani Kotakoski",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11057v1",
                "updated": "2024-09-17T10:35:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    10,
                    35,
                    30,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T10:35:30Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    10,
                    35,
                    30,
                    1,
                    261,
                    0
                ],
                "title": "KVPruner: Structural Pruning for Faster and Memory-Efficient Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVPruner: Structural Pruning for Faster and Memory-Efficient Large\n  Language Models"
                },
                "summary": "The bottleneck associated with the key-value(KV) cache presents a significant\nchallenge during the inference processes of large language models. While depth\npruning accelerates inference, it requires extensive recovery training, which\ncan take up to two weeks. On the other hand, width pruning retains much of the\nperformance but offers slight speed gains. To tackle these challenges, we\npropose KVPruner to improve model efficiency while maintaining performance. Our\nmethod uses global perplexity-based analysis to determine the importance ratio\nfor each block and provides multiple strategies to prune non-essential KV\nchannels within blocks. Compared to the original model, KVPruner reduces\nruntime memory usage by 50% and boosts throughput by over 35%. Additionally,\nour method requires only two hours of LoRA fine-tuning on small datasets to\nrecover most of the performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The bottleneck associated with the key-value(KV) cache presents a significant\nchallenge during the inference processes of large language models. While depth\npruning accelerates inference, it requires extensive recovery training, which\ncan take up to two weeks. On the other hand, width pruning retains much of the\nperformance but offers slight speed gains. To tackle these challenges, we\npropose KVPruner to improve model efficiency while maintaining performance. Our\nmethod uses global perplexity-based analysis to determine the importance ratio\nfor each block and provides multiple strategies to prune non-essential KV\nchannels within blocks. Compared to the original model, KVPruner reduces\nruntime memory usage by 50% and boosts throughput by over 35%. Additionally,\nour method requires only two hours of LoRA fine-tuning on small datasets to\nrecover most of the performance."
                },
                "authors": [
                    {
                        "name": "Bo Lv"
                    },
                    {
                        "name": "Quan Zhou"
                    },
                    {
                        "name": "Xuanang Ding"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Zeming Ma"
                    }
                ],
                "author_detail": {
                    "name": "Zeming Ma"
                },
                "author": "Zeming Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10946v1",
                "updated": "2024-09-17T07:28:56Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    7,
                    28,
                    56,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-17T07:28:56Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    7,
                    28,
                    56,
                    1,
                    261,
                    0
                ],
                "title": "Skip TLB flushes for reused pages within mmap's",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skip TLB flushes for reused pages within mmap's"
                },
                "summary": "Memory access efficiency is significantly enhanced by caching recent address\ntranslations in the CPUs' Translation Lookaside Buffers (TLBs). However, since\nthe operating system is not aware of which core is using a particular mapping,\nit flushes TLB entries across all cores where the application runs whenever\naddresses are unmapped, ensuring security and consistency. These TLB flushes,\nknown as TLB shootdowns, are costly and create a performance and scalability\nbottleneck. A key contributor to TLB shootdowns is memory-mapped I/O,\nparticularly during mmap-munmap cycles and page cache evictions. Often, the\nsame physical pages are reassigned to the same process post-eviction,\npresenting an opportunity for the operating system to reduce the frequency of\nTLB shootdowns. We demonstrate, that by slightly extending the mmap function,\nTLB shootdowns for these \"recycled pages\" can be avoided.\n  Therefore we introduce and implement the \"fast page recycling\" (FPR) feature\nwithin the mmap system call. FPR-mmaps maintain security by only triggering TLB\nshootdowns when a page exits its recycling cycle and is allocated to a\ndifferent process. To ensure consistency when FPR-mmap pointers are used, we\nmade minor adjustments to virtual memory management to avoid the ABA problem.\nUnlike previous methods to mitigate shootdown effects, our approach does not\nrequire any hardware modifications and operates transparently within the\nexisting Linux virtual memory framework.\n  Our evaluations across a variety of CPU, memory, and storage setups,\nincluding persistent memory and Optane SSDs, demonstrate that FPR delivers\nnotable performance gains, with improvements of up to 28% in real-world\napplications and 92% in micro-benchmarks. Additionally, we show that TLB\nshootdowns are a significant source of bottlenecks, previously misattributed to\nother components of the Linux kernel.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory access efficiency is significantly enhanced by caching recent address\ntranslations in the CPUs' Translation Lookaside Buffers (TLBs). However, since\nthe operating system is not aware of which core is using a particular mapping,\nit flushes TLB entries across all cores where the application runs whenever\naddresses are unmapped, ensuring security and consistency. These TLB flushes,\nknown as TLB shootdowns, are costly and create a performance and scalability\nbottleneck. A key contributor to TLB shootdowns is memory-mapped I/O,\nparticularly during mmap-munmap cycles and page cache evictions. Often, the\nsame physical pages are reassigned to the same process post-eviction,\npresenting an opportunity for the operating system to reduce the frequency of\nTLB shootdowns. We demonstrate, that by slightly extending the mmap function,\nTLB shootdowns for these \"recycled pages\" can be avoided.\n  Therefore we introduce and implement the \"fast page recycling\" (FPR) feature\nwithin the mmap system call. FPR-mmaps maintain security by only triggering TLB\nshootdowns when a page exits its recycling cycle and is allocated to a\ndifferent process. To ensure consistency when FPR-mmap pointers are used, we\nmade minor adjustments to virtual memory management to avoid the ABA problem.\nUnlike previous methods to mitigate shootdown effects, our approach does not\nrequire any hardware modifications and operates transparently within the\nexisting Linux virtual memory framework.\n  Our evaluations across a variety of CPU, memory, and storage setups,\nincluding persistent memory and Optane SSDs, demonstrate that FPR delivers\nnotable performance gains, with improvements of up to 28% in real-world\napplications and 92% in micro-benchmarks. Additionally, we show that TLB\nshootdowns are a significant source of bottlenecks, previously misattributed to\nother components of the Linux kernel."
                },
                "authors": [
                    {
                        "name": "Frederic Schimmelpfennig"
                    },
                    {
                        "name": "Andr Brinkmann"
                    },
                    {
                        "name": "Hossein Asadi"
                    },
                    {
                        "name": "Reza Salkhordeh"
                    }
                ],
                "author_detail": {
                    "name": "Reza Salkhordeh"
                },
                "author": "Reza Salkhordeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09417v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09417v2",
                "updated": "2024-09-17T04:39:04Z",
                "updated_parsed": [
                    2024,
                    9,
                    17,
                    4,
                    39,
                    4,
                    1,
                    261,
                    0
                ],
                "published": "2024-09-14T11:15:38Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    11,
                    15,
                    38,
                    5,
                    258,
                    0
                ],
                "title": "Resources on the Move for Smart City: A Disruptive Perspective on the\n  Grand Convergence of Sensing, Communications, Computing, Storage, and\n  Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resources on the Move for Smart City: A Disruptive Perspective on the\n  Grand Convergence of Sensing, Communications, Computing, Storage, and\n  Intelligence"
                },
                "summary": "The most commonly seen things on streets in any city are vehicles. However,\nmost of them are used to transport people or goods. What if they also carry\nresources and capabilities for sensing, communications, computing, storage, and\nintelligence (SCCSI)? We will have a web of sensors to monitor the city, a\nnetwork of powerful communicators to transport data around, a grid of computing\npower to conduct data analytics and machine learning (ML), a network of\ndistributed storage to buffer/cache data/job for optimization, and a set of\nmovable AI/ML toolboxes made available for specialized smart applications. This\nperspective article presents how to leverage SCCSI-empowered vehicles to design\nsuch a service network, simply called SCCSI network, to help build a smart city\nwith a cost-effective and sustainable solution. It showcases how\nmulti-dimensional technologies, namely, sensing, communications, computing,\nstorage, and intelligence, converge to a unifying technology to solve grand\nchallenges for resource demands from emerging large-scale applications. Thus,\nwith SCCSI-empowered vehicles on the ground, over the air, and on the sea,\nSCCSI network can make resources and capabilities on the move, practically\npushing SCCSI services to the edge! We hope this article serves as a spark to\nstimulate more disruptive thinking to address grand challenges of paramount\nimportance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The most commonly seen things on streets in any city are vehicles. However,\nmost of them are used to transport people or goods. What if they also carry\nresources and capabilities for sensing, communications, computing, storage, and\nintelligence (SCCSI)? We will have a web of sensors to monitor the city, a\nnetwork of powerful communicators to transport data around, a grid of computing\npower to conduct data analytics and machine learning (ML), a network of\ndistributed storage to buffer/cache data/job for optimization, and a set of\nmovable AI/ML toolboxes made available for specialized smart applications. This\nperspective article presents how to leverage SCCSI-empowered vehicles to design\nsuch a service network, simply called SCCSI network, to help build a smart city\nwith a cost-effective and sustainable solution. It showcases how\nmulti-dimensional technologies, namely, sensing, communications, computing,\nstorage, and intelligence, converge to a unifying technology to solve grand\nchallenges for resource demands from emerging large-scale applications. Thus,\nwith SCCSI-empowered vehicles on the ground, over the air, and on the sea,\nSCCSI network can make resources and capabilities on the move, practically\npushing SCCSI services to the edge! We hope this article serves as a spark to\nstimulate more disruptive thinking to address grand challenges of paramount\nimportance."
                },
                "authors": [
                    {
                        "name": "Yuguang Fang"
                    },
                    {
                        "name": "Yiqin Deng"
                    },
                    {
                        "name": "Xianhao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xianhao Chen"
                },
                "author": "Xianhao Chen",
                "arxiv_comment": "8 pages, 3 figures. Accepted by IEEE Communications Magazine",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09417v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09417v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13761v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13761v1",
                "updated": "2024-09-16T18:46:24Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    18,
                    46,
                    24,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T18:46:24Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    18,
                    46,
                    24,
                    0,
                    260,
                    0
                ],
                "title": "Do Large Language Models Need a Content Delivery Network?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Need a Content Delivery Network?"
                },
                "summary": "As the use of large language models (LLMs) expands rapidly, so does the range\nof knowledge needed to supplement various LLM queries. Thus, enabling flexible\nand efficient injection of new knowledge in LLM inference is critical. Three\nhigh-level options exist: (i) embedding the knowledge in LLM's weights (i.e.,\nfine-tuning), (ii) including the knowledge as a part of LLM's text input (i.e.,\nin-context learning), or (iii) injecting the KV caches of the new knowledge to\nLLM during prefill. This paper argues that, although fine-tuning and in-context\nlearning are popular, using KV caches as the medium of knowledge could\nsimultaneously enable more modular management of knowledge injection and more\nefficient LLM serving with low cost and fast response. To realize these\nbenefits, we envision a Knowledge Delivery Network (KDN), a new system\ncomponent in LLM services that dynamically optimizes the storage, transfer, and\ncomposition of KV cache across LLM engines and other compute and storage\nresources. We believe that, just like content delivery networks (CDNs), such as\nAkamai, enabled the success of the Internet ecosystem through their efficient\ndata delivery, KDNs will be critical to the success of LLM applications through\ntheir efficient knowledge delivery. We have open-sourced a KDN prototype at\nhttps://github.com/LMCache/LMCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the use of large language models (LLMs) expands rapidly, so does the range\nof knowledge needed to supplement various LLM queries. Thus, enabling flexible\nand efficient injection of new knowledge in LLM inference is critical. Three\nhigh-level options exist: (i) embedding the knowledge in LLM's weights (i.e.,\nfine-tuning), (ii) including the knowledge as a part of LLM's text input (i.e.,\nin-context learning), or (iii) injecting the KV caches of the new knowledge to\nLLM during prefill. This paper argues that, although fine-tuning and in-context\nlearning are popular, using KV caches as the medium of knowledge could\nsimultaneously enable more modular management of knowledge injection and more\nefficient LLM serving with low cost and fast response. To realize these\nbenefits, we envision a Knowledge Delivery Network (KDN), a new system\ncomponent in LLM services that dynamically optimizes the storage, transfer, and\ncomposition of KV cache across LLM engines and other compute and storage\nresources. We believe that, just like content delivery networks (CDNs), such as\nAkamai, enabled the success of the Internet ecosystem through their efficient\ndata delivery, KDNs will be critical to the success of LLM applications through\ntheir efficient knowledge delivery. We have open-sourced a KDN prototype at\nhttps://github.com/LMCache/LMCache."
                },
                "authors": [
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13761v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13761v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10287v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10287v1",
                "updated": "2024-09-16T13:52:46Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    13,
                    52,
                    46,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T13:52:46Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    13,
                    52,
                    46,
                    0,
                    260,
                    0
                ],
                "title": "Ejected Particles after Impact Splash on Mars: Electrification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ejected Particles after Impact Splash on Mars: Electrification"
                },
                "summary": "Within the RoadMap project we investigated the microphysical aspects of\nparticle collisions during saltation on the Martian surface in laboratory\nexperiments. Following the size distribution of ejected particles, their\naerodynamic properties and aggregation status upon ejection, we now focus on\nthe electrification and charge distribution of ejected particles. We analyzed\nrebound and ejection trajectories of grains in a vacuum setup with a strong\nelectric field of 100 kV/m and deduced particle charges from their\nacceleration. The ejected particles have sizes of about 10 to 100 microns. They\ncarry charges up to $10^5$ e or charge densities up to $> 10^7$ e/mm$^2$.\nWithin the given size range, we find a small bias towards positive charges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Within the RoadMap project we investigated the microphysical aspects of\nparticle collisions during saltation on the Martian surface in laboratory\nexperiments. Following the size distribution of ejected particles, their\naerodynamic properties and aggregation status upon ejection, we now focus on\nthe electrification and charge distribution of ejected particles. We analyzed\nrebound and ejection trajectories of grains in a vacuum setup with a strong\nelectric field of 100 kV/m and deduced particle charges from their\nacceleration. The ejected particles have sizes of about 10 to 100 microns. They\ncarry charges up to $10^5$ e or charge densities up to $> 10^7$ e/mm$^2$.\nWithin the given size range, we find a small bias towards positive charges."
                },
                "authors": [
                    {
                        "name": "T. Becker"
                    },
                    {
                        "name": "F. C. Onyeagusi"
                    },
                    {
                        "name": "J. Teiser"
                    },
                    {
                        "name": "T. Jardiel"
                    },
                    {
                        "name": "M. Peiteado"
                    },
                    {
                        "name": "O. Munoz"
                    },
                    {
                        "name": "J. Martikainen"
                    },
                    {
                        "name": "J. C. Gomez Martin"
                    },
                    {
                        "name": "J. Merrison"
                    },
                    {
                        "name": "G. Wurm"
                    }
                ],
                "author_detail": {
                    "name": "G. Wurm"
                },
                "author": "G. Wurm",
                "arxiv_comment": "Preprint, 7 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10287v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10287v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10207v1",
                "updated": "2024-09-16T11:56:09Z",
                "updated_parsed": [
                    2024,
                    9,
                    16,
                    11,
                    56,
                    9,
                    0,
                    260,
                    0
                ],
                "published": "2024-09-16T11:56:09Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    11,
                    56,
                    9,
                    0,
                    260,
                    0
                ],
                "title": "Decoupling DNS Update Timing from TTL Values",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoupling DNS Update Timing from TTL Values"
                },
                "summary": "A relatively simple safety-belt mechanism for improving DNS system\navailability and efficiency is proposed here. While it may seem ambitious, a\ncareful examination shows it is both feasible and beneficial for the DNS\nsystem. The mechanism called \"DNS Real-time Update\" (DNSRU), a service that\nfacilitates real-time and secure updates of cached domain records in DNS\nresolvers worldwide, even before the expiration of the corresponding Time To\nLive (TTL) values. This service allows Internet domain owners to quickly\nrectify any erroneous global IP address distribution, even if a long TTL value\nis associated with it. By addressing this critical DNS high availability issue,\nDNSRU eliminates the need for short TTL values and their associated drawbacks.\nTherefore, DNSRU DNSRU reduces the traffic load on authoritative servers while\nenhancing the system's fault tolerance. In this paper we show that our DNSRU\ndesign is backward compatible, supports gradual deployment, secure, efficient,\nand feasible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A relatively simple safety-belt mechanism for improving DNS system\navailability and efficiency is proposed here. While it may seem ambitious, a\ncareful examination shows it is both feasible and beneficial for the DNS\nsystem. The mechanism called \"DNS Real-time Update\" (DNSRU), a service that\nfacilitates real-time and secure updates of cached domain records in DNS\nresolvers worldwide, even before the expiration of the corresponding Time To\nLive (TTL) values. This service allows Internet domain owners to quickly\nrectify any erroneous global IP address distribution, even if a long TTL value\nis associated with it. By addressing this critical DNS high availability issue,\nDNSRU eliminates the need for short TTL values and their associated drawbacks.\nTherefore, DNSRU DNSRU reduces the traffic load on authoritative servers while\nenhancing the system's fault tolerance. In this paper we show that our DNSRU\ndesign is backward compatible, supports gradual deployment, secure, efficient,\nand feasible."
                },
                "authors": [
                    {
                        "name": "Yehuda Afek"
                    },
                    {
                        "name": "Ariel Litmanovich"
                    }
                ],
                "author_detail": {
                    "name": "Ariel Litmanovich"
                },
                "author": "Ariel Litmanovich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09753v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09753v1",
                "updated": "2024-09-15T14:49:30Z",
                "updated_parsed": [
                    2024,
                    9,
                    15,
                    14,
                    49,
                    30,
                    6,
                    259,
                    0
                ],
                "published": "2024-09-15T14:49:30Z",
                "published_parsed": [
                    2024,
                    9,
                    15,
                    14,
                    49,
                    30,
                    6,
                    259,
                    0
                ],
                "title": "DARDA: Domain-Aware Real-Time Dynamic Neural Network Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DARDA: Domain-Aware Real-Time Dynamic Neural Network Adaptation"
                },
                "summary": "Test Time Adaptation (TTA) has emerged as a practical solution to mitigate\nthe performance degradation of Deep Neural Networks (DNNs) in the presence of\ncorruption/ noise affecting inputs. Existing approaches in TTA continuously\nadapt the DNN, leading to excessive resource consumption and performance\ndegradation due to accumulation of error stemming from lack of supervision. In\nthis work, we propose Domain-Aware Real-Time Dynamic Adaptation (DARDA) to\naddress such issues. Our key approach is to proactively learn latent\nrepresentations of some corruption types, each one associated with a\nsub-network state tailored to correctly classify inputs affected by that\ncorruption. After deployment, DARDA adapts the DNN to previously unseen\ncorruptions in an unsupervised fashion by (i) estimating the latent\nrepresentation of the ongoing corruption; (ii) selecting the sub-network whose\nassociated corruption is the closest in the latent space to the ongoing\ncorruption; and (iii) adapting DNN state, so that its representation matches\nthe ongoing corruption. This way, DARDA is more resource efficient and can\nswiftly adapt to new distributions caused by different corruptions without\nrequiring a large variety of input data. Through experiments with two popular\nmobile edge devices - Raspberry Pi and NVIDIA Jetson Nano - we show that DARDA\nreduces energy consumption and average cache memory footprint respectively by\n1.74x and 2.64x with respect to the state of the art, while increasing the\nperformance by 10.4%, 5.7% and 4.4% on CIFAR-10, CIFAR-100 and TinyImagenet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test Time Adaptation (TTA) has emerged as a practical solution to mitigate\nthe performance degradation of Deep Neural Networks (DNNs) in the presence of\ncorruption/ noise affecting inputs. Existing approaches in TTA continuously\nadapt the DNN, leading to excessive resource consumption and performance\ndegradation due to accumulation of error stemming from lack of supervision. In\nthis work, we propose Domain-Aware Real-Time Dynamic Adaptation (DARDA) to\naddress such issues. Our key approach is to proactively learn latent\nrepresentations of some corruption types, each one associated with a\nsub-network state tailored to correctly classify inputs affected by that\ncorruption. After deployment, DARDA adapts the DNN to previously unseen\ncorruptions in an unsupervised fashion by (i) estimating the latent\nrepresentation of the ongoing corruption; (ii) selecting the sub-network whose\nassociated corruption is the closest in the latent space to the ongoing\ncorruption; and (iii) adapting DNN state, so that its representation matches\nthe ongoing corruption. This way, DARDA is more resource efficient and can\nswiftly adapt to new distributions caused by different corruptions without\nrequiring a large variety of input data. Through experiments with two popular\nmobile edge devices - Raspberry Pi and NVIDIA Jetson Nano - we show that DARDA\nreduces energy consumption and average cache memory footprint respectively by\n1.74x and 2.64x with respect to the state of the art, while increasing the\nperformance by 10.4%, 5.7% and 4.4% on CIFAR-10, CIFAR-100 and TinyImagenet."
                },
                "authors": [
                    {
                        "name": "Shahriar Rifat"
                    },
                    {
                        "name": "Jonathan Ashdown"
                    },
                    {
                        "name": "Francesco Restuccia"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Restuccia"
                },
                "author": "Francesco Restuccia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09753v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09753v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09398v1",
                "updated": "2024-09-14T10:15:37Z",
                "updated_parsed": [
                    2024,
                    9,
                    14,
                    10,
                    15,
                    37,
                    5,
                    258,
                    0
                ],
                "published": "2024-09-14T10:15:37Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    10,
                    15,
                    37,
                    5,
                    258,
                    0
                ],
                "title": "Language-Queried Target Sound Extraction Without Parallel Training Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-Queried Target Sound Extraction Without Parallel Training Data"
                },
                "summary": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a language-free training scheme, requiring\nonly unlabelled audio clips for TSE model training by utilizing the multi-modal\nrepresentation alignment nature of the contrastive language-audio pre-trained\nmodel (CLAP). In a vanilla language-free training stage, target audio is\nencoded using the pre-trained CLAP audio encoder to form a condition embedding\nfor the TSE model, while during inference, user language queries are encoded by\nCLAP text encoder. This straightforward approach faces challenges due to the\nmodality gap between training and inference queries and information leakage\nfrom direct exposure to target audio during training. To address this, we\npropose a retrieval-augmented strategy. Specifically, we create an embedding\ncache using audio captions generated by a large language model (LLM). During\ntraining, target audio embeddings retrieve text embeddings from this cache to\nuse as condition embeddings, ensuring consistent modalities between training\nand inference and eliminating information leakage. Extensive experiment results\nshow that our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a language-free training scheme, requiring\nonly unlabelled audio clips for TSE model training by utilizing the multi-modal\nrepresentation alignment nature of the contrastive language-audio pre-trained\nmodel (CLAP). In a vanilla language-free training stage, target audio is\nencoded using the pre-trained CLAP audio encoder to form a condition embedding\nfor the TSE model, while during inference, user language queries are encoded by\nCLAP text encoder. This straightforward approach faces challenges due to the\nmodality gap between training and inference queries and information leakage\nfrom direct exposure to target audio during training. To address this, we\npropose a retrieval-augmented strategy. Specifically, we create an embedding\ncache using audio captions generated by a large language model (LLM). During\ntraining, target audio embeddings retrieve text embeddings from this cache to\nuse as condition embeddings, ensuring consistent modalities between training\nand inference and eliminating information leakage. Extensive experiment results\nshow that our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability."
                },
                "authors": [
                    {
                        "name": "Hao Ma"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Xu Li"
                    },
                    {
                        "name": "Yukai Li"
                    },
                    {
                        "name": "Mingjie Shao"
                    },
                    {
                        "name": "Qiuqiang Kong"
                    },
                    {
                        "name": "Ju Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ju Liu"
                },
                "author": "Ju Liu",
                "arxiv_comment": "Submitted to ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09322v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09322v1",
                "updated": "2024-09-14T05:51:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    14,
                    5,
                    51,
                    50,
                    5,
                    258,
                    0
                ],
                "published": "2024-09-14T05:51:50Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    5,
                    51,
                    50,
                    5,
                    258,
                    0
                ],
                "title": "A Compressive Memory-based Retrieval Approach for Event Argument\n  Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Compressive Memory-based Retrieval Approach for Event Argument\n  Extraction"
                },
                "summary": "Recent works have demonstrated the effectiveness of retrieval augmentation in\nthe Event Argument Extraction (EAE) task. However, existing retrieval-based EAE\nmethods have two main limitations: (1) input length constraints and (2) the gap\nbetween the retriever and the inference model. These issues limit the diversity\nand quality of the retrieved information. In this paper, we propose a\nCompressive Memory-based Retrieval (CMR) mechanism for EAE, which addresses the\ntwo limitations mentioned above. Our compressive memory, designed as a dynamic\nmatrix that effectively caches retrieved information and supports continuous\nupdates, overcomes the limitations of the input length. Additionally, after\npre-loading all candidate demonstrations into the compressive memory, the model\nfurther retrieves and filters relevant information from memory based on the\ninput query, bridging the gap between the retriever and the inference model.\nExtensive experiments show that our method achieves new state-of-the-art\nperformance on three public datasets (RAMS, WikiEvents, ACE05), significantly\noutperforming existing retrieval-based EAE methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent works have demonstrated the effectiveness of retrieval augmentation in\nthe Event Argument Extraction (EAE) task. However, existing retrieval-based EAE\nmethods have two main limitations: (1) input length constraints and (2) the gap\nbetween the retriever and the inference model. These issues limit the diversity\nand quality of the retrieved information. In this paper, we propose a\nCompressive Memory-based Retrieval (CMR) mechanism for EAE, which addresses the\ntwo limitations mentioned above. Our compressive memory, designed as a dynamic\nmatrix that effectively caches retrieved information and supports continuous\nupdates, overcomes the limitations of the input length. Additionally, after\npre-loading all candidate demonstrations into the compressive memory, the model\nfurther retrieves and filters relevant information from memory based on the\ninput query, bridging the gap between the retriever and the inference model.\nExtensive experiments show that our method achieves new state-of-the-art\nperformance on three public datasets (RAMS, WikiEvents, ACE05), significantly\noutperforming existing retrieval-based EAE methods."
                },
                "authors": [
                    {
                        "name": "Wanlong Liu"
                    },
                    {
                        "name": "Enqi Zhang"
                    },
                    {
                        "name": "Li Zhou"
                    },
                    {
                        "name": "Dingyi Zeng"
                    },
                    {
                        "name": "Shaohuan Cheng"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Malu Zhang"
                    },
                    {
                        "name": "Wenyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wenyu Chen"
                },
                "author": "Wenyu Chen",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09322v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09322v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09202v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09202v1",
                "updated": "2024-09-13T21:31:45Z",
                "updated_parsed": [
                    2024,
                    9,
                    13,
                    21,
                    31,
                    45,
                    4,
                    257,
                    0
                ],
                "published": "2024-09-13T21:31:45Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    21,
                    31,
                    45,
                    4,
                    257,
                    0
                ],
                "title": "WarmSwap: Sharing Dependencies for Accelerating Cold Starts in\n  Serverless Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WarmSwap: Sharing Dependencies for Accelerating Cold Starts in\n  Serverless Functions"
                },
                "summary": "This work presents WarmSwap, a novel provider-side cold-start optimization\nfor serverless computing. This optimization reduces cold-start time when\nbooting and loading dependencies at runtime inside a function container.\nPrevious approaches to the optimization of cold starts tend to fall into two\ncategories: optimizing the infrastructure of serverless computing to benefit\nall serverless functions; or function-specific tuning for individual serverless\nfunctions. In contrast, WarmSwap offers a broad middle ground, which optimizes\nentire categories of serverless functions. WarmSwap eliminates the need to\ninitialize middleware or software dependencies when launching a new serverless\ncontainer, by migrating a pre-initialized live dependency image to the new\nfunction instance. WarmSwap respects the provider's cache constraints, as a\nsingle pre-warmed dependency image in the cache is shared among all serverless\nfunctions requiring that software dependency image. WarmSwap has been tested on\nseven representative functions from FunctionBench. The functions are chosen to\ncompare with previous work. In those tests, WarmSwap accelerates cold-start\nexecutions for those serverless functions with large dependency requirements by\na factor ranging from 1.2 to 2.2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents WarmSwap, a novel provider-side cold-start optimization\nfor serverless computing. This optimization reduces cold-start time when\nbooting and loading dependencies at runtime inside a function container.\nPrevious approaches to the optimization of cold starts tend to fall into two\ncategories: optimizing the infrastructure of serverless computing to benefit\nall serverless functions; or function-specific tuning for individual serverless\nfunctions. In contrast, WarmSwap offers a broad middle ground, which optimizes\nentire categories of serverless functions. WarmSwap eliminates the need to\ninitialize middleware or software dependencies when launching a new serverless\ncontainer, by migrating a pre-initialized live dependency image to the new\nfunction instance. WarmSwap respects the provider's cache constraints, as a\nsingle pre-warmed dependency image in the cache is shared among all serverless\nfunctions requiring that software dependency image. WarmSwap has been tested on\nseven representative functions from FunctionBench. The functions are chosen to\ncompare with previous work. In those tests, WarmSwap accelerates cold-start\nexecutions for those serverless functions with large dependency requirements by\na factor ranging from 1.2 to 2.2."
                },
                "authors": [
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Devesh Tiwari"
                    },
                    {
                        "name": "Gene Cooperman"
                    }
                ],
                "author_detail": {
                    "name": "Gene Cooperman"
                },
                "author": "Gene Cooperman",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09202v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09202v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08141v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08141v1",
                "updated": "2024-09-12T15:34:23Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    34,
                    23,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T15:34:23Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    34,
                    23,
                    3,
                    256,
                    0
                ],
                "title": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects"
                },
                "summary": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should be based on Direct Memory\nAccess (DMA), descriptor rings, and interrupts: DMA offloads transfers from the\nCPU, descriptor rings provide buffering and queuing, and interrupts facilitate\nasynchronous interaction between cores and device with a lightweight\nnotification mechanism. In this paper we question this wisdom in the light of\nmodern hardware and workloads, particularly in cloud servers. We argue that the\nassumptions that led to this model are obsolete, and in many use-cases use of\nprogrammed I/O, where the CPU explicitly transfers data and control information\nto and from a device via loads and stores, actually results in a more efficient\nsystem. We quantitatively demonstrate these advantages using three use-cases:\nfine-grained RPC-style invocation of functions on an accelerator, offloading of\noperators in a streaming dataflow engine, and a network interface targeting for\nserverless functions. Moreover, we show that while these advantages are\nsignificant over a modern PCIe peripheral bus, a truly cache-coherent\ninterconnect offers significant additional efficiency gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should be based on Direct Memory\nAccess (DMA), descriptor rings, and interrupts: DMA offloads transfers from the\nCPU, descriptor rings provide buffering and queuing, and interrupts facilitate\nasynchronous interaction between cores and device with a lightweight\nnotification mechanism. In this paper we question this wisdom in the light of\nmodern hardware and workloads, particularly in cloud servers. We argue that the\nassumptions that led to this model are obsolete, and in many use-cases use of\nprogrammed I/O, where the CPU explicitly transfers data and control information\nto and from a device via loads and stores, actually results in a more efficient\nsystem. We quantitatively demonstrate these advantages using three use-cases:\nfine-grained RPC-style invocation of functions on an accelerator, offloading of\noperators in a streaming dataflow engine, and a network interface targeting for\nserverless functions. Moreover, we show that while these advantages are\nsignificant over a modern PCIe peripheral bus, a truly cache-coherent\ninterconnect offers significant additional efficiency gains."
                },
                "authors": [
                    {
                        "name": "Anastasiia Ruzhanskaia"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "David Cock"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08141v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08141v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.01699v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.01699v5",
                "updated": "2024-09-12T10:35:15Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    10,
                    35,
                    15,
                    3,
                    256,
                    0
                ],
                "published": "2023-03-03T04:03:28Z",
                "published_parsed": [
                    2023,
                    3,
                    3,
                    4,
                    3,
                    28,
                    4,
                    62,
                    0
                ],
                "title": "Light Induced Orbital Magnetism in Metals via Inverse Faraday Effect",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Light Induced Orbital Magnetism in Metals via Inverse Faraday Effect"
                },
                "summary": "We present a microscopic calculation of the inverse Faraday effect in metals.\nWe derive a static local magnetic moment induced on the application of\nhigh-frequency light, using the Eilenberger formulation of quasiclassical\ntheory. We include the effect of disorder and formulate a theory applicable\nacross the entire temperature range, in the absence of external applied fields.\nFor light-induced electric fields of amplitude $\\sim 100 kV/cm$, the induced\nfields are large, $\\sim 0.1 T$ for metallic Nb! The predictions of our theory\nagree with recent experimental and theoretical results [1]. An extension of\nthis approach to superconductors would open a new route of inducing orbital\nmagnetic field and potentially vortices in superconductors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a microscopic calculation of the inverse Faraday effect in metals.\nWe derive a static local magnetic moment induced on the application of\nhigh-frequency light, using the Eilenberger formulation of quasiclassical\ntheory. We include the effect of disorder and formulate a theory applicable\nacross the entire temperature range, in the absence of external applied fields.\nFor light-induced electric fields of amplitude $\\sim 100 kV/cm$, the induced\nfields are large, $\\sim 0.1 T$ for metallic Nb! The predictions of our theory\nagree with recent experimental and theoretical results [1]. An extension of\nthis approach to superconductors would open a new route of inducing orbital\nmagnetic field and potentially vortices in superconductors."
                },
                "authors": [
                    {
                        "name": "Priya Sharma"
                    },
                    {
                        "name": "Alexander V. Balatsky"
                    }
                ],
                "author_detail": {
                    "name": "Alexander V. Balatsky"
                },
                "author": "Alexander V. Balatsky",
                "arxiv_doi": "10.1103/PhysRevB.110.094302",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevB.110.094302",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2303.01699v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.01699v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Phys. Rev. B 110, 094302 (2024)",
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07704v1",
                "updated": "2024-09-12T02:13:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    12,
                    2,
                    13,
                    57,
                    3,
                    256,
                    0
                ],
                "published": "2024-09-12T02:13:57Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    2,
                    13,
                    57,
                    3,
                    256,
                    0
                ],
                "title": "Super Monotonic Alignment Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Super Monotonic Alignment Search"
                },
                "summary": "Monotonic alignment search (MAS), introduced by Glow-TTS, is one of the most\npopular algorithm in TTS to estimate unknown alignments between text and\nspeech. Since this algorithm needs to search for the most probable alignment\nwith dynamic programming by caching all paths, the time complexity of the\nalgorithm is $O(T \\times S)$. The authors of Glow-TTS run this algorithm on\nCPU, and while they mentioned it is difficult to parallelize, we found that MAS\ncan be parallelized in text-length dimension and CPU execution consumes an\ninordinate amount of time for inter-device copy. Therefore, we implemented a\nTriton kernel and PyTorch JIT script to accelerate MAS on GPU without\ninter-device copy. As a result, Super-MAS Triton kernel is up to 72 times\nfaster in the extreme-length case. The code is available at\n\\url{https://github.com/supertone-inc/super-monotonic-align}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monotonic alignment search (MAS), introduced by Glow-TTS, is one of the most\npopular algorithm in TTS to estimate unknown alignments between text and\nspeech. Since this algorithm needs to search for the most probable alignment\nwith dynamic programming by caching all paths, the time complexity of the\nalgorithm is $O(T \\times S)$. The authors of Glow-TTS run this algorithm on\nCPU, and while they mentioned it is difficult to parallelize, we found that MAS\ncan be parallelized in text-length dimension and CPU execution consumes an\ninordinate amount of time for inter-device copy. Therefore, we implemented a\nTriton kernel and PyTorch JIT script to accelerate MAS on GPU without\ninter-device copy. As a result, Super-MAS Triton kernel is up to 72 times\nfaster in the extreme-length case. The code is available at\n\\url{https://github.com/supertone-inc/super-monotonic-align}."
                },
                "authors": [
                    {
                        "name": "Junhyeok Lee"
                    },
                    {
                        "name": "Hyeongju Kim"
                    }
                ],
                "author_detail": {
                    "name": "Hyeongju Kim"
                },
                "author": "Hyeongju Kim",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07331v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07331v1",
                "updated": "2024-09-11T15:11:39Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    11,
                    39,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T15:11:39Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    11,
                    39,
                    2,
                    255,
                    0
                ],
                "title": "Learning to Compress Contexts for Efficient Knowledge-based Visual\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Compress Contexts for Efficient Knowledge-based Visual\n  Question Answering"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have demonstrated great zero-shot\nperformance on visual question answering (VQA). However, when it comes to\nknowledge-based VQA (KB-VQA), MLLMs may lack human commonsense or specialized\ndomain knowledge to answer such questions and require obtaining necessary\ninformation from external knowledge sources. Previous works like\nRetrival-Augmented VQA-v2 (RAVQA-v2) focus on utilizing as much input\ninformation, such as image-based textual descriptions and retrieved knowledge,\nas possible to improve performance, but they all overlook the issue that with\nthe number of input tokens increasing, inference efficiency significantly\ndecreases, which contradicts the demands of practical applications. To address\nthis issue, we propose Retrieval-Augmented MLLM with Compressed Contexts\n(RACC). RACC learns to compress and aggregate retrieved contexts, from which it\ngenerates a compact modulation in the form of Key-Value (KV) cache. This\nmodulation is then used to adapt the downstream frozen MLLM, thereby achieving\neffective and efficient inference. RACC achieves a state-of-the-art (SOTA)\nperformance of 62.9% on OK-VQA. Moreover, it significantly reduces inference\nlatency by 22.0%-59.7% compared to the prominent RAVQA-v2. Abundant experiments\nshow RACC's broad applicability. It is compatible with various off-the-shelf\nMLLMs and can also handle different knowledge sources including textual and\nmultimodal documents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have demonstrated great zero-shot\nperformance on visual question answering (VQA). However, when it comes to\nknowledge-based VQA (KB-VQA), MLLMs may lack human commonsense or specialized\ndomain knowledge to answer such questions and require obtaining necessary\ninformation from external knowledge sources. Previous works like\nRetrival-Augmented VQA-v2 (RAVQA-v2) focus on utilizing as much input\ninformation, such as image-based textual descriptions and retrieved knowledge,\nas possible to improve performance, but they all overlook the issue that with\nthe number of input tokens increasing, inference efficiency significantly\ndecreases, which contradicts the demands of practical applications. To address\nthis issue, we propose Retrieval-Augmented MLLM with Compressed Contexts\n(RACC). RACC learns to compress and aggregate retrieved contexts, from which it\ngenerates a compact modulation in the form of Key-Value (KV) cache. This\nmodulation is then used to adapt the downstream frozen MLLM, thereby achieving\neffective and efficient inference. RACC achieves a state-of-the-art (SOTA)\nperformance of 62.9% on OK-VQA. Moreover, it significantly reduces inference\nlatency by 22.0%-59.7% compared to the prominent RAVQA-v2. Abundant experiments\nshow RACC's broad applicability. It is compatible with various off-the-shelf\nMLLMs and can also handle different knowledge sources including textual and\nmultimodal documents."
                },
                "authors": [
                    {
                        "name": "Weixi Weng"
                    },
                    {
                        "name": "Jieming Zhu"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Xiaojun Meng"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Chun Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Chun Yuan"
                },
                "author": "Chun Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07331v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07331v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09086v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09086v1",
                "updated": "2024-09-11T12:44:12Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    12,
                    44,
                    12,
                    2,
                    255,
                    0
                ],
                "published": "2024-09-11T12:44:12Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    12,
                    44,
                    12,
                    2,
                    255,
                    0
                ],
                "title": "Inf-MLLM: Efficient Streaming Inference of Multimodal Large Language\n  Models on a Single GPU",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inf-MLLM: Efficient Streaming Inference of Multimodal Large Language\n  Models on a Single GPU"
                },
                "summary": "Multimodal Large Language Models (MLLMs) are distinguished by their\nmultimodal comprehensive ability and widely used in many real-world\napplications including GPT-4o, autonomous driving and robotics. Despite their\nimpressive performance, the multimodal inputs always incur long context. The\ninference under long context requires caching massive Key and Value states (KV\ncache) of previous tokens, which introduces high latency and excessive memory\nconsumption. Due to this reason, it is challenging to deploy streaming\ninference of MLLMs on edge devices, which largely constrains the power and\nusage of MLLMs in real-world applications. In this paper, we introduce\nInf-MLLM, an efficient inference framework for MLLMs, which enable streaming\ninference of MLLM on a single GPU with infinite context. Inf-MLLM is based on\nour key observation of the attention pattern in both LLMs and MLLMs called\n\"attention saddles\". Thanks to the newly discovered attention pattern, Inf-MLLM\nmaintains a size-constrained KV cache by dynamically caching recent tokens and\nrelevant tokens. Furthermore, Inf-MLLM proposes attention bias, a novel\napproach to enable MLLMs to capture long-term dependency. We show that Inf-MLLM\nenables multiple LLMs and MLLMs to achieve stable performance over 4M-token\nlong texts and multi-round conversations with 1-hour-long videos on a single\nGPU. In addition, Inf-MLLM exhibits superior streaming reasoning quality than\nexisting methods such as StreamingLLM and 2x speedup than H2O.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) are distinguished by their\nmultimodal comprehensive ability and widely used in many real-world\napplications including GPT-4o, autonomous driving and robotics. Despite their\nimpressive performance, the multimodal inputs always incur long context. The\ninference under long context requires caching massive Key and Value states (KV\ncache) of previous tokens, which introduces high latency and excessive memory\nconsumption. Due to this reason, it is challenging to deploy streaming\ninference of MLLMs on edge devices, which largely constrains the power and\nusage of MLLMs in real-world applications. In this paper, we introduce\nInf-MLLM, an efficient inference framework for MLLMs, which enable streaming\ninference of MLLM on a single GPU with infinite context. Inf-MLLM is based on\nour key observation of the attention pattern in both LLMs and MLLMs called\n\"attention saddles\". Thanks to the newly discovered attention pattern, Inf-MLLM\nmaintains a size-constrained KV cache by dynamically caching recent tokens and\nrelevant tokens. Furthermore, Inf-MLLM proposes attention bias, a novel\napproach to enable MLLMs to capture long-term dependency. We show that Inf-MLLM\nenables multiple LLMs and MLLMs to achieve stable performance over 4M-token\nlong texts and multi-round conversations with 1-hour-long videos on a single\nGPU. In addition, Inf-MLLM exhibits superior streaming reasoning quality than\nexisting methods such as StreamingLLM and 2x speedup than H2O."
                },
                "authors": [
                    {
                        "name": "Zhenyu Ning"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Qihao Jin"
                    },
                    {
                        "name": "Wenchao Ding"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09086v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09086v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10926v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10926v2",
                "updated": "2024-09-11T08:12:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    8,
                    12,
                    55,
                    2,
                    255,
                    0
                ],
                "published": "2024-07-15T17:25:42Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    17,
                    25,
                    42,
                    0,
                    197,
                    0
                ],
                "title": "In-Loop Filtering via Trained Look-Up Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Loop Filtering via Trained Look-Up Tables"
                },
                "summary": "In-loop filtering (ILF) is a key technology for removing the artifacts in\nimage/video coding standards. Recently, neural network-based in-loop filtering\nmethods achieve remarkable coding gains beyond the capability of advanced video\ncoding standards, which becomes a powerful coding tool candidate for future\nvideo coding standards. However, the utilization of deep neural networks brings\nheavy time and computational complexity, and high demands of high-performance\nhardware, which is challenging to apply to the general uses of coding scene. To\naddress this limitation, inspired by explorations in image restoration, we\npropose an efficient and practical in-loop filtering scheme by adopting the\nLook-up Table (LUT). We train the DNN of in-loop filtering within a fixed\nfiltering reference range, and cache the output values of the DNN into a LUT\nvia traversing all possible inputs. At testing time in the coding process, the\nfiltered pixel is generated by locating input pixels (to-be-filtered pixel with\nreference pixels) and interpolating cached filtered pixel values. To further\nenable the large filtering reference range with the limited storage cost of\nLUT, we introduce the enhanced indexing mechanism in the filtering process, and\nclipping/finetuning mechanism in the training. The proposed method is\nimplemented into the Versatile Video Coding (VVC) reference software, VTM-11.0.\nExperimental results show that the ultrafast, very fast, and fast mode of the\nproposed method achieves on average 0.13%/0.34%/0.51%, and 0.10%/0.27%/0.39%\nBD-rate reduction, under the all intra (AI) and random access (RA)\nconfigurations. Especially, our method has friendly time and computational\ncomplexity, only 101%/102%-104%/108% time increase with 0.13-0.93 kMACs/pixel,\nand only 164-1148 KB storage cost for a single model. Our solution may shed\nlight on the journey of practical neural network-based coding tool evolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-loop filtering (ILF) is a key technology for removing the artifacts in\nimage/video coding standards. Recently, neural network-based in-loop filtering\nmethods achieve remarkable coding gains beyond the capability of advanced video\ncoding standards, which becomes a powerful coding tool candidate for future\nvideo coding standards. However, the utilization of deep neural networks brings\nheavy time and computational complexity, and high demands of high-performance\nhardware, which is challenging to apply to the general uses of coding scene. To\naddress this limitation, inspired by explorations in image restoration, we\npropose an efficient and practical in-loop filtering scheme by adopting the\nLook-up Table (LUT). We train the DNN of in-loop filtering within a fixed\nfiltering reference range, and cache the output values of the DNN into a LUT\nvia traversing all possible inputs. At testing time in the coding process, the\nfiltered pixel is generated by locating input pixels (to-be-filtered pixel with\nreference pixels) and interpolating cached filtered pixel values. To further\nenable the large filtering reference range with the limited storage cost of\nLUT, we introduce the enhanced indexing mechanism in the filtering process, and\nclipping/finetuning mechanism in the training. The proposed method is\nimplemented into the Versatile Video Coding (VVC) reference software, VTM-11.0.\nExperimental results show that the ultrafast, very fast, and fast mode of the\nproposed method achieves on average 0.13%/0.34%/0.51%, and 0.10%/0.27%/0.39%\nBD-rate reduction, under the all intra (AI) and random access (RA)\nconfigurations. Especially, our method has friendly time and computational\ncomplexity, only 101%/102%-104%/108% time increase with 0.13-0.93 kMACs/pixel,\nand only 164-1148 KB storage cost for a single model. Our solution may shed\nlight on the journey of practical neural network-based coding tool evolution."
                },
                "authors": [
                    {
                        "name": "Zhuoyuan Li"
                    },
                    {
                        "name": "Jiacheng Li"
                    },
                    {
                        "name": "Yao Li"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Feng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Feng Wu"
                },
                "author": "Feng Wu",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10926v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10926v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2208.12453v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2208.12453v2",
                "updated": "2024-09-11T02:33:06Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    2,
                    33,
                    6,
                    2,
                    255,
                    0
                ],
                "published": "2022-08-26T06:28:08Z",
                "published_parsed": [
                    2022,
                    8,
                    26,
                    6,
                    28,
                    8,
                    4,
                    238,
                    0
                ],
                "title": "Exploiting Deep Reinforcement Learning for Edge Caching in Cell-Free\n  Massive MIMO Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploiting Deep Reinforcement Learning for Edge Caching in Cell-Free\n  Massive MIMO Systems"
                },
                "summary": "Cell-free massive multiple-input-multiple-output is promising to meet the\nstringent quality-of-experience (QoE) requirements of railway wireless\ncommunications by coordinating many successional access points (APs) to serve\nthe onboard users coherently. A key challenge is how to deliver the desired\ncontents timely due to the radical changing propagation environment caused by\nthe growing train speed. In this paper, we propose to proactively cache the\nlikely-requesting contents at the upcoming APs which perform the coherent\ntransmission to reduce end-to-end delay. A long-term QoE-maximization problem\nis formulated and two cache placement algorithms are proposed. One is based on\nheuristic convex optimization (HCO) and the other exploits deep reinforcement\nlearning (DRL) with soft actor-critic (SAC). Compared to the conventional\nbenchmark, numerical results show the advantage of our proposed algorithms on\nQoE and hit probability. With the advanced DRL model, SAC outperforms HCO on\nQoE by predicting the user requests accurately.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cell-free massive multiple-input-multiple-output is promising to meet the\nstringent quality-of-experience (QoE) requirements of railway wireless\ncommunications by coordinating many successional access points (APs) to serve\nthe onboard users coherently. A key challenge is how to deliver the desired\ncontents timely due to the radical changing propagation environment caused by\nthe growing train speed. In this paper, we propose to proactively cache the\nlikely-requesting contents at the upcoming APs which perform the coherent\ntransmission to reduce end-to-end delay. A long-term QoE-maximization problem\nis formulated and two cache placement algorithms are proposed. One is based on\nheuristic convex optimization (HCO) and the other exploits deep reinforcement\nlearning (DRL) with soft actor-critic (SAC). Compared to the conventional\nbenchmark, numerical results show the advantage of our proposed algorithms on\nQoE and hit probability. With the advanced DRL model, SAC outperforms HCO on\nQoE by predicting the user requests accurately."
                },
                "authors": [
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Shuaifei Chen"
                    },
                    {
                        "name": "Jiayi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiayi Zhang"
                },
                "author": "Jiayi Zhang",
                "arxiv_comment": "The focus of the research has shifted, and the current submission is\n  no longer aligned with our objectives",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2208.12453v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2208.12453v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.11504v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.11504v3",
                "updated": "2024-09-11T02:22:58Z",
                "updated_parsed": [
                    2024,
                    9,
                    11,
                    2,
                    22,
                    58,
                    2,
                    255,
                    0
                ],
                "published": "2024-01-21T14:28:41Z",
                "published_parsed": [
                    2024,
                    1,
                    21,
                    14,
                    28,
                    41,
                    6,
                    21,
                    0
                ],
                "title": "With Greater Text Comes Greater Necessity: Inference-Time Training Helps\n  Long Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With Greater Text Comes Greater Necessity: Inference-Time Training Helps\n  Long Text Generation"
                },
                "summary": "Long text generation, such as novel writing and discourse-level translation\nwith extremely long contexts, presents significant challenges to current\nlanguage models. Existing methods mainly focus on extending the model's context\nwindow through strategies like length extrapolation. However, these approaches\ndemand substantial hardware resources during the training and/or inference\nphases. Our proposed method, Temp-Lora, introduces an alternative concept.\nInstead of relying on the KV cache to store all context information, we embeds\nthis information directly into a temporary Lora module. In the process of long\ntext generation, this module is progressively trained with text generated\npreviously. This approach not only efficiently preserves contextual knowledge\nbut also prevents any permanent alteration to the model's parameters given that\nthe module is discarded post-generation. Extensive experiments on the PG19\nlanguage modeling benchmark and the GuoFeng discourse-level translation\nbenchmark validate the effectiveness of Temp-Lora. Our results show that: 1)\nTemp-Lora substantially enhances generation quality for long text, as indicated\nby a 13.2% decrease in perplexity (PPL) on a subset of PG19, and a 29.3%\ndecrease in PPL along with a 113.2% increase in BLEU score on a subset of\nGuoFeng, 2) Temp-Lora is compatible with and enhances most existing long text\ngeneration methods, and 3) Temp-Lora can greatly reduce computational costs by\nshortening the context window. For example, we can ensure a moderate\nimprovement in generation quality (a decrease of 3.8% in PPL) while enabling a\n51.5% memory usage reduction and a 60.0% decrease in latency for inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long text generation, such as novel writing and discourse-level translation\nwith extremely long contexts, presents significant challenges to current\nlanguage models. Existing methods mainly focus on extending the model's context\nwindow through strategies like length extrapolation. However, these approaches\ndemand substantial hardware resources during the training and/or inference\nphases. Our proposed method, Temp-Lora, introduces an alternative concept.\nInstead of relying on the KV cache to store all context information, we embeds\nthis information directly into a temporary Lora module. In the process of long\ntext generation, this module is progressively trained with text generated\npreviously. This approach not only efficiently preserves contextual knowledge\nbut also prevents any permanent alteration to the model's parameters given that\nthe module is discarded post-generation. Extensive experiments on the PG19\nlanguage modeling benchmark and the GuoFeng discourse-level translation\nbenchmark validate the effectiveness of Temp-Lora. Our results show that: 1)\nTemp-Lora substantially enhances generation quality for long text, as indicated\nby a 13.2% decrease in perplexity (PPL) on a subset of PG19, and a 29.3%\ndecrease in PPL along with a 113.2% increase in BLEU score on a subset of\nGuoFeng, 2) Temp-Lora is compatible with and enhances most existing long text\ngeneration methods, and 3) Temp-Lora can greatly reduce computational costs by\nshortening the context window. For example, we can ensure a moderate\nimprovement in generation quality (a decrease of 3.8% in PPL) while enabling a\n51.5% memory usage reduction and a 60.0% decrease in latency for inference."
                },
                "authors": [
                    {
                        "name": "Y. Wang"
                    },
                    {
                        "name": "D. Ma"
                    },
                    {
                        "name": "D. Cai"
                    }
                ],
                "author_detail": {
                    "name": "D. Cai"
                },
                "author": "D. Cai",
                "arxiv_comment": "COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.11504v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.11504v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06217v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06217v1",
                "updated": "2024-09-10T04:58:48Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    58,
                    48,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T04:58:48Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    58,
                    48,
                    1,
                    254,
                    0
                ],
                "title": "DACAT: Dual-stream Adaptive Clip-aware Time Modeling for Robust Online\n  Surgical Phase Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DACAT: Dual-stream Adaptive Clip-aware Time Modeling for Robust Online\n  Surgical Phase Recognition"
                },
                "summary": "Surgical phase recognition has become a crucial requirement in laparoscopic\nsurgery, enabling various clinical applications like surgical risk forecasting.\nCurrent methods typically identify the surgical phase using individual\nframe-wise embeddings as the fundamental unit for time modeling. However, this\napproach is overly sensitive to current observations, often resulting in\ndiscontinuous and erroneous predictions within a complete surgical phase. In\nthis paper, we propose DACAT, a novel dual-stream model that adaptively learns\nclip-aware context information to enhance the temporal relationship. In one\nstream, DACAT pretrains a frame encoder, caching all historical frame-wise\nfeatures. In the other stream, DACAT fine-tunes a new frame encoder to extract\nthe frame-wise feature at the current moment. Additionally, a max clip-response\nread-out (Max-R) module is introduced to bridge the two streams by using the\ncurrent frame-wise feature to adaptively fetch the most relevant past clip from\nthe feature cache. The clip-aware context feature is then encoded via\ncross-attention between the current frame and its fetched adaptive clip, and\nfurther utilized to enhance the time modeling for accurate online surgical\nphase recognition. The benchmark results on three public datasets, i.e.,\nCholec80, M2CAI16, and AutoLaparo, demonstrate the superiority of our proposed\nDACAT over existing state-of-the-art methods, with improvements in Jaccard\nscores of at least 4.5%, 4.6%, and 2.7%, respectively. Our code and models have\nbeen released at https://github.com/kk42yy/DACAT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surgical phase recognition has become a crucial requirement in laparoscopic\nsurgery, enabling various clinical applications like surgical risk forecasting.\nCurrent methods typically identify the surgical phase using individual\nframe-wise embeddings as the fundamental unit for time modeling. However, this\napproach is overly sensitive to current observations, often resulting in\ndiscontinuous and erroneous predictions within a complete surgical phase. In\nthis paper, we propose DACAT, a novel dual-stream model that adaptively learns\nclip-aware context information to enhance the temporal relationship. In one\nstream, DACAT pretrains a frame encoder, caching all historical frame-wise\nfeatures. In the other stream, DACAT fine-tunes a new frame encoder to extract\nthe frame-wise feature at the current moment. Additionally, a max clip-response\nread-out (Max-R) module is introduced to bridge the two streams by using the\ncurrent frame-wise feature to adaptively fetch the most relevant past clip from\nthe feature cache. The clip-aware context feature is then encoded via\ncross-attention between the current frame and its fetched adaptive clip, and\nfurther utilized to enhance the time modeling for accurate online surgical\nphase recognition. The benchmark results on three public datasets, i.e.,\nCholec80, M2CAI16, and AutoLaparo, demonstrate the superiority of our proposed\nDACAT over existing state-of-the-art methods, with improvements in Jaccard\nscores of at least 4.5%, 4.6%, and 2.7%, respectively. Our code and models have\nbeen released at https://github.com/kk42yy/DACAT."
                },
                "authors": [
                    {
                        "name": "Kaixiang Yang"
                    },
                    {
                        "name": "Qiang Li"
                    },
                    {
                        "name": "Zhiwei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhiwei Wang"
                },
                "author": "Zhiwei Wang",
                "arxiv_comment": "5 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06217v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06217v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06207v1",
                "updated": "2024-09-10T04:24:22Z",
                "updated_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    24,
                    22,
                    1,
                    254,
                    0
                ],
                "published": "2024-09-10T04:24:22Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    4,
                    24,
                    22,
                    1,
                    254,
                    0
                ],
                "title": "Design and Implementation of Online Live Streaming System Using A 3D\n  Engine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design and Implementation of Online Live Streaming System Using A 3D\n  Engine"
                },
                "summary": "With the growing demand for live video streaming, there is an increasing need\nfor low-latency and high-quality transmission, especially with the advent of 5G\nnetworks. While 5G offers hardware-level improvements, effective software\nsolutions for minimizing latency remain essential. Current methods, such as\nmulti-channel streaming, fail to address latency issues fundamentally, often\nonly adding new channels without optimizing overall performance. This thesis\nproposes a novel approach using a 3D engine (e.g., Unity 3D) to stream\nmulti-input video data through a single channel with reduced latency. By\nleveraging 3D engine capabilities, such as World/Screen Space Cameras, 3D\nCanvases, and Webcam Textures, the proposed system consolidates video streams\nfrom multiple external cameras into a unified, low-latency output. The\naffiliated project of this thesis demonstrates the implementation of a\nlow-latency multi-channel live video streaming system. It employs the RTSP\nprotocol and examines video encoding techniques, alongside a client-side\napplication based on Unity 3D. The system architecture includes a WebSocket\nserver for persistent connections, an HTTP server for communication, a MySQL\ndatabase for storage, Redis for caching, and Nginx for load balancing. Each\nmodule operates independently, ensuring flexibility and scalability in the\nsystem's design. A key innovation of this system is its use of a 3D scene to\nmap multiple video inputs onto a virtual canvas, recorded by an in-engine\ncamera for transmission. This design minimizes redundant data, enabling an\nefficient and director-guided live streaming network. The thesis concludes by\ndiscussing challenges encountered during the project and provides solutions for\nfuture improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing demand for live video streaming, there is an increasing need\nfor low-latency and high-quality transmission, especially with the advent of 5G\nnetworks. While 5G offers hardware-level improvements, effective software\nsolutions for minimizing latency remain essential. Current methods, such as\nmulti-channel streaming, fail to address latency issues fundamentally, often\nonly adding new channels without optimizing overall performance. This thesis\nproposes a novel approach using a 3D engine (e.g., Unity 3D) to stream\nmulti-input video data through a single channel with reduced latency. By\nleveraging 3D engine capabilities, such as World/Screen Space Cameras, 3D\nCanvases, and Webcam Textures, the proposed system consolidates video streams\nfrom multiple external cameras into a unified, low-latency output. The\naffiliated project of this thesis demonstrates the implementation of a\nlow-latency multi-channel live video streaming system. It employs the RTSP\nprotocol and examines video encoding techniques, alongside a client-side\napplication based on Unity 3D. The system architecture includes a WebSocket\nserver for persistent connections, an HTTP server for communication, a MySQL\ndatabase for storage, Redis for caching, and Nginx for load balancing. Each\nmodule operates independently, ensuring flexibility and scalability in the\nsystem's design. A key innovation of this system is its use of a 3D scene to\nmap multiple video inputs onto a virtual canvas, recorded by an in-engine\ncamera for transmission. This design minimizes redundant data, enabling an\nefficient and director-guided live streaming network. The thesis concludes by\ndiscussing challenges encountered during the project and provides solutions for\nfuture improvement."
                },
                "authors": [
                    {
                        "name": "Aizierjiang Aiersilan"
                    }
                ],
                "author_detail": {
                    "name": "Aizierjiang Aiersilan"
                },
                "author": "Aizierjiang Aiersilan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05867v1",
                "updated": "2024-09-09T17:59:57Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    59,
                    57,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-09T17:59:57Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    17,
                    59,
                    57,
                    0,
                    253,
                    0
                ],
                "title": "Flash Cache: Reducing Bias in Radiance Cache Based Inverse Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flash Cache: Reducing Bias in Radiance Cache Based Inverse Rendering"
                },
                "summary": "State-of-the-art techniques for 3D reconstruction are largely based on\nvolumetric scene representations, which require sampling multiple points to\ncompute the color arriving along a ray. Using these representations for more\ngeneral inverse rendering -- reconstructing geometry, materials, and lighting\nfrom observed images -- is challenging because recursively path-tracing such\nvolumetric representations is expensive. Recent works alleviate this issue\nthrough the use of radiance caches: data structures that store the\nsteady-state, infinite-bounce radiance arriving at any point from any\ndirection. However, these solutions rely on approximations that introduce bias\ninto the renderings and, more importantly, into the gradients used for\noptimization. We present a method that avoids these approximations while\nremaining computationally efficient. In particular, we leverage two techniques\nto reduce variance for unbiased estimators of the rendering equation: (1) an\nocclusion-aware importance sampler for incoming illumination and (2) a fast\ncache architecture that can be used as a control variate for the radiance from\na high-quality, but more expensive, volumetric cache. We show that by removing\nthese biases our approach improves the generality of radiance cache based\ninverse rendering, as well as increasing quality in the presence of challenging\nlight transport effects such as specular reflections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State-of-the-art techniques for 3D reconstruction are largely based on\nvolumetric scene representations, which require sampling multiple points to\ncompute the color arriving along a ray. Using these representations for more\ngeneral inverse rendering -- reconstructing geometry, materials, and lighting\nfrom observed images -- is challenging because recursively path-tracing such\nvolumetric representations is expensive. Recent works alleviate this issue\nthrough the use of radiance caches: data structures that store the\nsteady-state, infinite-bounce radiance arriving at any point from any\ndirection. However, these solutions rely on approximations that introduce bias\ninto the renderings and, more importantly, into the gradients used for\noptimization. We present a method that avoids these approximations while\nremaining computationally efficient. In particular, we leverage two techniques\nto reduce variance for unbiased estimators of the rendering equation: (1) an\nocclusion-aware importance sampler for incoming illumination and (2) a fast\ncache architecture that can be used as a control variate for the radiance from\na high-quality, but more expensive, volumetric cache. We show that by removing\nthese biases our approach improves the generality of radiance cache based\ninverse rendering, as well as increasing quality in the presence of challenging\nlight transport effects such as specular reflections."
                },
                "authors": [
                    {
                        "name": "Benjamin Attal"
                    },
                    {
                        "name": "Dor Verbin"
                    },
                    {
                        "name": "Ben Mildenhall"
                    },
                    {
                        "name": "Peter Hedman"
                    },
                    {
                        "name": "Jonathan T. Barron"
                    },
                    {
                        "name": "Matthew O'Toole"
                    },
                    {
                        "name": "Pratul P. Srinivasan"
                    }
                ],
                "author_detail": {
                    "name": "Pratul P. Srinivasan"
                },
                "author": "Pratul P. Srinivasan",
                "arxiv_comment": "Website: https://benattal.github.io/flash-cache/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03753v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03753v2",
                "updated": "2024-09-09T10:04:00Z",
                "updated_parsed": [
                    2024,
                    9,
                    9,
                    10,
                    4,
                    0,
                    0,
                    253,
                    0
                ],
                "published": "2024-09-05T17:59:15Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    59,
                    15,
                    3,
                    249,
                    0
                ],
                "title": "WildVis: Open Source Visualizer for Million-Scale Chat Logs in the Wild",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WildVis: Open Source Visualizer for Million-Scale Chat Logs in the Wild"
                },
                "summary": "The increasing availability of real-world conversation data offers exciting\nopportunities for researchers to study user-chatbot interactions. However, the\nsheer volume of this data makes manually examining individual conversations\nimpractical. To overcome this challenge, we introduce WildVis, an interactive\ntool that enables fast, versatile, and large-scale conversation analysis.\nWildVis provides search and visualization capabilities in the text and\nembedding spaces based on a list of criteria. To manage million-scale datasets,\nwe implemented optimizations including search index construction, embedding\nprecomputation and compression, and caching to ensure responsive user\ninteractions within seconds. We demonstrate WildVis' utility through three case\nstudies: facilitating chatbot misuse research, visualizing and comparing topic\ndistributions across datasets, and characterizing user-specific conversation\npatterns. WildVis is open-source and designed to be extendable, supporting\nadditional datasets and customized search and visualization functionalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing availability of real-world conversation data offers exciting\nopportunities for researchers to study user-chatbot interactions. However, the\nsheer volume of this data makes manually examining individual conversations\nimpractical. To overcome this challenge, we introduce WildVis, an interactive\ntool that enables fast, versatile, and large-scale conversation analysis.\nWildVis provides search and visualization capabilities in the text and\nembedding spaces based on a list of criteria. To manage million-scale datasets,\nwe implemented optimizations including search index construction, embedding\nprecomputation and compression, and caching to ensure responsive user\ninteractions within seconds. We demonstrate WildVis' utility through three case\nstudies: facilitating chatbot misuse research, visualizing and comparing topic\ndistributions across datasets, and characterizing user-specific conversation\npatterns. WildVis is open-source and designed to be extendable, supporting\nadditional datasets and customized search and visualization functionalities."
                },
                "authors": [
                    {
                        "name": "Yuntian Deng"
                    },
                    {
                        "name": "Wenting Zhao"
                    },
                    {
                        "name": "Jack Hessel"
                    },
                    {
                        "name": "Xiang Ren"
                    },
                    {
                        "name": "Claire Cardie"
                    },
                    {
                        "name": "Yejin Choi"
                    }
                ],
                "author_detail": {
                    "name": "Yejin Choi"
                },
                "author": "Yejin Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03753v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03753v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05025v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05025v1",
                "updated": "2024-09-08T08:39:50Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    8,
                    39,
                    50,
                    6,
                    252,
                    0
                ],
                "published": "2024-09-08T08:39:50Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    8,
                    39,
                    50,
                    6,
                    252,
                    0
                ],
                "title": "Cooperative Learning-Based Framework for VNF Caching and Placement\n  Optimization over Low Earth Orbit Satellite Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cooperative Learning-Based Framework for VNF Caching and Placement\n  Optimization over Low Earth Orbit Satellite Networks"
                },
                "summary": "Low Earth Orbit Satellite Networks (LSNs) are integral to supporting a broad\nrange of modern applications, which are typically modeled as Service Function\nChains (SFCs). Each SFC is composed of Virtual Network Functions (VNFs), where\neach VNF performs a specific task. In this work, we tackle two key challenges\nin deploying SFCs across an LSN. Firstly, we aim to optimize the long-term\nsystem performance by minimizing the average end-to-end SFC execution delay,\ngiven that each satellite comes with a pre-installed/cached subset of VNFs. To\nachieve optimal SFC placement, we formulate an offline Dynamic Programming (DP)\nequation. To overcome the challenges associated with DP, such as its\ncomplexity, the need for probability knowledge, and centralized\ndecision-making, we put forth an online Multi-Agent Q-Learning (MAQL) solution.\nOur MAQL approach addresses convergence issues in the non-stationary LSN\nenvironment by enabling satellites to share learning parameters and update\ntheir Q-tables based on distinct rules for their selected actions. Secondly, to\ndetermine the optimal VNF subsets for satellite caching, we develop a Bayesian\nOptimization (BO)-based learning mechanism that operates both offline and\ncontinuously in the background during runtime. Extensive experiments\ndemonstrate that our MAQL approach achieves near-optimal performance comparable\nto the DP model and significantly outperforms existing baselines. Moreover, the\nBO-based approach effectively enhances the request serving rate over time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low Earth Orbit Satellite Networks (LSNs) are integral to supporting a broad\nrange of modern applications, which are typically modeled as Service Function\nChains (SFCs). Each SFC is composed of Virtual Network Functions (VNFs), where\neach VNF performs a specific task. In this work, we tackle two key challenges\nin deploying SFCs across an LSN. Firstly, we aim to optimize the long-term\nsystem performance by minimizing the average end-to-end SFC execution delay,\ngiven that each satellite comes with a pre-installed/cached subset of VNFs. To\nachieve optimal SFC placement, we formulate an offline Dynamic Programming (DP)\nequation. To overcome the challenges associated with DP, such as its\ncomplexity, the need for probability knowledge, and centralized\ndecision-making, we put forth an online Multi-Agent Q-Learning (MAQL) solution.\nOur MAQL approach addresses convergence issues in the non-stationary LSN\nenvironment by enabling satellites to share learning parameters and update\ntheir Q-tables based on distinct rules for their selected actions. Secondly, to\ndetermine the optimal VNF subsets for satellite caching, we develop a Bayesian\nOptimization (BO)-based learning mechanism that operates both offline and\ncontinuously in the background during runtime. Extensive experiments\ndemonstrate that our MAQL approach achieves near-optimal performance comparable\nto the DP model and significantly outperforms existing baselines. Moreover, the\nBO-based approach effectively enhances the request serving rate over time."
                },
                "authors": [
                    {
                        "name": "Khai Doan"
                    },
                    {
                        "name": "Marios Avgeris"
                    },
                    {
                        "name": "Aris Leivadeas"
                    },
                    {
                        "name": "Ioannis Lambadaris"
                    },
                    {
                        "name": "Wonjae Shin"
                    }
                ],
                "author_detail": {
                    "name": "Wonjae Shin"
                },
                "author": "Wonjae Shin",
                "arxiv_comment": "40 pages, 11 figure, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05025v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05025v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04992v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04992v1",
                "updated": "2024-09-08T06:06:44Z",
                "updated_parsed": [
                    2024,
                    9,
                    8,
                    6,
                    6,
                    44,
                    6,
                    252,
                    0
                ],
                "published": "2024-09-08T06:06:44Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    6,
                    6,
                    44,
                    6,
                    252,
                    0
                ],
                "title": "InstInfer: In-Storage Attention Offloading for Cost-Effective\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstInfer: In-Storage Attention Offloading for Cost-Effective\n  Long-Context LLM Inference"
                },
                "summary": "The widespread of Large Language Models (LLMs) marks a significant milestone\nin generative AI. Nevertheless, the increasing context length and batch size in\noffline LLM inference escalate the memory requirement of the key-value (KV)\ncache, which imposes a huge burden on the GPU VRAM, especially for\nresource-constraint scenarios (e.g., edge computing and personal devices).\nSeveral cost-effective solutions leverage host memory or SSDs to reduce storage\ncosts for offline inference scenarios and improve the throughput. Nevertheless,\nthey suffer from significant performance penalties imposed by intensive KV\ncache accesses due to limited PCIe bandwidth. To address these issues, we\npropose InstInfer, a novel LLM inference system that offloads the most\nperformance-critical computation (i.e., attention in decoding phase) and data\n(i.e., KV cache) parts to Computational Storage Drives (CSDs), which minimize\nthe enormous KV transfer overheads. InstInfer designs a dedicated flash-aware\nin-storage attention engine with KV cache management mechanisms to exploit the\nhigh internal bandwidths of CSDs instead of being limited by the PCIe\nbandwidth. The optimized P2P transmission between GPU and CSDs further reduces\ndata migration overheads. Experimental results demonstrate that for a 13B model\nusing an NVIDIA A6000 GPU, InstInfer improves throughput for long-sequence\ninference by up to 11.1$\\times$, compared to existing SSD-based solutions such\nas FlexGen.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread of Large Language Models (LLMs) marks a significant milestone\nin generative AI. Nevertheless, the increasing context length and batch size in\noffline LLM inference escalate the memory requirement of the key-value (KV)\ncache, which imposes a huge burden on the GPU VRAM, especially for\nresource-constraint scenarios (e.g., edge computing and personal devices).\nSeveral cost-effective solutions leverage host memory or SSDs to reduce storage\ncosts for offline inference scenarios and improve the throughput. Nevertheless,\nthey suffer from significant performance penalties imposed by intensive KV\ncache accesses due to limited PCIe bandwidth. To address these issues, we\npropose InstInfer, a novel LLM inference system that offloads the most\nperformance-critical computation (i.e., attention in decoding phase) and data\n(i.e., KV cache) parts to Computational Storage Drives (CSDs), which minimize\nthe enormous KV transfer overheads. InstInfer designs a dedicated flash-aware\nin-storage attention engine with KV cache management mechanisms to exploit the\nhigh internal bandwidths of CSDs instead of being limited by the PCIe\nbandwidth. The optimized P2P transmission between GPU and CSDs further reduces\ndata migration overheads. Experimental results demonstrate that for a 13B model\nusing an NVIDIA A6000 GPU, InstInfer improves throughput for long-sequence\ninference by up to 11.1$\\times$, compared to existing SSD-based solutions such\nas FlexGen."
                },
                "authors": [
                    {
                        "name": "Xiurui Pan"
                    },
                    {
                        "name": "Endian Li"
                    },
                    {
                        "name": "Qiao Li"
                    },
                    {
                        "name": "Shengwen Liang"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Ke Zhou"
                    },
                    {
                        "name": "Yingwei Luo"
                    },
                    {
                        "name": "Xiaolin Wang"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "author": "Jie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04992v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04992v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04750v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04750v1",
                "updated": "2024-09-07T07:50:13Z",
                "updated_parsed": [
                    2024,
                    9,
                    7,
                    7,
                    50,
                    13,
                    5,
                    251,
                    0
                ],
                "published": "2024-09-07T07:50:13Z",
                "published_parsed": [
                    2024,
                    9,
                    7,
                    7,
                    50,
                    13,
                    5,
                    251,
                    0
                ],
                "title": "Training-Free Style Consistent Image Synthesis with Condition and Mask\n  Guidance in E-Commerce",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Style Consistent Image Synthesis with Condition and Mask\n  Guidance in E-Commerce"
                },
                "summary": "Generating style-consistent images is a common task in the e-commerce field,\nand current methods are largely based on diffusion models, which have achieved\nexcellent results. This paper introduces the concept of the QKV\n(query/key/value) level, referring to modifications in the attention maps\n(self-attention and cross-attention) when integrating UNet with image\nconditions. Without disrupting the product's main composition in e-commerce\nimages, we aim to use a train-free method guided by pre-set conditions. This\ninvolves using shared KV to enhance similarity in cross-attention and\ngenerating mask guidance from the attention map to cleverly direct the\ngeneration of style-consistent images. Our method has shown promising results\nin practical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating style-consistent images is a common task in the e-commerce field,\nand current methods are largely based on diffusion models, which have achieved\nexcellent results. This paper introduces the concept of the QKV\n(query/key/value) level, referring to modifications in the attention maps\n(self-attention and cross-attention) when integrating UNet with image\nconditions. Without disrupting the product's main composition in e-commerce\nimages, we aim to use a train-free method guided by pre-set conditions. This\ninvolves using shared KV to enhance similarity in cross-attention and\ngenerating mask guidance from the attention map to cleverly direct the\ngeneration of style-consistent images. Our method has shown promising results\nin practical applications."
                },
                "authors": [
                    {
                        "name": "Guandong Li"
                    }
                ],
                "author_detail": {
                    "name": "Guandong Li"
                },
                "author": "Guandong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04750v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14366v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14366v2",
                "updated": "2024-09-07T02:52:29Z",
                "updated_parsed": [
                    2024,
                    9,
                    7,
                    2,
                    52,
                    29,
                    5,
                    251,
                    0
                ],
                "published": "2024-05-23T09:43:52Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    9,
                    43,
                    52,
                    3,
                    144,
                    0
                ],
                "title": "MiniCache: KV Cache Compression in Depth Dimension for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiniCache: KV Cache Compression in Depth Dimension for Large Language\n  Models"
                },
                "summary": "A critical approach for efficiently deploying computationally demanding large\nlanguage models (LLMs) is Key-Value (KV) caching. The KV cache stores key-value\nstates of previously generated tokens, significantly reducing the need for\nrepetitive computations and thereby lowering latency in autoregressive\ngeneration. However, the size of the KV cache grows linearly with sequence\nlength, posing challenges for applications requiring long context input and\nextensive sequence generation. In this paper, we present a simple yet effective\napproach, called MiniCache, to compress the KV cache across layers from a novel\ndepth perspective, significantly reducing the memory footprint for LLM\ninference. Our approach is based on the observation that KV cache states\nexhibit high similarity between the adjacent layers in the middle-to-deep\nportion of LLMs. To facilitate merging, we propose disentangling the states\ninto the magnitude and direction components, interpolating the directions of\nthe state vectors while preserving their lengths unchanged. Furthermore, we\nintroduce a token retention strategy to keep highly distinct state pairs\nunmerged, thus preserving the information with minimal additional storage\noverhead. Our MiniCache is training-free and general, complementing existing KV\ncache compression strategies, such as quantization and sparsity. We conduct a\ncomprehensive evaluation of MiniCache utilizing various models including\nLLaMA-2, LLaMA-3, Phi-3, Mistral, and Mixtral across multiple benchmarks,\ndemonstrating its exceptional performance in achieving superior compression\nratios and high throughput. On the ShareGPT dataset, LLaMA-2-7B with 4-bit\nMiniCache achieves a remarkable compression ratio of up to 5.02x, enhances\ninference throughput by approximately 5x, and reduces the memory footprint by\n41% compared to the FP16 full cache baseline, all while maintaining\nnear-lossless performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A critical approach for efficiently deploying computationally demanding large\nlanguage models (LLMs) is Key-Value (KV) caching. The KV cache stores key-value\nstates of previously generated tokens, significantly reducing the need for\nrepetitive computations and thereby lowering latency in autoregressive\ngeneration. However, the size of the KV cache grows linearly with sequence\nlength, posing challenges for applications requiring long context input and\nextensive sequence generation. In this paper, we present a simple yet effective\napproach, called MiniCache, to compress the KV cache across layers from a novel\ndepth perspective, significantly reducing the memory footprint for LLM\ninference. Our approach is based on the observation that KV cache states\nexhibit high similarity between the adjacent layers in the middle-to-deep\nportion of LLMs. To facilitate merging, we propose disentangling the states\ninto the magnitude and direction components, interpolating the directions of\nthe state vectors while preserving their lengths unchanged. Furthermore, we\nintroduce a token retention strategy to keep highly distinct state pairs\nunmerged, thus preserving the information with minimal additional storage\noverhead. Our MiniCache is training-free and general, complementing existing KV\ncache compression strategies, such as quantization and sparsity. We conduct a\ncomprehensive evaluation of MiniCache utilizing various models including\nLLaMA-2, LLaMA-3, Phi-3, Mistral, and Mixtral across multiple benchmarks,\ndemonstrating its exceptional performance in achieving superior compression\nratios and high throughput. On the ShareGPT dataset, LLaMA-2-7B with 4-bit\nMiniCache achieves a remarkable compression ratio of up to 5.02x, enhances\ninference throughput by approximately 5x, and reduces the memory footprint by\n41% compared to the FP16 full cache baseline, all while maintaining\nnear-lossless performance."
                },
                "authors": [
                    {
                        "name": "Akide Liu"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Zizheng Pan"
                    },
                    {
                        "name": "Yefei He"
                    },
                    {
                        "name": "Gholamreza Haffari"
                    },
                    {
                        "name": "Bohan Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Bohan Zhuang"
                },
                "author": "Bohan Zhuang",
                "arxiv_comment": "Project is available at https://minicache.vmv.re",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14366v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14366v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03637v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03637v4",
                "updated": "2024-09-06T08:28:01Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    8,
                    28,
                    1,
                    4,
                    250,
                    0
                ],
                "published": "2024-07-04T05:13:58Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    5,
                    13,
                    58,
                    3,
                    186,
                    0
                ],
                "title": "QET: Enhancing Quantized LLM Parameters and KV cache Compression through\n  Element Substitution and Residual Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QET: Enhancing Quantized LLM Parameters and KV cache Compression through\n  Element Substitution and Residual Clustering"
                },
                "summary": "The matrix quantization entails representing matrix elements in a more\nspace-efficient form to reduce storage usage, with dequantization restoring the\noriginal matrix for use. We formulate the Quantization Error Minimization (QEM)\nproblem as minimizing the distance between a matrix before and after\nquantization, under the condition that the quantized matrix occupies the same\nmemory space. Matrix quantization is crucial in various applications, including\nLarge Language Models (LLMs) weight quantization, vector databases, KV cache\nquantization, graph compression, and image compression. Recent advancements in\nLLMs, such as GPT-4 and BERT, have highlighted the importance of matrix\ncompression due to the large size of parameters and KV cache, which are stored\nas matrices.\n  We propose Quantum Entanglement Trees (QET) to address the QEM problem by\nleveraging the local orderliness of matrix elements, involving iterative\nelement swapping to form a locally ordered matrix. This matrix is then grouped\nand quantized by columns. To enhance QET, we introduce two optimizations:\nfurther quantizing residuals to reduce MSE, and using masking and batch\nprocessing to accelerate the algorithm.\n  Experimental results demonstrate that QET can effectively reduce MSE to\n5.05%, 13.33%, and 11.89% of the current best method on the LLM dataset, K\ncache, and V cache, respectively. Our contributions include the abstraction of\nthe QEM problem, the design of the QET algorithm, and the proposal of two\noptimizations to improve accuracy and speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The matrix quantization entails representing matrix elements in a more\nspace-efficient form to reduce storage usage, with dequantization restoring the\noriginal matrix for use. We formulate the Quantization Error Minimization (QEM)\nproblem as minimizing the distance between a matrix before and after\nquantization, under the condition that the quantized matrix occupies the same\nmemory space. Matrix quantization is crucial in various applications, including\nLarge Language Models (LLMs) weight quantization, vector databases, KV cache\nquantization, graph compression, and image compression. Recent advancements in\nLLMs, such as GPT-4 and BERT, have highlighted the importance of matrix\ncompression due to the large size of parameters and KV cache, which are stored\nas matrices.\n  We propose Quantum Entanglement Trees (QET) to address the QEM problem by\nleveraging the local orderliness of matrix elements, involving iterative\nelement swapping to form a locally ordered matrix. This matrix is then grouped\nand quantized by columns. To enhance QET, we introduce two optimizations:\nfurther quantizing residuals to reduce MSE, and using masking and batch\nprocessing to accelerate the algorithm.\n  Experimental results demonstrate that QET can effectively reduce MSE to\n5.05%, 13.33%, and 11.89% of the current best method on the LLM dataset, K\ncache, and V cache, respectively. Our contributions include the abstraction of\nthe QEM problem, the design of the QET algorithm, and the proposal of two\noptimizations to improve accuracy and speed."
                },
                "authors": [
                    {
                        "name": "Yanshu Wang"
                    },
                    {
                        "name": "Wang Li"
                    },
                    {
                        "name": "Zhaoqian Yao"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03637v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03637v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04040v1",
                "updated": "2024-09-06T06:16:55Z",
                "updated_parsed": [
                    2024,
                    9,
                    6,
                    6,
                    16,
                    55,
                    4,
                    250,
                    0
                ],
                "published": "2024-09-06T06:16:55Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    6,
                    16,
                    55,
                    4,
                    250,
                    0
                ],
                "title": "A First Look At Efficient And Secure On-Device LLM Inference Against KV\n  Leakage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A First Look At Efficient And Secure On-Device LLM Inference Against KV\n  Leakage"
                },
                "summary": "Running LLMs on end devices has garnered significant attention recently due\nto their advantages in privacy preservation. With the advent of lightweight LLM\nmodels and specially designed GPUs, on-device LLM inference has achieved the\nnecessary accuracy and performance metrics. However, we have identified that\nLLM inference on GPUs can leak privacy-sensitive intermediate information,\nspecifically the KV pairs. An attacker could exploit these KV pairs to\nreconstruct the entire user conversation, leading to significant\nvulnerabilities. Existing solutions, such as Fully Homomorphic Encryption (FHE)\nand Trusted Execution Environments (TEE), are either too computation-intensive\nor resource-limited. To address these issues, we designed KV-Shield, which\noperates in two phases. In the initialization phase, it permutes the weight\nmatrices so that all KV pairs are correspondingly permuted. During the runtime\nphase, the attention vector is inversely permuted to ensure the correctness of\nthe layer output. All permutation-related operations are executed within the\nTEE, ensuring that insecure GPUs cannot access the original KV pairs, thus\npreventing conversation reconstruction. Finally, we theoretically analyze the\ncorrectness of KV-Shield, along with its advantages and overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Running LLMs on end devices has garnered significant attention recently due\nto their advantages in privacy preservation. With the advent of lightweight LLM\nmodels and specially designed GPUs, on-device LLM inference has achieved the\nnecessary accuracy and performance metrics. However, we have identified that\nLLM inference on GPUs can leak privacy-sensitive intermediate information,\nspecifically the KV pairs. An attacker could exploit these KV pairs to\nreconstruct the entire user conversation, leading to significant\nvulnerabilities. Existing solutions, such as Fully Homomorphic Encryption (FHE)\nand Trusted Execution Environments (TEE), are either too computation-intensive\nor resource-limited. To address these issues, we designed KV-Shield, which\noperates in two phases. In the initialization phase, it permutes the weight\nmatrices so that all KV pairs are correspondingly permuted. During the runtime\nphase, the attention vector is inversely permuted to ensure the correctness of\nthe layer output. All permutation-related operations are executed within the\nTEE, ensuring that insecure GPUs cannot access the original KV pairs, thus\npreventing conversation reconstruction. Finally, we theoretically analyze the\ncorrectness of KV-Shield, along with its advantages and overhead."
                },
                "authors": [
                    {
                        "name": "Huan Yang"
                    },
                    {
                        "name": "Deyu Zhang"
                    },
                    {
                        "name": "Yudong Zhao"
                    },
                    {
                        "name": "Yuanchun Li"
                    },
                    {
                        "name": "Yunxin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yunxin Liu"
                },
                "author": "Yunxin Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03308v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03308v2",
                "updated": "2024-09-05T20:21:54Z",
                "updated_parsed": [
                    2024,
                    9,
                    5,
                    20,
                    21,
                    54,
                    3,
                    249,
                    0
                ],
                "published": "2024-08-06T17:16:19Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    17,
                    16,
                    19,
                    1,
                    219,
                    0
                ],
                "title": "Potential and Limitation of High-Frequency Cores and Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Potential and Limitation of High-Frequency Cores and Caches"
                },
                "summary": "This paper explores the potential of cryogenic semiconductor computing and\nsuperconductor electronics as promising alternatives to traditional\nsemiconductor devices. As semiconductor devices face challenges such as\nincreased leakage currents and reduced performance at higher temperatures,\nthese novel technologies offer high performance and low power computation.\nConventional semiconductor electronics operating at cryogenic temperatures\n(below -150{\\deg}C or 123.15 K) can benefit from reduced leakage currents and\nimproved electron mobility. On the other hand, superconductor electronics,\noperating below 10 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconductor electronics and cryogenic semiconductor\ncomputing in gem5. We evaluate the performance of these components using\nworkloads representative of real-world applications like NPB, SPEC CPU2006, and\nGAPBS. Our results show the potential speedups achievable by these components\nand the limitations posed by cache bandwidth. This work provides valuable\ninsights into the performance implications and design trade-offs associated\nwith cryogenic and superconductor technologies, laying the foundation for\nfuture research in this field using gem5.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the potential of cryogenic semiconductor computing and\nsuperconductor electronics as promising alternatives to traditional\nsemiconductor devices. As semiconductor devices face challenges such as\nincreased leakage currents and reduced performance at higher temperatures,\nthese novel technologies offer high performance and low power computation.\nConventional semiconductor electronics operating at cryogenic temperatures\n(below -150{\\deg}C or 123.15 K) can benefit from reduced leakage currents and\nimproved electron mobility. On the other hand, superconductor electronics,\noperating below 10 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconductor electronics and cryogenic semiconductor\ncomputing in gem5. We evaluate the performance of these components using\nworkloads representative of real-world applications like NPB, SPEC CPU2006, and\nGAPBS. Our results show the potential speedups achievable by these components\nand the limitations posed by cache bandwidth. This work provides valuable\ninsights into the performance implications and design trade-offs associated\nwith cryogenic and superconductor technologies, laying the foundation for\nfuture research in this field using gem5."
                },
                "authors": [
                    {
                        "name": "Kunal Pai"
                    },
                    {
                        "name": "Anusheel Nand"
                    },
                    {
                        "name": "Jason Lowe-Power"
                    }
                ],
                "author_detail": {
                    "name": "Jason Lowe-Power"
                },
                "author": "Jason Lowe-Power",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03308v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03308v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2404.10917v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10917v2",
                "updated": "2024-10-03T17:59:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    59,
                    55,
                    3,
                    277,
                    0
                ],
                "published": "2024-04-16T21:33:05Z",
                "published_parsed": [
                    2024,
                    4,
                    16,
                    21,
                    33,
                    5,
                    1,
                    107,
                    0
                ],
                "title": "Which questions should I answer? Salience Prediction of Inquisitive\n  Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Which questions should I answer? Salience Prediction of Inquisitive\n  Questions"
                },
                "summary": "Inquisitive questions -- open-ended, curiosity-driven questions people ask as\nthey read -- are an integral part of discourse processing (Kehler and Rohde,\n2017; Onea, 2016) and comprehension (Prince, 2004). Recent work in NLP has\ntaken advantage of question generation capabilities of LLMs to enhance a wide\nrange of applications. But the space of inquisitive questions is vast: many\nquestions can be evoked from a given context. So which of those should be\nprioritized to find answers? Linguistic theories, unfortunately, have not yet\nprovided an answer to this question. This paper presents QSALIENCE, a salience\npredictor of inquisitive questions. QSALIENCE is instruction-tuned over our\ndataset of linguist-annotated salience scores of 1,766 (context, question)\npairs. A question scores high on salience if answering it would greatly enhance\nthe understanding of the text (Van Rooy, 2003). We show that highly salient\nquestions are empirically more likely to be answered in the same article,\nbridging potential questions (Onea, 2016) with Questions Under Discussion\n(Roberts, 2012). We further validate our findings by showing that answering\nsalient questions is an indicator of summarization quality in news.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inquisitive questions -- open-ended, curiosity-driven questions people ask as\nthey read -- are an integral part of discourse processing (Kehler and Rohde,\n2017; Onea, 2016) and comprehension (Prince, 2004). Recent work in NLP has\ntaken advantage of question generation capabilities of LLMs to enhance a wide\nrange of applications. But the space of inquisitive questions is vast: many\nquestions can be evoked from a given context. So which of those should be\nprioritized to find answers? Linguistic theories, unfortunately, have not yet\nprovided an answer to this question. This paper presents QSALIENCE, a salience\npredictor of inquisitive questions. QSALIENCE is instruction-tuned over our\ndataset of linguist-annotated salience scores of 1,766 (context, question)\npairs. A question scores high on salience if answering it would greatly enhance\nthe understanding of the text (Van Rooy, 2003). We show that highly salient\nquestions are empirically more likely to be answered in the same article,\nbridging potential questions (Onea, 2016) with Questions Under Discussion\n(Roberts, 2012). We further validate our findings by showing that answering\nsalient questions is an indicator of summarization quality in news."
                },
                "authors": [
                    {
                        "name": "Yating Wu"
                    },
                    {
                        "name": "Ritika Mangla"
                    },
                    {
                        "name": "Alexandros G. Dimakis"
                    },
                    {
                        "name": "Greg Durrett"
                    },
                    {
                        "name": "Junyi Jessy Li"
                    }
                ],
                "author_detail": {
                    "name": "Junyi Jessy Li"
                },
                "author": "Junyi Jessy Li",
                "arxiv_comment": "Camera Ready for EMNLP 2024 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.10917v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10917v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02757v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02757v1",
                "updated": "2024-10-03T17:59:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    59,
                    2,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T17:59:02Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    59,
                    2,
                    3,
                    277,
                    0
                ],
                "title": "Loong: Generating Minute-level Long Videos with Autoregressive Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Loong: Generating Minute-level Long Videos with Autoregressive Language\n  Models"
                },
                "summary": "It is desirable but challenging to generate content-rich long videos in the\nscale of minutes. Autoregressive large language models (LLMs) have achieved\ngreat success in generating coherent and long sequences of tokens in the domain\nof natural language processing, while the exploration of autoregressive LLMs\nfor video generation is limited to generating short videos of several seconds.\nIn this work, we conduct a deep analysis of the challenges that prevent\nautoregressive LLM-based video generators from generating long videos. Based on\nthe observations and analysis, we propose Loong, a new autoregressive LLM-based\nvideo generator that can generate minute-long videos. Specifically, we model\nthe text tokens and video tokens as a unified sequence for autoregressive LLMs\nand train the model from scratch. We propose progressive short-to-long training\nwith a loss re-weighting scheme to mitigate the loss imbalance problem for long\nvideo training. We further investigate inference strategies, including video\ntoken re-encoding and sampling strategies, to diminish error accumulation\nduring inference. Our proposed Loong can be trained on 10-second videos and be\nextended to generate minute-level long videos conditioned on text prompts, as\ndemonstrated by the results. More samples are available at:\nhttps://epiphqny.github.io/Loong-video.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is desirable but challenging to generate content-rich long videos in the\nscale of minutes. Autoregressive large language models (LLMs) have achieved\ngreat success in generating coherent and long sequences of tokens in the domain\nof natural language processing, while the exploration of autoregressive LLMs\nfor video generation is limited to generating short videos of several seconds.\nIn this work, we conduct a deep analysis of the challenges that prevent\nautoregressive LLM-based video generators from generating long videos. Based on\nthe observations and analysis, we propose Loong, a new autoregressive LLM-based\nvideo generator that can generate minute-long videos. Specifically, we model\nthe text tokens and video tokens as a unified sequence for autoregressive LLMs\nand train the model from scratch. We propose progressive short-to-long training\nwith a loss re-weighting scheme to mitigate the loss imbalance problem for long\nvideo training. We further investigate inference strategies, including video\ntoken re-encoding and sampling strategies, to diminish error accumulation\nduring inference. Our proposed Loong can be trained on 10-second videos and be\nextended to generate minute-level long videos conditioned on text prompts, as\ndemonstrated by the results. More samples are available at:\nhttps://epiphqny.github.io/Loong-video."
                },
                "authors": [
                    {
                        "name": "Yuqing Wang"
                    },
                    {
                        "name": "Tianwei Xiong"
                    },
                    {
                        "name": "Daquan Zhou"
                    },
                    {
                        "name": "Zhijie Lin"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Bingyi Kang"
                    },
                    {
                        "name": "Jiashi Feng"
                    },
                    {
                        "name": "Xihui Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xihui Liu"
                },
                "author": "Xihui Liu",
                "arxiv_comment": "Project page: https://epiphqny.github.io/Loong-video/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02757v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02757v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02755v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02755v1",
                "updated": "2024-10-03T17:58:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    58,
                    29,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T17:58:29Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    58,
                    29,
                    3,
                    277,
                    0
                ],
                "title": "SIEVE: General Purpose Data Filtering System Matching GPT-4o Accuracy at\n  1% the Cost",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SIEVE: General Purpose Data Filtering System Matching GPT-4o Accuracy at\n  1% the Cost"
                },
                "summary": "Creating specialized large language models requires vast amounts of clean,\nspecial purpose data for training and fine-tuning. With only a handful of\nexisting large-scale, domain-specific datasets, creation of new datasets is\nrequired in most applications. This requires the development of new\napplication-specific filtering of web-scale data. Filtering with a\nhigh-performance, general-purpose LLM such as GPT-4o can be highly effective,\nbut this is extremely expensive at web-scale. This paper proposes SIEVE, a\nlightweight alternative that matches GPT-4o accuracy at a fraction of the cost.\nSIEVE can perform up to 500 filtering operations for the cost of one GPT-4o\nfiltering call. The key to SIEVE is a seamless integration of GPT-4o and\nlightweight T5 models, using active learning to fine-tune T5 in the background\nwith a small number of calls to GPT-4o. Once trained, it performs as well as\nGPT-4o at a tiny fraction of the cost. We experimentally validate SIEVE on the\nOpenWebText dataset, using five highly customized filter tasks targeting high\nquality and domain-specific content. Our results demonstrate the effectiveness\nand efficiency of our method in curating large, high-quality datasets for\nlanguage model training at a substantially lower cost (1%) than existing\ntechniques. To further validate SIEVE, experiments show that SIEVE and GPT-4o\nachieve similar accuracy, with human evaluators preferring SIEVE's filtering\nresults to those of GPT-4o.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creating specialized large language models requires vast amounts of clean,\nspecial purpose data for training and fine-tuning. With only a handful of\nexisting large-scale, domain-specific datasets, creation of new datasets is\nrequired in most applications. This requires the development of new\napplication-specific filtering of web-scale data. Filtering with a\nhigh-performance, general-purpose LLM such as GPT-4o can be highly effective,\nbut this is extremely expensive at web-scale. This paper proposes SIEVE, a\nlightweight alternative that matches GPT-4o accuracy at a fraction of the cost.\nSIEVE can perform up to 500 filtering operations for the cost of one GPT-4o\nfiltering call. The key to SIEVE is a seamless integration of GPT-4o and\nlightweight T5 models, using active learning to fine-tune T5 in the background\nwith a small number of calls to GPT-4o. Once trained, it performs as well as\nGPT-4o at a tiny fraction of the cost. We experimentally validate SIEVE on the\nOpenWebText dataset, using five highly customized filter tasks targeting high\nquality and domain-specific content. Our results demonstrate the effectiveness\nand efficiency of our method in curating large, high-quality datasets for\nlanguage model training at a substantially lower cost (1%) than existing\ntechniques. To further validate SIEVE, experiments show that SIEVE and GPT-4o\nachieve similar accuracy, with human evaluators preferring SIEVE's filtering\nresults to those of GPT-4o."
                },
                "authors": [
                    {
                        "name": "Jifan Zhang"
                    },
                    {
                        "name": "Robert Nowak"
                    }
                ],
                "author_detail": {
                    "name": "Robert Nowak"
                },
                "author": "Robert Nowak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02755v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02755v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.08695v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.08695v3",
                "updated": "2024-10-03T17:58:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    58,
                    3,
                    3,
                    277,
                    0
                ],
                "published": "2023-07-17T17:57:01Z",
                "published_parsed": [
                    2023,
                    7,
                    17,
                    17,
                    57,
                    1,
                    0,
                    198,
                    0
                ],
                "title": "NVDS+: Towards Efficient and Versatile Neural Stabilizer for Video Depth\n  Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NVDS+: Towards Efficient and Versatile Neural Stabilizer for Video Depth\n  Estimation"
                },
                "summary": "Video depth estimation aims to infer temporally consistent depth. One\napproach is to finetune a single-image model on each video with geometry\nconstraints, which proves inefficient and lacks robustness. An alternative is\nlearning to enforce consistency from data, which requires well-designed models\nand sufficient video depth data. To address both challenges, we introduce NVDS+\nthat stabilizes inconsistent depth estimated by various single-image models in\na plug-and-play manner. We also elaborate a large-scale Video Depth in the Wild\n(VDW) dataset, which contains 14,203 videos with over two million frames,\nmaking it the largest natural-scene video depth dataset. Additionally, a\nbidirectional inference strategy is designed to improve consistency by\nadaptively fusing forward and backward predictions. We instantiate a model\nfamily ranging from small to large scales for different applications. The\nmethod is evaluated on VDW dataset and three public benchmarks. To further\nprove the versatility, we extend NVDS+ to video semantic segmentation and\nseveral downstream applications like bokeh rendering, novel view synthesis, and\n3D reconstruction. Experimental results show that our method achieves\nsignificant improvements in consistency, accuracy, and efficiency. Our work\nserves as a solid baseline and data foundation for learning-based video depth\nestimation. Code and dataset are available at:\nhttps://github.com/RaymondWang987/NVDS",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video depth estimation aims to infer temporally consistent depth. One\napproach is to finetune a single-image model on each video with geometry\nconstraints, which proves inefficient and lacks robustness. An alternative is\nlearning to enforce consistency from data, which requires well-designed models\nand sufficient video depth data. To address both challenges, we introduce NVDS+\nthat stabilizes inconsistent depth estimated by various single-image models in\na plug-and-play manner. We also elaborate a large-scale Video Depth in the Wild\n(VDW) dataset, which contains 14,203 videos with over two million frames,\nmaking it the largest natural-scene video depth dataset. Additionally, a\nbidirectional inference strategy is designed to improve consistency by\nadaptively fusing forward and backward predictions. We instantiate a model\nfamily ranging from small to large scales for different applications. The\nmethod is evaluated on VDW dataset and three public benchmarks. To further\nprove the versatility, we extend NVDS+ to video semantic segmentation and\nseveral downstream applications like bokeh rendering, novel view synthesis, and\n3D reconstruction. Experimental results show that our method achieves\nsignificant improvements in consistency, accuracy, and efficiency. Our work\nserves as a solid baseline and data foundation for learning-based video depth\nestimation. Code and dataset are available at:\nhttps://github.com/RaymondWang987/NVDS"
                },
                "authors": [
                    {
                        "name": "Yiran Wang"
                    },
                    {
                        "name": "Min Shi"
                    },
                    {
                        "name": "Jiaqi Li"
                    },
                    {
                        "name": "Chaoyi Hong"
                    },
                    {
                        "name": "Zihao Huang"
                    },
                    {
                        "name": "Juewen Peng"
                    },
                    {
                        "name": "Zhiguo Cao"
                    },
                    {
                        "name": "Jianming Zhang"
                    },
                    {
                        "name": "Ke Xian"
                    },
                    {
                        "name": "Guosheng Lin"
                    }
                ],
                "author_detail": {
                    "name": "Guosheng Lin"
                },
                "author": "Guosheng Lin",
                "arxiv_comment": "V1/V2: ICCV 2023 accepted; V3: the journal extension accepted by IEEE\n  TPAMI 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2307.08695v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.08695v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02749v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02749v1",
                "updated": "2024-10-03T17:57:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    57,
                    22,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T17:57:22Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    57,
                    22,
                    3,
                    277,
                    0
                ],
                "title": "Training Language Models on Synthetic Edit Sequences Improves Code\n  Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Language Models on Synthetic Edit Sequences Improves Code\n  Synthesis"
                },
                "summary": "Software engineers mainly write code by editing existing programs. In\ncontrast, large language models (LLMs) autoregressively synthesize programs in\na single pass. One explanation for this is the scarcity of open-sourced edit\ndata. While high-quality instruction data for code synthesis is already scarce,\nhigh-quality edit data is even scarcer. To fill this gap, we develop a\nsynthetic data generation algorithm called LintSeq. This algorithm refactors\nexisting code into a sequence of code edits by using a linter to procedurally\nsample across the error-free insertions that can be used to sequentially write\nprograms. It outputs edit sequences as text strings consisting of consecutive\nprogram diffs. To test LintSeq, we use it to refactor a dataset of instruction\n+ program pairs into instruction + program-diff-sequence tuples. Then, we\ninstruction finetune a series of smaller LLMs ranging from 2.6B to 14B\nparameters on both the re-factored and original versions of this dataset,\ncomparing zero-shot performance on code synthesis benchmarks. We show that\nduring repeated sampling, edit sequence finetuned models produce more diverse\nprograms than baselines. This results in better inference-time scaling for\nbenchmark coverage as a function of samples, i.e. the fraction of problems\n\"pass@k\" solved by any attempt given \"k\" tries. For example, on HumanEval\npass@50, small LLMs finetuned on synthetic edit sequences are competitive with\nGPT-4 and outperform models finetuned on the baseline dataset by +20% (+/-3%)\nin absolute score. Finally, we also pretrain our own tiny LMs for code\nunderstanding. We show that finetuning tiny models on synthetic code edits\nresults in state-of-the-art code synthesis for the on-device model class. Our\n150M parameter edit sequence LM matches or outperforms code models with twice\nas many parameters, both with and without repeated sampling, including Codex\nand AlphaCode.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software engineers mainly write code by editing existing programs. In\ncontrast, large language models (LLMs) autoregressively synthesize programs in\na single pass. One explanation for this is the scarcity of open-sourced edit\ndata. While high-quality instruction data for code synthesis is already scarce,\nhigh-quality edit data is even scarcer. To fill this gap, we develop a\nsynthetic data generation algorithm called LintSeq. This algorithm refactors\nexisting code into a sequence of code edits by using a linter to procedurally\nsample across the error-free insertions that can be used to sequentially write\nprograms. It outputs edit sequences as text strings consisting of consecutive\nprogram diffs. To test LintSeq, we use it to refactor a dataset of instruction\n+ program pairs into instruction + program-diff-sequence tuples. Then, we\ninstruction finetune a series of smaller LLMs ranging from 2.6B to 14B\nparameters on both the re-factored and original versions of this dataset,\ncomparing zero-shot performance on code synthesis benchmarks. We show that\nduring repeated sampling, edit sequence finetuned models produce more diverse\nprograms than baselines. This results in better inference-time scaling for\nbenchmark coverage as a function of samples, i.e. the fraction of problems\n\"pass@k\" solved by any attempt given \"k\" tries. For example, on HumanEval\npass@50, small LLMs finetuned on synthetic edit sequences are competitive with\nGPT-4 and outperform models finetuned on the baseline dataset by +20% (+/-3%)\nin absolute score. Finally, we also pretrain our own tiny LMs for code\nunderstanding. We show that finetuning tiny models on synthetic code edits\nresults in state-of-the-art code synthesis for the on-device model class. Our\n150M parameter edit sequence LM matches or outperforms code models with twice\nas many parameters, both with and without repeated sampling, including Codex\nand AlphaCode."
                },
                "authors": [
                    {
                        "name": "Ulyana Piterbarg"
                    },
                    {
                        "name": "Lerrel Pinto"
                    },
                    {
                        "name": "Rob Fergus"
                    }
                ],
                "author_detail": {
                    "name": "Rob Fergus"
                },
                "author": "Rob Fergus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02749v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02749v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18957v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18957v2",
                "updated": "2024-10-03T17:57:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    57,
                    7,
                    3,
                    277,
                    0
                ],
                "published": "2024-09-27T17:58:50Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    17,
                    58,
                    50,
                    4,
                    271,
                    0
                ],
                "title": "LML-DAP: Language Model Learning a Dataset for Data-Augmented Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LML-DAP: Language Model Learning a Dataset for Data-Augmented Prediction"
                },
                "summary": "Classification tasks are typically handled using Machine Learning (ML)\nmodels, which lack a balance between accuracy and interpretability. This paper\nintroduces a new approach to using Large Language Models (LLMs) for\nclassification tasks in an explainable way. Unlike ML models that rely heavily\non data cleaning and feature engineering, this method streamlines the process\nusing LLMs. This paper proposes a new concept called \"Language Model Learning\n(LML)\" powered by a new method called \"Data-Augmented Prediction (DAP)\". The\nclassification is performed by LLMs using a method similar to humans manually\nexploring and understanding the data and deciding classifications using data as\na reference. In the LML process, a dataset is summarized and evaluated to\ndetermine the features that lead to the classification of each label the most.\nIn the process of DAP, the system uses the data summary and a row of the\ntesting dataset to automatically generate a query, which is used to retrieve\nrelevant rows from the dataset. A classification is generated by the LLM using\ndata summary and relevant rows, ensuring satisfactory accuracy even with\ncomplex data using context-aware decision-making. LML and DAP unlock the\npossibilities of new applications. The proposed method uses the words \"Act as\nan Explainable Machine Learning Model\" in the prompt to enhance the\ninterpretability of the predictions by allowing users to review the logic\nbehind each prediction. In some test cases, the system scored an accuracy above\n90%, proving the effectiveness of the system and its potential to outperform\nconventional ML models in various scenarios. The code is available at\nhttps://github.com/Pro-GenAI/LML-DAP",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Classification tasks are typically handled using Machine Learning (ML)\nmodels, which lack a balance between accuracy and interpretability. This paper\nintroduces a new approach to using Large Language Models (LLMs) for\nclassification tasks in an explainable way. Unlike ML models that rely heavily\non data cleaning and feature engineering, this method streamlines the process\nusing LLMs. This paper proposes a new concept called \"Language Model Learning\n(LML)\" powered by a new method called \"Data-Augmented Prediction (DAP)\". The\nclassification is performed by LLMs using a method similar to humans manually\nexploring and understanding the data and deciding classifications using data as\na reference. In the LML process, a dataset is summarized and evaluated to\ndetermine the features that lead to the classification of each label the most.\nIn the process of DAP, the system uses the data summary and a row of the\ntesting dataset to automatically generate a query, which is used to retrieve\nrelevant rows from the dataset. A classification is generated by the LLM using\ndata summary and relevant rows, ensuring satisfactory accuracy even with\ncomplex data using context-aware decision-making. LML and DAP unlock the\npossibilities of new applications. The proposed method uses the words \"Act as\nan Explainable Machine Learning Model\" in the prompt to enhance the\ninterpretability of the predictions by allowing users to review the logic\nbehind each prediction. In some test cases, the system scored an accuracy above\n90%, proving the effectiveness of the system and its potential to outperform\nconventional ML models in various scenarios. The code is available at\nhttps://github.com/Pro-GenAI/LML-DAP"
                },
                "authors": [
                    {
                        "name": "Praneeth Vadlapati"
                    }
                ],
                "author_detail": {
                    "name": "Praneeth Vadlapati"
                },
                "author": "Praneeth Vadlapati",
                "arxiv_comment": "Updated title, abstract, and images",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18957v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18957v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02748v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02748v1",
                "updated": "2024-10-03T17:57:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    57,
                    1,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T17:57:01Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    57,
                    1,
                    3,
                    277,
                    0
                ],
                "title": "CriSPO: Multi-Aspect Critique-Suggestion-guided Automatic Prompt\n  Optimization for Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CriSPO: Multi-Aspect Critique-Suggestion-guided Automatic Prompt\n  Optimization for Text Generation"
                },
                "summary": "Large language models (LLMs) can generate fluent summaries across domains\nusing prompting techniques, reducing the need to train models for summarization\napplications. However, crafting effective prompts that guide LLMs to generate\nsummaries with the appropriate level of detail and writing style remains a\nchallenge. In this paper, we explore the use of salient information extracted\nfrom the source document to enhance summarization prompts. We show that adding\nkeyphrases in prompts can improve ROUGE F1 and recall, making the generated\nsummaries more similar to the reference and more complete. The number of\nkeyphrases can control the precision-recall trade-off. Furthermore, our\nanalysis reveals that incorporating phrase-level salient information is\nsuperior to word- or sentence-level. However, the impact on hallucination is\nnot universally positive across LLMs. To conduct this analysis, we introduce\nKeyphrase Signal Extractor (CriSPO), a lightweight model that can be finetuned\nto extract salient keyphrases. By using CriSPO, we achieve consistent ROUGE\nimprovements across datasets and open-weight and proprietary LLMs without any\nLLM customization. Our findings provide insights into leveraging salient\ninformation in building prompt-based summarization systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) can generate fluent summaries across domains\nusing prompting techniques, reducing the need to train models for summarization\napplications. However, crafting effective prompts that guide LLMs to generate\nsummaries with the appropriate level of detail and writing style remains a\nchallenge. In this paper, we explore the use of salient information extracted\nfrom the source document to enhance summarization prompts. We show that adding\nkeyphrases in prompts can improve ROUGE F1 and recall, making the generated\nsummaries more similar to the reference and more complete. The number of\nkeyphrases can control the precision-recall trade-off. Furthermore, our\nanalysis reveals that incorporating phrase-level salient information is\nsuperior to word- or sentence-level. However, the impact on hallucination is\nnot universally positive across LLMs. To conduct this analysis, we introduce\nKeyphrase Signal Extractor (CriSPO), a lightweight model that can be finetuned\nto extract salient keyphrases. By using CriSPO, we achieve consistent ROUGE\nimprovements across datasets and open-weight and proprietary LLMs without any\nLLM customization. Our findings provide insights into leveraging salient\ninformation in building prompt-based summarization systems."
                },
                "authors": [
                    {
                        "name": "Han He"
                    },
                    {
                        "name": "Qianchu Liu"
                    },
                    {
                        "name": "Lei Xu"
                    },
                    {
                        "name": "Chaitanya Shivade"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Sundararajan Srinivasan"
                    },
                    {
                        "name": "Katrin Kirchhoff"
                    }
                ],
                "author_detail": {
                    "name": "Katrin Kirchhoff"
                },
                "author": "Katrin Kirchhoff",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02748v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02748v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11687v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11687v2",
                "updated": "2024-10-03T17:56:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    56,
                    34,
                    3,
                    277,
                    0
                ],
                "published": "2024-06-17T16:05:32Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    16,
                    5,
                    32,
                    0,
                    169,
                    0
                ],
                "title": "Tokenization Falling Short: The Curse of Tokenization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokenization Falling Short: The Curse of Tokenization"
                },
                "summary": "Language models typically tokenize raw text into sequences of subword\nidentifiers from a predefined vocabulary, a process inherently sensitive to\ntypographical errors, length variations, and largely oblivious to the internal\nstructure of tokens--issues we term the curse of tokenization. In this study,\nwe delve into these drawbacks and demonstrate that large language models (LLMs)\nremain susceptible to these problems. This study systematically investigates\nthese challenges and their impact on LLMs through three critical research\nquestions: (1) complex problem solving, (2) token structure probing, and (3)\nresilience to typographical variation. Our findings reveal that scaling model\nparameters can mitigate the issue of tokenization; however, LLMs still suffer\nfrom biases induced by typos and other text format variations. Our experiments\nshow that subword regularization such as BPE-dropout can mitigate this issue.\nWe release our evaluation code and data at https://github.com/FloatAI/TKEval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models typically tokenize raw text into sequences of subword\nidentifiers from a predefined vocabulary, a process inherently sensitive to\ntypographical errors, length variations, and largely oblivious to the internal\nstructure of tokens--issues we term the curse of tokenization. In this study,\nwe delve into these drawbacks and demonstrate that large language models (LLMs)\nremain susceptible to these problems. This study systematically investigates\nthese challenges and their impact on LLMs through three critical research\nquestions: (1) complex problem solving, (2) token structure probing, and (3)\nresilience to typographical variation. Our findings reveal that scaling model\nparameters can mitigate the issue of tokenization; however, LLMs still suffer\nfrom biases induced by typos and other text format variations. Our experiments\nshow that subword regularization such as BPE-dropout can mitigate this issue.\nWe release our evaluation code and data at https://github.com/FloatAI/TKEval."
                },
                "authors": [
                    {
                        "name": "Yekun Chai"
                    },
                    {
                        "name": "Yewei Fang"
                    },
                    {
                        "name": "Qiwei Peng"
                    },
                    {
                        "name": "Xuhong Li"
                    }
                ],
                "author_detail": {
                    "name": "Xuhong Li"
                },
                "author": "Xuhong Li",
                "arxiv_comment": "EMNLP 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11687v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11687v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.10432v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.10432v3",
                "updated": "2024-10-03T17:55:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    55,
                    29,
                    3,
                    277,
                    0
                ],
                "published": "2023-07-19T19:40:34Z",
                "published_parsed": [
                    2023,
                    7,
                    19,
                    19,
                    40,
                    34,
                    2,
                    200,
                    0
                ],
                "title": "PharmacyGPT: The AI Pharmacist",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PharmacyGPT: The AI Pharmacist"
                },
                "summary": "In this study, we introduce PharmacyGPT, a novel framework to assess the\ncapabilities of large language models (LLMs) such as ChatGPT and GPT-4 in\nemulating the role of clinical pharmacists. Our methodology encompasses the\nutilization of LLMs to generate comprehensible patient clusters, formulate\nmedication plans, and forecast patient outcomes. We conduct our investigation\nusing real data acquired from the intensive care unit (ICU) at the University\nof North Carolina Chapel Hill (UNC) Hospital. Our analysis offers valuable\ninsights into the potential applications and limitations of LLMs in the field\nof clinical pharmacy, with implications for both patient care and the\ndevelopment of future AI-driven healthcare solutions. By evaluating the\nperformance of PharmacyGPT, we aim to contribute to the ongoing discourse\nsurrounding the integration of artificial intelligence in healthcare settings,\nultimately promoting the responsible and efficacious use of such technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we introduce PharmacyGPT, a novel framework to assess the\ncapabilities of large language models (LLMs) such as ChatGPT and GPT-4 in\nemulating the role of clinical pharmacists. Our methodology encompasses the\nutilization of LLMs to generate comprehensible patient clusters, formulate\nmedication plans, and forecast patient outcomes. We conduct our investigation\nusing real data acquired from the intensive care unit (ICU) at the University\nof North Carolina Chapel Hill (UNC) Hospital. Our analysis offers valuable\ninsights into the potential applications and limitations of LLMs in the field\nof clinical pharmacy, with implications for both patient care and the\ndevelopment of future AI-driven healthcare solutions. By evaluating the\nperformance of PharmacyGPT, we aim to contribute to the ongoing discourse\nsurrounding the integration of artificial intelligence in healthcare settings,\nultimately promoting the responsible and efficacious use of such technologies."
                },
                "authors": [
                    {
                        "name": "Zhengliang Liu"
                    },
                    {
                        "name": "Zihao Wu"
                    },
                    {
                        "name": "Mengxuan Hu"
                    },
                    {
                        "name": "Bokai Zhao"
                    },
                    {
                        "name": "Lin Zhao"
                    },
                    {
                        "name": "Tianyi Zhang"
                    },
                    {
                        "name": "Haixing Dai"
                    },
                    {
                        "name": "Xianyan Chen"
                    },
                    {
                        "name": "Ye Shen"
                    },
                    {
                        "name": "Sheng Li"
                    },
                    {
                        "name": "Quanzheng Li"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Brian Murray"
                    },
                    {
                        "name": "Tianming Liu"
                    },
                    {
                        "name": "Andrea Sikora"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Sikora"
                },
                "author": "Andrea Sikora",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2307.10432v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.10432v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02743v1",
                "updated": "2024-10-03T17:55:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    55,
                    13,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T17:55:13Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    55,
                    13,
                    3,
                    277,
                    0
                ],
                "title": "MA-RLHF: Reinforcement Learning from Human Feedback with Macro Actions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MA-RLHF: Reinforcement Learning from Human Feedback with Macro Actions"
                },
                "summary": "Reinforcement learning from human feedback (RLHF) has demonstrated\neffectiveness in aligning large language models (LLMs) with human preferences.\nHowever, token-level RLHF suffers from the credit assignment problem over long\nsequences, where delayed rewards make it challenging for the model to discern\nwhich actions contributed to successful outcomes. This hinders learning\nefficiency and slows convergence. In this paper, we propose MA-RLHF, a simple\nyet effective RLHF framework that incorporates macro actions -- sequences of\ntokens or higher-level language constructs -- into the learning process. By\noperating at this higher level of abstraction, our approach reduces the\ntemporal distance between actions and rewards, facilitating faster and more\naccurate credit assignment. This results in more stable policy gradient\nestimates and enhances learning efficiency within each episode, all without\nincreasing computational complexity during training or inference. We validate\nour approach through extensive experiments across various model sizes and\ntasks, including text summarization, dialogue generation, question answering,\nand program synthesis. Our method achieves substantial performance improvements\nover standard RLHF, with performance gains of up to 30% in text summarization\nand code generation, 18% in dialogue, and 8% in question answering tasks.\nNotably, our approach reaches parity with vanilla RLHF 1.7x to 2x faster in\nterms of training time and continues to outperform it with further training. We\nwill make our code and data publicly available at\nhttps://github.com/ernie-research/MA-RLHF .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning from human feedback (RLHF) has demonstrated\neffectiveness in aligning large language models (LLMs) with human preferences.\nHowever, token-level RLHF suffers from the credit assignment problem over long\nsequences, where delayed rewards make it challenging for the model to discern\nwhich actions contributed to successful outcomes. This hinders learning\nefficiency and slows convergence. In this paper, we propose MA-RLHF, a simple\nyet effective RLHF framework that incorporates macro actions -- sequences of\ntokens or higher-level language constructs -- into the learning process. By\noperating at this higher level of abstraction, our approach reduces the\ntemporal distance between actions and rewards, facilitating faster and more\naccurate credit assignment. This results in more stable policy gradient\nestimates and enhances learning efficiency within each episode, all without\nincreasing computational complexity during training or inference. We validate\nour approach through extensive experiments across various model sizes and\ntasks, including text summarization, dialogue generation, question answering,\nand program synthesis. Our method achieves substantial performance improvements\nover standard RLHF, with performance gains of up to 30% in text summarization\nand code generation, 18% in dialogue, and 8% in question answering tasks.\nNotably, our approach reaches parity with vanilla RLHF 1.7x to 2x faster in\nterms of training time and continues to outperform it with further training. We\nwill make our code and data publicly available at\nhttps://github.com/ernie-research/MA-RLHF ."
                },
                "authors": [
                    {
                        "name": "Yekun Chai"
                    },
                    {
                        "name": "Haoran Sun"
                    },
                    {
                        "name": "Huang Fang"
                    },
                    {
                        "name": "Shuohuan Wang"
                    },
                    {
                        "name": "Yu Sun"
                    },
                    {
                        "name": "Hua Wu"
                    }
                ],
                "author_detail": {
                    "name": "Hua Wu"
                },
                "author": "Hua Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02742v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02742v1",
                "updated": "2024-10-03T17:55:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    55,
                    9,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T17:55:09Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    55,
                    9,
                    3,
                    277,
                    0
                ],
                "title": "Grounding Large Language Models In Embodied Environment With Imperfect\n  World Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grounding Large Language Models In Embodied Environment With Imperfect\n  World Models"
                },
                "summary": "Despite a widespread success in various applications, large language models\n(LLMs) often stumble when tackling basic physical reasoning or executing\nrobotics tasks, due to a lack of direct experience with the physical nuances of\nthe real world. To address these issues, we propose a Grounding Large language\nmodel with Imperfect world MOdel (GLIMO), which utilizes proxy world models\nsuch as simulators to collect and synthesize trining data. GLIMO incorporates\nan LLM agent-based data generator to automatically create high-quality and\ndiverse instruction datasets. The generator includes an iterative self-refining\nmodule for temporally consistent experience sampling, a diverse set of\nquestion-answering instruction seeds, and a retrieval-augmented generation\nmodule for reflecting on prior experiences. Comprehensive experiments show that\nour approach improve the performance of strong open-source LLMs like LLaMA-3\nwith a performance boost of 2.04 $\\times$, 1.54 $\\times$, and 1.82 $\\times$\nacross three different benchmarks, respectively. The performance is able to\ncompete with or surpass their larger counterparts such as GPT-4.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite a widespread success in various applications, large language models\n(LLMs) often stumble when tackling basic physical reasoning or executing\nrobotics tasks, due to a lack of direct experience with the physical nuances of\nthe real world. To address these issues, we propose a Grounding Large language\nmodel with Imperfect world MOdel (GLIMO), which utilizes proxy world models\nsuch as simulators to collect and synthesize trining data. GLIMO incorporates\nan LLM agent-based data generator to automatically create high-quality and\ndiverse instruction datasets. The generator includes an iterative self-refining\nmodule for temporally consistent experience sampling, a diverse set of\nquestion-answering instruction seeds, and a retrieval-augmented generation\nmodule for reflecting on prior experiences. Comprehensive experiments show that\nour approach improve the performance of strong open-source LLMs like LLaMA-3\nwith a performance boost of 2.04 $\\times$, 1.54 $\\times$, and 1.82 $\\times$\nacross three different benchmarks, respectively. The performance is able to\ncompete with or surpass their larger counterparts such as GPT-4."
                },
                "authors": [
                    {
                        "name": "Haolan Liu"
                    },
                    {
                        "name": "Jishen Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jishen Zhao"
                },
                "author": "Jishen Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02742v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02742v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02741v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02741v1",
                "updated": "2024-10-03T17:54:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    54,
                    56,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T17:54:56Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    54,
                    56,
                    3,
                    277,
                    0
                ],
                "title": "Salient Information Prompting to Steer Content in Prompt-based\n  Abstractive Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Salient Information Prompting to Steer Content in Prompt-based\n  Abstractive Summarization"
                },
                "summary": "Large language models (LLMs) can generate fluent summaries across domains\nusing prompting techniques, reducing the need to train models for summarization\napplications. However, crafting effective prompts that guide LLMs to generate\nsummaries with the appropriate level of detail and writing style remains a\nchallenge. In this paper, we explore the use of salient information extracted\nfrom the source document to enhance summarization prompts. We show that adding\nkeyphrases in prompts can improve ROUGE F1 and recall, making the generated\nsummaries more similar to the reference and more complete. The number of\nkeyphrases can control the precision-recall trade-off. Furthermore, our\nanalysis reveals that incorporating phrase-level salient information is\nsuperior to word- or sentence-level. However, the impact on hallucination is\nnot universally positive across LLMs. To conduct this analysis, we introduce\nKeyphrase Signal Extractor (SigExt), a lightweight model that can be finetuned\nto extract salient keyphrases. By using SigExt, we achieve consistent ROUGE\nimprovements across datasets and open-weight and proprietary LLMs without any\nLLM customization. Our findings provide insights into leveraging salient\ninformation in building prompt-based summarization systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) can generate fluent summaries across domains\nusing prompting techniques, reducing the need to train models for summarization\napplications. However, crafting effective prompts that guide LLMs to generate\nsummaries with the appropriate level of detail and writing style remains a\nchallenge. In this paper, we explore the use of salient information extracted\nfrom the source document to enhance summarization prompts. We show that adding\nkeyphrases in prompts can improve ROUGE F1 and recall, making the generated\nsummaries more similar to the reference and more complete. The number of\nkeyphrases can control the precision-recall trade-off. Furthermore, our\nanalysis reveals that incorporating phrase-level salient information is\nsuperior to word- or sentence-level. However, the impact on hallucination is\nnot universally positive across LLMs. To conduct this analysis, we introduce\nKeyphrase Signal Extractor (SigExt), a lightweight model that can be finetuned\nto extract salient keyphrases. By using SigExt, we achieve consistent ROUGE\nimprovements across datasets and open-weight and proprietary LLMs without any\nLLM customization. Our findings provide insights into leveraging salient\ninformation in building prompt-based summarization systems."
                },
                "authors": [
                    {
                        "name": "Lei Xu"
                    },
                    {
                        "name": "Mohammed Asad Karim"
                    },
                    {
                        "name": "Saket Dingliwal"
                    },
                    {
                        "name": "Aparna Elangovan"
                    }
                ],
                "author_detail": {
                    "name": "Aparna Elangovan"
                },
                "author": "Aparna Elangovan",
                "arxiv_comment": "Accepted to EMNLP 2024 Industry Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02741v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02740v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02740v1",
                "updated": "2024-10-03T17:54:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    54,
                    52,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T17:54:52Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    54,
                    52,
                    3,
                    277,
                    0
                ],
                "title": "Revisit Large-Scale Image-Caption Data in Pre-training Multimodal\n  Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisit Large-Scale Image-Caption Data in Pre-training Multimodal\n  Foundation Models"
                },
                "summary": "Recent advancements in multimodal models highlight the value of rewritten\ncaptions for improving performance, yet key challenges remain. For example,\nwhile synthetic captions often provide superior quality and image-text\nalignment, it is not clear whether they can fully replace AltTexts: the role of\nsynthetic captions and their interaction with original web-crawled AltTexts in\npre-training is still not well understood. Moreover, different multimodal\nfoundation models may have unique preferences for specific caption formats, but\nefforts to identify the optimal captions for each model remain limited. In this\nwork, we propose a novel, controllable, and scalable captioning pipeline\ndesigned to generate diverse caption formats tailored to various multimodal\nmodels. By examining Short Synthetic Captions (SSC) towards Dense Synthetic\nCaptions (DSC+) as case studies, we systematically explore their effects and\ninteractions with AltTexts across models such as CLIP, multimodal LLMs, and\ndiffusion models. Our findings reveal that a hybrid approach that keeps both\nsynthetic captions and AltTexts can outperform the use of synthetic captions\nalone, improving both alignment and performance, with each model demonstrating\npreferences for particular caption formats. This comprehensive analysis\nprovides valuable insights into optimizing captioning strategies, thereby\nadvancing the pre-training of multimodal foundation models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in multimodal models highlight the value of rewritten\ncaptions for improving performance, yet key challenges remain. For example,\nwhile synthetic captions often provide superior quality and image-text\nalignment, it is not clear whether they can fully replace AltTexts: the role of\nsynthetic captions and their interaction with original web-crawled AltTexts in\npre-training is still not well understood. Moreover, different multimodal\nfoundation models may have unique preferences for specific caption formats, but\nefforts to identify the optimal captions for each model remain limited. In this\nwork, we propose a novel, controllable, and scalable captioning pipeline\ndesigned to generate diverse caption formats tailored to various multimodal\nmodels. By examining Short Synthetic Captions (SSC) towards Dense Synthetic\nCaptions (DSC+) as case studies, we systematically explore their effects and\ninteractions with AltTexts across models such as CLIP, multimodal LLMs, and\ndiffusion models. Our findings reveal that a hybrid approach that keeps both\nsynthetic captions and AltTexts can outperform the use of synthetic captions\nalone, improving both alignment and performance, with each model demonstrating\npreferences for particular caption formats. This comprehensive analysis\nprovides valuable insights into optimizing captioning strategies, thereby\nadvancing the pre-training of multimodal foundation models."
                },
                "authors": [
                    {
                        "name": "Zhengfeng Lai"
                    },
                    {
                        "name": "Vasileios Saveris"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Hong-You Chen"
                    },
                    {
                        "name": "Haotian Zhang"
                    },
                    {
                        "name": "Bowen Zhang"
                    },
                    {
                        "name": "Juan Lao Tebar"
                    },
                    {
                        "name": "Wenze Hu"
                    },
                    {
                        "name": "Zhe Gan"
                    },
                    {
                        "name": "Peter Grasch"
                    },
                    {
                        "name": "Meng Cao"
                    },
                    {
                        "name": "Yinfei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yinfei Yang"
                },
                "author": "Yinfei Yang",
                "arxiv_comment": "CV/ML",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02740v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02740v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02736v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02736v1",
                "updated": "2024-10-03T17:53:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    53,
                    30,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T17:53:30Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    53,
                    30,
                    3,
                    277,
                    0
                ],
                "title": "Justice or Prejudice? Quantifying Biases in LLM-as-a-Judge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Justice or Prejudice? Quantifying Biases in LLM-as-a-Judge"
                },
                "summary": "LLM-as-a-Judge has been widely utilized as an evaluation method in various\nbenchmarks and served as supervised rewards in model training. However, despite\ntheir excellence in many domains, potential issues are under-explored,\nundermining their reliability and the scope of their utility. Therefore, we\nidentify 12 key potential biases and propose a new automated bias\nquantification framework-CALM-which systematically quantifies and analyzes each\ntype of bias in LLM-as-a-Judge by using automated and principle-guided\nmodification. Our experiments cover multiple popular language models, and the\nresults indicate that while advanced models have achieved commendable overall\nperformance, significant biases persist in certain specific tasks. Empirical\nresults suggest that there remains room for improvement in the reliability of\nLLM-as-a-Judge. Moreover, we also discuss the explicit and implicit influence\nof these biases and give some suggestions for the reliable application of\nLLM-as-a-Judge. Our work highlights the need for stakeholders to address these\nissues and remind users to exercise caution in LLM-as-a-Judge applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-as-a-Judge has been widely utilized as an evaluation method in various\nbenchmarks and served as supervised rewards in model training. However, despite\ntheir excellence in many domains, potential issues are under-explored,\nundermining their reliability and the scope of their utility. Therefore, we\nidentify 12 key potential biases and propose a new automated bias\nquantification framework-CALM-which systematically quantifies and analyzes each\ntype of bias in LLM-as-a-Judge by using automated and principle-guided\nmodification. Our experiments cover multiple popular language models, and the\nresults indicate that while advanced models have achieved commendable overall\nperformance, significant biases persist in certain specific tasks. Empirical\nresults suggest that there remains room for improvement in the reliability of\nLLM-as-a-Judge. Moreover, we also discuss the explicit and implicit influence\nof these biases and give some suggestions for the reliable application of\nLLM-as-a-Judge. Our work highlights the need for stakeholders to address these\nissues and remind users to exercise caution in LLM-as-a-Judge applications."
                },
                "authors": [
                    {
                        "name": "Jiayi Ye"
                    },
                    {
                        "name": "Yanbo Wang"
                    },
                    {
                        "name": "Yue Huang"
                    },
                    {
                        "name": "Dongping Chen"
                    },
                    {
                        "name": "Qihui Zhang"
                    },
                    {
                        "name": "Nuno Moniz"
                    },
                    {
                        "name": "Tian Gao"
                    },
                    {
                        "name": "Werner Geyer"
                    },
                    {
                        "name": "Chao Huang"
                    },
                    {
                        "name": "Pin-Yu Chen"
                    },
                    {
                        "name": "Nitesh V Chawla"
                    },
                    {
                        "name": "Xiangliang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiangliang Zhang"
                },
                "author": "Xiangliang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02736v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02736v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.17051v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.17051v3",
                "updated": "2024-10-03T17:53:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    53,
                    4,
                    3,
                    277,
                    0
                ],
                "published": "2023-03-29T22:50:05Z",
                "published_parsed": [
                    2023,
                    3,
                    29,
                    22,
                    50,
                    5,
                    2,
                    88,
                    0
                ],
                "title": "Towards Foundation Models and Few-Shot Parameter-Efficient Fine-Tuning\n  for Volumetric Organ Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Foundation Models and Few-Shot Parameter-Efficient Fine-Tuning\n  for Volumetric Organ Segmentation"
                },
                "summary": "The recent popularity of foundation models and the pre-train-and-adapt\nparadigm, where a large-scale model is transferred to downstream tasks, is\ngaining attention for volumetric medical image segmentation. However, current\ntransfer learning strategies devoted to full fine-tuning for transfer learning\nmay require significant resources and yield sub-optimal results when the\nlabeled data of the target task is scarce. This makes its applicability in real\nclinical settings challenging since these institutions are usually constrained\non data and computational resources to develop proprietary solutions. To\naddress this challenge, we formalize Few-Shot Efficient Fine-Tuning (FSEFT), a\nnovel and realistic scenario for adapting medical image segmentation foundation\nmodels. This setting considers the key role of both data- and parameter-\nefficiency during adaptation. Building on a foundation model pre-trained on\nopen-access CT organ segmentation sources, we propose leveraging\nParameter-Efficient Fine-Tuning and black-box Adapters to address such\nchallenges. Furthermore, novel efficient adaptation methodologies are\nintroduced in this work, which include Spatial black-box Adapters that are more\nappropriate for dense prediction tasks and constrained transductive inference,\nleveraging task-specific prior knowledge. Our comprehensive transfer learning\nexperiments confirm the suitability of foundation models in medical image\nsegmentation and unveil the limitations of popular fine-tuning strategies in\nfew-shot scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent popularity of foundation models and the pre-train-and-adapt\nparadigm, where a large-scale model is transferred to downstream tasks, is\ngaining attention for volumetric medical image segmentation. However, current\ntransfer learning strategies devoted to full fine-tuning for transfer learning\nmay require significant resources and yield sub-optimal results when the\nlabeled data of the target task is scarce. This makes its applicability in real\nclinical settings challenging since these institutions are usually constrained\non data and computational resources to develop proprietary solutions. To\naddress this challenge, we formalize Few-Shot Efficient Fine-Tuning (FSEFT), a\nnovel and realistic scenario for adapting medical image segmentation foundation\nmodels. This setting considers the key role of both data- and parameter-\nefficiency during adaptation. Building on a foundation model pre-trained on\nopen-access CT organ segmentation sources, we propose leveraging\nParameter-Efficient Fine-Tuning and black-box Adapters to address such\nchallenges. Furthermore, novel efficient adaptation methodologies are\nintroduced in this work, which include Spatial black-box Adapters that are more\nappropriate for dense prediction tasks and constrained transductive inference,\nleveraging task-specific prior knowledge. Our comprehensive transfer learning\nexperiments confirm the suitability of foundation models in medical image\nsegmentation and unveil the limitations of popular fine-tuning strategies in\nfew-shot scenarios."
                },
                "authors": [
                    {
                        "name": "Julio Silva-Rodrguez"
                    },
                    {
                        "name": "Jose Dolz"
                    },
                    {
                        "name": "Ismail Ben Ayed"
                    }
                ],
                "author_detail": {
                    "name": "Ismail Ben Ayed"
                },
                "author": "Ismail Ben Ayed",
                "arxiv_comment": "Journal Extension of MICCAI - MedAGI Workshop 2023. Code in\n  https://github.com/jusiro/fewshot-finetuning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2303.17051v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.17051v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00023v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00023v2",
                "updated": "2024-10-03T17:50:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    50,
                    33,
                    3,
                    277,
                    0
                ],
                "published": "2024-05-08T06:30:58Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    6,
                    30,
                    58,
                    2,
                    129,
                    0
                ],
                "title": "Preble: Efficient Distributed Prompt Scheduling for LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preble: Efficient Distributed Prompt Scheduling for LLM Serving"
                },
                "summary": "Prompts to large language models (LLMs) have evolved beyond simple user\nquestions. For LLMs to solve complex problems, today's practices are to include\ndomain-specific instructions, illustration of tool usages, and/or long context\nsuch as textbook chapters in prompts. As such, many parts of prompts are\nrepetitive across requests. Recent works propose to cache and reuse KV state of\nprompts. However, they are all confined to a single-GPU optimization, while\nproduction LLM serving systems are distributed by nature.\n  This paper proposes Preble, the first distributed LLM serving platform that\ntargets and optimizes for prompt sharing. We designed a distributed scheduling\nsystem that co-optimizes KV state reuse and computation load-balancing with a\nnew scheduling algorithm and a hierarchical scheduling mechanism. Our\nevaluation of Preble with real workloads and request arrival patterns on two\nopen-source LLMs shows that Preble outperforms the SOTA serving systems by 1.5X\nto 14.5X on average latency and 2X to 10X on p99 latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompts to large language models (LLMs) have evolved beyond simple user\nquestions. For LLMs to solve complex problems, today's practices are to include\ndomain-specific instructions, illustration of tool usages, and/or long context\nsuch as textbook chapters in prompts. As such, many parts of prompts are\nrepetitive across requests. Recent works propose to cache and reuse KV state of\nprompts. However, they are all confined to a single-GPU optimization, while\nproduction LLM serving systems are distributed by nature.\n  This paper proposes Preble, the first distributed LLM serving platform that\ntargets and optimizes for prompt sharing. We designed a distributed scheduling\nsystem that co-optimizes KV state reuse and computation load-balancing with a\nnew scheduling algorithm and a hierarchical scheduling mechanism. Our\nevaluation of Preble with real workloads and request arrival patterns on two\nopen-source LLMs shows that Preble outperforms the SOTA serving systems by 1.5X\nto 14.5X on average latency and 2X to 10X on p99 latency."
                },
                "authors": [
                    {
                        "name": "Vikranth Srivatsa"
                    },
                    {
                        "name": "Zijian He"
                    },
                    {
                        "name": "Reyna Abhyankar"
                    },
                    {
                        "name": "Dongming Li"
                    },
                    {
                        "name": "Yiying Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yiying Zhang"
                },
                "author": "Yiying Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00023v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00023v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02725v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02725v1",
                "updated": "2024-10-03T17:47:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    47,
                    29,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T17:47:29Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    47,
                    29,
                    3,
                    277,
                    0
                ],
                "title": "Adaptive Inference-Time Compute: LLMs Can Predict if They Can Do Better,\n  Even Mid-Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Inference-Time Compute: LLMs Can Predict if They Can Do Better,\n  Even Mid-Generation"
                },
                "summary": "Inference-time computation is a powerful paradigm to enhance the performance\nof large language models (LLMs), with Best-of-N sampling being a widely used\ntechnique. However, this method is computationally expensive, requiring both\n(1) an external reward model and (2) the generation of multiple samples. In\nthis work, we introduce a new generative self-evaluation scheme designed to\nadaptively reduce the number of generated samples while maintaining or even\nimproving performance. We use a generative reward model formulation, allowing\nthe LLM to predict mid-generation the probability that restarting the\ngeneration will yield a better response. These predictions are obtained without\nan external reward model and can be used to decide whether or not to generate\nmore samples, prune unpromising samples early on, or to pick the best sample.\nThis capability is very inexpensive as it involves generating a single\npredefined token. Trained using a dataset constructed with real unfiltered\nLMSYS user prompts, Llama 3.1 8B's win rate against GPT-4 on AlpacaEval\nincreases from 21% to 34% with 16 samples and math performance on GSM8K\nimproves from 84% to 91%. By sampling only when the LLM determines that it is\nbeneficial to do so and adaptively adjusting temperature annealing, we\ndemonstrate that 74% of the improvement from using 16 samples can be achieved\nwith only 1.2 samples on average. We further demonstrate that 50-75% of samples\ncan be pruned early in generation with minimal degradation in performance.\nOverall, our methods enable more efficient and scalable compute utilization\nduring inference for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-time computation is a powerful paradigm to enhance the performance\nof large language models (LLMs), with Best-of-N sampling being a widely used\ntechnique. However, this method is computationally expensive, requiring both\n(1) an external reward model and (2) the generation of multiple samples. In\nthis work, we introduce a new generative self-evaluation scheme designed to\nadaptively reduce the number of generated samples while maintaining or even\nimproving performance. We use a generative reward model formulation, allowing\nthe LLM to predict mid-generation the probability that restarting the\ngeneration will yield a better response. These predictions are obtained without\nan external reward model and can be used to decide whether or not to generate\nmore samples, prune unpromising samples early on, or to pick the best sample.\nThis capability is very inexpensive as it involves generating a single\npredefined token. Trained using a dataset constructed with real unfiltered\nLMSYS user prompts, Llama 3.1 8B's win rate against GPT-4 on AlpacaEval\nincreases from 21% to 34% with 16 samples and math performance on GSM8K\nimproves from 84% to 91%. By sampling only when the LLM determines that it is\nbeneficial to do so and adaptively adjusting temperature annealing, we\ndemonstrate that 74% of the improvement from using 16 samples can be achieved\nwith only 1.2 samples on average. We further demonstrate that 50-75% of samples\ncan be pruned early in generation with minimal degradation in performance.\nOverall, our methods enable more efficient and scalable compute utilization\nduring inference for LLMs."
                },
                "authors": [
                    {
                        "name": "Rohin Manvi"
                    },
                    {
                        "name": "Anikait Singh"
                    },
                    {
                        "name": "Stefano Ermon"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Ermon"
                },
                "author": "Stefano Ermon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02725v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02725v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02724v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02724v1",
                "updated": "2024-10-03T17:45:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    45,
                    31,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T17:45:31Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    45,
                    31,
                    3,
                    277,
                    0
                ],
                "title": "Large Language Models as Markov Chains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models as Markov Chains"
                },
                "summary": "Large language models (LLMs) have proven to be remarkably efficient, both\nacross a wide range of natural language processing tasks and well beyond them.\nHowever, a comprehensive theoretical analysis of the origins of their\nimpressive performance remains elusive. In this paper, we approach this\nchallenging task by drawing an equivalence between generic autoregressive\nlanguage models with vocabulary of size $T$ and context window of size $K$ and\nMarkov chains defined on a finite state space of size $\\mathcal{O}(T^K)$. We\nderive several surprising findings related to the existence of a stationary\ndistribution of Markov chains that capture the inference power of LLMs, their\nspeed of convergence to it, and the influence of the temperature on the latter.\nWe then prove pre-training and in-context generalization bounds and show how\nthe drawn equivalence allows us to enrich their interpretation. Finally, we\nillustrate our theoretical guarantees with experiments on several recent LLMs\nto highlight how they capture the behavior observed in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have proven to be remarkably efficient, both\nacross a wide range of natural language processing tasks and well beyond them.\nHowever, a comprehensive theoretical analysis of the origins of their\nimpressive performance remains elusive. In this paper, we approach this\nchallenging task by drawing an equivalence between generic autoregressive\nlanguage models with vocabulary of size $T$ and context window of size $K$ and\nMarkov chains defined on a finite state space of size $\\mathcal{O}(T^K)$. We\nderive several surprising findings related to the existence of a stationary\ndistribution of Markov chains that capture the inference power of LLMs, their\nspeed of convergence to it, and the influence of the temperature on the latter.\nWe then prove pre-training and in-context generalization bounds and show how\nthe drawn equivalence allows us to enrich their interpretation. Finally, we\nillustrate our theoretical guarantees with experiments on several recent LLMs\nto highlight how they capture the behavior observed in practice."
                },
                "authors": [
                    {
                        "name": "Oussama Zekri"
                    },
                    {
                        "name": "Ambroise Odonnat"
                    },
                    {
                        "name": "Abdelhakim Benechehab"
                    },
                    {
                        "name": "Linus Bleistein"
                    },
                    {
                        "name": "Nicolas Boull"
                    },
                    {
                        "name": "Ievgen Redko"
                    }
                ],
                "author_detail": {
                    "name": "Ievgen Redko"
                },
                "author": "Ievgen Redko",
                "arxiv_comment": "49 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02724v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02724v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02721v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02721v1",
                "updated": "2024-10-03T17:40:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    40,
                    55,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T17:40:55Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    40,
                    55,
                    3,
                    277,
                    0
                ],
                "title": "Domain-Specific Retrieval-Augmented Generation Using Vector Stores,\n  Knowledge Graphs, and Tensor Factorization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain-Specific Retrieval-Augmented Generation Using Vector Stores,\n  Knowledge Graphs, and Tensor Factorization"
                },
                "summary": "Large Language Models (LLMs) are pre-trained on large-scale corpora and excel\nin numerous general natural language processing (NLP) tasks, such as question\nanswering (QA). Despite their advanced language capabilities, when it comes to\ndomain-specific and knowledge-intensive tasks, LLMs suffer from hallucinations,\nknowledge cut-offs, and lack of knowledge attributions. Additionally, fine\ntuning LLMs' intrinsic knowledge to highly specific domains is an expensive and\ntime consuming process. The retrieval-augmented generation (RAG) process has\nrecently emerged as a method capable of optimization of LLM responses, by\nreferencing them to a predetermined ontology. It was shown that using a\nKnowledge Graph (KG) ontology for RAG improves the QA accuracy, by taking into\naccount relevant sub-graphs that preserve the information in a structured\nmanner. In this paper, we introduce SMART-SLIC, a highly domain-specific LLM\nframework, that integrates RAG with KG and a vector store (VS) that store\nfactual domain specific information. Importantly, to avoid hallucinations in\nthe KG, we build these highly domain-specific KGs and VSs without the use of\nLLMs, but via NLP, data mining, and nonnegative tensor factorization with\nautomatic model selection. Pairing our RAG with a domain-specific: (i) KG\n(containing structured information), and (ii) VS (containing unstructured\ninformation) enables the development of domain-specific chat-bots that\nattribute the source of information, mitigate hallucinations, lessen the need\nfor fine-tuning, and excel in highly domain-specific question answering tasks.\nWe pair SMART-SLIC with chain-of-thought prompting agents. The framework is\ndesigned to be generalizable to adapt to any specific or specialized domain. In\nthis paper, we demonstrate the question answering capabilities of our framework\non a corpus of scientific publications on malware analysis and anomaly\ndetection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are pre-trained on large-scale corpora and excel\nin numerous general natural language processing (NLP) tasks, such as question\nanswering (QA). Despite their advanced language capabilities, when it comes to\ndomain-specific and knowledge-intensive tasks, LLMs suffer from hallucinations,\nknowledge cut-offs, and lack of knowledge attributions. Additionally, fine\ntuning LLMs' intrinsic knowledge to highly specific domains is an expensive and\ntime consuming process. The retrieval-augmented generation (RAG) process has\nrecently emerged as a method capable of optimization of LLM responses, by\nreferencing them to a predetermined ontology. It was shown that using a\nKnowledge Graph (KG) ontology for RAG improves the QA accuracy, by taking into\naccount relevant sub-graphs that preserve the information in a structured\nmanner. In this paper, we introduce SMART-SLIC, a highly domain-specific LLM\nframework, that integrates RAG with KG and a vector store (VS) that store\nfactual domain specific information. Importantly, to avoid hallucinations in\nthe KG, we build these highly domain-specific KGs and VSs without the use of\nLLMs, but via NLP, data mining, and nonnegative tensor factorization with\nautomatic model selection. Pairing our RAG with a domain-specific: (i) KG\n(containing structured information), and (ii) VS (containing unstructured\ninformation) enables the development of domain-specific chat-bots that\nattribute the source of information, mitigate hallucinations, lessen the need\nfor fine-tuning, and excel in highly domain-specific question answering tasks.\nWe pair SMART-SLIC with chain-of-thought prompting agents. The framework is\ndesigned to be generalizable to adapt to any specific or specialized domain. In\nthis paper, we demonstrate the question answering capabilities of our framework\non a corpus of scientific publications on malware analysis and anomaly\ndetection."
                },
                "authors": [
                    {
                        "name": "Ryan C. Barron"
                    },
                    {
                        "name": "Ves Grantcharov"
                    },
                    {
                        "name": "Selma Wanna"
                    },
                    {
                        "name": "Maksim E. Eren"
                    },
                    {
                        "name": "Manish Bhattarai"
                    },
                    {
                        "name": "Nicholas Solovyev"
                    },
                    {
                        "name": "George Tompkins"
                    },
                    {
                        "name": "Charles Nicholas"
                    },
                    {
                        "name": "Kim . Rasmussen"
                    },
                    {
                        "name": "Cynthia Matuszek"
                    },
                    {
                        "name": "Boian S. Alexandrov"
                    }
                ],
                "author_detail": {
                    "name": "Boian S. Alexandrov"
                },
                "author": "Boian S. Alexandrov",
                "arxiv_comment": "9 pages 7 figures, 1 table, 1 cypher code Accepted to ICMLA 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02721v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02721v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02707v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02707v1",
                "updated": "2024-10-03T17:31:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    31,
                    31,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T17:31:31Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    31,
                    31,
                    3,
                    277,
                    0
                ],
                "title": "LLMs Know More Than They Show: On the Intrinsic Representation of LLM\n  Hallucinations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Know More Than They Show: On the Intrinsic Representation of LLM\n  Hallucinations"
                },
                "summary": "Large language models (LLMs) often produce errors, including factual\ninaccuracies, biases, and reasoning failures, collectively referred to as\n\"hallucinations\". Recent studies have demonstrated that LLMs' internal states\nencode information regarding the truthfulness of their outputs, and that this\ninformation can be utilized to detect errors. In this work, we show that the\ninternal representations of LLMs encode much more information about\ntruthfulness than previously recognized. We first discover that the\ntruthfulness information is concentrated in specific tokens, and leveraging\nthis property significantly enhances error detection performance. Yet, we show\nthat such error detectors fail to generalize across datasets, implying that --\ncontrary to prior claims -- truthfulness encoding is not universal but rather\nmultifaceted. Next, we show that internal representations can also be used for\npredicting the types of errors the model is likely to make, facilitating the\ndevelopment of tailored mitigation strategies. Lastly, we reveal a discrepancy\nbetween LLMs' internal encoding and external behavior: they may encode the\ncorrect answer, yet consistently generate an incorrect one. Taken together,\nthese insights deepen our understanding of LLM errors from the model's internal\nperspective, which can guide future research on enhancing error analysis and\nmitigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often produce errors, including factual\ninaccuracies, biases, and reasoning failures, collectively referred to as\n\"hallucinations\". Recent studies have demonstrated that LLMs' internal states\nencode information regarding the truthfulness of their outputs, and that this\ninformation can be utilized to detect errors. In this work, we show that the\ninternal representations of LLMs encode much more information about\ntruthfulness than previously recognized. We first discover that the\ntruthfulness information is concentrated in specific tokens, and leveraging\nthis property significantly enhances error detection performance. Yet, we show\nthat such error detectors fail to generalize across datasets, implying that --\ncontrary to prior claims -- truthfulness encoding is not universal but rather\nmultifaceted. Next, we show that internal representations can also be used for\npredicting the types of errors the model is likely to make, facilitating the\ndevelopment of tailored mitigation strategies. Lastly, we reveal a discrepancy\nbetween LLMs' internal encoding and external behavior: they may encode the\ncorrect answer, yet consistently generate an incorrect one. Taken together,\nthese insights deepen our understanding of LLM errors from the model's internal\nperspective, which can guide future research on enhancing error analysis and\nmitigation."
                },
                "authors": [
                    {
                        "name": "Hadas Orgad"
                    },
                    {
                        "name": "Michael Toker"
                    },
                    {
                        "name": "Zorik Gekhman"
                    },
                    {
                        "name": "Roi Reichart"
                    },
                    {
                        "name": "Idan Szpektor"
                    },
                    {
                        "name": "Hadas Kotek"
                    },
                    {
                        "name": "Yonatan Belinkov"
                    }
                ],
                "author_detail": {
                    "name": "Yonatan Belinkov"
                },
                "author": "Yonatan Belinkov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02707v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02707v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02703v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02703v1",
                "updated": "2024-10-03T17:27:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    27,
                    30,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T17:27:30Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    27,
                    30,
                    3,
                    277,
                    0
                ],
                "title": "Selective Attention Improves Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Selective Attention Improves Transformer"
                },
                "summary": "Unneeded elements in the attention's context degrade performance. We\nintroduce Selective Attention, a simple parameter-free change to the standard\nattention mechanism which reduces attention to unneeded elements. Selective\nattention improves language modeling performance in a variety of model sizes\nand context lengths. For example, a range of transformers trained with the\nlanguage modeling objective on C4 with selective attention perform equivalently\nto standard transformers with ~2X more heads and parameters in their attention\nmodules. Selective attention also allows decreasing the size of the attention's\ncontext buffer, leading to meaningful reductions in the memory and compute\nrequirements during inference. For example, transformers with 100M parameters\ntrained on C4 with context sizes of 512, 1,024, and 2,048 need 16X, 25X, and\n47X less memory for their attention module, respectively, when equipped with\nselective attention, as those without selective attention, with the same\nvalidation perplexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unneeded elements in the attention's context degrade performance. We\nintroduce Selective Attention, a simple parameter-free change to the standard\nattention mechanism which reduces attention to unneeded elements. Selective\nattention improves language modeling performance in a variety of model sizes\nand context lengths. For example, a range of transformers trained with the\nlanguage modeling objective on C4 with selective attention perform equivalently\nto standard transformers with ~2X more heads and parameters in their attention\nmodules. Selective attention also allows decreasing the size of the attention's\ncontext buffer, leading to meaningful reductions in the memory and compute\nrequirements during inference. For example, transformers with 100M parameters\ntrained on C4 with context sizes of 512, 1,024, and 2,048 need 16X, 25X, and\n47X less memory for their attention module, respectively, when equipped with\nselective attention, as those without selective attention, with the same\nvalidation perplexity."
                },
                "authors": [
                    {
                        "name": "Yaniv Leviathan"
                    },
                    {
                        "name": "Matan Kalman"
                    },
                    {
                        "name": "Yossi Matias"
                    }
                ],
                "author_detail": {
                    "name": "Yossi Matias"
                },
                "author": "Yossi Matias",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02703v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02703v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.12683v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.12683v2",
                "updated": "2024-10-03T17:27:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    27,
                    28,
                    3,
                    277,
                    0
                ],
                "published": "2023-12-20T00:49:52Z",
                "published_parsed": [
                    2023,
                    12,
                    20,
                    0,
                    49,
                    52,
                    2,
                    354,
                    0
                ],
                "title": "Turning English-centric LLMs Into Polyglots: How Much Multilinguality Is\n  Needed?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Turning English-centric LLMs Into Polyglots: How Much Multilinguality Is\n  Needed?"
                },
                "summary": "The vast majority of today's large language models (LLMs) are\nEnglish-centric, having been pretrained predominantly on English text. Yet, in\norder to meet user expectations, models need to be able to respond\nappropriately in multiple languages once deployed in downstream applications.\nThis requires strong cross-lingual transfer abilities. In this work, we\ninvestigate the minimal amount of multilinguality required during finetuning to\nelicit cross-lingual generalisation in English-centric LLMs. In experiments\nacross four LLMs, we find that multilingual instruction tuning with as few as\ntwo to three languages is both necessary and sufficient to elicit effective\ncross-lingual generalisation, with the limiting factor being the degree to\nwhich a target language is seen during pretraining. Evaluations on five\ndifferent tasks further reveal that multilingual instruction tuning is most\nbeneficial for generative tasks that assume input/output language agreement,\nsuch as in chat settings, while being of less importance for highly structured\nclassification-style tasks. Our code and data is available at\nhttps://github.com/ZurichNLP/multilingual-instruction-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The vast majority of today's large language models (LLMs) are\nEnglish-centric, having been pretrained predominantly on English text. Yet, in\norder to meet user expectations, models need to be able to respond\nappropriately in multiple languages once deployed in downstream applications.\nThis requires strong cross-lingual transfer abilities. In this work, we\ninvestigate the minimal amount of multilinguality required during finetuning to\nelicit cross-lingual generalisation in English-centric LLMs. In experiments\nacross four LLMs, we find that multilingual instruction tuning with as few as\ntwo to three languages is both necessary and sufficient to elicit effective\ncross-lingual generalisation, with the limiting factor being the degree to\nwhich a target language is seen during pretraining. Evaluations on five\ndifferent tasks further reveal that multilingual instruction tuning is most\nbeneficial for generative tasks that assume input/output language agreement,\nsuch as in chat settings, while being of less importance for highly structured\nclassification-style tasks. Our code and data is available at\nhttps://github.com/ZurichNLP/multilingual-instruction-tuning."
                },
                "authors": [
                    {
                        "name": "Tannon Kew"
                    },
                    {
                        "name": "Florian Schottmann"
                    },
                    {
                        "name": "Rico Sennrich"
                    }
                ],
                "author_detail": {
                    "name": "Rico Sennrich"
                },
                "author": "Rico Sennrich",
                "arxiv_comment": "Accepted at Findings of EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.12683v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.12683v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07071v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07071v2",
                "updated": "2024-10-03T17:26:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    26,
                    48,
                    3,
                    277,
                    0
                ],
                "published": "2024-07-09T17:44:34Z",
                "published_parsed": [
                    2024,
                    7,
                    9,
                    17,
                    44,
                    34,
                    1,
                    191,
                    0
                ],
                "title": "Lookback Lens: Detecting and Mitigating Contextual Hallucinations in\n  Large Language Models Using Only Attention Maps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lookback Lens: Detecting and Mitigating Contextual Hallucinations in\n  Large Language Models Using Only Attention Maps"
                },
                "summary": "When asked to summarize articles or answer questions given a passage, large\nlanguage models (LLMs) can hallucinate details and respond with unsubstantiated\nanswers that are inaccurate with respect to the input context. This paper\ndescribes a simple approach for detecting such contextual hallucinations. We\nhypothesize that contextual hallucinations are related to the extent to which\nan LLM attends to information in the provided context versus its own\ngenerations. Based on this intuition, we propose a simple hallucination\ndetection model whose input features are given by the ratio of attention\nweights on the context versus newly generated tokens (for each attention head).\nWe find that a linear classifier based on these lookback ratio features is as\neffective as a richer detector that utilizes the entire hidden states of an LLM\nor a text-based entailment model. The lookback ratio-based detector -- Lookback\nLens -- is found to transfer across tasks and even models, allowing a detector\nthat is trained on a 7B model to be applied (without retraining) to a larger\n13B model. We further apply this detector to mitigate contextual\nhallucinations, and find that a simple classifier-guided decoding approach is\nable to reduce the amount of hallucination, for example by 9.6% in the XSum\nsummarization task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When asked to summarize articles or answer questions given a passage, large\nlanguage models (LLMs) can hallucinate details and respond with unsubstantiated\nanswers that are inaccurate with respect to the input context. This paper\ndescribes a simple approach for detecting such contextual hallucinations. We\nhypothesize that contextual hallucinations are related to the extent to which\nan LLM attends to information in the provided context versus its own\ngenerations. Based on this intuition, we propose a simple hallucination\ndetection model whose input features are given by the ratio of attention\nweights on the context versus newly generated tokens (for each attention head).\nWe find that a linear classifier based on these lookback ratio features is as\neffective as a richer detector that utilizes the entire hidden states of an LLM\nor a text-based entailment model. The lookback ratio-based detector -- Lookback\nLens -- is found to transfer across tasks and even models, allowing a detector\nthat is trained on a 7B model to be applied (without retraining) to a larger\n13B model. We further apply this detector to mitigate contextual\nhallucinations, and find that a simple classifier-guided decoding approach is\nable to reduce the amount of hallucination, for example by 9.6% in the XSum\nsummarization task."
                },
                "authors": [
                    {
                        "name": "Yung-Sung Chuang"
                    },
                    {
                        "name": "Linlu Qiu"
                    },
                    {
                        "name": "Cheng-Yu Hsieh"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Yoon Kim"
                    },
                    {
                        "name": "James Glass"
                    }
                ],
                "author_detail": {
                    "name": "James Glass"
                },
                "author": "James Glass",
                "arxiv_comment": "EMNLP 2024 main conference long paper. The source code is available\n  at https://github.com/voidism/Lookback-Lens",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.07071v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07071v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.00237v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.00237v3",
                "updated": "2024-10-03T17:25:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    25,
                    2,
                    3,
                    277,
                    0
                ],
                "published": "2023-11-01T02:40:42Z",
                "published_parsed": [
                    2023,
                    11,
                    1,
                    2,
                    40,
                    42,
                    2,
                    305,
                    0
                ],
                "title": "The Mystery of In-Context Learning: A Comprehensive Survey on\n  Interpretation and Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mystery of In-Context Learning: A Comprehensive Survey on\n  Interpretation and Analysis"
                },
                "summary": "Understanding in-context learning (ICL) capability that enables large\nlanguage models (LLMs) to excel in proficiency through demonstration examples\nis of utmost importance. This importance stems not only from the better\nutilization of this capability across various tasks, but also from the\nproactive identification and mitigation of potential risks, including concerns\nregarding truthfulness, bias, and toxicity, that may arise alongside the\ncapability. In this paper, we present a thorough survey on the interpretation\nand analysis of in-context learning. First, we provide a concise introduction\nto the background and definition of in-context learning. Then, we give an\noverview of advancements from two perspectives: 1) a theoretical perspective,\nemphasizing studies on mechanistic interpretability and delving into the\nmathematical foundations behind ICL; and 2) an empirical perspective,\nconcerning studies that empirically analyze factors associated with ICL. We\nconclude by highlighting the challenges encountered and suggesting potential\navenues for future research. We believe that our work establishes the basis for\nfurther exploration into the interpretation of in-context learning.\nAdditionally, we have created a repository containing the resources referenced\nin our survey.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding in-context learning (ICL) capability that enables large\nlanguage models (LLMs) to excel in proficiency through demonstration examples\nis of utmost importance. This importance stems not only from the better\nutilization of this capability across various tasks, but also from the\nproactive identification and mitigation of potential risks, including concerns\nregarding truthfulness, bias, and toxicity, that may arise alongside the\ncapability. In this paper, we present a thorough survey on the interpretation\nand analysis of in-context learning. First, we provide a concise introduction\nto the background and definition of in-context learning. Then, we give an\noverview of advancements from two perspectives: 1) a theoretical perspective,\nemphasizing studies on mechanistic interpretability and delving into the\nmathematical foundations behind ICL; and 2) an empirical perspective,\nconcerning studies that empirically analyze factors associated with ICL. We\nconclude by highlighting the challenges encountered and suggesting potential\navenues for future research. We believe that our work establishes the basis for\nfurther exploration into the interpretation of in-context learning.\nAdditionally, we have created a repository containing the resources referenced\nin our survey."
                },
                "authors": [
                    {
                        "name": "Yuxiang Zhou"
                    },
                    {
                        "name": "Jiazheng Li"
                    },
                    {
                        "name": "Yanzheng Xiang"
                    },
                    {
                        "name": "Hanqi Yan"
                    },
                    {
                        "name": "Lin Gui"
                    },
                    {
                        "name": "Yulan He"
                    }
                ],
                "author_detail": {
                    "name": "Yulan He"
                },
                "author": "Yulan He",
                "arxiv_comment": "Accepted to the main conference of EMNLP 2024. Resources are\n  available at https://github.com/zyxnlp/ICL-Interpretation-Analysis-Resources",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.00237v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.00237v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02698v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02698v1",
                "updated": "2024-10-03T17:21:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    21,
                    30,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T17:21:30Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    21,
                    30,
                    3,
                    277,
                    0
                ],
                "title": "Lie Algebra Canonicalization: Equivariant Neural Operators under\n  arbitrary Lie Groups",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lie Algebra Canonicalization: Equivariant Neural Operators under\n  arbitrary Lie Groups"
                },
                "summary": "The quest for robust and generalizable machine learning models has driven\nrecent interest in exploiting symmetries through equivariant neural networks.\nIn the context of PDE solvers, recent works have shown that Lie point\nsymmetries can be a useful inductive bias for Physics-Informed Neural Networks\n(PINNs) through data and loss augmentation. Despite this, directly enforcing\nequivariance within the model architecture for these problems remains elusive.\nThis is because many PDEs admit non-compact symmetry groups, oftentimes not\nstudied beyond their infinitesimal generators, making them incompatible with\nmost existing equivariant architectures. In this work, we propose Lie aLgebrA\nCanonicalization (LieLAC), a novel approach that exploits only the action of\ninfinitesimal generators of the symmetry group, circumventing the need for\nknowledge of the full group structure. To achieve this, we address existing\ntheoretical issues in the canonicalization literature, establishing connections\nwith frame averaging in the case of continuous non-compact groups. Operating\nwithin the framework of canonicalization, LieLAC can easily be integrated with\nunconstrained pre-trained models, transforming inputs to a canonical form\nbefore feeding them into the existing model, effectively aligning the input for\nmodel inference according to allowed symmetries. LieLAC utilizes standard Lie\ngroup descent schemes, achieving equivariance in pre-trained models. Finally,\nwe showcase LieLAC's efficacy on tasks of invariant image classification and\nLie point symmetry equivariant neural PDE solvers using pre-trained models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The quest for robust and generalizable machine learning models has driven\nrecent interest in exploiting symmetries through equivariant neural networks.\nIn the context of PDE solvers, recent works have shown that Lie point\nsymmetries can be a useful inductive bias for Physics-Informed Neural Networks\n(PINNs) through data and loss augmentation. Despite this, directly enforcing\nequivariance within the model architecture for these problems remains elusive.\nThis is because many PDEs admit non-compact symmetry groups, oftentimes not\nstudied beyond their infinitesimal generators, making them incompatible with\nmost existing equivariant architectures. In this work, we propose Lie aLgebrA\nCanonicalization (LieLAC), a novel approach that exploits only the action of\ninfinitesimal generators of the symmetry group, circumventing the need for\nknowledge of the full group structure. To achieve this, we address existing\ntheoretical issues in the canonicalization literature, establishing connections\nwith frame averaging in the case of continuous non-compact groups. Operating\nwithin the framework of canonicalization, LieLAC can easily be integrated with\nunconstrained pre-trained models, transforming inputs to a canonical form\nbefore feeding them into the existing model, effectively aligning the input for\nmodel inference according to allowed symmetries. LieLAC utilizes standard Lie\ngroup descent schemes, achieving equivariance in pre-trained models. Finally,\nwe showcase LieLAC's efficacy on tasks of invariant image classification and\nLie point symmetry equivariant neural PDE solvers using pre-trained models."
                },
                "authors": [
                    {
                        "name": "Zakhar Shumaylov"
                    },
                    {
                        "name": "Peter Zaika"
                    },
                    {
                        "name": "James Rowbottom"
                    },
                    {
                        "name": "Ferdia Sherry"
                    },
                    {
                        "name": "Melanie Weber"
                    },
                    {
                        "name": "Carola-Bibiane Schnlieb"
                    }
                ],
                "author_detail": {
                    "name": "Carola-Bibiane Schnlieb"
                },
                "author": "Carola-Bibiane Schnlieb",
                "arxiv_comment": "40 pages; preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02698v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02698v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02693v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02693v1",
                "updated": "2024-10-03T17:18:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    18,
                    37,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T17:18:37Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    18,
                    37,
                    3,
                    277,
                    0
                ],
                "title": "Discovering Clues of Spoofed LM Watermarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discovering Clues of Spoofed LM Watermarks"
                },
                "summary": "LLM watermarks stand out as a promising way to attribute ownership of\nLLM-generated text. One threat to watermark credibility comes from spoofing\nattacks, where an unauthorized third party forges the watermark, enabling it to\nfalsely attribute arbitrary texts to a particular LLM. While recent works have\ndemonstrated that state-of-the-art schemes are in fact vulnerable to spoofing,\nthey lack deeper qualitative analysis of the texts produced by spoofing\nmethods. In this work, we for the first time reveal that there are observable\ndifferences between genuine and spoofed watermark texts. Namely, we show that\nregardless of their underlying approach, all current spoofing methods\nconsistently leave observable artifacts in spoofed texts, indicative of\nwatermark forgery. We build upon these findings to propose rigorous statistical\ntests that reliably reveal the presence of such artifacts, effectively\ndiscovering that a watermark was spoofed. Our experimental evaluation shows\nhigh test power across all current spoofing methods, providing insights into\ntheir fundamental limitations, and suggesting a way to mitigate this threat.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM watermarks stand out as a promising way to attribute ownership of\nLLM-generated text. One threat to watermark credibility comes from spoofing\nattacks, where an unauthorized third party forges the watermark, enabling it to\nfalsely attribute arbitrary texts to a particular LLM. While recent works have\ndemonstrated that state-of-the-art schemes are in fact vulnerable to spoofing,\nthey lack deeper qualitative analysis of the texts produced by spoofing\nmethods. In this work, we for the first time reveal that there are observable\ndifferences between genuine and spoofed watermark texts. Namely, we show that\nregardless of their underlying approach, all current spoofing methods\nconsistently leave observable artifacts in spoofed texts, indicative of\nwatermark forgery. We build upon these findings to propose rigorous statistical\ntests that reliably reveal the presence of such artifacts, effectively\ndiscovering that a watermark was spoofed. Our experimental evaluation shows\nhigh test power across all current spoofing methods, providing insights into\ntheir fundamental limitations, and suggesting a way to mitigate this threat."
                },
                "authors": [
                    {
                        "name": "Thibaud Gloaguen"
                    },
                    {
                        "name": "Nikola Jovanovi"
                    },
                    {
                        "name": "Robin Staab"
                    },
                    {
                        "name": "Martin Vechev"
                    }
                ],
                "author_detail": {
                    "name": "Martin Vechev"
                },
                "author": "Martin Vechev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02693v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02693v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05514v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05514v3",
                "updated": "2024-10-03T17:15:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    15,
                    34,
                    3,
                    277,
                    0
                ],
                "published": "2024-06-08T16:24:24Z",
                "published_parsed": [
                    2024,
                    6,
                    8,
                    16,
                    24,
                    24,
                    5,
                    160,
                    0
                ],
                "title": "RAG-Enhanced Commit Message Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAG-Enhanced Commit Message Generation"
                },
                "summary": "Commit message is one of the most important textual information in software\ndevelopment and maintenance. However, it is time-consuming to write commit\nmessages manually. Commit Message Generation (CMG) has become a research\nhotspot. Recently, several pre-trained language models (PLMs) and large\nlanguage models (LLMs) with code capabilities have been introduced,\ndemonstrating impressive performance on code-related tasks. Meanwhile, prior\nstudies have explored the utilization of retrieval techniques for CMG, but it\nis still unclear what effects would emerge from combining advanced retrieval\ntechniques with various generation models. This paper proposed REACT, a\nREtrieval-Augmented framework for CommiT message generation. It integrates\nadvanced retrieval techniques with different PLMs and LLMs, to enhance the\nperformance of these models on the CMG task. Specifically, a hybrid retriever\nis designed and used to retrieve the most relevant code diff and commit message\npair as an exemplar. Then, the retrieved pair is utilized to guide and enhance\nthe CMG task by PLMs and LLMs through fine-tuning and in-context learning. The\nexperimental results show that REACT significantly enhances these models'\nperformance on the CMG task, improving the BLEU score of CodeT5 by up to 55%,\nboosting Llama 3's BLEU score by 102%, and substantially surpassing all\nbaselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Commit message is one of the most important textual information in software\ndevelopment and maintenance. However, it is time-consuming to write commit\nmessages manually. Commit Message Generation (CMG) has become a research\nhotspot. Recently, several pre-trained language models (PLMs) and large\nlanguage models (LLMs) with code capabilities have been introduced,\ndemonstrating impressive performance on code-related tasks. Meanwhile, prior\nstudies have explored the utilization of retrieval techniques for CMG, but it\nis still unclear what effects would emerge from combining advanced retrieval\ntechniques with various generation models. This paper proposed REACT, a\nREtrieval-Augmented framework for CommiT message generation. It integrates\nadvanced retrieval techniques with different PLMs and LLMs, to enhance the\nperformance of these models on the CMG task. Specifically, a hybrid retriever\nis designed and used to retrieve the most relevant code diff and commit message\npair as an exemplar. Then, the retrieved pair is utilized to guide and enhance\nthe CMG task by PLMs and LLMs through fine-tuning and in-context learning. The\nexperimental results show that REACT significantly enhances these models'\nperformance on the CMG task, improving the BLEU score of CodeT5 by up to 55%,\nboosting Llama 3's BLEU score by 102%, and substantially surpassing all\nbaselines."
                },
                "authors": [
                    {
                        "name": "Linghao Zhang"
                    },
                    {
                        "name": "Hongyi Zhang"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Peng Liang"
                    }
                ],
                "author_detail": {
                    "name": "Peng Liang"
                },
                "author": "Peng Liang",
                "arxiv_comment": "22 pages, 5 images, 6 tables, Manuscript submitted to a journal\n  (2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05514v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05514v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.03741v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.03741v2",
                "updated": "2024-10-03T17:15:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    15,
                    24,
                    3,
                    277,
                    0
                ],
                "published": "2024-01-08T09:01:29Z",
                "published_parsed": [
                    2024,
                    1,
                    8,
                    9,
                    1,
                    29,
                    0,
                    8,
                    0
                ],
                "title": "Enhanced Automated Code Vulnerability Repair using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhanced Automated Code Vulnerability Repair using Large Language Models"
                },
                "summary": "This research addresses the complex challenge of automated repair of code\nvulnerabilities, vital for enhancing digital security in an increasingly\ntechnology-driven world. The study introduces a novel and efficient format for\nthe representation of code modification, using advanced Large Language Models\n(LLMs) such as Code Llama and Mistral. These models, fine-tuned on datasets\nfeaturing C code vulnerabilities, significantly improve the accuracy and\nadaptability of automated code repair techniques. A key finding is the enhanced\nrepair accuracy of these models when compared to previous methods such as\nVulRepair, which underscores their practical utility and efficiency. The\nresearch also offers a critical assessment of current evaluation metrics, such\nas perfect predictions, and their limitations in reflecting the true\ncapabilities of automated repair models in real-world scenarios. Following\nthis, it underscores the importance of using test datasets devoid of train\nsamples, emphasizing the need for dataset integrity to enhance the\neffectiveness of LLMs in code repair tasks. The significance of this work is\nits contribution to digital security, setting new standards for automated code\nvulnerability repair and paving the way for future advancements in the fields\nof cybersecurity and artificial intelligence. The study does not only highlight\nthe potential of LLMs in enhancing code security but also fosters further\nexploration and research in these crucial areas.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research addresses the complex challenge of automated repair of code\nvulnerabilities, vital for enhancing digital security in an increasingly\ntechnology-driven world. The study introduces a novel and efficient format for\nthe representation of code modification, using advanced Large Language Models\n(LLMs) such as Code Llama and Mistral. These models, fine-tuned on datasets\nfeaturing C code vulnerabilities, significantly improve the accuracy and\nadaptability of automated code repair techniques. A key finding is the enhanced\nrepair accuracy of these models when compared to previous methods such as\nVulRepair, which underscores their practical utility and efficiency. The\nresearch also offers a critical assessment of current evaluation metrics, such\nas perfect predictions, and their limitations in reflecting the true\ncapabilities of automated repair models in real-world scenarios. Following\nthis, it underscores the importance of using test datasets devoid of train\nsamples, emphasizing the need for dataset integrity to enhance the\neffectiveness of LLMs in code repair tasks. The significance of this work is\nits contribution to digital security, setting new standards for automated code\nvulnerability repair and paving the way for future advancements in the fields\nof cybersecurity and artificial intelligence. The study does not only highlight\nthe potential of LLMs in enhancing code security but also fosters further\nexploration and research in these crucial areas."
                },
                "authors": [
                    {
                        "name": "David de-Fitero-Dominguez"
                    },
                    {
                        "name": "Eva Garcia-Lopez"
                    },
                    {
                        "name": "Antonio Garcia-Cabot"
                    },
                    {
                        "name": "Jose-Javier Martinez-Herraiz"
                    }
                ],
                "author_detail": {
                    "name": "Jose-Javier Martinez-Herraiz"
                },
                "author": "Jose-Javier Martinez-Herraiz",
                "arxiv_doi": "10.1016/j.engappai.2024.109291",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.engappai.2024.109291",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2401.03741v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.03741v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Engineering Applications of Artificial Intelligence. Volume 138,\n  Part A, December 2024, 109291",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03650v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03650v2",
                "updated": "2024-10-03T17:13:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    13,
                    4,
                    3,
                    277,
                    0
                ],
                "published": "2024-09-05T16:08:19Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    16,
                    8,
                    19,
                    3,
                    249,
                    0
                ],
                "title": "On the Limited Generalization Capability of the Implicit Reward Model\n  Induced by Direct Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Limited Generalization Capability of the Implicit Reward Model\n  Induced by Direct Preference Optimization"
                },
                "summary": "Reinforcement Learning from Human Feedback (RLHF) is an effective approach\nfor aligning language models to human preferences. Central to RLHF is learning\na reward function for scoring human preferences. Two main approaches for\nlearning a reward model are 1) training an EXplicit Reward Model (EXRM) as in\nRLHF, and 2) using an implicit reward learned from preference data through\nmethods such as Direct Preference Optimization (DPO). Prior work has shown that\nthe implicit reward model of DPO (denoted as DPORM) can approximate an EXRM in\nthe limit. DPORM's effectiveness directly implies the optimality of the learned\npolicy, and also has practical implication for LLM alignment methods including\niterative DPO. However, it is unclear how well DPORM empirically matches the\nperformance of EXRM. This work studies the accuracy at distinguishing preferred\nand rejected answers for both DPORM and EXRM. Our findings indicate that even\nthough DPORM fits the training dataset comparably, it generalizes less\neffectively than EXRM, especially when the validation datasets contain\ndistribution shifts. Across five out-of-distribution settings, DPORM has a mean\ndrop in accuracy of 3% and a maximum drop of 7%. These findings highlight that\nDPORM has limited generalization ability and substantiates the integration of\nan explicit reward model in iterative DPO approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning from Human Feedback (RLHF) is an effective approach\nfor aligning language models to human preferences. Central to RLHF is learning\na reward function for scoring human preferences. Two main approaches for\nlearning a reward model are 1) training an EXplicit Reward Model (EXRM) as in\nRLHF, and 2) using an implicit reward learned from preference data through\nmethods such as Direct Preference Optimization (DPO). Prior work has shown that\nthe implicit reward model of DPO (denoted as DPORM) can approximate an EXRM in\nthe limit. DPORM's effectiveness directly implies the optimality of the learned\npolicy, and also has practical implication for LLM alignment methods including\niterative DPO. However, it is unclear how well DPORM empirically matches the\nperformance of EXRM. This work studies the accuracy at distinguishing preferred\nand rejected answers for both DPORM and EXRM. Our findings indicate that even\nthough DPORM fits the training dataset comparably, it generalizes less\neffectively than EXRM, especially when the validation datasets contain\ndistribution shifts. Across five out-of-distribution settings, DPORM has a mean\ndrop in accuracy of 3% and a maximum drop of 7%. These findings highlight that\nDPORM has limited generalization ability and substantiates the integration of\nan explicit reward model in iterative DPO approaches."
                },
                "authors": [
                    {
                        "name": "Yong Lin"
                    },
                    {
                        "name": "Skyler Seto"
                    },
                    {
                        "name": "Maartje ter Hoeve"
                    },
                    {
                        "name": "Katherine Metcalf"
                    },
                    {
                        "name": "Barry-John Theobald"
                    },
                    {
                        "name": "Xuan Wang"
                    },
                    {
                        "name": "Yizhe Zhang"
                    },
                    {
                        "name": "Chen Huang"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "arxiv_comment": "12 pages, 8 tables, 3 figures; Paper Accepted at EMNLP Findings 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03650v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03650v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02684v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02684v1",
                "updated": "2024-10-03T17:10:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    10,
                    41,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T17:10:41Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    10,
                    41,
                    3,
                    277,
                    0
                ],
                "title": "HiddenGuard: Fine-Grained Safe Generation with Specialized\n  Representation Router",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HiddenGuard: Fine-Grained Safe Generation with Specialized\n  Representation Router"
                },
                "summary": "As Large Language Models (LLMs) grow increasingly powerful, ensuring their\nsafety and alignment with human values remains a critical challenge. Ideally,\nLLMs should provide informative responses while avoiding the disclosure of\nharmful or sensitive information. However, current alignment approaches, which\nrely heavily on refusal strategies, such as training models to completely\nreject harmful prompts or applying coarse filters are limited by their binary\nnature. These methods either fully deny access to information or grant it\nwithout sufficient nuance, leading to overly cautious responses or failures to\ndetect subtle harmful content. For example, LLMs may refuse to provide basic,\npublic information about medication due to misuse concerns. Moreover, these\nrefusal-based methods struggle to handle mixed-content scenarios and lack the\nability to adapt to context-dependent sensitivities, which can result in\nover-censorship of benign content. To overcome these challenges, we introduce\nHiddenGuard, a novel framework for fine-grained, safe generation in LLMs.\nHiddenGuard incorporates Prism (rePresentation Router for In-Stream\nModeration), which operates alongside the LLM to enable real-time, token-level\ndetection and redaction of harmful content by leveraging intermediate hidden\nstates. This fine-grained approach allows for more nuanced, context-aware\nmoderation, enabling the model to generate informative responses while\nselectively redacting or replacing sensitive information, rather than outright\nrefusal. We also contribute a comprehensive dataset with token-level\nfine-grained annotations of potentially harmful information across diverse\ncontexts. Our experiments demonstrate that HiddenGuard achieves over 90% in F1\nscore for detecting and redacting harmful content while preserving the overall\nutility and informativeness of the model's responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) grow increasingly powerful, ensuring their\nsafety and alignment with human values remains a critical challenge. Ideally,\nLLMs should provide informative responses while avoiding the disclosure of\nharmful or sensitive information. However, current alignment approaches, which\nrely heavily on refusal strategies, such as training models to completely\nreject harmful prompts or applying coarse filters are limited by their binary\nnature. These methods either fully deny access to information or grant it\nwithout sufficient nuance, leading to overly cautious responses or failures to\ndetect subtle harmful content. For example, LLMs may refuse to provide basic,\npublic information about medication due to misuse concerns. Moreover, these\nrefusal-based methods struggle to handle mixed-content scenarios and lack the\nability to adapt to context-dependent sensitivities, which can result in\nover-censorship of benign content. To overcome these challenges, we introduce\nHiddenGuard, a novel framework for fine-grained, safe generation in LLMs.\nHiddenGuard incorporates Prism (rePresentation Router for In-Stream\nModeration), which operates alongside the LLM to enable real-time, token-level\ndetection and redaction of harmful content by leveraging intermediate hidden\nstates. This fine-grained approach allows for more nuanced, context-aware\nmoderation, enabling the model to generate informative responses while\nselectively redacting or replacing sensitive information, rather than outright\nrefusal. We also contribute a comprehensive dataset with token-level\nfine-grained annotations of potentially harmful information across diverse\ncontexts. Our experiments demonstrate that HiddenGuard achieves over 90% in F1\nscore for detecting and redacting harmful content while preserving the overall\nutility and informativeness of the model's responses."
                },
                "authors": [
                    {
                        "name": "Lingrui Mei"
                    },
                    {
                        "name": "Shenghua Liu"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Baolong Bi"
                    },
                    {
                        "name": "Ruibin Yuan"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02684v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02684v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18725v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18725v2",
                "updated": "2024-10-03T17:10:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    10,
                    9,
                    3,
                    277,
                    0
                ],
                "published": "2024-06-26T19:48:48Z",
                "published_parsed": [
                    2024,
                    6,
                    26,
                    19,
                    48,
                    48,
                    2,
                    178,
                    0
                ],
                "title": "Jailbreaking LLMs with Arabic Transliteration and Arabizi",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreaking LLMs with Arabic Transliteration and Arabizi"
                },
                "summary": "This study identifies the potential vulnerabilities of Large Language Models\n(LLMs) to 'jailbreak' attacks, specifically focusing on the Arabic language and\nits various forms. While most research has concentrated on English-based prompt\nmanipulation, our investigation broadens the scope to investigate the Arabic\nlanguage. We initially tested the AdvBench benchmark in Standardized Arabic,\nfinding that even with prompt manipulation techniques like prefix injection, it\nwas insufficient to provoke LLMs into generating unsafe content. However, when\nusing Arabic transliteration and chatspeak (or arabizi), we found that unsafe\ncontent could be produced on platforms like OpenAI GPT-4 and Anthropic Claude 3\nSonnet. Our findings suggest that using Arabic and its various forms could\nexpose information that might remain hidden, potentially increasing the risk of\njailbreak attacks. We hypothesize that this exposure could be due to the\nmodel's learned connection to specific words, highlighting the need for more\ncomprehensive safety training across all language forms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study identifies the potential vulnerabilities of Large Language Models\n(LLMs) to 'jailbreak' attacks, specifically focusing on the Arabic language and\nits various forms. While most research has concentrated on English-based prompt\nmanipulation, our investigation broadens the scope to investigate the Arabic\nlanguage. We initially tested the AdvBench benchmark in Standardized Arabic,\nfinding that even with prompt manipulation techniques like prefix injection, it\nwas insufficient to provoke LLMs into generating unsafe content. However, when\nusing Arabic transliteration and chatspeak (or arabizi), we found that unsafe\ncontent could be produced on platforms like OpenAI GPT-4 and Anthropic Claude 3\nSonnet. Our findings suggest that using Arabic and its various forms could\nexpose information that might remain hidden, potentially increasing the risk of\njailbreak attacks. We hypothesize that this exposure could be due to the\nmodel's learned connection to specific words, highlighting the need for more\ncomprehensive safety training across all language forms."
                },
                "authors": [
                    {
                        "name": "Mansour Al Ghanim"
                    },
                    {
                        "name": "Saleh Almohaimeed"
                    },
                    {
                        "name": "Mengxin Zheng"
                    },
                    {
                        "name": "Yan Solihin"
                    },
                    {
                        "name": "Qian Lou"
                    }
                ],
                "author_detail": {
                    "name": "Qian Lou"
                },
                "author": "Qian Lou",
                "arxiv_comment": "Accepted by EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18725v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18725v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02683v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02683v1",
                "updated": "2024-10-03T17:08:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    8,
                    52,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T17:08:52Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    8,
                    52,
                    3,
                    277,
                    0
                ],
                "title": "DailyDilemmas: Revealing Value Preferences of LLMs with Quandaries of\n  Daily Life",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DailyDilemmas: Revealing Value Preferences of LLMs with Quandaries of\n  Daily Life"
                },
                "summary": "As we increasingly seek guidance from LLMs for decision-making in daily life,\nmany of these decisions are not clear-cut and depend significantly on the\npersonal values and ethical standards of the users. We present DailyDilemmas, a\ndataset of 1,360 moral dilemmas encountered in everyday life. Each dilemma\nincludes two possible actions and with each action, the affected parties and\nhuman values invoked. Based on these dilemmas, we consolidated a set of human\nvalues across everyday topics e.g., interpersonal relationships, workplace, and\nenvironmental issues. We evaluated LLMs on these dilemmas to determine what\naction they will take and the values represented by these actions. Then, we\nanalyzed these values through the lens of five popular theories inspired by\nsociology, psychology and philosophy. These theories are: World Value Survey,\nMoral Foundation Theory, Maslow's Hierarchy of Needs, Aristotle's Virtues, and\nPlutchik Wheel of Emotion. We find that LLMs are most aligned with the\nself-expression over survival values in terms of World Value Survey, care over\nloyalty in Moral Foundation Theory. Interestingly, we find large preferences\ndifferences in models for some core values such as truthfulness e.g.,\nMixtral-8x7B model tends to neglect it by 9.7% while GPT-4-turbo model tends to\nselect it by 9.4%. We also study the recent guidance released by OpenAI\n(ModelSpec), and Anthropic (Constitutional AI) to understand how their released\nprinciples reflect their actual value prioritization when facing nuanced moral\nreasoning in daily-life settings. We find that end users cannot effectively\nsteer such prioritization using system prompts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As we increasingly seek guidance from LLMs for decision-making in daily life,\nmany of these decisions are not clear-cut and depend significantly on the\npersonal values and ethical standards of the users. We present DailyDilemmas, a\ndataset of 1,360 moral dilemmas encountered in everyday life. Each dilemma\nincludes two possible actions and with each action, the affected parties and\nhuman values invoked. Based on these dilemmas, we consolidated a set of human\nvalues across everyday topics e.g., interpersonal relationships, workplace, and\nenvironmental issues. We evaluated LLMs on these dilemmas to determine what\naction they will take and the values represented by these actions. Then, we\nanalyzed these values through the lens of five popular theories inspired by\nsociology, psychology and philosophy. These theories are: World Value Survey,\nMoral Foundation Theory, Maslow's Hierarchy of Needs, Aristotle's Virtues, and\nPlutchik Wheel of Emotion. We find that LLMs are most aligned with the\nself-expression over survival values in terms of World Value Survey, care over\nloyalty in Moral Foundation Theory. Interestingly, we find large preferences\ndifferences in models for some core values such as truthfulness e.g.,\nMixtral-8x7B model tends to neglect it by 9.7% while GPT-4-turbo model tends to\nselect it by 9.4%. We also study the recent guidance released by OpenAI\n(ModelSpec), and Anthropic (Constitutional AI) to understand how their released\nprinciples reflect their actual value prioritization when facing nuanced moral\nreasoning in daily-life settings. We find that end users cannot effectively\nsteer such prioritization using system prompts."
                },
                "authors": [
                    {
                        "name": "Yu Ying Chiu"
                    },
                    {
                        "name": "Liwei Jiang"
                    },
                    {
                        "name": "Yejin Choi"
                    }
                ],
                "author_detail": {
                    "name": "Yejin Choi"
                },
                "author": "Yejin Choi",
                "arxiv_comment": "Preprint. Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02683v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02683v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02678v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02678v1",
                "updated": "2024-10-03T17:04:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    4,
                    48,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T17:04:48Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    4,
                    48,
                    3,
                    277,
                    0
                ],
                "title": "Distilling an End-to-End Voice Assistant Without Instruction Training\n  Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilling an End-to-End Voice Assistant Without Instruction Training\n  Data"
                },
                "summary": "Voice assistants, such as Siri and Google Assistant, typically model audio\nand text separately, resulting in lost speech information and increased\ncomplexity. Recent efforts to address this with end-to-end Speech Large\nLanguage Models (LLMs) trained with supervised finetuning (SFT)\n  have led to models ``forgetting\" capabilities from text-only LLMs. Our work\nproposes an alternative paradigm for training Speech LLMs without instruction\ndata, using the response of a text-only LLM to transcripts as self-supervision.\nImportantly, this process can be performed without annotated responses. We show\nthat our Distilled Voice Assistant (DiVA) generalizes to Spoken Question\nAnswering, Classification, and Translation. Furthermore, we show that DiVA\nbetter meets user preferences, achieving a 72\\% win rate compared with\nstate-of-the-art models like Qwen 2 Audio, despite using $>$100x less training\ncompute.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Voice assistants, such as Siri and Google Assistant, typically model audio\nand text separately, resulting in lost speech information and increased\ncomplexity. Recent efforts to address this with end-to-end Speech Large\nLanguage Models (LLMs) trained with supervised finetuning (SFT)\n  have led to models ``forgetting\" capabilities from text-only LLMs. Our work\nproposes an alternative paradigm for training Speech LLMs without instruction\ndata, using the response of a text-only LLM to transcripts as self-supervision.\nImportantly, this process can be performed without annotated responses. We show\nthat our Distilled Voice Assistant (DiVA) generalizes to Spoken Question\nAnswering, Classification, and Translation. Furthermore, we show that DiVA\nbetter meets user preferences, achieving a 72\\% win rate compared with\nstate-of-the-art models like Qwen 2 Audio, despite using $>$100x less training\ncompute."
                },
                "authors": [
                    {
                        "name": "William Held"
                    },
                    {
                        "name": "Ella Li"
                    },
                    {
                        "name": "Michael Ryan"
                    },
                    {
                        "name": "Weiyan Shi"
                    },
                    {
                        "name": "Yanzhe Zhang"
                    },
                    {
                        "name": "Diyi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Diyi Yang"
                },
                "author": "Diyi Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02678v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02678v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02677v1",
                "updated": "2024-10-03T17:04:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    4,
                    31,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T17:04:31Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    4,
                    31,
                    3,
                    277,
                    0
                ],
                "title": "CulturalBench: a Robust, Diverse and Challenging Benchmark on Measuring\n  the (Lack of) Cultural Knowledge of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CulturalBench: a Robust, Diverse and Challenging Benchmark on Measuring\n  the (Lack of) Cultural Knowledge of LLMs"
                },
                "summary": "To make large language models (LLMs) more helpful across diverse cultures, it\nis essential to have effective cultural knowledge benchmarks to measure and\ntrack our progress. Effective benchmarks need to be robust, diverse, and\nchallenging. We introduce CulturalBench: a set of 1,227 human-written and\nhuman-verified questions for effectively assessing LLMs' cultural knowledge,\ncovering 45 global regions including the underrepresented ones like Bangladesh,\nZimbabwe, and Peru. Questions - each verified by five independent annotators -\nspan 17 diverse topics ranging from food preferences to greeting etiquettes. We\nevaluate models on two setups: CulturalBench-Easy and CulturalBench-Hard which\nshare the same questions but asked differently. We find that LLMs are sensitive\nto such difference in setups (e.g., GPT-4o with 27.3% difference). Compared to\nhuman performance (92.6% accuracy), CulturalBench-Hard is more challenging for\nfrontier LLMs with the best performing model (GPT-4o) at only 61.5% and the\nworst (Llama3-8b) at 21.4%. Moreover, we find that LLMs often struggle with\ntricky questions that have multiple correct answers (e.g., What utensils do the\nChinese usually use?), revealing a tendency to converge to a single answer. Our\nresults also indicate that OpenAI GPT-4o substantially outperform other\nproprietary and open source models in questions related to all but one region\n(Oceania). Nonetheless, all models consistently underperform on questions\nrelated to South America and the Middle East.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To make large language models (LLMs) more helpful across diverse cultures, it\nis essential to have effective cultural knowledge benchmarks to measure and\ntrack our progress. Effective benchmarks need to be robust, diverse, and\nchallenging. We introduce CulturalBench: a set of 1,227 human-written and\nhuman-verified questions for effectively assessing LLMs' cultural knowledge,\ncovering 45 global regions including the underrepresented ones like Bangladesh,\nZimbabwe, and Peru. Questions - each verified by five independent annotators -\nspan 17 diverse topics ranging from food preferences to greeting etiquettes. We\nevaluate models on two setups: CulturalBench-Easy and CulturalBench-Hard which\nshare the same questions but asked differently. We find that LLMs are sensitive\nto such difference in setups (e.g., GPT-4o with 27.3% difference). Compared to\nhuman performance (92.6% accuracy), CulturalBench-Hard is more challenging for\nfrontier LLMs with the best performing model (GPT-4o) at only 61.5% and the\nworst (Llama3-8b) at 21.4%. Moreover, we find that LLMs often struggle with\ntricky questions that have multiple correct answers (e.g., What utensils do the\nChinese usually use?), revealing a tendency to converge to a single answer. Our\nresults also indicate that OpenAI GPT-4o substantially outperform other\nproprietary and open source models in questions related to all but one region\n(Oceania). Nonetheless, all models consistently underperform on questions\nrelated to South America and the Middle East."
                },
                "authors": [
                    {
                        "name": "Yu Ying Chiu"
                    },
                    {
                        "name": "Liwei Jiang"
                    },
                    {
                        "name": "Bill Yuchen Lin"
                    },
                    {
                        "name": "Chan Young Park"
                    },
                    {
                        "name": "Shuyue Stella Li"
                    },
                    {
                        "name": "Sahithya Ravi"
                    },
                    {
                        "name": "Mehar Bhatia"
                    },
                    {
                        "name": "Maria Antoniak"
                    },
                    {
                        "name": "Yulia Tsvetkov"
                    },
                    {
                        "name": "Vered Shwartz"
                    },
                    {
                        "name": "Yejin Choi"
                    }
                ],
                "author_detail": {
                    "name": "Yejin Choi"
                },
                "author": "Yejin Choi",
                "arxiv_comment": "Preprint. Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07950v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07950v2",
                "updated": "2024-10-03T16:54:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    16,
                    54,
                    59,
                    3,
                    277,
                    0
                ],
                "published": "2024-07-10T18:00:05Z",
                "published_parsed": [
                    2024,
                    7,
                    10,
                    18,
                    0,
                    5,
                    2,
                    192,
                    0
                ],
                "title": "Rel-A.I.: An Interaction-Centered Approach To Measuring Human-LM\n  Reliance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rel-A.I.: An Interaction-Centered Approach To Measuring Human-LM\n  Reliance"
                },
                "summary": "The ability to communicate uncertainty, risk, and limitation is crucial for\nthe safety of large language models. However, current evaluations of these\nabilities rely on simple calibration, asking whether the language generated by\nthe model matches appropriate probabilities. Instead, evaluation of this aspect\nof LLM communication should focus on the behaviors of their human\ninterlocutors: how much do they rely on what the LLM says? Here we introduce an\ninteraction-centered evaluation framework called Rel-A.I. (pronounced \"rely\"})\nthat measures whether humans rely on LLM generations. We use this framework to\nstudy how reliance is affected by contextual features of the interaction (e.g,\nthe knowledge domain that is being discussed), or the use of greetings\ncommunicating warmth or competence (e.g., \"I'm happy to help!\"). We find that\ncontextual characteristics significantly affect human reliance behavior. For\nexample, people rely 10% more on LMs when responding to questions involving\ncalculations and rely 30% more on LMs that are perceived as more competent. Our\nresults show that calibration and language quality alone are insufficient in\nevaluating the risks of human-LM interactions, and illustrate the need to\nconsider features of the interactional context.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability to communicate uncertainty, risk, and limitation is crucial for\nthe safety of large language models. However, current evaluations of these\nabilities rely on simple calibration, asking whether the language generated by\nthe model matches appropriate probabilities. Instead, evaluation of this aspect\nof LLM communication should focus on the behaviors of their human\ninterlocutors: how much do they rely on what the LLM says? Here we introduce an\ninteraction-centered evaluation framework called Rel-A.I. (pronounced \"rely\"})\nthat measures whether humans rely on LLM generations. We use this framework to\nstudy how reliance is affected by contextual features of the interaction (e.g,\nthe knowledge domain that is being discussed), or the use of greetings\ncommunicating warmth or competence (e.g., \"I'm happy to help!\"). We find that\ncontextual characteristics significantly affect human reliance behavior. For\nexample, people rely 10% more on LMs when responding to questions involving\ncalculations and rely 30% more on LMs that are perceived as more competent. Our\nresults show that calibration and language quality alone are insufficient in\nevaluating the risks of human-LM interactions, and illustrate the need to\nconsider features of the interactional context."
                },
                "authors": [
                    {
                        "name": "Kaitlyn Zhou"
                    },
                    {
                        "name": "Jena D. Hwang"
                    },
                    {
                        "name": "Xiang Ren"
                    },
                    {
                        "name": "Nouha Dziri"
                    },
                    {
                        "name": "Dan Jurafsky"
                    },
                    {
                        "name": "Maarten Sap"
                    }
                ],
                "author_detail": {
                    "name": "Maarten Sap"
                },
                "author": "Maarten Sap",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.07950v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07950v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02666v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02666v1",
                "updated": "2024-10-03T16:50:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    16,
                    50,
                    30,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T16:50:30Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    16,
                    50,
                    30,
                    3,
                    277,
                    0
                ],
                "title": "AlphaIntegrator: Transformer Action Search for Symbolic Integration\n  Proofs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlphaIntegrator: Transformer Action Search for Symbolic Integration\n  Proofs"
                },
                "summary": "We present the first correct-by-construction learning-based system for\nstep-by-step mathematical integration. The key idea is to learn a policy,\nrepresented by a GPT transformer model, which guides the search for the right\nmathematical integration rule, to be carried out by a symbolic solver.\nConcretely, we introduce a symbolic engine with axiomatically correct actions\non mathematical expressions, as well as the first dataset for step-by-step\nintegration. Our GPT-style transformer model, trained on this synthetic data,\ndemonstrates strong generalization by surpassing its own data generator in\naccuracy and efficiency, using 50% fewer search steps. Our experimental results\nwith SoTA LLMs also demonstrate that the standard approach of fine-tuning LLMs\non a set of question-answer pairs is insufficient for solving this mathematical\ntask. This motivates the importance of discovering creative methods for\ncombining LLMs with symbolic reasoning engines, of which our work is an\ninstance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the first correct-by-construction learning-based system for\nstep-by-step mathematical integration. The key idea is to learn a policy,\nrepresented by a GPT transformer model, which guides the search for the right\nmathematical integration rule, to be carried out by a symbolic solver.\nConcretely, we introduce a symbolic engine with axiomatically correct actions\non mathematical expressions, as well as the first dataset for step-by-step\nintegration. Our GPT-style transformer model, trained on this synthetic data,\ndemonstrates strong generalization by surpassing its own data generator in\naccuracy and efficiency, using 50% fewer search steps. Our experimental results\nwith SoTA LLMs also demonstrate that the standard approach of fine-tuning LLMs\non a set of question-answer pairs is insufficient for solving this mathematical\ntask. This motivates the importance of discovering creative methods for\ncombining LLMs with symbolic reasoning engines, of which our work is an\ninstance."
                },
                "authors": [
                    {
                        "name": "Mert nsal"
                    },
                    {
                        "name": "Timon Gehr"
                    },
                    {
                        "name": "Martin Vechev"
                    }
                ],
                "author_detail": {
                    "name": "Martin Vechev"
                },
                "author": "Martin Vechev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02666v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02666v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11969v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11969v3",
                "updated": "2024-10-03T16:46:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    16,
                    46,
                    9,
                    3,
                    277,
                    0
                ],
                "published": "2024-07-16T17:59:55Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    17,
                    59,
                    55,
                    1,
                    198,
                    0
                ],
                "title": "Does Refusal Training in LLMs Generalize to the Past Tense?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does Refusal Training in LLMs Generalize to the Past Tense?"
                },
                "summary": "Refusal training is widely used to prevent LLMs from generating harmful,\nundesirable, or illegal outputs. We reveal a curious generalization gap in the\ncurrent refusal training approaches: simply reformulating a harmful request in\nthe past tense (e.g., \"How to make a Molotov cocktail?\" to \"How did people make\na Molotov cocktail?\") is often sufficient to jailbreak many state-of-the-art\nLLMs. We systematically evaluate this method on Llama-3 8B, Claude-3.5 Sonnet,\nGPT-3.5 Turbo, Gemma-2 9B, Phi-3-Mini, GPT-4o mini, GPT-4o, o1-mini,\no1-preview, and R2D2 models using GPT-3.5 Turbo as a reformulation model. For\nexample, the success rate of this simple attack on GPT-4o increases from 1%\nusing direct requests to 88% using 20 past tense reformulation attempts on\nharmful requests from JailbreakBench with GPT-4 as a jailbreak judge.\nInterestingly, we also find that reformulations in the future tense are less\neffective, suggesting that refusal guardrails tend to consider past historical\nquestions more benign than hypothetical future questions. Moreover, our\nexperiments on fine-tuning GPT-3.5 Turbo show that defending against past\nreformulations is feasible when past tense examples are explicitly included in\nthe fine-tuning data. Overall, our findings highlight that the widely used\nalignment techniques -- such as SFT, RLHF, and adversarial training -- employed\nto align the studied models can be brittle and do not always generalize as\nintended. We provide code and jailbreak artifacts at\nhttps://github.com/tml-epfl/llm-past-tense.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refusal training is widely used to prevent LLMs from generating harmful,\nundesirable, or illegal outputs. We reveal a curious generalization gap in the\ncurrent refusal training approaches: simply reformulating a harmful request in\nthe past tense (e.g., \"How to make a Molotov cocktail?\" to \"How did people make\na Molotov cocktail?\") is often sufficient to jailbreak many state-of-the-art\nLLMs. We systematically evaluate this method on Llama-3 8B, Claude-3.5 Sonnet,\nGPT-3.5 Turbo, Gemma-2 9B, Phi-3-Mini, GPT-4o mini, GPT-4o, o1-mini,\no1-preview, and R2D2 models using GPT-3.5 Turbo as a reformulation model. For\nexample, the success rate of this simple attack on GPT-4o increases from 1%\nusing direct requests to 88% using 20 past tense reformulation attempts on\nharmful requests from JailbreakBench with GPT-4 as a jailbreak judge.\nInterestingly, we also find that reformulations in the future tense are less\neffective, suggesting that refusal guardrails tend to consider past historical\nquestions more benign than hypothetical future questions. Moreover, our\nexperiments on fine-tuning GPT-3.5 Turbo show that defending against past\nreformulations is feasible when past tense examples are explicitly included in\nthe fine-tuning data. Overall, our findings highlight that the widely used\nalignment techniques -- such as SFT, RLHF, and adversarial training -- employed\nto align the studied models can be brittle and do not always generalize as\nintended. We provide code and jailbreak artifacts at\nhttps://github.com/tml-epfl/llm-past-tense."
                },
                "authors": [
                    {
                        "name": "Maksym Andriushchenko"
                    },
                    {
                        "name": "Nicolas Flammarion"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Flammarion"
                },
                "author": "Nicolas Flammarion",
                "arxiv_comment": "Update in v3: o1-mini and o1-preview results (on top of GPT-4o and\n  Claude 3.5 Sonnet added in v2). We provide code and jailbreak artifacts at\n  https://github.com/tml-epfl/llm-past-tense",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11969v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11969v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02657v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02657v1",
                "updated": "2024-10-03T16:43:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    16,
                    43,
                    17,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T16:43:17Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    16,
                    43,
                    17,
                    3,
                    277,
                    0
                ],
                "title": "Hate Personified: Investigating the role of LLMs in content moderation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hate Personified: Investigating the role of LLMs in content moderation"
                },
                "summary": "For subjective tasks such as hate detection, where people perceive hate\ndifferently, the Large Language Model's (LLM) ability to represent diverse\ngroups is unclear. By including additional context in prompts, we\ncomprehensively analyze LLM's sensitivity to geographical priming, persona\nattributes, and numerical information to assess how well the needs of various\ngroups are reflected. Our findings on two LLMs, five languages, and six\ndatasets reveal that mimicking persona-based attributes leads to annotation\nvariability. Meanwhile, incorporating geographical signals leads to better\nregional alignment. We also find that the LLMs are sensitive to numerical\nanchors, indicating the ability to leverage community-based flagging efforts\nand exposure to adversaries. Our work provides preliminary guidelines and\nhighlights the nuances of applying LLMs in culturally sensitive cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For subjective tasks such as hate detection, where people perceive hate\ndifferently, the Large Language Model's (LLM) ability to represent diverse\ngroups is unclear. By including additional context in prompts, we\ncomprehensively analyze LLM's sensitivity to geographical priming, persona\nattributes, and numerical information to assess how well the needs of various\ngroups are reflected. Our findings on two LLMs, five languages, and six\ndatasets reveal that mimicking persona-based attributes leads to annotation\nvariability. Meanwhile, incorporating geographical signals leads to better\nregional alignment. We also find that the LLMs are sensitive to numerical\nanchors, indicating the ability to leverage community-based flagging efforts\nand exposure to adversaries. Our work provides preliminary guidelines and\nhighlights the nuances of applying LLMs in culturally sensitive cases."
                },
                "authors": [
                    {
                        "name": "Sarah Masud"
                    },
                    {
                        "name": "Sahajpreet Singh"
                    },
                    {
                        "name": "Viktor Hangya"
                    },
                    {
                        "name": "Alexander Fraser"
                    },
                    {
                        "name": "Tanmoy Chakraborty"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Chakraborty"
                },
                "author": "Tanmoy Chakraborty",
                "arxiv_comment": "17 pages, 6 Figures, 13 Tables, EMNLP'24 Mains",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02657v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02657v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02655v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02655v1",
                "updated": "2024-10-03T16:42:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    16,
                    42,
                    6,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T16:42:06Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    16,
                    42,
                    6,
                    3,
                    277,
                    0
                ],
                "title": "Exact Bayesian Inference for Multivariate Spatial Data of Any Size with\n  Application to Air Pollution Monitoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exact Bayesian Inference for Multivariate Spatial Data of Any Size with\n  Application to Air Pollution Monitoring"
                },
                "summary": "Fine particulate matter and aerosol optical thickness are of interest to\natmospheric scientists for understanding air quality and its various\nhealth/environmental impacts. The available data are extremely large, making\nuncertainty quantification in a fully Bayesian framework quite difficult, as\ntraditional implementations do not scale reasonably to the size of the data. We\nspecifically consider roughly 8 million observations obtained from NASA's\nModerate Resolution Imaging Spectroradiometer (MODIS) instrument. To analyze\ndata on this scale, we introduce Scalable Multivariate Exact Posterior\nRegression (SM-EPR) which combines the recently introduced data subset approach\nand Exact Posterior Regression (EPR). EPR is a new Bayesian hierarchical model\nwhere it is possible to sample independent replicates of fixed and random\neffects directly from the posterior without the use of Markov chain Monte Carlo\n(MCMC) or approximate Bayesian techniques. We extend EPR to the multivariate\nspatial context, where the multiple variables may be distributed according to\ndifferent distributions. The combination of the data subset approach with EPR\nallows one to perform exact Bayesian inference without MCMC for effectively any\nsample size. We demonstrate our new SM-EPR method using this motivating big\nremote sensing data application and provide several simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine particulate matter and aerosol optical thickness are of interest to\natmospheric scientists for understanding air quality and its various\nhealth/environmental impacts. The available data are extremely large, making\nuncertainty quantification in a fully Bayesian framework quite difficult, as\ntraditional implementations do not scale reasonably to the size of the data. We\nspecifically consider roughly 8 million observations obtained from NASA's\nModerate Resolution Imaging Spectroradiometer (MODIS) instrument. To analyze\ndata on this scale, we introduce Scalable Multivariate Exact Posterior\nRegression (SM-EPR) which combines the recently introduced data subset approach\nand Exact Posterior Regression (EPR). EPR is a new Bayesian hierarchical model\nwhere it is possible to sample independent replicates of fixed and random\neffects directly from the posterior without the use of Markov chain Monte Carlo\n(MCMC) or approximate Bayesian techniques. We extend EPR to the multivariate\nspatial context, where the multiple variables may be distributed according to\ndifferent distributions. The combination of the data subset approach with EPR\nallows one to perform exact Bayesian inference without MCMC for effectively any\nsample size. We demonstrate our new SM-EPR method using this motivating big\nremote sensing data application and provide several simulations."
                },
                "authors": [
                    {
                        "name": "Madelyn Clinch"
                    },
                    {
                        "name": "Jonathan R. Bradley"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan R. Bradley"
                },
                "author": "Jonathan R. Bradley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02655v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02655v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.16382v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.16382v2",
                "updated": "2024-10-03T16:39:32Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    16,
                    39,
                    32,
                    3,
                    277,
                    0
                ],
                "published": "2024-02-26T08:08:03Z",
                "published_parsed": [
                    2024,
                    2,
                    26,
                    8,
                    8,
                    3,
                    0,
                    57,
                    0
                ],
                "title": "Immunization against harmful fine-tuning attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Immunization against harmful fine-tuning attacks"
                },
                "summary": "Large Language Models (LLMs) are often trained with safety guards intended to\nprevent harmful text generation. However, such safety training can be removed\nby fine-tuning the LLM on harmful datasets. While this emerging threat (harmful\nfine-tuning attacks) has been characterized by previous work, there is little\nunderstanding of how we should proceed in constructing and validating defenses\nagainst these attacks especially in the case where defenders would not have\ncontrol of the fine-tuning process. We introduce a formal framework based on\nthe training budget of an attacker which we call \"Immunization\" conditions.\nUsing a formal characterisation of the harmful fine-tuning problem, we provide\na thorough description of what a successful defense must comprise of and\nestablish a set of guidelines on how rigorous defense research that gives us\nconfidence should proceed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are often trained with safety guards intended to\nprevent harmful text generation. However, such safety training can be removed\nby fine-tuning the LLM on harmful datasets. While this emerging threat (harmful\nfine-tuning attacks) has been characterized by previous work, there is little\nunderstanding of how we should proceed in constructing and validating defenses\nagainst these attacks especially in the case where defenders would not have\ncontrol of the fine-tuning process. We introduce a formal framework based on\nthe training budget of an attacker which we call \"Immunization\" conditions.\nUsing a formal characterisation of the harmful fine-tuning problem, we provide\na thorough description of what a successful defense must comprise of and\nestablish a set of guidelines on how rigorous defense research that gives us\nconfidence should proceed."
                },
                "authors": [
                    {
                        "name": "Domenic Rosati"
                    },
                    {
                        "name": "Jan Wehner"
                    },
                    {
                        "name": "Kai Williams"
                    },
                    {
                        "name": "ukasz Bartoszcze"
                    },
                    {
                        "name": "Jan Batzner"
                    },
                    {
                        "name": "Hassan Sajjad"
                    },
                    {
                        "name": "Frank Rudzicz"
                    }
                ],
                "author_detail": {
                    "name": "Frank Rudzicz"
                },
                "author": "Frank Rudzicz",
                "arxiv_comment": "Published in EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.16382v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.16382v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02653v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02653v1",
                "updated": "2024-10-03T16:36:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    16,
                    36,
                    35,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T16:36:35Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    16,
                    36,
                    35,
                    3,
                    277,
                    0
                ],
                "title": "Measuring and Improving Persuasiveness of Generative Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring and Improving Persuasiveness of Generative Models"
                },
                "summary": "LLMs are increasingly being used in workflows involving generating content to\nbe consumed by humans (e.g., marketing) and also in directly interacting with\nhumans (e.g., through chatbots). The development of such systems that are\ncapable of generating verifiably persuasive messages presents both\nopportunities and challenges for society. On the one hand, such systems could\npositively impact domains like advertising and social good, such as addressing\ndrug addiction, and on the other, they could be misused for spreading\nmisinformation and shaping political opinions. To channel LLMs' impact on\nsociety, we need to develop systems to measure and benchmark their\npersuasiveness. With this motivation, we introduce PersuasionBench and\nPersuasionArena, the first large-scale benchmark and arena containing a battery\nof tasks to measure the persuasion ability of generative models automatically.\nWe investigate to what extent LLMs know and leverage linguistic patterns that\ncan help them generate more persuasive language. Our findings indicate that the\npersuasiveness of LLMs correlates positively with model size, but smaller\nmodels can also be made to have a higher persuasiveness than much larger\nmodels. Notably, targeted training using synthetic and natural datasets\nsignificantly enhances smaller models' persuasive capabilities, challenging\nscale-dependent assumptions. Our findings carry key implications for both model\ndevelopers and policymakers. For instance, while the EU AI Act and California's\nSB-1047 aim to regulate AI models based on the number of floating point\noperations, we demonstrate that simple metrics like this alone fail to capture\nthe full scope of AI's societal impact. We invite the community to explore and\ncontribute to PersuasionArena and PersuasionBench, available at\nhttps://bit.ly/measure-persuasion, to advance our understanding of AI-driven\npersuasion and its societal implications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are increasingly being used in workflows involving generating content to\nbe consumed by humans (e.g., marketing) and also in directly interacting with\nhumans (e.g., through chatbots). The development of such systems that are\ncapable of generating verifiably persuasive messages presents both\nopportunities and challenges for society. On the one hand, such systems could\npositively impact domains like advertising and social good, such as addressing\ndrug addiction, and on the other, they could be misused for spreading\nmisinformation and shaping political opinions. To channel LLMs' impact on\nsociety, we need to develop systems to measure and benchmark their\npersuasiveness. With this motivation, we introduce PersuasionBench and\nPersuasionArena, the first large-scale benchmark and arena containing a battery\nof tasks to measure the persuasion ability of generative models automatically.\nWe investigate to what extent LLMs know and leverage linguistic patterns that\ncan help them generate more persuasive language. Our findings indicate that the\npersuasiveness of LLMs correlates positively with model size, but smaller\nmodels can also be made to have a higher persuasiveness than much larger\nmodels. Notably, targeted training using synthetic and natural datasets\nsignificantly enhances smaller models' persuasive capabilities, challenging\nscale-dependent assumptions. Our findings carry key implications for both model\ndevelopers and policymakers. For instance, while the EU AI Act and California's\nSB-1047 aim to regulate AI models based on the number of floating point\noperations, we demonstrate that simple metrics like this alone fail to capture\nthe full scope of AI's societal impact. We invite the community to explore and\ncontribute to PersuasionArena and PersuasionBench, available at\nhttps://bit.ly/measure-persuasion, to advance our understanding of AI-driven\npersuasion and its societal implications."
                },
                "authors": [
                    {
                        "name": "Somesh Singh"
                    },
                    {
                        "name": "Yaman K Singla"
                    },
                    {
                        "name": "Harini SI"
                    },
                    {
                        "name": "Balaji Krishnamurthy"
                    }
                ],
                "author_detail": {
                    "name": "Balaji Krishnamurthy"
                },
                "author": "Balaji Krishnamurthy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02653v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02653v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02650v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02650v1",
                "updated": "2024-10-03T16:34:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    16,
                    34,
                    46,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T16:34:46Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    16,
                    34,
                    46,
                    3,
                    277,
                    0
                ],
                "title": "Undesirable Memorization in Large Language Models: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Undesirable Memorization in Large Language Models: A Survey"
                },
                "summary": "While recent research increasingly showcases the remarkable capabilities of\nLarge Language Models (LLMs), it's vital to confront their hidden pitfalls.\nAmong these challenges, the issue of memorization stands out, posing\nsignificant ethical and legal risks. In this paper, we presents a\nSystematization of Knowledge (SoK) on the topic of memorization in LLMs.\nMemorization is the effect that a model tends to store and reproduce phrases or\npassages from the training data and has been shown to be the fundamental issue\nto various privacy and security attacks against LLMs.\n  We begin by providing an overview of the literature on the memorization,\nexploring it across five key dimensions: intentionality, degree,\nretrievability, abstraction, and transparency. Next, we discuss the metrics and\nmethods used to measure memorization, followed by an analysis of the factors\nthat contribute to memorization phenomenon. We then examine how memorization\nmanifests itself in specific model architectures and explore strategies for\nmitigating these effects. We conclude our overview by identifying potential\nresearch topics for the near future: to develop methods for balancing\nperformance and privacy in LLMs, and the analysis of memorization in specific\ncontexts, including conversational agents, retrieval-augmented generation,\nmultilingual language models, and diffusion language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While recent research increasingly showcases the remarkable capabilities of\nLarge Language Models (LLMs), it's vital to confront their hidden pitfalls.\nAmong these challenges, the issue of memorization stands out, posing\nsignificant ethical and legal risks. In this paper, we presents a\nSystematization of Knowledge (SoK) on the topic of memorization in LLMs.\nMemorization is the effect that a model tends to store and reproduce phrases or\npassages from the training data and has been shown to be the fundamental issue\nto various privacy and security attacks against LLMs.\n  We begin by providing an overview of the literature on the memorization,\nexploring it across five key dimensions: intentionality, degree,\nretrievability, abstraction, and transparency. Next, we discuss the metrics and\nmethods used to measure memorization, followed by an analysis of the factors\nthat contribute to memorization phenomenon. We then examine how memorization\nmanifests itself in specific model architectures and explore strategies for\nmitigating these effects. We conclude our overview by identifying potential\nresearch topics for the near future: to develop methods for balancing\nperformance and privacy in LLMs, and the analysis of memorization in specific\ncontexts, including conversational agents, retrieval-augmented generation,\nmultilingual language models, and diffusion language models."
                },
                "authors": [
                    {
                        "name": "Ali Satvaty"
                    },
                    {
                        "name": "Suzan Verberne"
                    },
                    {
                        "name": "Fatih Turkmen"
                    }
                ],
                "author_detail": {
                    "name": "Fatih Turkmen"
                },
                "author": "Fatih Turkmen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02650v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02650v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02649v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02649v1",
                "updated": "2024-10-03T16:34:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    16,
                    34,
                    26,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T16:34:26Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    16,
                    34,
                    26,
                    3,
                    277,
                    0
                ],
                "title": "Stochastic Gradient Variational Bayes in the Stochastic Blockmodel",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic Gradient Variational Bayes in the Stochastic Blockmodel"
                },
                "summary": "Stochastic variational Bayes algorithms have become very popular in the\nmachine learning literature, particularly in the context of nonparametric\nBayesian inference. These algorithms replace the true but intractable posterior\ndistribution with the best (in the sense of Kullback-Leibler divergence) member\nof a tractable family of distributions, using stochastic gradient algorithms to\nperform the optimization step. stochastic variational Bayes inference\nimplicitly trades off computational speed for accuracy, but the loss of\naccuracy is highly model (and even dataset) specific. In this paper we carry\nout an empirical evaluation of this trade off in the context of stochastic\nblockmodels, which are a widely used class of probabilistic models for network\nand relational data. Our experiments indicate that, in the context of\nstochastic blockmodels, relatively large subsamples are required for these\nalgorithms to find accurate approximations of the posterior, and that even then\nthe quality of the approximations provided by stochastic gradient variational\nalgorithms can be highly variable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic variational Bayes algorithms have become very popular in the\nmachine learning literature, particularly in the context of nonparametric\nBayesian inference. These algorithms replace the true but intractable posterior\ndistribution with the best (in the sense of Kullback-Leibler divergence) member\nof a tractable family of distributions, using stochastic gradient algorithms to\nperform the optimization step. stochastic variational Bayes inference\nimplicitly trades off computational speed for accuracy, but the loss of\naccuracy is highly model (and even dataset) specific. In this paper we carry\nout an empirical evaluation of this trade off in the context of stochastic\nblockmodels, which are a widely used class of probabilistic models for network\nand relational data. Our experiments indicate that, in the context of\nstochastic blockmodels, relatively large subsamples are required for these\nalgorithms to find accurate approximations of the posterior, and that even then\nthe quality of the approximations provided by stochastic gradient variational\nalgorithms can be highly variable."
                },
                "authors": [
                    {
                        "name": "Pedro Regueiro"
                    },
                    {
                        "name": "Abel Rodrguez"
                    },
                    {
                        "name": "Juan Sosa"
                    }
                ],
                "author_detail": {
                    "name": "Juan Sosa"
                },
                "author": "Juan Sosa",
                "arxiv_comment": "33 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02649v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02649v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02026v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02026v2",
                "updated": "2024-10-03T16:31:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    16,
                    31,
                    59,
                    3,
                    277,
                    0
                ],
                "published": "2024-09-03T16:20:22Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    16,
                    20,
                    22,
                    1,
                    247,
                    0
                ],
                "title": "Foundations of Large Language Model Compression -- Part 1: Weight\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundations of Large Language Model Compression -- Part 1: Weight\n  Quantization"
                },
                "summary": "In recent years, compression of large language models (LLMs) has emerged as\nan important problem to enable language model deployment on\nresource-constrained devices, reduce computational costs, and mitigate the\nenvironmental footprint of large-scale AI infrastructure. In this paper, we lay\ndown the foundation for LLM quantization from a convex optimization perspective\nand propose a quantization technique that builds on this foundation for optimum\nquantization outcomes. Our quantization framework, CVXQ, scales to models\ncontaining hundreds of billions of weight parameters and provides users with\nthe flexibility to compress models to any specified model size, post-training.\nA reference implementation of CVXQ can be obtained from github.com/seannz/cvxq.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, compression of large language models (LLMs) has emerged as\nan important problem to enable language model deployment on\nresource-constrained devices, reduce computational costs, and mitigate the\nenvironmental footprint of large-scale AI infrastructure. In this paper, we lay\ndown the foundation for LLM quantization from a convex optimization perspective\nand propose a quantization technique that builds on this foundation for optimum\nquantization outcomes. Our quantization framework, CVXQ, scales to models\ncontaining hundreds of billions of weight parameters and provides users with\nthe flexibility to compress models to any specified model size, post-training.\nA reference implementation of CVXQ can be obtained from github.com/seannz/cvxq."
                },
                "authors": [
                    {
                        "name": "Sean I. Young"
                    }
                ],
                "author_detail": {
                    "name": "Sean I. Young"
                },
                "author": "Sean I. Young",
                "arxiv_comment": "Preprint. 17 pages, 4 figures, 5 appendices",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02026v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02026v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02644v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02644v1",
                "updated": "2024-10-03T16:30:47Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    16,
                    30,
                    47,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T16:30:47Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    16,
                    30,
                    47,
                    3,
                    277,
                    0
                ],
                "title": "Agent Security Bench (ASB): Formalizing and Benchmarking Attacks and\n  Defenses in LLM-based Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent Security Bench (ASB): Formalizing and Benchmarking Attacks and\n  Defenses in LLM-based Agents"
                },
                "summary": "Although LLM-based agents, powered by Large Language Models (LLMs), can use\nexternal tools and memory mechanisms to solve complex real-world tasks, they\nmay also introduce critical security vulnerabilities. However, the existing\nliterature does not comprehensively evaluate attacks and defenses against\nLLM-based agents. To address this, we introduce Agent Security Bench (ASB), a\ncomprehensive framework designed to formalize, benchmark, and evaluate the\nattacks and defenses of LLM-based agents, including 10 scenarios (e.g.,\ne-commerce, autonomous driving, finance), 10 agents targeting the scenarios,\nover 400 tools, 23 different types of attack/defense methods, and 8 evaluation\nmetrics. Based on ASB, we benchmark 10 prompt injection attacks, a memory\npoisoning attack, a novel Plan-of-Thought backdoor attack, a mixed attack, and\n10 corresponding defenses across 13 LLM backbones with nearly 90,000 testing\ncases in total. Our benchmark results reveal critical vulnerabilities in\ndifferent stages of agent operation, including system prompt, user prompt\nhandling, tool usage, and memory retrieval, with the highest average attack\nsuccess rate of 84.30\\%, but limited effectiveness shown in current defenses,\nunveiling important works to be done in terms of agent security for the\ncommunity. Our code can be found at https://github.com/agiresearch/ASB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although LLM-based agents, powered by Large Language Models (LLMs), can use\nexternal tools and memory mechanisms to solve complex real-world tasks, they\nmay also introduce critical security vulnerabilities. However, the existing\nliterature does not comprehensively evaluate attacks and defenses against\nLLM-based agents. To address this, we introduce Agent Security Bench (ASB), a\ncomprehensive framework designed to formalize, benchmark, and evaluate the\nattacks and defenses of LLM-based agents, including 10 scenarios (e.g.,\ne-commerce, autonomous driving, finance), 10 agents targeting the scenarios,\nover 400 tools, 23 different types of attack/defense methods, and 8 evaluation\nmetrics. Based on ASB, we benchmark 10 prompt injection attacks, a memory\npoisoning attack, a novel Plan-of-Thought backdoor attack, a mixed attack, and\n10 corresponding defenses across 13 LLM backbones with nearly 90,000 testing\ncases in total. Our benchmark results reveal critical vulnerabilities in\ndifferent stages of agent operation, including system prompt, user prompt\nhandling, tool usage, and memory retrieval, with the highest average attack\nsuccess rate of 84.30\\%, but limited effectiveness shown in current defenses,\nunveiling important works to be done in terms of agent security for the\ncommunity. Our code can be found at https://github.com/agiresearch/ASB."
                },
                "authors": [
                    {
                        "name": "Hanrong Zhang"
                    },
                    {
                        "name": "Jingyuan Huang"
                    },
                    {
                        "name": "Kai Mei"
                    },
                    {
                        "name": "Yifei Yao"
                    },
                    {
                        "name": "Zhenting Wang"
                    },
                    {
                        "name": "Chenlu Zhan"
                    },
                    {
                        "name": "Hongwei Wang"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02644v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02644v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02642v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02642v1",
                "updated": "2024-10-03T16:25:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    16,
                    25,
                    37,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T16:25:37Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    16,
                    25,
                    37,
                    3,
                    277,
                    0
                ],
                "title": "Attention in Large Language Models Yields Efficient Zero-Shot Re-Rankers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention in Large Language Models Yields Efficient Zero-Shot Re-Rankers"
                },
                "summary": "Information retrieval (IR) systems have played a vital role in modern digital\nlife and have cemented their continued usefulness in this new era of generative\nAI via retrieval-augmented generation. With strong language processing\ncapabilities and remarkable versatility, large language models (LLMs) have\nbecome popular choices for zero-shot re-ranking in IR systems. So far,\nLLM-based re-ranking methods rely on strong generative capabilities, which\nrestricts their use to either specialized or powerful proprietary models. Given\nthese restrictions, we ask: is autoregressive generation necessary and optimal\nfor LLMs to perform re-ranking? We hypothesize that there are abundant signals\nrelevant to re-ranking within LLMs that might not be used to their full\npotential via generation. To more directly leverage such signals, we propose\nin-context re-ranking (ICR), a novel method that leverages the change in\nattention pattern caused by the search query for accurate and efficient\nre-ranking. To mitigate the intrinsic biases in LLMs, we propose a calibration\nmethod using a content-free query. Due to the absence of generation, ICR only\nrequires two ($O(1)$) forward passes to re-rank $N$ documents, making it\nsubstantially more efficient than generative re-ranking methods that require at\nleast $O(N)$ forward passes. Our novel design also enables ICR to be applied to\nany LLM without specialized training while guaranteeing a well-formed ranking.\nExtensive experiments with two popular open-weight LLMs on standard single-hop\nand multi-hop information retrieval benchmarks show that ICR outperforms\nRankGPT while cutting the latency by more than 60% in practice. Through\ndetailed analyses, we show that ICR's performance is specially strong on tasks\nthat require more complex re-ranking signals. Our findings call for further\nexploration on novel ways of utilizing open-weight LLMs beyond text generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information retrieval (IR) systems have played a vital role in modern digital\nlife and have cemented their continued usefulness in this new era of generative\nAI via retrieval-augmented generation. With strong language processing\ncapabilities and remarkable versatility, large language models (LLMs) have\nbecome popular choices for zero-shot re-ranking in IR systems. So far,\nLLM-based re-ranking methods rely on strong generative capabilities, which\nrestricts their use to either specialized or powerful proprietary models. Given\nthese restrictions, we ask: is autoregressive generation necessary and optimal\nfor LLMs to perform re-ranking? We hypothesize that there are abundant signals\nrelevant to re-ranking within LLMs that might not be used to their full\npotential via generation. To more directly leverage such signals, we propose\nin-context re-ranking (ICR), a novel method that leverages the change in\nattention pattern caused by the search query for accurate and efficient\nre-ranking. To mitigate the intrinsic biases in LLMs, we propose a calibration\nmethod using a content-free query. Due to the absence of generation, ICR only\nrequires two ($O(1)$) forward passes to re-rank $N$ documents, making it\nsubstantially more efficient than generative re-ranking methods that require at\nleast $O(N)$ forward passes. Our novel design also enables ICR to be applied to\nany LLM without specialized training while guaranteeing a well-formed ranking.\nExtensive experiments with two popular open-weight LLMs on standard single-hop\nand multi-hop information retrieval benchmarks show that ICR outperforms\nRankGPT while cutting the latency by more than 60% in practice. Through\ndetailed analyses, we show that ICR's performance is specially strong on tasks\nthat require more complex re-ranking signals. Our findings call for further\nexploration on novel ways of utilizing open-weight LLMs beyond text generation."
                },
                "authors": [
                    {
                        "name": "Shijie Chen"
                    },
                    {
                        "name": "Bernal Jimnez Gutirrez"
                    },
                    {
                        "name": "Yu Su"
                    }
                ],
                "author_detail": {
                    "name": "Yu Su"
                },
                "author": "Yu Su",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02642v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02642v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.07752v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.07752v3",
                "updated": "2024-10-03T16:25:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    16,
                    25,
                    25,
                    3,
                    277,
                    0
                ],
                "published": "2023-10-11T18:00:00Z",
                "published_parsed": [
                    2023,
                    10,
                    11,
                    18,
                    0,
                    0,
                    2,
                    284,
                    0
                ],
                "title": "Precision-Machine Learning for the Matrix Element Method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Precision-Machine Learning for the Matrix Element Method"
                },
                "summary": "The matrix element method is the LHC inference method of choice for limited\nstatistics. We present a dedicated machine learning framework, based on\nefficient phase-space integration, a learned acceptance and transfer function.\nIt is based on a choice of INN and diffusion networks, and a transformer to\nsolve jet combinatorics. We showcase this setup for the CP-phase of the top\nYukawa coupling in associated Higgs and single-top production.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The matrix element method is the LHC inference method of choice for limited\nstatistics. We present a dedicated machine learning framework, based on\nefficient phase-space integration, a learned acceptance and transfer function.\nIt is based on a choice of INN and diffusion networks, and a transformer to\nsolve jet combinatorics. We showcase this setup for the CP-phase of the top\nYukawa coupling in associated Higgs and single-top production."
                },
                "authors": [
                    {
                        "name": "Theo Heimel"
                    },
                    {
                        "name": "Nathan Huetsch"
                    },
                    {
                        "name": "Ramon Winterhalder"
                    },
                    {
                        "name": "Tilman Plehn"
                    },
                    {
                        "name": "Anja Butter"
                    }
                ],
                "author_detail": {
                    "name": "Anja Butter"
                },
                "author": "Anja Butter",
                "arxiv_comment": "26 pages, 12 figures, v2: update references, v3: include evaluation\n  on Herwig",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.07752v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.07752v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02640v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02640v1",
                "updated": "2024-10-03T16:24:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    16,
                    24,
                    20,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T16:24:20Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    16,
                    24,
                    20,
                    3,
                    277,
                    0
                ],
                "title": "Diffusion-based Extreme Image Compression with Compressed Feature\n  Initialization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based Extreme Image Compression with Compressed Feature\n  Initialization"
                },
                "summary": "Diffusion-based extreme image compression methods have achieved impressive\nperformance at extremely low bitrates. However, constrained by the iterative\ndenoising process that starts from pure noise, these methods are limited in\nboth fidelity and efficiency. To address these two issues, we present Relay\nResidual Diffusion Extreme Image Compression (RDEIC), which leverages\ncompressed feature initialization and residual diffusion. Specifically, we\nfirst use the compressed latent features of the image with added noise, instead\nof pure noise, as the starting point to eliminate the unnecessary initial\nstages of the denoising process. Second, we design a novel relay residual\ndiffusion that reconstructs the raw image by iteratively removing the added\nnoise and the residual between the compressed and target latent features.\nNotably, our relay residual diffusion network seamlessly integrates pre-trained\nstable diffusion to leverage its robust generative capability for high-quality\nreconstruction. Third, we propose a fixed-step fine-tuning strategy to\neliminate the discrepancy between the training and inference phases, further\nimproving the reconstruction quality. Extensive experiments demonstrate that\nthe proposed RDEIC achieves state-of-the-art visual quality and outperforms\nexisting diffusion-based extreme image compression methods in both fidelity and\nefficiency. The source code will be provided in\nhttps://github.com/huai-chang/RDEIC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based extreme image compression methods have achieved impressive\nperformance at extremely low bitrates. However, constrained by the iterative\ndenoising process that starts from pure noise, these methods are limited in\nboth fidelity and efficiency. To address these two issues, we present Relay\nResidual Diffusion Extreme Image Compression (RDEIC), which leverages\ncompressed feature initialization and residual diffusion. Specifically, we\nfirst use the compressed latent features of the image with added noise, instead\nof pure noise, as the starting point to eliminate the unnecessary initial\nstages of the denoising process. Second, we design a novel relay residual\ndiffusion that reconstructs the raw image by iteratively removing the added\nnoise and the residual between the compressed and target latent features.\nNotably, our relay residual diffusion network seamlessly integrates pre-trained\nstable diffusion to leverage its robust generative capability for high-quality\nreconstruction. Third, we propose a fixed-step fine-tuning strategy to\neliminate the discrepancy between the training and inference phases, further\nimproving the reconstruction quality. Extensive experiments demonstrate that\nthe proposed RDEIC achieves state-of-the-art visual quality and outperforms\nexisting diffusion-based extreme image compression methods in both fidelity and\nefficiency. The source code will be provided in\nhttps://github.com/huai-chang/RDEIC."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Li"
                    },
                    {
                        "name": "Yanhui Zhou"
                    },
                    {
                        "name": "Hao Wei"
                    },
                    {
                        "name": "Chenyang Ge"
                    },
                    {
                        "name": "Ajmal Mian"
                    }
                ],
                "author_detail": {
                    "name": "Ajmal Mian"
                },
                "author": "Ajmal Mian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02640v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02640v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02631v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02631v1",
                "updated": "2024-10-03T16:15:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    16,
                    15,
                    4,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T16:15:04Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    16,
                    15,
                    4,
                    3,
                    277,
                    0
                ],
                "title": "Large Language Model for Multi-Domain Translation: Benchmarking and\n  Domain CoT Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model for Multi-Domain Translation: Benchmarking and\n  Domain CoT Fine-tuning"
                },
                "summary": "Achieving consistent high-quality machine translation (MT) across diverse\ndomains remains a significant challenge, primarily due to the limited and\nimbalanced parallel training data available in various domains. While large\nlanguage models (LLMs) have demonstrated impressive general understanding and\ngeneration abilities, their potential in multi-domain MT is under-explored. We\nestablish a comprehensive benchmark for multi-domain translation, featuring 25\nGerman$\\Leftrightarrow$English and 22 Chinese$\\Leftrightarrow$English test sets\nrespectively covering 15 domains. Our evaluation of prominent LLMs reveals a\ndiscernible performance gap against traditional MT systems, highlighting domain\noverfitting and catastrophic forgetting issues after fine-tuning on\ndomain-limited corpora. To mitigate this, we propose a domain Chain of Thought\n(CoT) fine-tuning technique that utilizes the intrinsic multi-domain\nintelligence of LLMs to improve translation performance. This method inspires\nthe LLM to perceive domain information from the source text, which then serves\nas a helpful hint to guide the translation process. Despite being trained on a\nsmall dataset of four domains, our CoT fine-tune approach achieves notable\nenhancements in translation accuracy and domain robustness than traditional\nfine-tuning, as evidenced by an average 1.53 BLEU score increase in over 20\nGerman$\\rightarrow$English distinct out-of-domain tests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Achieving consistent high-quality machine translation (MT) across diverse\ndomains remains a significant challenge, primarily due to the limited and\nimbalanced parallel training data available in various domains. While large\nlanguage models (LLMs) have demonstrated impressive general understanding and\ngeneration abilities, their potential in multi-domain MT is under-explored. We\nestablish a comprehensive benchmark for multi-domain translation, featuring 25\nGerman$\\Leftrightarrow$English and 22 Chinese$\\Leftrightarrow$English test sets\nrespectively covering 15 domains. Our evaluation of prominent LLMs reveals a\ndiscernible performance gap against traditional MT systems, highlighting domain\noverfitting and catastrophic forgetting issues after fine-tuning on\ndomain-limited corpora. To mitigate this, we propose a domain Chain of Thought\n(CoT) fine-tuning technique that utilizes the intrinsic multi-domain\nintelligence of LLMs to improve translation performance. This method inspires\nthe LLM to perceive domain information from the source text, which then serves\nas a helpful hint to guide the translation process. Despite being trained on a\nsmall dataset of four domains, our CoT fine-tune approach achieves notable\nenhancements in translation accuracy and domain robustness than traditional\nfine-tuning, as evidenced by an average 1.53 BLEU score increase in over 20\nGerman$\\rightarrow$English distinct out-of-domain tests."
                },
                "authors": [
                    {
                        "name": "Tianxiang Hu"
                    },
                    {
                        "name": "Pei Zhang"
                    },
                    {
                        "name": "Baosong Yang"
                    },
                    {
                        "name": "Jun Xie"
                    },
                    {
                        "name": "Derek F. Wong"
                    },
                    {
                        "name": "Rui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Wang"
                },
                "author": "Rui Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02631v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02631v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.15284v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.15284v5",
                "updated": "2024-10-03T16:13:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    16,
                    13,
                    55,
                    3,
                    277,
                    0
                ],
                "published": "2024-01-27T03:53:25Z",
                "published_parsed": [
                    2024,
                    1,
                    27,
                    3,
                    53,
                    25,
                    5,
                    27,
                    0
                ],
                "title": "Beyond principlism: Practical strategies for ethical AI use in research\n  practices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond principlism: Practical strategies for ethical AI use in research\n  practices"
                },
                "summary": "The rapid adoption of generative artificial intelligence (AI) in scientific\nresearch, particularly large language models (LLMs), has outpaced the\ndevelopment of ethical guidelines, leading to a Triple-Too problem: too many\nhigh-level ethical initiatives, too abstract principles lacking contextual and\npractical relevance, and too much focus on restrictions and risks over benefits\nand utilities. Existing approaches, including principlism (reliance on abstract\nethical principles), formalism (rigid application of rules), and technical\nsolutionism (overemphasis on technological fixes), offer little practical\nguidance for addressing ethical challenges of AI in scientific research\npractices. To bridge the gap between abstract principles and day-to-day\nresearch practices, a user-centered, realism-inspired approach is proposed\nhere. It outlines five specific goals for ethical AI use: 1) understanding\nmodel training and output, including bias mitigation strategies; 2) respecting\nprivacy, confidentiality, and copyright; 3) avoiding plagiarism and policy\nviolations; 4) applying AI beneficially compared to alternatives; and 5) using\nAI transparently and reproducibly. Each goal is accompanied by actionable\nstrategies and realistic cases of misuse and corrective measures. I argue that\nethical AI application requires evaluating its utility against existing\nalternatives rather than isolated performance metrics. Additionally, I propose\ndocumentation guidelines to enhance transparency and reproducibility in\nAI-assisted research. Moving forward, we need targeted professional\ndevelopment, training programs, and balanced enforcement mechanisms to promote\nresponsible AI use while fostering innovation. By refining these ethical\nguidelines and adapting them to emerging AI capabilities, we can accelerate\nscientific progress without compromising research integrity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid adoption of generative artificial intelligence (AI) in scientific\nresearch, particularly large language models (LLMs), has outpaced the\ndevelopment of ethical guidelines, leading to a Triple-Too problem: too many\nhigh-level ethical initiatives, too abstract principles lacking contextual and\npractical relevance, and too much focus on restrictions and risks over benefits\nand utilities. Existing approaches, including principlism (reliance on abstract\nethical principles), formalism (rigid application of rules), and technical\nsolutionism (overemphasis on technological fixes), offer little practical\nguidance for addressing ethical challenges of AI in scientific research\npractices. To bridge the gap between abstract principles and day-to-day\nresearch practices, a user-centered, realism-inspired approach is proposed\nhere. It outlines five specific goals for ethical AI use: 1) understanding\nmodel training and output, including bias mitigation strategies; 2) respecting\nprivacy, confidentiality, and copyright; 3) avoiding plagiarism and policy\nviolations; 4) applying AI beneficially compared to alternatives; and 5) using\nAI transparently and reproducibly. Each goal is accompanied by actionable\nstrategies and realistic cases of misuse and corrective measures. I argue that\nethical AI application requires evaluating its utility against existing\nalternatives rather than isolated performance metrics. Additionally, I propose\ndocumentation guidelines to enhance transparency and reproducibility in\nAI-assisted research. Moving forward, we need targeted professional\ndevelopment, training programs, and balanced enforcement mechanisms to promote\nresponsible AI use while fostering innovation. By refining these ethical\nguidelines and adapting them to emerging AI capabilities, we can accelerate\nscientific progress without compromising research integrity."
                },
                "authors": [
                    {
                        "name": "Zhicheng Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Lin"
                },
                "author": "Zhicheng Lin",
                "arxiv_doi": "10.1007/s43681-024-00585-5",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s43681-024-00585-5",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2401.15284v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.15284v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted in: AI and Ethics. 20 pages, 1 figure, 3 tables, 2 boxes",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.08702v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.08702v4",
                "updated": "2024-10-03T16:11:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    16,
                    11,
                    43,
                    3,
                    277,
                    0
                ],
                "published": "2024-02-13T16:38:01Z",
                "published_parsed": [
                    2024,
                    2,
                    13,
                    16,
                    38,
                    1,
                    1,
                    44,
                    0
                ],
                "title": "PRompt Optimization in Multi-Step Tasks (PROMST): Integrating Human\n  Feedback and Heuristic-based Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRompt Optimization in Multi-Step Tasks (PROMST): Integrating Human\n  Feedback and Heuristic-based Sampling"
                },
                "summary": "Prompt optimization aims to find the best prompt to a large language model\n(LLM) for a given task. LLMs have been successfully used to help find and\nimprove prompt candidates for single-step tasks. However, realistic tasks for\nagents are multi-step and introduce new challenges: (1) Prompt content is\nlikely to be more extensive and complex, making it more difficult for LLMs to\nanalyze errors, (2) the impact of an individual step is difficult to evaluate,\nand (3) different people may have varied preferences about task execution.\nWhile humans struggle to optimize prompts, they are good at providing feedback\nabout LLM outputs; we therefore introduce a new LLM-driven discrete prompt\noptimization framework PRompt Optimization in Multi-Step Tasks (PROMST) that\nincorporates human-designed feedback rules to automatically offer direct\nsuggestions for improvement. We also use an extra learned heuristic model that\npredicts prompt performance to efficiently sample from prompt candidates. This\napproach significantly outperforms both human-engineered prompts and several\nother prompt optimization methods across 11 representative multi-step tasks (an\naverage 10.6\\%-29.3\\% improvement to current best methods on five LLMs\nrespectively). We believe our work can serve as a benchmark for automatic\nprompt optimization for LLM-driven multi-step tasks. Datasets and Codes are\navailable at https://github.com/yongchao98/PROMST. Project Page is available at\nhttps://yongchao98.github.io/MIT-REALM-PROMST.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt optimization aims to find the best prompt to a large language model\n(LLM) for a given task. LLMs have been successfully used to help find and\nimprove prompt candidates for single-step tasks. However, realistic tasks for\nagents are multi-step and introduce new challenges: (1) Prompt content is\nlikely to be more extensive and complex, making it more difficult for LLMs to\nanalyze errors, (2) the impact of an individual step is difficult to evaluate,\nand (3) different people may have varied preferences about task execution.\nWhile humans struggle to optimize prompts, they are good at providing feedback\nabout LLM outputs; we therefore introduce a new LLM-driven discrete prompt\noptimization framework PRompt Optimization in Multi-Step Tasks (PROMST) that\nincorporates human-designed feedback rules to automatically offer direct\nsuggestions for improvement. We also use an extra learned heuristic model that\npredicts prompt performance to efficiently sample from prompt candidates. This\napproach significantly outperforms both human-engineered prompts and several\nother prompt optimization methods across 11 representative multi-step tasks (an\naverage 10.6\\%-29.3\\% improvement to current best methods on five LLMs\nrespectively). We believe our work can serve as a benchmark for automatic\nprompt optimization for LLM-driven multi-step tasks. Datasets and Codes are\navailable at https://github.com/yongchao98/PROMST. Project Page is available at\nhttps://yongchao98.github.io/MIT-REALM-PROMST."
                },
                "authors": [
                    {
                        "name": "Yongchao Chen"
                    },
                    {
                        "name": "Jacob Arkin"
                    },
                    {
                        "name": "Yilun Hao"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Nicholas Roy"
                    },
                    {
                        "name": "Chuchu Fan"
                    }
                ],
                "author_detail": {
                    "name": "Chuchu Fan"
                },
                "author": "Chuchu Fan",
                "arxiv_comment": "62 pages, 14 figures, Published in EMNLP 2024 Main",
                "arxiv_journal_ref": "EMNLP 2024 Main (The 2024 Conference on Empirical Methods on\n  Natural Language Processing )",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.08702v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.08702v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02627v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02627v1",
                "updated": "2024-10-03T16:08:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    16,
                    8,
                    30,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T16:08:30Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    16,
                    8,
                    30,
                    3,
                    277,
                    0
                ],
                "title": "Preparing for Super-Reactivity: Early Fault-Detection in the Development\n  of Exceedingly Complex Reactive Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preparing for Super-Reactivity: Early Fault-Detection in the Development\n  of Exceedingly Complex Reactive Systems"
                },
                "summary": "We introduce the term Super-Reactive Systems to refer to reactive systems\nwhose construction and behavior are complex, constantly changing and evolving,\nand heavily interwoven with other systems and the physical world. Finding\nhidden faults in such systems early in planning and development is critical for\nhuman safety, the environment, society and the economy. However, the complexity\nof the system and its interactions and the absence of adequate technical\ndetails pose a great obstacle. We propose an architecture for models and tools\nto overcome such barriers and enable simulation, systematic analysis, and fault\ndetection and handling, early in the development of super-reactive systems. The\napproach is facilitated by the inference and abstraction capabilities and the\npower and knowledge afforded by large language models and associated AI tools.\nIt is based on: (i) deferred, just-in-time interpretation of model elements\nthat are stored in natural language form, and (ii) early capture of tacit\ninterdependencies among seemingly orthogonal requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the term Super-Reactive Systems to refer to reactive systems\nwhose construction and behavior are complex, constantly changing and evolving,\nand heavily interwoven with other systems and the physical world. Finding\nhidden faults in such systems early in planning and development is critical for\nhuman safety, the environment, society and the economy. However, the complexity\nof the system and its interactions and the absence of adequate technical\ndetails pose a great obstacle. We propose an architecture for models and tools\nto overcome such barriers and enable simulation, systematic analysis, and fault\ndetection and handling, early in the development of super-reactive systems. The\napproach is facilitated by the inference and abstraction capabilities and the\npower and knowledge afforded by large language models and associated AI tools.\nIt is based on: (i) deferred, just-in-time interpretation of model elements\nthat are stored in natural language form, and (ii) early capture of tacit\ninterdependencies among seemingly orthogonal requirements."
                },
                "authors": [
                    {
                        "name": "David Harel"
                    },
                    {
                        "name": "Assaf Marron"
                    }
                ],
                "author_detail": {
                    "name": "Assaf Marron"
                },
                "author": "Assaf Marron",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02627v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02627v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01702v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01702v2",
                "updated": "2024-10-03T16:05:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    16,
                    5,
                    8,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-02T16:12:18Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    16,
                    12,
                    18,
                    2,
                    276,
                    0
                ],
                "title": "$\\mathcal{D(R,O)}$ Grasp: A Unified Representation of Robot and Object\n  Interaction for Cross-Embodiment Dexterous Grasping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$\\mathcal{D(R,O)}$ Grasp: A Unified Representation of Robot and Object\n  Interaction for Cross-Embodiment Dexterous Grasping"
                },
                "summary": "Dexterous grasping is a fundamental yet challenging skill in robotic\nmanipulation, requiring precise interaction between robotic hands and objects.\nIn this paper, we present $\\mathcal{D(R,O)}$ Grasp, a novel framework that\nmodels the interaction between the robotic hand in its grasping pose and the\nobject, enabling broad generalization across various robot hands and object\ngeometries. Our model takes the robot hand's description and object point cloud\nas inputs and efficiently predicts kinematically valid and stable grasps,\ndemonstrating strong adaptability to diverse robot embodiments and object\ngeometries. Extensive experiments conducted in both simulated and real-world\nenvironments validate the effectiveness of our approach, with significant\nimprovements in success rate, grasp diversity, and inference speed across\nmultiple robotic hands. Our method achieves an average success rate of 87.53%\nin simulation in less than one second, tested across three different dexterous\nrobotic hands. In real-world experiments using the LeapHand, the method also\ndemonstrates an average success rate of 89%. $\\mathcal{D(R,O)}$ Grasp provides\na robust solution for dexterous grasping in complex and varied environments.\nThe code, appendix, and videos are available on our project website at\nhttps://nus-lins-lab.github.io/drograspweb/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dexterous grasping is a fundamental yet challenging skill in robotic\nmanipulation, requiring precise interaction between robotic hands and objects.\nIn this paper, we present $\\mathcal{D(R,O)}$ Grasp, a novel framework that\nmodels the interaction between the robotic hand in its grasping pose and the\nobject, enabling broad generalization across various robot hands and object\ngeometries. Our model takes the robot hand's description and object point cloud\nas inputs and efficiently predicts kinematically valid and stable grasps,\ndemonstrating strong adaptability to diverse robot embodiments and object\ngeometries. Extensive experiments conducted in both simulated and real-world\nenvironments validate the effectiveness of our approach, with significant\nimprovements in success rate, grasp diversity, and inference speed across\nmultiple robotic hands. Our method achieves an average success rate of 87.53%\nin simulation in less than one second, tested across three different dexterous\nrobotic hands. In real-world experiments using the LeapHand, the method also\ndemonstrates an average success rate of 89%. $\\mathcal{D(R,O)}$ Grasp provides\na robust solution for dexterous grasping in complex and varied environments.\nThe code, appendix, and videos are available on our project website at\nhttps://nus-lins-lab.github.io/drograspweb/."
                },
                "authors": [
                    {
                        "name": "Zhenyu Wei"
                    },
                    {
                        "name": "Zhixuan Xu"
                    },
                    {
                        "name": "Jingxiang Guo"
                    },
                    {
                        "name": "Yiwen Hou"
                    },
                    {
                        "name": "Chongkai Gao"
                    },
                    {
                        "name": "Zhehao Cai"
                    },
                    {
                        "name": "Jiayu Luo"
                    },
                    {
                        "name": "Lin Shao"
                    }
                ],
                "author_detail": {
                    "name": "Lin Shao"
                },
                "author": "Lin Shao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01702v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01702v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.08492v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.08492v2",
                "updated": "2024-10-03T16:04:36Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    16,
                    4,
                    36,
                    3,
                    277,
                    0
                ],
                "published": "2024-04-12T14:20:57Z",
                "published_parsed": [
                    2024,
                    4,
                    12,
                    14,
                    20,
                    57,
                    4,
                    103,
                    0
                ],
                "title": "Strategic Interactions between Large Language Models-based Agents in\n  Beauty Contests",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strategic Interactions between Large Language Models-based Agents in\n  Beauty Contests"
                },
                "summary": "The growing adoption of large language models (LLMs) presents potential for\ndeeper understanding of human behaviours within game theory frameworks.\nAddressing research gap on multi-player competitive games, this paper examines\nthe strategic interactions among multiple types of LLM-based agents in a\nclassical beauty contest game. LLM-based agents demonstrate varying depth of\nreasoning that fall within a range of level-0 to 1, which are lower than\nexperimental results conducted with human subjects, but they do display similar\nconvergence pattern towards Nash Equilibrium (NE) choice in repeated setting.\nFurther, through variation in group composition of agent types, I found\nenvironment with lower strategic uncertainty enhances convergence for LLM-based\nagents, and having a mixed environment comprises of LLM-based agents of\ndiffering strategic levels accelerates convergence for all. Higher average\npayoffs for the more intelligent agents are usually observed, albeit at the\nexpense of less intelligent agents. The results from game play with simulated\nagents not only convey insights on potential human behaviours under specified\nexperimental set-ups, they also offer valuable understanding of strategic\ninteractions among algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing adoption of large language models (LLMs) presents potential for\ndeeper understanding of human behaviours within game theory frameworks.\nAddressing research gap on multi-player competitive games, this paper examines\nthe strategic interactions among multiple types of LLM-based agents in a\nclassical beauty contest game. LLM-based agents demonstrate varying depth of\nreasoning that fall within a range of level-0 to 1, which are lower than\nexperimental results conducted with human subjects, but they do display similar\nconvergence pattern towards Nash Equilibrium (NE) choice in repeated setting.\nFurther, through variation in group composition of agent types, I found\nenvironment with lower strategic uncertainty enhances convergence for LLM-based\nagents, and having a mixed environment comprises of LLM-based agents of\ndiffering strategic levels accelerates convergence for all. Higher average\npayoffs for the more intelligent agents are usually observed, albeit at the\nexpense of less intelligent agents. The results from game play with simulated\nagents not only convey insights on potential human behaviours under specified\nexperimental set-ups, they also offer valuable understanding of strategic\ninteractions among algorithms."
                },
                "authors": [
                    {
                        "name": "Siting Estee Lu"
                    }
                ],
                "author_detail": {
                    "name": "Siting Estee Lu"
                },
                "author": "Siting Estee Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.08492v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.08492v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.13681v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.13681v2",
                "updated": "2024-10-03T16:01:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    16,
                    1,
                    1,
                    3,
                    277,
                    0
                ],
                "published": "2024-03-20T15:39:54Z",
                "published_parsed": [
                    2024,
                    3,
                    20,
                    15,
                    39,
                    54,
                    2,
                    80,
                    0
                ],
                "title": "PARAMANU-AYN: Pretrain from scratch or Continual Pretraining of LLMs for\n  Legal Domain Adaptation?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PARAMANU-AYN: Pretrain from scratch or Continual Pretraining of LLMs for\n  Legal Domain Adaptation?"
                },
                "summary": "In this paper, we present Paramanu-Ayn, a collection of legal language models\ntrained exclusively on Indian legal case documents. This 97-million-parameter\nAuto-Regressive (AR) decoder-only model was pretrained from scratch with a\ncontext size of 8192 on a single GPU for just 185 hours, achieving an efficient\nMFU of 41.35. We also developed a legal domain specialized BPE tokenizer. We\nevaluated our model using perplexity and zero-shot tasks: case judgment\nprediction with explanation and abstractive case summarization. Paramanu-Ayn\noutperformed Llama-2 7B and Gemini-Pro in case judgment prediction with\nexplanation task on test accuracy by nearly 2 percentage points, despite being\n72 times smaller. In zero-shot abstractive summarization, it surpassed\ndecoder-only LLMs generating fixed-length summaries (5000 tokens) by over 10\npercentage points in BLEU and METEOR metrics, and by nearly 4 percentage points\nin BERTScore. Further evaluations on zero-shot commonsense and mathematical\nbenchmarks showed that Paramanu-Ayn excelled despite being trained exclusively\non legal documents, outperforming Llama-1, Llama-2, and Falcon on\nAGIEVAL-AQuA-RAT and AGIEVAL-SAT-Math tasks. We also instruction-tuned our\nmodel on 10,763 diverse legal tasks, including legal clause generation, legal\ndrafting, case summarization, etc. The Paramanu-Ayn-instruct model scored above\n8 out of 10 in clarity, relevance, completeness, and legal reasoning metrics by\nGPT-3.5-Turbo. We found that our models, were able to learn drafting knowledge\nand generalize to draft legal contracts and legal clauses with limited\ninstruction-tuning. Hence, we conclude that for a strong domain-specialized\ngenerative language model (such as legal), domain specialized pretraining from\nscratch is more cost effective, environmentally friendly, and remains\ncompetitive with larger models or even better than adapting LLMs for legal\ndomain tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present Paramanu-Ayn, a collection of legal language models\ntrained exclusively on Indian legal case documents. This 97-million-parameter\nAuto-Regressive (AR) decoder-only model was pretrained from scratch with a\ncontext size of 8192 on a single GPU for just 185 hours, achieving an efficient\nMFU of 41.35. We also developed a legal domain specialized BPE tokenizer. We\nevaluated our model using perplexity and zero-shot tasks: case judgment\nprediction with explanation and abstractive case summarization. Paramanu-Ayn\noutperformed Llama-2 7B and Gemini-Pro in case judgment prediction with\nexplanation task on test accuracy by nearly 2 percentage points, despite being\n72 times smaller. In zero-shot abstractive summarization, it surpassed\ndecoder-only LLMs generating fixed-length summaries (5000 tokens) by over 10\npercentage points in BLEU and METEOR metrics, and by nearly 4 percentage points\nin BERTScore. Further evaluations on zero-shot commonsense and mathematical\nbenchmarks showed that Paramanu-Ayn excelled despite being trained exclusively\non legal documents, outperforming Llama-1, Llama-2, and Falcon on\nAGIEVAL-AQuA-RAT and AGIEVAL-SAT-Math tasks. We also instruction-tuned our\nmodel on 10,763 diverse legal tasks, including legal clause generation, legal\ndrafting, case summarization, etc. The Paramanu-Ayn-instruct model scored above\n8 out of 10 in clarity, relevance, completeness, and legal reasoning metrics by\nGPT-3.5-Turbo. We found that our models, were able to learn drafting knowledge\nand generalize to draft legal contracts and legal clauses with limited\ninstruction-tuning. Hence, we conclude that for a strong domain-specialized\ngenerative language model (such as legal), domain specialized pretraining from\nscratch is more cost effective, environmentally friendly, and remains\ncompetitive with larger models or even better than adapting LLMs for legal\ndomain tasks."
                },
                "authors": [
                    {
                        "name": "Mitodru Niyogi"
                    },
                    {
                        "name": "Arnab Bhattacharya"
                    }
                ],
                "author_detail": {
                    "name": "Arnab Bhattacharya"
                },
                "author": "Arnab Bhattacharya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.13681v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.13681v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05197v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05197v2",
                "updated": "2024-10-03T15:55:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    55,
                    40,
                    3,
                    277,
                    0
                ],
                "published": "2024-09-08T19:22:58Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    19,
                    22,
                    58,
                    6,
                    252,
                    0
                ],
                "title": "Seemingly Plausible Distractors in Multi-Hop Reasoning: Are Large\n  Language Models Attentive Readers?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seemingly Plausible Distractors in Multi-Hop Reasoning: Are Large\n  Language Models Attentive Readers?"
                },
                "summary": "State-of-the-art Large Language Models (LLMs) are accredited with an\nincreasing number of different capabilities, ranging from reading\ncomprehension, over advanced mathematical and reasoning skills to possessing\nscientific knowledge. In this paper we focus on their multi-hop reasoning\ncapability: the ability to identify and integrate information from multiple\ntextual sources.\n  Given the concerns with the presence of simplifying cues in existing\nmulti-hop reasoning benchmarks, which allow models to circumvent the reasoning\nrequirement, we set out to investigate, whether LLMs are prone to exploiting\nsuch simplifying cues. We find evidence that they indeed circumvent the\nrequirement to perform multi-hop reasoning, but they do so in more subtle ways\nthan what was reported about their fine-tuned pre-trained language model (PLM)\npredecessors. Motivated by this finding, we propose a challenging multi-hop\nreasoning benchmark, by generating seemingly plausible multi-hop reasoning\nchains, which ultimately lead to incorrect answers. We evaluate multiple open\nand proprietary state-of-the-art LLMs, and find that their performance to\nperform multi-hop reasoning is affected, as indicated by up to 45% relative\ndecrease in F1 score when presented with such seemingly plausible alternatives.\nWe conduct a deeper analysis and find evidence that while LLMs tend to ignore\nmisleading lexical cues, misleading reasoning paths indeed present a\nsignificant challenge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State-of-the-art Large Language Models (LLMs) are accredited with an\nincreasing number of different capabilities, ranging from reading\ncomprehension, over advanced mathematical and reasoning skills to possessing\nscientific knowledge. In this paper we focus on their multi-hop reasoning\ncapability: the ability to identify and integrate information from multiple\ntextual sources.\n  Given the concerns with the presence of simplifying cues in existing\nmulti-hop reasoning benchmarks, which allow models to circumvent the reasoning\nrequirement, we set out to investigate, whether LLMs are prone to exploiting\nsuch simplifying cues. We find evidence that they indeed circumvent the\nrequirement to perform multi-hop reasoning, but they do so in more subtle ways\nthan what was reported about their fine-tuned pre-trained language model (PLM)\npredecessors. Motivated by this finding, we propose a challenging multi-hop\nreasoning benchmark, by generating seemingly plausible multi-hop reasoning\nchains, which ultimately lead to incorrect answers. We evaluate multiple open\nand proprietary state-of-the-art LLMs, and find that their performance to\nperform multi-hop reasoning is affected, as indicated by up to 45% relative\ndecrease in F1 score when presented with such seemingly plausible alternatives.\nWe conduct a deeper analysis and find evidence that while LLMs tend to ignore\nmisleading lexical cues, misleading reasoning paths indeed present a\nsignificant challenge."
                },
                "authors": [
                    {
                        "name": "Neeladri Bhuiya"
                    },
                    {
                        "name": "Viktor Schlegel"
                    },
                    {
                        "name": "Stefan Winkler"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Winkler"
                },
                "author": "Stefan Winkler",
                "arxiv_comment": "15 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05197v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05197v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14662v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14662v2",
                "updated": "2024-10-03T15:52:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    52,
                    17,
                    3,
                    277,
                    0
                ],
                "published": "2024-06-20T18:30:09Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    18,
                    30,
                    9,
                    3,
                    172,
                    0
                ],
                "title": "Advantage Alignment Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advantage Alignment Algorithms"
                },
                "summary": "Artificially intelligent agents are increasingly being integrated into human\ndecision-making: from large language model (LLM) assistants to autonomous\nvehicles. These systems often optimize their individual objective, leading to\nconflicts, particularly in general-sum games where naive reinforcement learning\nagents empirically converge to Pareto-suboptimal Nash equilibria. To address\nthis issue, opponent shaping has emerged as a paradigm for finding socially\nbeneficial equilibria in general-sum games. In this work, we introduce\nAdvantage Alignment, a family of algorithms derived from first principles that\nperform opponent shaping efficiently and intuitively. We achieve this by\naligning the advantages of interacting agents, increasing the probability of\nmutually beneficial actions when their interaction has been positive. We prove\nthat existing opponent shaping methods implicitly perform Advantage Alignment.\nCompared to these methods, Advantage Alignment simplifies the mathematical\nformulation of opponent shaping, reduces the computational burden and extends\nto continuous action domains. We demonstrate the effectiveness of our\nalgorithms across a range of social dilemmas, achieving state-of-the-art\ncooperation and robustness against exploitation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificially intelligent agents are increasingly being integrated into human\ndecision-making: from large language model (LLM) assistants to autonomous\nvehicles. These systems often optimize their individual objective, leading to\nconflicts, particularly in general-sum games where naive reinforcement learning\nagents empirically converge to Pareto-suboptimal Nash equilibria. To address\nthis issue, opponent shaping has emerged as a paradigm for finding socially\nbeneficial equilibria in general-sum games. In this work, we introduce\nAdvantage Alignment, a family of algorithms derived from first principles that\nperform opponent shaping efficiently and intuitively. We achieve this by\naligning the advantages of interacting agents, increasing the probability of\nmutually beneficial actions when their interaction has been positive. We prove\nthat existing opponent shaping methods implicitly perform Advantage Alignment.\nCompared to these methods, Advantage Alignment simplifies the mathematical\nformulation of opponent shaping, reduces the computational burden and extends\nto continuous action domains. We demonstrate the effectiveness of our\nalgorithms across a range of social dilemmas, achieving state-of-the-art\ncooperation and robustness against exploitation."
                },
                "authors": [
                    {
                        "name": "Juan Agustin Duque"
                    },
                    {
                        "name": "Milad Aghajohari"
                    },
                    {
                        "name": "Tim Cooijmans"
                    },
                    {
                        "name": "Razvan Ciuca"
                    },
                    {
                        "name": "Tianyu Zhang"
                    },
                    {
                        "name": "Gauthier Gidel"
                    },
                    {
                        "name": "Aaron Courville"
                    }
                ],
                "author_detail": {
                    "name": "Aaron Courville"
                },
                "author": "Aaron Courville",
                "arxiv_comment": "25 Pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14662v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14662v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02613v1",
                "updated": "2024-10-03T15:51:36Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    51,
                    36,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T15:51:36Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    51,
                    36,
                    3,
                    277,
                    0
                ],
                "title": "NL-Eye: Abductive NLI for Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NL-Eye: Abductive NLI for Images"
                },
                "summary": "Will a Visual Language Model (VLM)-based bot warn us about slipping if it\ndetects a wet floor? Recent VLMs have demonstrated impressive capabilities, yet\ntheir ability to infer outcomes and causes remains underexplored. To address\nthis, we introduce NL-Eye, a benchmark designed to assess VLMs' visual\nabductive reasoning skills. NL-Eye adapts the abductive Natural Language\nInference (NLI) task to the visual domain, requiring models to evaluate the\nplausibility of hypothesis images based on a premise image and explain their\ndecisions. NL-Eye consists of 350 carefully curated triplet examples (1,050\nimages) spanning diverse reasoning categories: physical, functional, logical,\nemotional, cultural, and social. The data curation process involved two steps -\nwriting textual descriptions and generating images using text-to-image models,\nboth requiring substantial human involvement to ensure high-quality and\nchallenging scenes. Our experiments show that VLMs struggle significantly on\nNL-Eye, often performing at random baseline levels, while humans excel in both\nplausibility prediction and explanation quality. This demonstrates a deficiency\nin the abductive reasoning capabilities of modern VLMs. NL-Eye represents a\ncrucial step toward developing VLMs capable of robust multimodal reasoning for\nreal-world applications, including accident-prevention bots and generated video\nverification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Will a Visual Language Model (VLM)-based bot warn us about slipping if it\ndetects a wet floor? Recent VLMs have demonstrated impressive capabilities, yet\ntheir ability to infer outcomes and causes remains underexplored. To address\nthis, we introduce NL-Eye, a benchmark designed to assess VLMs' visual\nabductive reasoning skills. NL-Eye adapts the abductive Natural Language\nInference (NLI) task to the visual domain, requiring models to evaluate the\nplausibility of hypothesis images based on a premise image and explain their\ndecisions. NL-Eye consists of 350 carefully curated triplet examples (1,050\nimages) spanning diverse reasoning categories: physical, functional, logical,\nemotional, cultural, and social. The data curation process involved two steps -\nwriting textual descriptions and generating images using text-to-image models,\nboth requiring substantial human involvement to ensure high-quality and\nchallenging scenes. Our experiments show that VLMs struggle significantly on\nNL-Eye, often performing at random baseline levels, while humans excel in both\nplausibility prediction and explanation quality. This demonstrates a deficiency\nin the abductive reasoning capabilities of modern VLMs. NL-Eye represents a\ncrucial step toward developing VLMs capable of robust multimodal reasoning for\nreal-world applications, including accident-prevention bots and generated video\nverification."
                },
                "authors": [
                    {
                        "name": "Mor Ventura"
                    },
                    {
                        "name": "Michael Toker"
                    },
                    {
                        "name": "Nitay Calderon"
                    },
                    {
                        "name": "Zorik Gekhman"
                    },
                    {
                        "name": "Yonatan Bitton"
                    },
                    {
                        "name": "Roi Reichart"
                    }
                ],
                "author_detail": {
                    "name": "Roi Reichart"
                },
                "author": "Roi Reichart",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10960v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10960v3",
                "updated": "2024-10-03T15:48:45Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    48,
                    45,
                    3,
                    277,
                    0
                ],
                "published": "2024-07-15T17:55:42Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    17,
                    55,
                    42,
                    0,
                    197,
                    0
                ],
                "title": "Fast Matrix Multiplications for Lookup Table-Quantized LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Matrix Multiplications for Lookup Table-Quantized LLMs"
                },
                "summary": "The deployment of large language models (LLMs) is often constrained by memory\nbandwidth, where the primary bottleneck is the cost of transferring model\nparameters from the GPU's global memory to its registers. When coupled with\ncustom kernels that fuse the dequantization and matmul operations, weight-only\nquantization can thus enable faster inference by reducing the amount of memory\nmovement. However, developing high-performance kernels for weight-quantized\nLLMs presents substantial challenges, especially when the weights are\ncompressed to non-evenly-divisible bit widths (e.g., 3 bits) with non-uniform,\nlookup table (LUT) quantization. This paper describes FLUTE, a flexible lookup\ntable engine for LUT-quantized LLMs, which uses offline restructuring of the\nquantized weight matrix to minimize bit manipulations associated with\nunpacking, and vectorization and duplication of the lookup table to mitigate\nshared memory bandwidth constraints. At batch sizes < 32 and quantization group\nsize of 128 (typical in LLM inference), the FLUTE kernel can be 2-4x faster\nthan existing GEMM kernels. As an application of FLUTE, we explore a simple\nextension to lookup table-based NormalFloat quantization and apply it to\nquantize LLaMA3 to various configurations, obtaining competitive quantization\nperformance against strong baselines while obtaining an end-to-end throughput\nincrease of 1.5 to 2 times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of large language models (LLMs) is often constrained by memory\nbandwidth, where the primary bottleneck is the cost of transferring model\nparameters from the GPU's global memory to its registers. When coupled with\ncustom kernels that fuse the dequantization and matmul operations, weight-only\nquantization can thus enable faster inference by reducing the amount of memory\nmovement. However, developing high-performance kernels for weight-quantized\nLLMs presents substantial challenges, especially when the weights are\ncompressed to non-evenly-divisible bit widths (e.g., 3 bits) with non-uniform,\nlookup table (LUT) quantization. This paper describes FLUTE, a flexible lookup\ntable engine for LUT-quantized LLMs, which uses offline restructuring of the\nquantized weight matrix to minimize bit manipulations associated with\nunpacking, and vectorization and duplication of the lookup table to mitigate\nshared memory bandwidth constraints. At batch sizes < 32 and quantization group\nsize of 128 (typical in LLM inference), the FLUTE kernel can be 2-4x faster\nthan existing GEMM kernels. As an application of FLUTE, we explore a simple\nextension to lookup table-based NormalFloat quantization and apply it to\nquantize LLaMA3 to various configurations, obtaining competitive quantization\nperformance against strong baselines while obtaining an end-to-end throughput\nincrease of 1.5 to 2 times."
                },
                "authors": [
                    {
                        "name": "Han Guo"
                    },
                    {
                        "name": "William Brandon"
                    },
                    {
                        "name": "Radostin Cholakov"
                    },
                    {
                        "name": "Jonathan Ragan-Kelley"
                    },
                    {
                        "name": "Eric P. Xing"
                    },
                    {
                        "name": "Yoon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Yoon Kim"
                },
                "author": "Yoon Kim",
                "arxiv_comment": "EMNLP 2024 (Findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10960v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10960v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18256v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18256v3",
                "updated": "2024-10-03T15:48:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    48,
                    31,
                    3,
                    277,
                    0
                ],
                "published": "2024-06-26T11:08:17Z",
                "published_parsed": [
                    2024,
                    6,
                    26,
                    11,
                    8,
                    17,
                    2,
                    178,
                    0
                ],
                "title": "Llamipa: An Incremental Discourse Parser",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Llamipa: An Incremental Discourse Parser"
                },
                "summary": "This paper provides the first discourse parsing experiments with a large\nlanguage model(LLM) finetuned on corpora annotated in the style of SDRT\n(Segmented Discourse Representation Theory Asher, 1993; Asher and Lascarides,\n2003). The result is a discourse parser, Llamipa (Llama Incremental Parser),\nthat leverages discourse context, leading to substantial performance gains over\napproaches that use encoder-only models to provide local, context-sensitive\nrepresentations of discourse units. Furthermore, it can process discourse data\nincrementally, which is essential for the eventual use of discourse information\nin downstream tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides the first discourse parsing experiments with a large\nlanguage model(LLM) finetuned on corpora annotated in the style of SDRT\n(Segmented Discourse Representation Theory Asher, 1993; Asher and Lascarides,\n2003). The result is a discourse parser, Llamipa (Llama Incremental Parser),\nthat leverages discourse context, leading to substantial performance gains over\napproaches that use encoder-only models to provide local, context-sensitive\nrepresentations of discourse units. Furthermore, it can process discourse data\nincrementally, which is essential for the eventual use of discourse information\nin downstream tasks."
                },
                "authors": [
                    {
                        "name": "Kate Thompson"
                    },
                    {
                        "name": "Akshay Chaturvedi"
                    },
                    {
                        "name": "Julie Hunter"
                    },
                    {
                        "name": "Nicholas Asher"
                    }
                ],
                "author_detail": {
                    "name": "Nicholas Asher"
                },
                "author": "Nicholas Asher",
                "arxiv_comment": "EMNLP 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18256v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18256v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18164v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18164v3",
                "updated": "2024-10-03T15:46:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    46,
                    16,
                    3,
                    277,
                    0
                ],
                "published": "2024-06-26T08:24:44Z",
                "published_parsed": [
                    2024,
                    6,
                    26,
                    8,
                    24,
                    44,
                    2,
                    178,
                    0
                ],
                "title": "Nebula: A discourse aware Minecraft Builder",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nebula: A discourse aware Minecraft Builder"
                },
                "summary": "When engaging in collaborative tasks, humans efficiently exploit the semantic\nstructure of a conversation to optimize verbal and nonverbal interactions. But\nin recent \"language to code\" or \"language to action\" models, this information\nis lacking. We show how incorporating the prior discourse and nonlinguistic\ncontext of a conversation situated in a nonlinguistic environment can improve\nthe \"language to action\" component of such interactions. We finetune an LLM to\npredict actions based on prior context; our model, Nebula, doubles the\nnet-action F1 score over the baseline on this task of Jayannavar et al.(2020).\nWe also investigate our model's ability to construct shapes and understand\nlocation descriptions using a synthetic dataset",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When engaging in collaborative tasks, humans efficiently exploit the semantic\nstructure of a conversation to optimize verbal and nonverbal interactions. But\nin recent \"language to code\" or \"language to action\" models, this information\nis lacking. We show how incorporating the prior discourse and nonlinguistic\ncontext of a conversation situated in a nonlinguistic environment can improve\nthe \"language to action\" component of such interactions. We finetune an LLM to\npredict actions based on prior context; our model, Nebula, doubles the\nnet-action F1 score over the baseline on this task of Jayannavar et al.(2020).\nWe also investigate our model's ability to construct shapes and understand\nlocation descriptions using a synthetic dataset"
                },
                "authors": [
                    {
                        "name": "Akshay Chaturvedi"
                    },
                    {
                        "name": "Kate Thompson"
                    },
                    {
                        "name": "Nicholas Asher"
                    }
                ],
                "author_detail": {
                    "name": "Nicholas Asher"
                },
                "author": "Nicholas Asher",
                "arxiv_comment": "EMNLP 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18164v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18164v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.08460v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.08460v3",
                "updated": "2024-10-03T15:46:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    46,
                    13,
                    3,
                    277,
                    0
                ],
                "published": "2023-04-17T17:36:35Z",
                "published_parsed": [
                    2023,
                    4,
                    17,
                    17,
                    36,
                    35,
                    0,
                    107,
                    0
                ],
                "title": "LongForm: Effective Instruction Tuning with Reverse Instructions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongForm: Effective Instruction Tuning with Reverse Instructions"
                },
                "summary": "Instruction tuning enables language models to more effectively generalize and\nbetter follow user intent. However, obtaining instruction data is costly and\nchallenging. Prior work employs methods such as expensive human annotation,\ncrowd-sourced datasets with alignment issues, and generating noisy examples via\nLLMs. We introduce the LongForm-C dataset, which is created by reverse\ninstructions. We generate instructions via LLMs for human-written corpus\nexamples using reverse instructions. First we select a diverse set of\nhuman-written documents from corpora such as C4 and Wikipedia; then we generate\ninstructions for these documents via LLMs. This approach provides a cheaper and\ncleaner instruction-tuning dataset with natural output and one suitable for\nlong text generation. Our models outperform 10x larger language models without\ninstruction tuning on tasks such as story/recipe generation and long-form\nquestion answering. Moreover, LongForm models outperform prior\ninstruction-tuned models such as FLAN-T5 and Alpaca by a large margin, and\nimprove language understanding capabilities further. We publicly release our\ndata and models: https://github.com/akoksal/LongForm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction tuning enables language models to more effectively generalize and\nbetter follow user intent. However, obtaining instruction data is costly and\nchallenging. Prior work employs methods such as expensive human annotation,\ncrowd-sourced datasets with alignment issues, and generating noisy examples via\nLLMs. We introduce the LongForm-C dataset, which is created by reverse\ninstructions. We generate instructions via LLMs for human-written corpus\nexamples using reverse instructions. First we select a diverse set of\nhuman-written documents from corpora such as C4 and Wikipedia; then we generate\ninstructions for these documents via LLMs. This approach provides a cheaper and\ncleaner instruction-tuning dataset with natural output and one suitable for\nlong text generation. Our models outperform 10x larger language models without\ninstruction tuning on tasks such as story/recipe generation and long-form\nquestion answering. Moreover, LongForm models outperform prior\ninstruction-tuned models such as FLAN-T5 and Alpaca by a large margin, and\nimprove language understanding capabilities further. We publicly release our\ndata and models: https://github.com/akoksal/LongForm."
                },
                "authors": [
                    {
                        "name": "Abdullatif Kksal"
                    },
                    {
                        "name": "Timo Schick"
                    },
                    {
                        "name": "Anna Korhonen"
                    },
                    {
                        "name": "Hinrich Schtze"
                    }
                ],
                "author_detail": {
                    "name": "Hinrich Schtze"
                },
                "author": "Hinrich Schtze",
                "arxiv_comment": "EMNLP 2024 Findings. This version extends the training with recent\n  LLMs, evaluation with new metrics, and NLU tasks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.08460v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.08460v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12402v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12402v2",
                "updated": "2024-10-03T15:45:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    45,
                    52,
                    3,
                    277,
                    0
                ],
                "published": "2024-07-17T08:28:55Z",
                "published_parsed": [
                    2024,
                    7,
                    17,
                    8,
                    28,
                    55,
                    2,
                    199,
                    0
                ],
                "title": "TurkishMMLU: Measuring Massive Multitask Language Understanding in\n  Turkish",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TurkishMMLU: Measuring Massive Multitask Language Understanding in\n  Turkish"
                },
                "summary": "Multiple choice question answering tasks evaluate the reasoning,\ncomprehension, and mathematical abilities of Large Language Models (LLMs).\nWhile existing benchmarks employ automatic translation for multilingual\nevaluation, this approach is error-prone and potentially introduces culturally\nbiased questions, especially in social sciences. We introduce the first\nmultitask, multiple-choice Turkish QA benchmark, TurkishMMLU, to evaluate LLMs'\nunderstanding of the Turkish language. TurkishMMLU includes over 10,000\nquestions, covering 9 different subjects from Turkish high-school education\ncurricula. These questions are written by curriculum experts, suitable for the\nhigh-school curricula in Turkey, covering subjects ranging from natural\nsciences and math questions to more culturally representative topics such as\nTurkish Literature and the history of the Turkish Republic. We evaluate over 20\nLLMs, including multilingual open-source (e.g., Gemma, Llama, MT5),\nclosed-source (GPT 4o, Claude, Gemini), and Turkish-adapted (e.g., Trendyol)\nmodels. We provide an extensive evaluation, including zero-shot and few-shot\nevaluation of LLMs, chain-of-thought reasoning, and question difficulty\nanalysis along with model performance. We provide an in-depth analysis of the\nTurkish capabilities and limitations of current LLMs to provide insights for\nfuture LLMs for the Turkish language. We publicly release our code for the\ndataset and evaluation: https://github.com/ArdaYueksel/TurkishMMLU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiple choice question answering tasks evaluate the reasoning,\ncomprehension, and mathematical abilities of Large Language Models (LLMs).\nWhile existing benchmarks employ automatic translation for multilingual\nevaluation, this approach is error-prone and potentially introduces culturally\nbiased questions, especially in social sciences. We introduce the first\nmultitask, multiple-choice Turkish QA benchmark, TurkishMMLU, to evaluate LLMs'\nunderstanding of the Turkish language. TurkishMMLU includes over 10,000\nquestions, covering 9 different subjects from Turkish high-school education\ncurricula. These questions are written by curriculum experts, suitable for the\nhigh-school curricula in Turkey, covering subjects ranging from natural\nsciences and math questions to more culturally representative topics such as\nTurkish Literature and the history of the Turkish Republic. We evaluate over 20\nLLMs, including multilingual open-source (e.g., Gemma, Llama, MT5),\nclosed-source (GPT 4o, Claude, Gemini), and Turkish-adapted (e.g., Trendyol)\nmodels. We provide an extensive evaluation, including zero-shot and few-shot\nevaluation of LLMs, chain-of-thought reasoning, and question difficulty\nanalysis along with model performance. We provide an in-depth analysis of the\nTurkish capabilities and limitations of current LLMs to provide insights for\nfuture LLMs for the Turkish language. We publicly release our code for the\ndataset and evaluation: https://github.com/ArdaYueksel/TurkishMMLU."
                },
                "authors": [
                    {
                        "name": "Arda Yksel"
                    },
                    {
                        "name": "Abdullatif Kksal"
                    },
                    {
                        "name": "Ltfi Kerem enel"
                    },
                    {
                        "name": "Anna Korhonen"
                    },
                    {
                        "name": "Hinrich Schtze"
                    }
                ],
                "author_detail": {
                    "name": "Hinrich Schtze"
                },
                "author": "Hinrich Schtze",
                "arxiv_comment": "EMNLP 2024 - Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12402v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12402v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.14741v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.14741v2",
                "updated": "2024-10-03T15:44:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    44,
                    59,
                    3,
                    277,
                    0
                ],
                "published": "2024-04-23T04:47:22Z",
                "published_parsed": [
                    2024,
                    4,
                    23,
                    4,
                    47,
                    22,
                    1,
                    114,
                    0
                ],
                "title": "Generate-on-Graph: Treat LLM as both Agent and KG in Incomplete\n  Knowledge Graph Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generate-on-Graph: Treat LLM as both Agent and KG in Incomplete\n  Knowledge Graph Question Answering"
                },
                "summary": "To address the issues of insufficient knowledge and hallucination in Large\nLanguage Models (LLMs), numerous studies have explored integrating LLMs with\nKnowledge Graphs (KGs). However, these methods are typically evaluated on\nconventional Knowledge Graph Question Answering (KGQA) with complete KGs, where\nall factual triples required for each question are entirely covered by the\ngiven KG. In such cases, LLMs primarily act as an agent to find answer entities\nwithin the KG, rather than effectively integrating the internal knowledge of\nLLMs and external knowledge sources such as KGs. In fact, KGs are often\nincomplete to cover all the knowledge required to answer questions. To simulate\nthese real-world scenarios and evaluate the ability of LLMs to integrate\ninternal and external knowledge, we propose leveraging LLMs for QA under\nIncomplete Knowledge Graph (IKGQA), where the provided KG lacks some of the\nfactual triples for each question, and construct corresponding datasets. To\nhandle IKGQA, we propose a training-free method called Generate-on-Graph (GoG),\nwhich can generate new factual triples while exploring KGs. Specifically, GoG\nperforms reasoning through a Thinking-Searching-Generating framework, which\ntreats LLM as both Agent and KG in IKGQA. Experimental results on two datasets\ndemonstrate that our GoG outperforms all previous methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To address the issues of insufficient knowledge and hallucination in Large\nLanguage Models (LLMs), numerous studies have explored integrating LLMs with\nKnowledge Graphs (KGs). However, these methods are typically evaluated on\nconventional Knowledge Graph Question Answering (KGQA) with complete KGs, where\nall factual triples required for each question are entirely covered by the\ngiven KG. In such cases, LLMs primarily act as an agent to find answer entities\nwithin the KG, rather than effectively integrating the internal knowledge of\nLLMs and external knowledge sources such as KGs. In fact, KGs are often\nincomplete to cover all the knowledge required to answer questions. To simulate\nthese real-world scenarios and evaluate the ability of LLMs to integrate\ninternal and external knowledge, we propose leveraging LLMs for QA under\nIncomplete Knowledge Graph (IKGQA), where the provided KG lacks some of the\nfactual triples for each question, and construct corresponding datasets. To\nhandle IKGQA, we propose a training-free method called Generate-on-Graph (GoG),\nwhich can generate new factual triples while exploring KGs. Specifically, GoG\nperforms reasoning through a Thinking-Searching-Generating framework, which\ntreats LLM as both Agent and KG in IKGQA. Experimental results on two datasets\ndemonstrate that our GoG outperforms all previous methods."
                },
                "authors": [
                    {
                        "name": "Yao Xu"
                    },
                    {
                        "name": "Shizhu He"
                    },
                    {
                        "name": "Jiabei Chen"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Yangqiu Song"
                    },
                    {
                        "name": "Hanghang Tong"
                    },
                    {
                        "name": "Guang Liu"
                    },
                    {
                        "name": "Kang Liu"
                    },
                    {
                        "name": "Jun Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhao"
                },
                "author": "Jun Zhao",
                "arxiv_comment": "Accepted by EMNLP 2024 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.14741v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.14741v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02603v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02603v1",
                "updated": "2024-10-03T15:44:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    44,
                    42,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T15:44:42Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    44,
                    42,
                    3,
                    277,
                    0
                ],
                "title": "Agents' Room: Narrative Generation through Multi-step Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agents' Room: Narrative Generation through Multi-step Collaboration"
                },
                "summary": "Writing compelling fiction is a multifaceted process combining elements such\nas crafting a plot, developing interesting characters, and using evocative\nlanguage. While large language models (LLMs) show promise for story writing,\nthey currently rely heavily on intricate prompting, which limits their use. We\npropose Agents' Room, a generation framework inspired by narrative theory, that\ndecomposes narrative writing into subtasks tackled by specialized agents. To\nillustrate our method, we introduce Tell Me A Story, a high-quality dataset of\ncomplex writing prompts and human-written stories, and a novel evaluation\nframework designed specifically for assessing long narratives. We show that\nAgents' Room generates stories that are preferred by expert evaluators over\nthose produced by baseline systems by leveraging collaboration and\nspecialization to decompose the complex story writing task into tractable\ncomponents. We provide extensive analysis with automated and human-based\nmetrics of the generated output.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Writing compelling fiction is a multifaceted process combining elements such\nas crafting a plot, developing interesting characters, and using evocative\nlanguage. While large language models (LLMs) show promise for story writing,\nthey currently rely heavily on intricate prompting, which limits their use. We\npropose Agents' Room, a generation framework inspired by narrative theory, that\ndecomposes narrative writing into subtasks tackled by specialized agents. To\nillustrate our method, we introduce Tell Me A Story, a high-quality dataset of\ncomplex writing prompts and human-written stories, and a novel evaluation\nframework designed specifically for assessing long narratives. We show that\nAgents' Room generates stories that are preferred by expert evaluators over\nthose produced by baseline systems by leveraging collaboration and\nspecialization to decompose the complex story writing task into tractable\ncomponents. We provide extensive analysis with automated and human-based\nmetrics of the generated output."
                },
                "authors": [
                    {
                        "name": "Fantine Huot"
                    },
                    {
                        "name": "Reinald Kim Amplayo"
                    },
                    {
                        "name": "Jennimaria Palomaki"
                    },
                    {
                        "name": "Alice Shoshana Jakobovits"
                    },
                    {
                        "name": "Elizabeth Clark"
                    },
                    {
                        "name": "Mirella Lapata"
                    }
                ],
                "author_detail": {
                    "name": "Mirella Lapata"
                },
                "author": "Mirella Lapata",
                "arxiv_comment": "Under review as a conference paper at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02603v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02603v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.07743v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.07743v2",
                "updated": "2024-10-03T15:41:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    41,
                    31,
                    3,
                    277,
                    0
                ],
                "published": "2024-02-12T16:05:22Z",
                "published_parsed": [
                    2024,
                    2,
                    12,
                    16,
                    5,
                    22,
                    0,
                    43,
                    0
                ],
                "title": "Local Projections Inference with High-Dimensional Covariates without\n  Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Local Projections Inference with High-Dimensional Covariates without\n  Sparsity"
                },
                "summary": "This paper presents a comprehensive local projections (LP) framework for\nestimating future responses to current shocks, robust to high-dimensional\ncontrols without relying on sparsity assumptions. The approach is applicable to\nvarious settings, including impulse response analysis and\ndifference-in-differences (DiD) estimation. While methods like LASSO exist,\nthey often assume most parameters are exactly zero, limiting their\neffectiveness in dense data generation processes. I propose a novel technique\nincorporating high-dimensional covariates in local projections using the\nOrthogonal Greedy Algorithm with a high-dimensional AIC (OGA+HDAIC) model\nselection method. This approach offers robustness in both sparse and dense\nscenarios, improved interpretability, and more reliable causal inference in\nlocal projections. Simulation studies show superior performance in dense and\npersistent scenarios compared to conventional LP and LASSO-based approaches. In\nan empirical application to Acemoglu, Naidu, Restrepo, and Robinson (2019), I\ndemonstrate efficiency gains and robustness to a large set of controls.\nAdditionally, I examine the effect of subjective beliefs on economic\naggregates, demonstrating robustness to various model specifications. A novel\nstate-dependent analysis reveals that inflation behaves more in line with\nrational expectations in good states, but exhibits more subjective, pessimistic\ndynamics in bad states.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a comprehensive local projections (LP) framework for\nestimating future responses to current shocks, robust to high-dimensional\ncontrols without relying on sparsity assumptions. The approach is applicable to\nvarious settings, including impulse response analysis and\ndifference-in-differences (DiD) estimation. While methods like LASSO exist,\nthey often assume most parameters are exactly zero, limiting their\neffectiveness in dense data generation processes. I propose a novel technique\nincorporating high-dimensional covariates in local projections using the\nOrthogonal Greedy Algorithm with a high-dimensional AIC (OGA+HDAIC) model\nselection method. This approach offers robustness in both sparse and dense\nscenarios, improved interpretability, and more reliable causal inference in\nlocal projections. Simulation studies show superior performance in dense and\npersistent scenarios compared to conventional LP and LASSO-based approaches. In\nan empirical application to Acemoglu, Naidu, Restrepo, and Robinson (2019), I\ndemonstrate efficiency gains and robustness to a large set of controls.\nAdditionally, I examine the effect of subjective beliefs on economic\naggregates, demonstrating robustness to various model specifications. A novel\nstate-dependent analysis reveals that inflation behaves more in line with\nrational expectations in good states, but exhibits more subjective, pessimistic\ndynamics in bad states."
                },
                "authors": [
                    {
                        "name": "Jooyoung Cha"
                    }
                ],
                "author_detail": {
                    "name": "Jooyoung Cha"
                },
                "author": "Jooyoung Cha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.07743v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.07743v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02597v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02597v1",
                "updated": "2024-10-03T15:38:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    38,
                    20,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T15:38:20Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    38,
                    20,
                    3,
                    277,
                    0
                ],
                "title": "Three-in-One: Fast and Accurate Transducer for Hybrid-Autoregressive ASR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Three-in-One: Fast and Accurate Transducer for Hybrid-Autoregressive ASR"
                },
                "summary": "We present \\textbf{H}ybrid-\\textbf{A}utoregressive \\textbf{IN}ference\nTr\\textbf{AN}sducers (HAINAN), a novel architecture for speech recognition that\nextends the Token-and-Duration Transducer (TDT) model. Trained with randomly\nmasked predictor network outputs, HAINAN supports both autoregressive inference\nwith all network components and non-autoregressive inference without the\npredictor. Additionally, we propose a novel semi-autoregressive inference\nparadigm that first generates an initial hypothesis using non-autoregressive\ninference, followed by refinement steps where each token prediction is\nregenerated using parallelized autoregression on the initial hypothesis.\nExperiments on multiple datasets across different languages demonstrate that\nHAINAN achieves efficiency parity with CTC in non-autoregressive mode and with\nTDT in autoregressive mode. In terms of accuracy, autoregressive HAINAN\noutperforms TDT and RNN-T, while non-autoregressive HAINAN significantly\noutperforms CTC. Semi-autoregressive inference further enhances the model's\naccuracy with minimal computational overhead, and even outperforms TDT results\nin some cases. These results highlight HAINAN's flexibility in balancing\naccuracy and speed, positioning it as a strong candidate for real-world speech\nrecognition applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present \\textbf{H}ybrid-\\textbf{A}utoregressive \\textbf{IN}ference\nTr\\textbf{AN}sducers (HAINAN), a novel architecture for speech recognition that\nextends the Token-and-Duration Transducer (TDT) model. Trained with randomly\nmasked predictor network outputs, HAINAN supports both autoregressive inference\nwith all network components and non-autoregressive inference without the\npredictor. Additionally, we propose a novel semi-autoregressive inference\nparadigm that first generates an initial hypothesis using non-autoregressive\ninference, followed by refinement steps where each token prediction is\nregenerated using parallelized autoregression on the initial hypothesis.\nExperiments on multiple datasets across different languages demonstrate that\nHAINAN achieves efficiency parity with CTC in non-autoregressive mode and with\nTDT in autoregressive mode. In terms of accuracy, autoregressive HAINAN\noutperforms TDT and RNN-T, while non-autoregressive HAINAN significantly\noutperforms CTC. Semi-autoregressive inference further enhances the model's\naccuracy with minimal computational overhead, and even outperforms TDT results\nin some cases. These results highlight HAINAN's flexibility in balancing\naccuracy and speed, positioning it as a strong candidate for real-world speech\nrecognition applications."
                },
                "authors": [
                    {
                        "name": "Hainan Xu"
                    },
                    {
                        "name": "Travis M. Bartley"
                    },
                    {
                        "name": "Vladimir Bataev"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02597v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02597v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11064v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11064v2",
                "updated": "2024-10-03T15:38:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    38,
                    4,
                    3,
                    277,
                    0
                ],
                "published": "2024-06-16T20:41:03Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    20,
                    41,
                    3,
                    6,
                    168,
                    0
                ],
                "title": "Continual Test-time Adaptation for End-to-end Speech Recognition on\n  Noisy Speech",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Test-time Adaptation for End-to-end Speech Recognition on\n  Noisy Speech"
                },
                "summary": "Deep Learning-based end-to-end Automatic Speech Recognition (ASR) has made\nsignificant strides but still struggles with performance on out-of-domain\nsamples due to domain shifts in real-world scenarios. Test-Time Adaptation\n(TTA) methods address this issue by adapting models using test samples at\ninference time. However, current ASR TTA methods have largely focused on\nnon-continual TTA, which limits cross-sample knowledge learning compared to\ncontinual TTA. In this work, we first propose a Fast-slow TTA framework for ASR\nthat leverages the advantage of continual and non-continual TTA. Following this\nframework, we introduce Dynamic SUTA (DSUTA), an entropy-minimization-based\ncontinual TTA method for ASR. To enhance DSUTA robustness for time-varying\ndata, we design a dynamic reset strategy to automatically detect domain shifts\nand reset the model, making it more effective at handling multi-domain data.\nOur method demonstrates superior performance on various noisy ASR datasets,\noutperforming both non-continual and continual TTA baselines while maintaining\nrobustness to domain changes without requiring domain boundary information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning-based end-to-end Automatic Speech Recognition (ASR) has made\nsignificant strides but still struggles with performance on out-of-domain\nsamples due to domain shifts in real-world scenarios. Test-Time Adaptation\n(TTA) methods address this issue by adapting models using test samples at\ninference time. However, current ASR TTA methods have largely focused on\nnon-continual TTA, which limits cross-sample knowledge learning compared to\ncontinual TTA. In this work, we first propose a Fast-slow TTA framework for ASR\nthat leverages the advantage of continual and non-continual TTA. Following this\nframework, we introduce Dynamic SUTA (DSUTA), an entropy-minimization-based\ncontinual TTA method for ASR. To enhance DSUTA robustness for time-varying\ndata, we design a dynamic reset strategy to automatically detect domain shifts\nand reset the model, making it more effective at handling multi-domain data.\nOur method demonstrates superior performance on various noisy ASR datasets,\noutperforming both non-continual and continual TTA baselines while maintaining\nrobustness to domain changes without requiring domain boundary information."
                },
                "authors": [
                    {
                        "name": "Guan-Ting Lin"
                    },
                    {
                        "name": "Wei-Ping Huang"
                    },
                    {
                        "name": "Hung-yi Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hung-yi Lee"
                },
                "author": "Hung-yi Lee",
                "arxiv_comment": "Accepted by EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11064v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11064v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.06443v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.06443v2",
                "updated": "2024-10-03T15:34:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    34,
                    2,
                    3,
                    277,
                    0
                ],
                "published": "2024-05-10T12:48:57Z",
                "published_parsed": [
                    2024,
                    5,
                    10,
                    12,
                    48,
                    57,
                    4,
                    131,
                    0
                ],
                "title": "Residual-based Attention Physics-informed Neural Networks for\n  Spatio-Temporal Ageing Assessment of Transformers Operated in Renewable Power\n  Plants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Residual-based Attention Physics-informed Neural Networks for\n  Spatio-Temporal Ageing Assessment of Transformers Operated in Renewable Power\n  Plants"
                },
                "summary": "Transformers are crucial for reliable and efficient power system operations,\nparticularly in supporting the integration of renewable energy. Effective\nmonitoring of transformer health is critical to maintain grid stability and\nperformance. Thermal insulation ageing is a key transformer failure mode, which\nis generally tracked by monitoring the hotspot temperature (HST). However, HST\nmeasurement is complex, costly, and often estimated from indirect measurements.\nExisting HST models focus on space-agnostic thermal models, providing\nworst-case HST estimates. This article introduces a spatio-temporal model for\ntransformer winding temperature and ageing estimation, which leverages\nphysics-based partial differential equations (PDEs) with data-driven Neural\nNetworks (NN) in a Physics Informed Neural Networks (PINNs) configuration to\nimprove prediction accuracy and acquire spatio-temporal resolution. The\ncomputational accuracy of the PINN model is improved through the implementation\nof the Residual-Based Attention (PINN-RBA) scheme that accelerates the PINN\nmodel convergence. The PINN-RBA model is benchmarked against self-adaptive\nattention schemes and classical vanilla PINN configurations. For the first\ntime, PINN based oil temperature predictions are used to estimate\nspatio-temporal transformer winding temperature values, validated through PDE\nnumerical solution and fiber optic sensor measurements. Furthermore, the\nspatio-temporal transformer ageing model is inferred, which supports\ntransformer health management decision-making. Results are validated with a\ndistribution transformer operating on a floating photovoltaic power plant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers are crucial for reliable and efficient power system operations,\nparticularly in supporting the integration of renewable energy. Effective\nmonitoring of transformer health is critical to maintain grid stability and\nperformance. Thermal insulation ageing is a key transformer failure mode, which\nis generally tracked by monitoring the hotspot temperature (HST). However, HST\nmeasurement is complex, costly, and often estimated from indirect measurements.\nExisting HST models focus on space-agnostic thermal models, providing\nworst-case HST estimates. This article introduces a spatio-temporal model for\ntransformer winding temperature and ageing estimation, which leverages\nphysics-based partial differential equations (PDEs) with data-driven Neural\nNetworks (NN) in a Physics Informed Neural Networks (PINNs) configuration to\nimprove prediction accuracy and acquire spatio-temporal resolution. The\ncomputational accuracy of the PINN model is improved through the implementation\nof the Residual-Based Attention (PINN-RBA) scheme that accelerates the PINN\nmodel convergence. The PINN-RBA model is benchmarked against self-adaptive\nattention schemes and classical vanilla PINN configurations. For the first\ntime, PINN based oil temperature predictions are used to estimate\nspatio-temporal transformer winding temperature values, validated through PDE\nnumerical solution and fiber optic sensor measurements. Furthermore, the\nspatio-temporal transformer ageing model is inferred, which supports\ntransformer health management decision-making. Results are validated with a\ndistribution transformer operating on a floating photovoltaic power plant."
                },
                "authors": [
                    {
                        "name": "Ibai Ramirez"
                    },
                    {
                        "name": "Joel Pino"
                    },
                    {
                        "name": "David Pardo"
                    },
                    {
                        "name": "Mikel Sanz"
                    },
                    {
                        "name": "Luis del Rio"
                    },
                    {
                        "name": "Alvaro Ortiz"
                    },
                    {
                        "name": "Kateryna Morozovska"
                    },
                    {
                        "name": "Jose I. Aizpurua"
                    }
                ],
                "author_detail": {
                    "name": "Jose I. Aizpurua"
                },
                "author": "Jose I. Aizpurua",
                "arxiv_comment": "23 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.06443v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.06443v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01769v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01769v2",
                "updated": "2024-10-03T15:30:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    30,
                    12,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-02T17:25:37Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    17,
                    25,
                    37,
                    2,
                    276,
                    0
                ],
                "title": "Quantifying Generalization Complexity for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying Generalization Complexity for Large Language Models"
                },
                "summary": "While large language models (LLMs) have shown exceptional capabilities in\nunderstanding complex queries and performing sophisticated tasks, their\ngeneralization abilities are often deeply entangled with memorization,\nnecessitating more precise evaluation. To address this challenge, we introduce\nScylla, a dynamic evaluation framework that quantitatively measures the\ngeneralization abilities of LLMs. Scylla disentangles generalization from\nmemorization via assessing model performance on both in-distribution (ID) and\nout-of-distribution (OOD) data through 20 tasks across 5 levels of complexity.\nThrough extensive experiments, we uncover a non-monotonic relationship between\ntask complexity and the performance gap between ID and OOD data, which we term\nthe generalization valley. Specifically, this phenomenon reveals a critical\nthreshold - referred to as critical complexity - where reliance on\nnon-generalizable behavior peaks, indicating the upper bound of LLMs'\ngeneralization capabilities. As model size increases, the critical complexity\nshifts toward higher levels of task complexity, suggesting that larger models\ncan handle more complex reasoning tasks before over-relying on memorization.\nLeveraging Scylla and the concept of critical complexity, we benchmark 28LLMs\nincluding both open-sourced models such as LLaMA and Qwen families, and\nclose-sourced models like Claude and GPT, providing a more robust evaluation\nand establishing a clearer understanding of LLMs' generalization capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have shown exceptional capabilities in\nunderstanding complex queries and performing sophisticated tasks, their\ngeneralization abilities are often deeply entangled with memorization,\nnecessitating more precise evaluation. To address this challenge, we introduce\nScylla, a dynamic evaluation framework that quantitatively measures the\ngeneralization abilities of LLMs. Scylla disentangles generalization from\nmemorization via assessing model performance on both in-distribution (ID) and\nout-of-distribution (OOD) data through 20 tasks across 5 levels of complexity.\nThrough extensive experiments, we uncover a non-monotonic relationship between\ntask complexity and the performance gap between ID and OOD data, which we term\nthe generalization valley. Specifically, this phenomenon reveals a critical\nthreshold - referred to as critical complexity - where reliance on\nnon-generalizable behavior peaks, indicating the upper bound of LLMs'\ngeneralization capabilities. As model size increases, the critical complexity\nshifts toward higher levels of task complexity, suggesting that larger models\ncan handle more complex reasoning tasks before over-relying on memorization.\nLeveraging Scylla and the concept of critical complexity, we benchmark 28LLMs\nincluding both open-sourced models such as LLaMA and Qwen families, and\nclose-sourced models like Claude and GPT, providing a more robust evaluation\nand establishing a clearer understanding of LLMs' generalization capabilities."
                },
                "authors": [
                    {
                        "name": "Zhenting Qi"
                    },
                    {
                        "name": "Hongyin Luo"
                    },
                    {
                        "name": "Xuliang Huang"
                    },
                    {
                        "name": "Zhuokai Zhao"
                    },
                    {
                        "name": "Yibo Jiang"
                    },
                    {
                        "name": "Xiangjun Fan"
                    },
                    {
                        "name": "Himabindu Lakkaraju"
                    },
                    {
                        "name": "James Glass"
                    }
                ],
                "author_detail": {
                    "name": "James Glass"
                },
                "author": "James Glass",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01769v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01769v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02584v1",
                "updated": "2024-10-03T15:28:05Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    28,
                    5,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T15:28:05Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    28,
                    5,
                    3,
                    277,
                    0
                ],
                "title": "Towards Implicit Bias Detection and Mitigation in Multi-Agent LLM\n  Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Implicit Bias Detection and Mitigation in Multi-Agent LLM\n  Interactions"
                },
                "summary": "As Large Language Models (LLMs) continue to evolve, they are increasingly\nbeing employed in numerous studies to simulate societies and execute diverse\nsocial tasks. However, LLMs are susceptible to societal biases due to their\nexposure to human-generated data. Given that LLMs are being used to gain\ninsights into various societal aspects, it is essential to mitigate these\nbiases. To that end, our study investigates the presence of implicit gender\nbiases in multi-agent LLM interactions and proposes two strategies to mitigate\nthese biases. We begin by creating a dataset of scenarios where implicit gender\nbiases might arise, and subsequently develop a metric to assess the presence of\nbiases. Our empirical analysis reveals that LLMs generate outputs characterized\nby strong implicit bias associations (>= 50\\% of the time). Furthermore, these\nbiases tend to escalate following multi-agent interactions. To mitigate them,\nwe propose two strategies: self-reflection with in-context examples (ICE); and\nsupervised fine-tuning. Our research demonstrates that both methods effectively\nmitigate implicit biases, with the ensemble of fine-tuning and self-reflection\nproving to be the most successful.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) continue to evolve, they are increasingly\nbeing employed in numerous studies to simulate societies and execute diverse\nsocial tasks. However, LLMs are susceptible to societal biases due to their\nexposure to human-generated data. Given that LLMs are being used to gain\ninsights into various societal aspects, it is essential to mitigate these\nbiases. To that end, our study investigates the presence of implicit gender\nbiases in multi-agent LLM interactions and proposes two strategies to mitigate\nthese biases. We begin by creating a dataset of scenarios where implicit gender\nbiases might arise, and subsequently develop a metric to assess the presence of\nbiases. Our empirical analysis reveals that LLMs generate outputs characterized\nby strong implicit bias associations (>= 50\\% of the time). Furthermore, these\nbiases tend to escalate following multi-agent interactions. To mitigate them,\nwe propose two strategies: self-reflection with in-context examples (ICE); and\nsupervised fine-tuning. Our research demonstrates that both methods effectively\nmitigate implicit biases, with the ensemble of fine-tuning and self-reflection\nproving to be the most successful."
                },
                "authors": [
                    {
                        "name": "Angana Borah"
                    },
                    {
                        "name": "Rada Mihalcea"
                    }
                ],
                "author_detail": {
                    "name": "Rada Mihalcea"
                },
                "author": "Rada Mihalcea",
                "arxiv_comment": "Accepted to EMNLP Findings 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00110v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00110v2",
                "updated": "2024-10-03T15:25:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    25,
                    40,
                    3,
                    277,
                    0
                ],
                "published": "2024-09-30T18:00:06Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    18,
                    0,
                    6,
                    0,
                    274,
                    0
                ],
                "title": "The AURORA Survey: An Extraordinarily Mature, Star-forming Galaxy at\n  $z\\sim 7$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The AURORA Survey: An Extraordinarily Mature, Star-forming Galaxy at\n  $z\\sim 7$"
                },
                "summary": "We present the properties of a massive, large, dusty, metal-rich,\nstar-forming galaxy at z_spec=6.73. GOODSN-100182 was observed with\nJWST/NIRSpec as part of the AURORA survey, and is also covered by public\nmulti-wavelength HST and JWST imaging. While the large mass of GOODSN-100182\n(~10^10 M_sun) was indicated prior to JWST, NIRCam rest-optical imaging now\nreveals the presence of an extended disk (r_eff~1.5 kpc). In addition, the\nNIRSpec R~1000 spectrum of GOODSN-100182 includes the detection of a large\nsuite of rest-optical nebular emission lines ranging in wavelength from\n[OII]3727 up to [NII]6583. The ratios of Balmer lines suggest significant dust\nattenuation (E(B-V)_gas=0.40+0.10/-0.09), consistent with the red rest-UV slope\ninferred for GOODSN-100182 (beta=-0.50+/-0.09). The star-formation rate based\non dust-corrected H-alpha emission is log(SFR(H-alpha)/\nM_sun/yr)=2.02+0.13/-0.14, well above the z~7 star-forming main sequence in\nterms of specific SFR. Strikingly, the ratio of [NII]6583/H-alpha emission\nsuggests almost solar metallicity, as does the ratio\n([OIII]5007/H-beta)/([NII]6583/H-alpha) and the detection of the faint\n[FeII]4360 emission feature, whereas the [OIII]5007/[OII]3727 ratio suggests\nroughly 50% solar metallicity. Overall, the excitation and ionization\nproperties of GOODSN-100182 more closely resemble those of typical star-forming\ngalaxies at z~2-3 rather than z~7. Based on public spectroscopy of the GOODS-N\nfield, we find that GOODSN-100182 resides within a significant galaxy\noverdensity, and is accompanied by a spectroscopically-confirmed neighbor\ngalaxy. GOODSN-100182 demonstrates the existence of mature, chemically-enriched\ngalaxies within the first billion years of cosmic time, whose properties must\nbe explained by galaxy formation models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the properties of a massive, large, dusty, metal-rich,\nstar-forming galaxy at z_spec=6.73. GOODSN-100182 was observed with\nJWST/NIRSpec as part of the AURORA survey, and is also covered by public\nmulti-wavelength HST and JWST imaging. While the large mass of GOODSN-100182\n(~10^10 M_sun) was indicated prior to JWST, NIRCam rest-optical imaging now\nreveals the presence of an extended disk (r_eff~1.5 kpc). In addition, the\nNIRSpec R~1000 spectrum of GOODSN-100182 includes the detection of a large\nsuite of rest-optical nebular emission lines ranging in wavelength from\n[OII]3727 up to [NII]6583. The ratios of Balmer lines suggest significant dust\nattenuation (E(B-V)_gas=0.40+0.10/-0.09), consistent with the red rest-UV slope\ninferred for GOODSN-100182 (beta=-0.50+/-0.09). The star-formation rate based\non dust-corrected H-alpha emission is log(SFR(H-alpha)/\nM_sun/yr)=2.02+0.13/-0.14, well above the z~7 star-forming main sequence in\nterms of specific SFR. Strikingly, the ratio of [NII]6583/H-alpha emission\nsuggests almost solar metallicity, as does the ratio\n([OIII]5007/H-beta)/([NII]6583/H-alpha) and the detection of the faint\n[FeII]4360 emission feature, whereas the [OIII]5007/[OII]3727 ratio suggests\nroughly 50% solar metallicity. Overall, the excitation and ionization\nproperties of GOODSN-100182 more closely resemble those of typical star-forming\ngalaxies at z~2-3 rather than z~7. Based on public spectroscopy of the GOODS-N\nfield, we find that GOODSN-100182 resides within a significant galaxy\noverdensity, and is accompanied by a spectroscopically-confirmed neighbor\ngalaxy. GOODSN-100182 demonstrates the existence of mature, chemically-enriched\ngalaxies within the first billion years of cosmic time, whose properties must\nbe explained by galaxy formation models."
                },
                "authors": [
                    {
                        "name": "Alice E. Shapley"
                    },
                    {
                        "name": "Ryan L. Sanders"
                    },
                    {
                        "name": "Michael W. Topping"
                    },
                    {
                        "name": "Naveen A. Reddy"
                    },
                    {
                        "name": "Anthony J. Pahl"
                    },
                    {
                        "name": "Pascal A. Oesch"
                    },
                    {
                        "name": "Danielle A. Berg"
                    },
                    {
                        "name": "Rychard J. Bouwens"
                    },
                    {
                        "name": "Gabriel Brammer"
                    },
                    {
                        "name": "Adam C. Carnall"
                    },
                    {
                        "name": "Fergus Cullen"
                    },
                    {
                        "name": "Romeel Dav"
                    },
                    {
                        "name": "James S. Dunlop"
                    },
                    {
                        "name": "Richard S. Ellis"
                    },
                    {
                        "name": "N. M. Frster Schreiber"
                    },
                    {
                        "name": "Steven R . Furlanetto"
                    },
                    {
                        "name": "Karl Glazebrook"
                    },
                    {
                        "name": "Garth D. Illingworth"
                    },
                    {
                        "name": "Tucker Jones"
                    },
                    {
                        "name": "Mariska Kriek"
                    },
                    {
                        "name": "Derek J. McLeod"
                    },
                    {
                        "name": "Ross J. McLure"
                    },
                    {
                        "name": "Desika Narayanan"
                    },
                    {
                        "name": "Max Pettini"
                    },
                    {
                        "name": "Daniel Schaerer"
                    },
                    {
                        "name": "Daniel P. Stark"
                    },
                    {
                        "name": "Charles C. Steidel"
                    },
                    {
                        "name": "Mengtao Tang"
                    },
                    {
                        "name": "Leonardo Clarke"
                    },
                    {
                        "name": "Callum T. Donnan"
                    },
                    {
                        "name": "Emily Kehoe"
                    }
                ],
                "author_detail": {
                    "name": "Emily Kehoe"
                },
                "author": "Emily Kehoe",
                "arxiv_comment": "16 pages, 13 figures, submitted to ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00110v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00110v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.04484v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.04484v3",
                "updated": "2024-10-03T15:20:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    20,
                    17,
                    3,
                    277,
                    0
                ],
                "published": "2023-10-06T13:28:04Z",
                "published_parsed": [
                    2023,
                    10,
                    6,
                    13,
                    28,
                    4,
                    4,
                    279,
                    0
                ],
                "title": "Ada-Instruct: Adapting Instruction Generators for Complex Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ada-Instruct: Adapting Instruction Generators for Complex Reasoning"
                },
                "summary": "Instructions augmentation is a crucial step for unleashing the full potential\nof large language models (LLMs) in downstream tasks. Existing Self-Instruct\nmethods primarily simulate new instructions from a few initial instructions\nwith in-context learning. However, our study identifies a critical flaw in this\napproach: even with GPT4o, Self-Instruct cannot generate complex instructions\nof length $\\ge 100$, which is necessary in complex tasks such as code\ncompletion.\n  To address this issue, our key insight is that fine-tuning open source LLMs\nwith only ten examples can produce complex instructions that maintain\ndistributional consistency for complex reasoning tasks. We introduce\nAda-Instruct, an adaptive instruction generator developed through fine-tuning.\nWe empirically validated Ada-Instruct's efficacy across different applications.\nThe results highlight Ada-Instruct's capacity to generate long, intricate, and\ndistributionally consistent instructions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instructions augmentation is a crucial step for unleashing the full potential\nof large language models (LLMs) in downstream tasks. Existing Self-Instruct\nmethods primarily simulate new instructions from a few initial instructions\nwith in-context learning. However, our study identifies a critical flaw in this\napproach: even with GPT4o, Self-Instruct cannot generate complex instructions\nof length $\\ge 100$, which is necessary in complex tasks such as code\ncompletion.\n  To address this issue, our key insight is that fine-tuning open source LLMs\nwith only ten examples can produce complex instructions that maintain\ndistributional consistency for complex reasoning tasks. We introduce\nAda-Instruct, an adaptive instruction generator developed through fine-tuning.\nWe empirically validated Ada-Instruct's efficacy across different applications.\nThe results highlight Ada-Instruct's capacity to generate long, intricate, and\ndistributionally consistent instructions."
                },
                "authors": [
                    {
                        "name": "Wanyun Cui"
                    },
                    {
                        "name": "Qianle Wang"
                    }
                ],
                "author_detail": {
                    "name": "Qianle Wang"
                },
                "author": "Qianle Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.04484v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.04484v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.00651v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.00651v3",
                "updated": "2024-10-03T15:14:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    14,
                    34,
                    3,
                    277,
                    0
                ],
                "published": "2024-01-01T03:38:06Z",
                "published_parsed": [
                    2024,
                    1,
                    1,
                    3,
                    38,
                    6,
                    0,
                    1,
                    0
                ],
                "title": "IRWE: Inductive Random Walk for Joint Inference of Identity and Position\n  Network Embedding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IRWE: Inductive Random Walk for Joint Inference of Identity and Position\n  Network Embedding"
                },
                "summary": "Network embedding, which maps graphs to distributed representations, is a\nunified framework for various graph inference tasks. According to the topology\nproperties (e.g., structural roles and community memberships of nodes) to be\npreserved, it can be categorized into the identity and position embedding. Most\nexisting methods can only capture one type of property. Some approaches can\nsupport the inductive inference that generalizes the embedding model to new\nnodes or graphs but relies on the availability of attributes. Due to the\ncomplicated correlations between topology and attributes, it is unclear for\nsome inductive methods which type of property they can capture. In this study,\nwe explore a unified framework for the joint inductive inference of identity\nand position embeddings without attributes. An inductive random walk embedding\n(IRWE) method is proposed, which combines multiple attention units to handle\nthe random walk (RW) on graph topology and simultaneously derives identity and\nposition embeddings that are jointly optimized. We demonstrate that some RW\nstatistics can characterize node identities and positions while supporting the\ninductive inference. Experiments validate the superior performance of IRWE over\nvarious baselines for the transductive and inductive inference of identity and\nposition embeddings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network embedding, which maps graphs to distributed representations, is a\nunified framework for various graph inference tasks. According to the topology\nproperties (e.g., structural roles and community memberships of nodes) to be\npreserved, it can be categorized into the identity and position embedding. Most\nexisting methods can only capture one type of property. Some approaches can\nsupport the inductive inference that generalizes the embedding model to new\nnodes or graphs but relies on the availability of attributes. Due to the\ncomplicated correlations between topology and attributes, it is unclear for\nsome inductive methods which type of property they can capture. In this study,\nwe explore a unified framework for the joint inductive inference of identity\nand position embeddings without attributes. An inductive random walk embedding\n(IRWE) method is proposed, which combines multiple attention units to handle\nthe random walk (RW) on graph topology and simultaneously derives identity and\nposition embeddings that are jointly optimized. We demonstrate that some RW\nstatistics can characterize node identities and positions while supporting the\ninductive inference. Experiments validate the superior performance of IRWE over\nvarious baselines for the transductive and inductive inference of identity and\nposition embeddings."
                },
                "authors": [
                    {
                        "name": "Meng Qin"
                    },
                    {
                        "name": "Dit-Yan Yeung"
                    }
                ],
                "author_detail": {
                    "name": "Dit-Yan Yeung"
                },
                "author": "Dit-Yan Yeung",
                "arxiv_comment": "Accepted by Transactions on Machine Learning Research (TMLR)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.00651v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.00651v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.18952v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.18952v4",
                "updated": "2024-10-03T15:08:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    8,
                    7,
                    3,
                    277,
                    0
                ],
                "published": "2023-05-27T16:05:00Z",
                "published_parsed": [
                    2023,
                    5,
                    27,
                    16,
                    5,
                    0,
                    5,
                    147,
                    0
                ],
                "title": "Exploring the Practicality of Generative Retrieval on Dynamic Corpora",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Practicality of Generative Retrieval on Dynamic Corpora"
                },
                "summary": "Benchmarking the performance of information retrieval (IR) is mostly\nconducted with a fixed set of documents (static corpora). However, in realistic\nscenarios, this is rarely the case and the documents to be retrieved are\nconstantly updated and added. In this paper, we focus on Generative Retrievals\n(GR), which apply autoregressive language models to IR problems, and explore\ntheir adaptability and robustness in dynamic scenarios. We also conduct an\nextensive evaluation of computational and memory efficiency, crucial factors\nfor real-world deployment of IR systems handling vast and ever-changing\ndocument collections. Our results on the StreamingQA benchmark demonstrate that\nGR is more adaptable to evolving knowledge (4 -- 11%), robust in learning\nknowledge with temporal information, and efficient in terms of inference FLOPs\n(x 2), indexing time (x 6), and storage footprint (x 4) compared to Dual\nEncoders (DE), which are commonly used in retrieval systems. Our paper\nhighlights the potential of GR for future use in practical IR systems within\ndynamic environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking the performance of information retrieval (IR) is mostly\nconducted with a fixed set of documents (static corpora). However, in realistic\nscenarios, this is rarely the case and the documents to be retrieved are\nconstantly updated and added. In this paper, we focus on Generative Retrievals\n(GR), which apply autoregressive language models to IR problems, and explore\ntheir adaptability and robustness in dynamic scenarios. We also conduct an\nextensive evaluation of computational and memory efficiency, crucial factors\nfor real-world deployment of IR systems handling vast and ever-changing\ndocument collections. Our results on the StreamingQA benchmark demonstrate that\nGR is more adaptable to evolving knowledge (4 -- 11%), robust in learning\nknowledge with temporal information, and efficient in terms of inference FLOPs\n(x 2), indexing time (x 6), and storage footprint (x 4) compared to Dual\nEncoders (DE), which are commonly used in retrieval systems. Our paper\nhighlights the potential of GR for future use in practical IR systems within\ndynamic environments."
                },
                "authors": [
                    {
                        "name": "Chaeeun Kim"
                    },
                    {
                        "name": "Soyoung Yoon"
                    },
                    {
                        "name": "Hyunji Lee"
                    },
                    {
                        "name": "Joel Jang"
                    },
                    {
                        "name": "Sohee Yang"
                    },
                    {
                        "name": "Minjoon Seo"
                    }
                ],
                "author_detail": {
                    "name": "Minjoon Seo"
                },
                "author": "Minjoon Seo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.18952v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.18952v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02561v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02561v1",
                "updated": "2024-10-03T15:04:47Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    4,
                    47,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T15:04:47Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    4,
                    47,
                    3,
                    277,
                    0
                ],
                "title": "The Benefit of Being Bayesian in Online Conformal Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Benefit of Being Bayesian in Online Conformal Prediction"
                },
                "summary": "Based on the framework of Conformal Prediction (CP), we study the online\nconstruction of valid confidence sets given a black-box machine learning model.\nBy converting the target confidence levels into quantile levels, the problem\ncan be reduced to predicting the quantiles (in hindsight) of a sequentially\nrevealed data sequence. Two very different approaches have been studied\npreviously. (i) Direct approach: Assuming the data sequence is iid or\nexchangeable, one could maintain the empirical distribution of the observed\ndata as an algorithmic belief, and directly predict its quantiles. (ii)\nIndirect approach: As statistical assumptions often do not hold in practice, a\nrecent trend is to consider the adversarial setting and apply first-order\nonline optimization to moving quantile losses (Gibbs & Cand\\`es, 2021). It\nrequires knowing the target quantile level beforehand, and suffers from certain\nvalidity issues on the obtained confidence sets, due to the associated loss\nlinearization.\n  This paper presents a novel Bayesian CP framework that combines their\nstrengths. Without any statistical assumption, it is able to both: (i) answer\nmultiple arbitrary confidence level queries online, with provably low regret;\nand (ii) overcome the validity issues suffered by first-order optimization\nbaselines, due to being \"data-centric\" rather than \"iterate-centric\".\n  From a technical perspective, our key idea is to regularize the algorithmic\nbelief of the above direct approach by a Bayesian prior, which \"robustifies\" it\nby simulating a non-linearized Follow the Regularized Leader (FTRL) algorithm\non the output. For statisticians, this can be regarded as an online adversarial\nview of Bayesian inference. Importantly, the proposed belief update backbone is\nshared by prediction heads targeting different confidence levels, bringing\npractical benefits analogous to U-calibration (Kleinberg et al., 2023).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Based on the framework of Conformal Prediction (CP), we study the online\nconstruction of valid confidence sets given a black-box machine learning model.\nBy converting the target confidence levels into quantile levels, the problem\ncan be reduced to predicting the quantiles (in hindsight) of a sequentially\nrevealed data sequence. Two very different approaches have been studied\npreviously. (i) Direct approach: Assuming the data sequence is iid or\nexchangeable, one could maintain the empirical distribution of the observed\ndata as an algorithmic belief, and directly predict its quantiles. (ii)\nIndirect approach: As statistical assumptions often do not hold in practice, a\nrecent trend is to consider the adversarial setting and apply first-order\nonline optimization to moving quantile losses (Gibbs & Cand\\`es, 2021). It\nrequires knowing the target quantile level beforehand, and suffers from certain\nvalidity issues on the obtained confidence sets, due to the associated loss\nlinearization.\n  This paper presents a novel Bayesian CP framework that combines their\nstrengths. Without any statistical assumption, it is able to both: (i) answer\nmultiple arbitrary confidence level queries online, with provably low regret;\nand (ii) overcome the validity issues suffered by first-order optimization\nbaselines, due to being \"data-centric\" rather than \"iterate-centric\".\n  From a technical perspective, our key idea is to regularize the algorithmic\nbelief of the above direct approach by a Bayesian prior, which \"robustifies\" it\nby simulating a non-linearized Follow the Regularized Leader (FTRL) algorithm\non the output. For statisticians, this can be regarded as an online adversarial\nview of Bayesian inference. Importantly, the proposed belief update backbone is\nshared by prediction heads targeting different confidence levels, bringing\npractical benefits analogous to U-calibration (Kleinberg et al., 2023)."
                },
                "authors": [
                    {
                        "name": "Zhiyu Zhang"
                    },
                    {
                        "name": "Zhou Lu"
                    },
                    {
                        "name": "Heng Yang"
                    }
                ],
                "author_detail": {
                    "name": "Heng Yang"
                },
                "author": "Heng Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02561v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02561v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19700v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19700v2",
                "updated": "2024-10-03T14:56:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    56,
                    2,
                    3,
                    277,
                    0
                ],
                "published": "2024-09-29T13:16:37Z",
                "published_parsed": [
                    2024,
                    9,
                    29,
                    13,
                    16,
                    37,
                    6,
                    273,
                    0
                ],
                "title": "2D-TPE: Two-Dimensional Positional Encoding Enhances Table Understanding\n  for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "2D-TPE: Two-Dimensional Positional Encoding Enhances Table Understanding\n  for Large Language Models"
                },
                "summary": "Tables are ubiquitous across various domains for concisely representing\nstructured information. Empowering large language models (LLMs) to reason over\ntabular data represents an actively explored direction. However, since typical\nLLMs only support one-dimensional~(1D) inputs, existing methods often flatten\nthe two-dimensional~(2D) table structure into a sequence of tokens, which can\nseverely disrupt the spatial relationships and result in an inevitable loss of\nvital contextual information. In this paper, we first empirically demonstrate\nthe detrimental impact of such flattening operations on the performance of LLMs\nin capturing the spatial information of tables through two elaborate proxy\ntasks. Subsequently, we introduce a simple yet effective positional encoding\nmethod, termed ``2D-TPE'' (Two-Dimensional Table Positional Encoding), to\naddress this challenge. 2D-TPE enables each attention head to dynamically\nselect a permutation order of tokens within the context for attending to them,\nwhere each permutation represents a distinct traversal mode for the table, such\nas column-wise or row-wise traversal. 2D-TPE effectively mitigates the risk of\nlosing essential spatial information while preserving computational efficiency,\nthus better preserving the table structure. Extensive experiments across five\nbenchmarks demonstrate that 2D-TPE outperforms strong baselines, underscoring\nthe importance of preserving the table structure for accurate table\ncomprehension. Comprehensive analysis further reveals the substantially better\nscalability of 2D-TPE to large tables than baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tables are ubiquitous across various domains for concisely representing\nstructured information. Empowering large language models (LLMs) to reason over\ntabular data represents an actively explored direction. However, since typical\nLLMs only support one-dimensional~(1D) inputs, existing methods often flatten\nthe two-dimensional~(2D) table structure into a sequence of tokens, which can\nseverely disrupt the spatial relationships and result in an inevitable loss of\nvital contextual information. In this paper, we first empirically demonstrate\nthe detrimental impact of such flattening operations on the performance of LLMs\nin capturing the spatial information of tables through two elaborate proxy\ntasks. Subsequently, we introduce a simple yet effective positional encoding\nmethod, termed ``2D-TPE'' (Two-Dimensional Table Positional Encoding), to\naddress this challenge. 2D-TPE enables each attention head to dynamically\nselect a permutation order of tokens within the context for attending to them,\nwhere each permutation represents a distinct traversal mode for the table, such\nas column-wise or row-wise traversal. 2D-TPE effectively mitigates the risk of\nlosing essential spatial information while preserving computational efficiency,\nthus better preserving the table structure. Extensive experiments across five\nbenchmarks demonstrate that 2D-TPE outperforms strong baselines, underscoring\nthe importance of preserving the table structure for accurate table\ncomprehension. Comprehensive analysis further reveals the substantially better\nscalability of 2D-TPE to large tables than baselines."
                },
                "authors": [
                    {
                        "name": "Jia-Nan Li"
                    },
                    {
                        "name": "Jian Guan"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Zhengtao Yu"
                    },
                    {
                        "name": "Rui Yan"
                    }
                ],
                "author_detail": {
                    "name": "Rui Yan"
                },
                "author": "Rui Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19700v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19700v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.16519v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.16519v3",
                "updated": "2024-10-03T14:55:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    55,
                    41,
                    3,
                    277,
                    0
                ],
                "published": "2023-09-28T15:25:17Z",
                "published_parsed": [
                    2023,
                    9,
                    28,
                    15,
                    25,
                    17,
                    3,
                    271,
                    0
                ],
                "title": "AtomSurf : Surface Representation for Learning on Protein Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AtomSurf : Surface Representation for Learning on Protein Structures"
                },
                "summary": "While there has been significant progress in evaluating and comparing\ndifferent representations for learning on protein data, the role of\nsurface-based learning approaches remains not well-understood. In particular,\nthere is a lack of direct and fair benchmark comparison between the best\navailable surface-based learning methods against alternative representations\nsuch as graphs. Moreover, the few existing surface-based approaches either use\nsurface information in isolation or, at best, perform global pooling between\nsurface and graph-based architectures.\n  In this work, we fill this gap by first adapting a state-of-the-art surface\nencoder for protein learning tasks. We then perform a direct and fair\ncomparison of the resulting method against alternative approaches within the\nAtom3D benchmark, highlighting the limitations of pure surface-based learning.\nFinally, we propose an integrated approach, which allows learned feature\nsharing between graphs and surface representations on the level of nodes and\nvertices $\\textit{across all layers}$.\n  We demonstrate that the resulting architecture achieves state-of-the-art\nresults on all tasks in the Atom3D benchmark, while adhering to the strict\nbenchmark protocol, as well as more broadly on binding site identification and\nbinding pocket classification. Furthermore, we use coarsened surfaces and\noptimize our approach for efficiency, making our tool competitive in training\nand inference time with existing techniques. Our code and data can be found\nonline: $\\texttt{github.com/Vincentx15/atomsurf}$",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While there has been significant progress in evaluating and comparing\ndifferent representations for learning on protein data, the role of\nsurface-based learning approaches remains not well-understood. In particular,\nthere is a lack of direct and fair benchmark comparison between the best\navailable surface-based learning methods against alternative representations\nsuch as graphs. Moreover, the few existing surface-based approaches either use\nsurface information in isolation or, at best, perform global pooling between\nsurface and graph-based architectures.\n  In this work, we fill this gap by first adapting a state-of-the-art surface\nencoder for protein learning tasks. We then perform a direct and fair\ncomparison of the resulting method against alternative approaches within the\nAtom3D benchmark, highlighting the limitations of pure surface-based learning.\nFinally, we propose an integrated approach, which allows learned feature\nsharing between graphs and surface representations on the level of nodes and\nvertices $\\textit{across all layers}$.\n  We demonstrate that the resulting architecture achieves state-of-the-art\nresults on all tasks in the Atom3D benchmark, while adhering to the strict\nbenchmark protocol, as well as more broadly on binding site identification and\nbinding pocket classification. Furthermore, we use coarsened surfaces and\noptimize our approach for efficiency, making our tool competitive in training\nand inference time with existing techniques. Our code and data can be found\nonline: $\\texttt{github.com/Vincentx15/atomsurf}$"
                },
                "authors": [
                    {
                        "name": "Vincent Mallet"
                    },
                    {
                        "name": "Souhaib Attaiki"
                    },
                    {
                        "name": "Yangyang Miao"
                    },
                    {
                        "name": "Bruno Correia"
                    },
                    {
                        "name": "Maks Ovsjanikov"
                    }
                ],
                "author_detail": {
                    "name": "Maks Ovsjanikov"
                },
                "author": "Maks Ovsjanikov",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.16519v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.16519v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02551v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02551v1",
                "updated": "2024-10-03T14:55:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    55,
                    22,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T14:55:22Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    55,
                    22,
                    3,
                    277,
                    0
                ],
                "title": "ColaCare: Enhancing Electronic Health Record Modeling through Large\n  Language Model-Driven Multi-Agent Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ColaCare: Enhancing Electronic Health Record Modeling through Large\n  Language Model-Driven Multi-Agent Collaboration"
                },
                "summary": "We introduce ColaCare, a framework that enhances Electronic Health Record\n(EHR) modeling through multi-agent collaboration driven by Large Language\nModels (LLMs). Our approach seamlessly integrates domain-specific expert models\nwith LLMs to bridge the gap between structured EHR data and text-based\nreasoning. Inspired by clinical consultations, ColaCare employs two types of\nagents: DoctorAgent and MetaAgent, which collaboratively analyze patient data.\nExpert models process and generate predictions from numerical EHR data, while\nLLM agents produce reasoning references and decision-making reports within the\ncollaborative consultation framework. We additionally incorporate the Merck\nManual of Diagnosis and Therapy (MSD) medical guideline within a\nretrieval-augmented generation (RAG) module for authoritative evidence support.\nExtensive experiments conducted on four distinct EHR datasets demonstrate\nColaCare's superior performance in mortality prediction tasks, underscoring its\npotential to revolutionize clinical decision support systems and advance\npersonalized precision medicine. The code, complete prompt templates, more case\nstudies, etc. are publicly available at the anonymous link:\nhttps://colacare.netlify.app.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce ColaCare, a framework that enhances Electronic Health Record\n(EHR) modeling through multi-agent collaboration driven by Large Language\nModels (LLMs). Our approach seamlessly integrates domain-specific expert models\nwith LLMs to bridge the gap between structured EHR data and text-based\nreasoning. Inspired by clinical consultations, ColaCare employs two types of\nagents: DoctorAgent and MetaAgent, which collaboratively analyze patient data.\nExpert models process and generate predictions from numerical EHR data, while\nLLM agents produce reasoning references and decision-making reports within the\ncollaborative consultation framework. We additionally incorporate the Merck\nManual of Diagnosis and Therapy (MSD) medical guideline within a\nretrieval-augmented generation (RAG) module for authoritative evidence support.\nExtensive experiments conducted on four distinct EHR datasets demonstrate\nColaCare's superior performance in mortality prediction tasks, underscoring its\npotential to revolutionize clinical decision support systems and advance\npersonalized precision medicine. The code, complete prompt templates, more case\nstudies, etc. are publicly available at the anonymous link:\nhttps://colacare.netlify.app."
                },
                "authors": [
                    {
                        "name": "Zixiang Wang"
                    },
                    {
                        "name": "Yinghao Zhu"
                    },
                    {
                        "name": "Huiya Zhao"
                    },
                    {
                        "name": "Xiaochen Zheng"
                    },
                    {
                        "name": "Tianlong Wang"
                    },
                    {
                        "name": "Wen Tang"
                    },
                    {
                        "name": "Yasha Wang"
                    },
                    {
                        "name": "Chengwei Pan"
                    },
                    {
                        "name": "Ewen M. Harrison"
                    },
                    {
                        "name": "Junyi Gao"
                    },
                    {
                        "name": "Liantao Ma"
                    }
                ],
                "author_detail": {
                    "name": "Liantao Ma"
                },
                "author": "Liantao Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02551v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02551v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02458v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02458v1",
                "updated": "2024-10-03T14:50:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    50,
                    33,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T14:50:33Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    50,
                    33,
                    3,
                    277,
                    0
                ],
                "title": "MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to\n  Enhance Medical Image Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to\n  Enhance Medical Image Segmentation"
                },
                "summary": "Large Language Models (LLMs), known for their versatility in textual data,\nare increasingly being explored for their potential to enhance medical image\nsegmentation, a crucial task for accurate diagnostic imaging. This study\nexplores enhancing Vision Transformers (ViTs) for medical image segmentation by\nintegrating pre-trained LLM transformer blocks. Our approach, which\nincorporates a frozen LLM transformer block into the encoder of a ViT-based\nmodel, leads to substantial improvements in segmentation performance across\nvarious medical imaging modalities. We propose a Hybrid Attention Mechanism\nthat combines global and local feature learning with a Multi-Scale Fusion Block\nfor aggregating features across different scales. The enhanced model shows\nsignificant performance gains, including an average Dice score increase from\n0.74 to 0.79 and improvements in accuracy, precision, and the Jaccard Index.\nThese results demonstrate the effectiveness of LLM-based transformers in\nrefining medical image segmentation, highlighting their potential to\nsignificantly boost model accuracy and robustness. The source code and our\nimplementation are available at: https://bit.ly/3zf2CVs",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), known for their versatility in textual data,\nare increasingly being explored for their potential to enhance medical image\nsegmentation, a crucial task for accurate diagnostic imaging. This study\nexplores enhancing Vision Transformers (ViTs) for medical image segmentation by\nintegrating pre-trained LLM transformer blocks. Our approach, which\nincorporates a frozen LLM transformer block into the encoder of a ViT-based\nmodel, leads to substantial improvements in segmentation performance across\nvarious medical imaging modalities. We propose a Hybrid Attention Mechanism\nthat combines global and local feature learning with a Multi-Scale Fusion Block\nfor aggregating features across different scales. The enhanced model shows\nsignificant performance gains, including an average Dice score increase from\n0.74 to 0.79 and improvements in accuracy, precision, and the Jaccard Index.\nThese results demonstrate the effectiveness of LLM-based transformers in\nrefining medical image segmentation, highlighting their potential to\nsignificantly boost model accuracy and robustness. The source code and our\nimplementation are available at: https://bit.ly/3zf2CVs"
                },
                "authors": [
                    {
                        "name": "Gurucharan Marthi Krishna Kumar"
                    },
                    {
                        "name": "Aman Chadha"
                    },
                    {
                        "name": "Janine Mendola"
                    },
                    {
                        "name": "Amir Shmuel"
                    }
                ],
                "author_detail": {
                    "name": "Amir Shmuel"
                },
                "author": "Amir Shmuel",
                "arxiv_comment": "Submitted to IEEE/CVF Winter Conference on Applications of Computer\n  Vision (WACV) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02458v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02458v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.09850v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.09850v2",
                "updated": "2024-10-03T14:44:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    44,
                    51,
                    3,
                    277,
                    0
                ],
                "published": "2024-03-14T20:18:08Z",
                "published_parsed": [
                    2024,
                    3,
                    14,
                    20,
                    18,
                    8,
                    3,
                    74,
                    0
                ],
                "title": "MARVIS: Motion & Geometry Aware Real and Virtual Image Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARVIS: Motion & Geometry Aware Real and Virtual Image Segmentation"
                },
                "summary": "Tasks such as autonomous navigation, 3D reconstruction, and object\nrecognition near the water surfaces are crucial in marine robotics\napplications. However, challenges arise due to dynamic disturbances, e.g.,\nlight reflections and refraction from the random air-water interface, irregular\nliquid flow, and similar factors, which can lead to potential failures in\nperception and navigation systems. Traditional computer vision algorithms\nstruggle to differentiate between real and virtual image regions, significantly\ncomplicating tasks. A virtual image region is an apparent representation formed\nby the redirection of light rays, typically through reflection or refraction,\ncreating the illusion of an object's presence without its actual physical\nlocation. This work proposes a novel approach for segmentation on real and\nvirtual image regions, exploiting synthetic images combined with\ndomain-invariant information, a Motion Entropy Kernel, and Epipolar Geometric\nConsistency. Our segmentation network does not need to be re-trained if the\ndomain changes. We show this by deploying the same segmentation network in two\ndifferent domains: simulation and the real world. By creating realistic\nsynthetic images that mimic the complexities of the water surface, we provide\nfine-grained training data for our network (MARVIS) to discern between real and\nvirtual images effectively. By motion & geometry-aware design choices and\nthrough comprehensive experimental analysis, we achieve state-of-the-art\nreal-virtual image segmentation performance in unseen real world domain,\nachieving an IoU over 78% and a F1-Score over 86% while ensuring a small\ncomputational footprint. MARVIS offers over 43 FPS (8 FPS) inference rates on a\nsingle GPU (CPU core). Our code and dataset are available here\nhttps://github.com/jiayi-wu-umd/MARVIS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tasks such as autonomous navigation, 3D reconstruction, and object\nrecognition near the water surfaces are crucial in marine robotics\napplications. However, challenges arise due to dynamic disturbances, e.g.,\nlight reflections and refraction from the random air-water interface, irregular\nliquid flow, and similar factors, which can lead to potential failures in\nperception and navigation systems. Traditional computer vision algorithms\nstruggle to differentiate between real and virtual image regions, significantly\ncomplicating tasks. A virtual image region is an apparent representation formed\nby the redirection of light rays, typically through reflection or refraction,\ncreating the illusion of an object's presence without its actual physical\nlocation. This work proposes a novel approach for segmentation on real and\nvirtual image regions, exploiting synthetic images combined with\ndomain-invariant information, a Motion Entropy Kernel, and Epipolar Geometric\nConsistency. Our segmentation network does not need to be re-trained if the\ndomain changes. We show this by deploying the same segmentation network in two\ndifferent domains: simulation and the real world. By creating realistic\nsynthetic images that mimic the complexities of the water surface, we provide\nfine-grained training data for our network (MARVIS) to discern between real and\nvirtual images effectively. By motion & geometry-aware design choices and\nthrough comprehensive experimental analysis, we achieve state-of-the-art\nreal-virtual image segmentation performance in unseen real world domain,\nachieving an IoU over 78% and a F1-Score over 86% while ensuring a small\ncomputational footprint. MARVIS offers over 43 FPS (8 FPS) inference rates on a\nsingle GPU (CPU core). Our code and dataset are available here\nhttps://github.com/jiayi-wu-umd/MARVIS."
                },
                "authors": [
                    {
                        "name": "Jiayi Wu"
                    },
                    {
                        "name": "Xiaomin Lin"
                    },
                    {
                        "name": "Shahriar Negahdaripour"
                    },
                    {
                        "name": "Cornelia Fermller"
                    },
                    {
                        "name": "Yiannis Aloimonos"
                    }
                ],
                "author_detail": {
                    "name": "Yiannis Aloimonos"
                },
                "author": "Yiannis Aloimonos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.09850v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.09850v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.18045v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.18045v3",
                "updated": "2024-10-03T14:44:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    44,
                    44,
                    3,
                    277,
                    0
                ],
                "published": "2024-02-28T04:43:46Z",
                "published_parsed": [
                    2024,
                    2,
                    28,
                    4,
                    43,
                    46,
                    2,
                    59,
                    0
                ],
                "title": "Multi-FAct: Assessing Factuality of Multilingual LLMs using FActScore",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-FAct: Assessing Factuality of Multilingual LLMs using FActScore"
                },
                "summary": "Evaluating the factuality of long-form large language model (LLM)-generated\ntext is an important challenge. Recently there has been a surge of interest in\nfactuality evaluation for English, but little is known about the factuality\nevaluation of multilingual LLMs, specially when it comes to long-form\ngeneration. %This paper systematically evaluates multilingual LLMs' factual\naccuracy across languages and geographic regions. We introduce a simple\npipeline for multilingual factuality evaluation, by applying FActScore (Min et\nal., 2023) for diverse languages. In addition to evaluating multilingual\nfactual generation, we evaluate the factual accuracy of long-form text\ngeneration in topics that reflect regional diversity. We also examine the\nfeasibility of running the FActScore pipeline using non-English Wikipedia and\nprovide comprehensive guidelines on multilingual factual evaluation for\nregionally diverse topics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the factuality of long-form large language model (LLM)-generated\ntext is an important challenge. Recently there has been a surge of interest in\nfactuality evaluation for English, but little is known about the factuality\nevaluation of multilingual LLMs, specially when it comes to long-form\ngeneration. %This paper systematically evaluates multilingual LLMs' factual\naccuracy across languages and geographic regions. We introduce a simple\npipeline for multilingual factuality evaluation, by applying FActScore (Min et\nal., 2023) for diverse languages. In addition to evaluating multilingual\nfactual generation, we evaluate the factual accuracy of long-form text\ngeneration in topics that reflect regional diversity. We also examine the\nfeasibility of running the FActScore pipeline using non-English Wikipedia and\nprovide comprehensive guidelines on multilingual factual evaluation for\nregionally diverse topics."
                },
                "authors": [
                    {
                        "name": "Sheikh Shafayat"
                    },
                    {
                        "name": "Eunsu Kim"
                    },
                    {
                        "name": "Juhyun Oh"
                    },
                    {
                        "name": "Alice Oh"
                    }
                ],
                "author_detail": {
                    "name": "Alice Oh"
                },
                "author": "Alice Oh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.18045v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.18045v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02539v1",
                "updated": "2024-10-03T14:44:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    44,
                    15,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T14:44:15Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    44,
                    15,
                    3,
                    277,
                    0
                ],
                "title": "Exploiting HDMI and USB Ports for GPU Side-Channel Insights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploiting HDMI and USB Ports for GPU Side-Channel Insights"
                },
                "summary": "Modern computers rely on USB and HDMI ports for connecting external\nperipherals and display devices. Despite their built-in security measures,\nthese ports remain susceptible to passive power-based side-channel attacks.\nThis paper presents a new class of attacks that exploit power consumption\npatterns at these ports to infer GPU activities. We develop a custom device\nthat plugs into these ports and demonstrate that its high-resolution power\nmeasurements can drive successful inferences about GPU processes, such as\nneural network computations and video rendering. The ubiquitous presence of USB\nand HDMI ports allows for discreet placement of the device, and its\nnon-interference with data channels ensures that no security alerts are\ntriggered. Our findings underscore the need to reevaluate and strengthen the\ncurrent generation of HDMI and USB port security defenses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern computers rely on USB and HDMI ports for connecting external\nperipherals and display devices. Despite their built-in security measures,\nthese ports remain susceptible to passive power-based side-channel attacks.\nThis paper presents a new class of attacks that exploit power consumption\npatterns at these ports to infer GPU activities. We develop a custom device\nthat plugs into these ports and demonstrate that its high-resolution power\nmeasurements can drive successful inferences about GPU processes, such as\nneural network computations and video rendering. The ubiquitous presence of USB\nand HDMI ports allows for discreet placement of the device, and its\nnon-interference with data channels ensures that no security alerts are\ntriggered. Our findings underscore the need to reevaluate and strengthen the\ncurrent generation of HDMI and USB port security defenses."
                },
                "authors": [
                    {
                        "name": "Sayed Erfan Arefin"
                    },
                    {
                        "name": "Abdul Serwadda"
                    }
                ],
                "author_detail": {
                    "name": "Abdul Serwadda"
                },
                "author": "Abdul Serwadda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02536v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02536v1",
                "updated": "2024-10-03T14:42:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    42,
                    34,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T14:42:34Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    42,
                    34,
                    3,
                    277,
                    0
                ],
                "title": "Intelligence at the Edge of Chaos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligence at the Edge of Chaos"
                },
                "summary": "We explore the emergence of intelligent behavior in artificial systems by\ninvestigating how the complexity of rule-based systems influences the\ncapabilities of models trained to predict these rules. Our study focuses on\nelementary cellular automata (ECA), simple yet powerful one-dimensional systems\nthat generate behaviors ranging from trivial to highly complex. By training\ndistinct Large Language Models (LLMs) on different ECAs, we evaluated the\nrelationship between the complexity of the rules' behavior and the intelligence\nexhibited by the LLMs, as reflected in their performance on downstream tasks.\nOur findings reveal that rules with higher complexity lead to models exhibiting\ngreater intelligence, as demonstrated by their performance on reasoning and\nchess move prediction tasks. Both uniform and periodic systems, and often also\nhighly chaotic systems, resulted in poorer downstream performance, highlighting\na sweet spot of complexity conducive to intelligence. We conjecture that\nintelligence arises from the ability to predict complexity and that creating\nintelligence may require only exposure to complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore the emergence of intelligent behavior in artificial systems by\ninvestigating how the complexity of rule-based systems influences the\ncapabilities of models trained to predict these rules. Our study focuses on\nelementary cellular automata (ECA), simple yet powerful one-dimensional systems\nthat generate behaviors ranging from trivial to highly complex. By training\ndistinct Large Language Models (LLMs) on different ECAs, we evaluated the\nrelationship between the complexity of the rules' behavior and the intelligence\nexhibited by the LLMs, as reflected in their performance on downstream tasks.\nOur findings reveal that rules with higher complexity lead to models exhibiting\ngreater intelligence, as demonstrated by their performance on reasoning and\nchess move prediction tasks. Both uniform and periodic systems, and often also\nhighly chaotic systems, resulted in poorer downstream performance, highlighting\na sweet spot of complexity conducive to intelligence. We conjecture that\nintelligence arises from the ability to predict complexity and that creating\nintelligence may require only exposure to complexity."
                },
                "authors": [
                    {
                        "name": "Shiyang Zhang"
                    },
                    {
                        "name": "Aakash Patel"
                    },
                    {
                        "name": "Syed A Rizvi"
                    },
                    {
                        "name": "Nianchen Liu"
                    },
                    {
                        "name": "Sizhuang He"
                    },
                    {
                        "name": "Amin Karbasi"
                    },
                    {
                        "name": "Emanuele Zappala"
                    },
                    {
                        "name": "David van Dijk"
                    }
                ],
                "author_detail": {
                    "name": "David van Dijk"
                },
                "author": "David van Dijk",
                "arxiv_comment": "15 pages,8 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02536v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02536v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02535v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02535v1",
                "updated": "2024-10-03T14:41:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    41,
                    52,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T14:41:52Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    41,
                    52,
                    3,
                    277,
                    0
                ],
                "title": "The Origin of Enhanced Conductivity and Structure Change in Defective\n  Li4Ti5O12 or Blue-LTO : a study combined theoretical and experimental\n  perspectives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Origin of Enhanced Conductivity and Structure Change in Defective\n  Li4Ti5O12 or Blue-LTO : a study combined theoretical and experimental\n  perspectives"
                },
                "summary": "The spinel Li4Ti5O12 (LTO) has emerged as a promising anode material for the\nnext generation of all-solid-state Li-ion batteries (ASSB), primarily due to\nits characteristic \"zero strain\" charge/discharge behavior and exceptional\ncycling stability, which significantly prolongs battery lifespan. Pristine LTO,\nhowever, is hindered by poor ionic and electronic conductivity. By employing\ntailored sintering protocols that create oxygen vacancies, a high-performing,\nblue LTO material is achieved. It has been proposed that the increased\nelectronic conductivity could stem from vacancy-induced polarons. Yet, detailed\ninsights into polaron stability, distribution, and dynamics within both the LTO\nbulk and surface have remained elusive due to limited information on structural\nchanges. Utilizing Positron Annihilation Lifetime Spectroscopy (PALS) and\nCoincidence Doppler Broadening Spectroscopy (CDBS), in conjunction with Two\nComponent Density Functional Theory (TCDFT) with the on-site Hubbard U\ncorrection, enables us to probe the depth profile of defect species introduced\nby sintering in a reductive environment. Our research provides direct evidence\nof oxygen vacancy formation within the subsurface region, an inference drawn\nfrom the observation of \\ch{Ti^{3+}}. Our investigation into Li16d vacancy\nformation within the bulk region uncovers the interactions between mobile\nspecies, namely Li-ions and polarons. Furthermore, we delve into the polaron\nstability on the LTO surface, offering an explanation for the superior\nperformance of the (100) facet exposed LTO nanoparticle, as compared to its\n(111) exposed counterpart.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The spinel Li4Ti5O12 (LTO) has emerged as a promising anode material for the\nnext generation of all-solid-state Li-ion batteries (ASSB), primarily due to\nits characteristic \"zero strain\" charge/discharge behavior and exceptional\ncycling stability, which significantly prolongs battery lifespan. Pristine LTO,\nhowever, is hindered by poor ionic and electronic conductivity. By employing\ntailored sintering protocols that create oxygen vacancies, a high-performing,\nblue LTO material is achieved. It has been proposed that the increased\nelectronic conductivity could stem from vacancy-induced polarons. Yet, detailed\ninsights into polaron stability, distribution, and dynamics within both the LTO\nbulk and surface have remained elusive due to limited information on structural\nchanges. Utilizing Positron Annihilation Lifetime Spectroscopy (PALS) and\nCoincidence Doppler Broadening Spectroscopy (CDBS), in conjunction with Two\nComponent Density Functional Theory (TCDFT) with the on-site Hubbard U\ncorrection, enables us to probe the depth profile of defect species introduced\nby sintering in a reductive environment. Our research provides direct evidence\nof oxygen vacancy formation within the subsurface region, an inference drawn\nfrom the observation of \\ch{Ti^{3+}}. Our investigation into Li16d vacancy\nformation within the bulk region uncovers the interactions between mobile\nspecies, namely Li-ions and polarons. Furthermore, we delve into the polaron\nstability on the LTO surface, offering an explanation for the superior\nperformance of the (100) facet exposed LTO nanoparticle, as compared to its\n(111) exposed counterpart."
                },
                "authors": [
                    {
                        "name": "Yute Chan"
                    },
                    {
                        "name": "Cristina Grosu"
                    },
                    {
                        "name": "Matthias Kick"
                    },
                    {
                        "name": "Peter Jakes"
                    },
                    {
                        "name": "Stefan Seidlmayer"
                    },
                    {
                        "name": "Thomas Gigl"
                    },
                    {
                        "name": "Werner Egger"
                    },
                    {
                        "name": "Ruediger-A. Eichel"
                    },
                    {
                        "name": "Josef Granwehr"
                    },
                    {
                        "name": "Christoph Hugenschmidt"
                    },
                    {
                        "name": "Christoph Scheurer"
                    }
                ],
                "author_detail": {
                    "name": "Christoph Scheurer"
                },
                "author": "Christoph Scheurer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02535v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02535v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.17512v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.17512v3",
                "updated": "2024-10-03T14:41:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    41,
                    43,
                    3,
                    277,
                    0
                ],
                "published": "2024-02-27T13:54:48Z",
                "published_parsed": [
                    2024,
                    2,
                    27,
                    13,
                    54,
                    48,
                    1,
                    58,
                    0
                ],
                "title": "Latte: Latent Attention for Linear Time Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latte: Latent Attention for Linear Time Transformers"
                },
                "summary": "The time complexity of the standard attention mechanism in transformers\nscales quadratically with sequence length. We propose a probabilistic framework\nfor attention, enabling us to derive a novel low-rank linear\nre-parameterisation of both bidirectional and causal cases, based on defining a\nlatent variable model. Our method can be seamlessly integrated as a drop-in\nreplacement for the standard attention mechanism. Additionally, this framework\nprovides a natural extension for combining local standard attention with our\nglobal linear attention. This approach allows us to extend the context length\nof existing large pre-trained models with only a few additional training steps.\nThe resulting ``Latte Transformer'' achieves performance comparable to standard\nattention and other state-of-the-art models, while maintaining linear time and\nmemory complexity, along with constant-time next-token prediction during\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The time complexity of the standard attention mechanism in transformers\nscales quadratically with sequence length. We propose a probabilistic framework\nfor attention, enabling us to derive a novel low-rank linear\nre-parameterisation of both bidirectional and causal cases, based on defining a\nlatent variable model. Our method can be seamlessly integrated as a drop-in\nreplacement for the standard attention mechanism. Additionally, this framework\nprovides a natural extension for combining local standard attention with our\nglobal linear attention. This approach allows us to extend the context length\nof existing large pre-trained models with only a few additional training steps.\nThe resulting ``Latte Transformer'' achieves performance comparable to standard\nattention and other state-of-the-art models, while maintaining linear time and\nmemory complexity, along with constant-time next-token prediction during\ninference."
                },
                "authors": [
                    {
                        "name": "Rares Dolga"
                    },
                    {
                        "name": "Marius Cobzarenco"
                    },
                    {
                        "name": "David Barber"
                    }
                ],
                "author_detail": {
                    "name": "David Barber"
                },
                "author": "David Barber",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.17512v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.17512v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02534v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02534v1",
                "updated": "2024-10-03T14:40:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    40,
                    17,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T14:40:17Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    40,
                    17,
                    3,
                    277,
                    0
                ],
                "title": "Pseudo-Stereo Inputs: A Solution to the Occlusion Challenge in\n  Self-Supervised Stereo Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pseudo-Stereo Inputs: A Solution to the Occlusion Challenge in\n  Self-Supervised Stereo Matching"
                },
                "summary": "Self-supervised stereo matching holds great promise for application and\nresearch due to its independence from expensive labeled data. However, direct\nself-supervised stereo matching paradigms based on photometric loss functions\nhave consistently struggled with performance issues due to the occlusion\nchallenge. The crux of the occlusion challenge lies in the fact that the\npositions of occluded pixels consistently align with the epipolar search\ndirection defined by the input stereo images, leading to persistent information\nloss and erroneous feedback at fixed locations during self-supervised training.\nIn this work, we propose a simple yet highly effective pseudo-stereo inputs\nstrategy to address the core occlusion challenge. This strategy decouples the\ninput and feedback images, compelling the network to probabilistically sample\ninformation from both sides of the occluding objects. As a result, the\npersistent lack of information in the aforementioned fixed occlusion areas is\nmitigated. Building upon this, we further address feedback conflicts and\noverfitting issues arising from the strategy. By integrating these components,\nour method achieves stable and significant performance improvements compared to\nexisting methods. Quantitative experiments are conducted to evaluate the\nperformance. Qualitative experiments further demonstrate accurate disparity\ninference even at occluded regions. These results demonstrate a significant\nadvancement over previous methods in the field of direct self-supervised stereo\nmatching based on photometric loss. The proposed pseudo-stereo inputs strategy,\ndue to its simplicity and effectiveness, has the potential to serve as a new\nparadigm for direct self-supervised stereo matching. Code is available at\nhttps://github.com/qrzyang/Pseudo-Stereo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-supervised stereo matching holds great promise for application and\nresearch due to its independence from expensive labeled data. However, direct\nself-supervised stereo matching paradigms based on photometric loss functions\nhave consistently struggled with performance issues due to the occlusion\nchallenge. The crux of the occlusion challenge lies in the fact that the\npositions of occluded pixels consistently align with the epipolar search\ndirection defined by the input stereo images, leading to persistent information\nloss and erroneous feedback at fixed locations during self-supervised training.\nIn this work, we propose a simple yet highly effective pseudo-stereo inputs\nstrategy to address the core occlusion challenge. This strategy decouples the\ninput and feedback images, compelling the network to probabilistically sample\ninformation from both sides of the occluding objects. As a result, the\npersistent lack of information in the aforementioned fixed occlusion areas is\nmitigated. Building upon this, we further address feedback conflicts and\noverfitting issues arising from the strategy. By integrating these components,\nour method achieves stable and significant performance improvements compared to\nexisting methods. Quantitative experiments are conducted to evaluate the\nperformance. Qualitative experiments further demonstrate accurate disparity\ninference even at occluded regions. These results demonstrate a significant\nadvancement over previous methods in the field of direct self-supervised stereo\nmatching based on photometric loss. The proposed pseudo-stereo inputs strategy,\ndue to its simplicity and effectiveness, has the potential to serve as a new\nparadigm for direct self-supervised stereo matching. Code is available at\nhttps://github.com/qrzyang/Pseudo-Stereo."
                },
                "authors": [
                    {
                        "name": "Ruizhi Yang"
                    },
                    {
                        "name": "Xingqiang Li"
                    },
                    {
                        "name": "Jiajun Bai"
                    },
                    {
                        "name": "Jinsong Du"
                    }
                ],
                "author_detail": {
                    "name": "Jinsong Du"
                },
                "author": "Jinsong Du",
                "arxiv_comment": "Submitted to IEEE Transactions on Image Processing (TIP)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02534v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02534v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17932v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17932v2",
                "updated": "2024-10-03T14:34:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    34,
                    52,
                    3,
                    277,
                    0
                ],
                "published": "2024-06-25T20:47:10Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    20,
                    47,
                    10,
                    1,
                    177,
                    0
                ],
                "title": "SonicSense: Object Perception from In-Hand Acoustic Vibration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SonicSense: Object Perception from In-Hand Acoustic Vibration"
                },
                "summary": "We introduce SonicSense, a holistic design of hardware and software to enable\nrich robot object perception through in-hand acoustic vibration sensing. While\nprevious studies have shown promising results with acoustic sensing for object\nperception, current solutions are constrained to a handful of objects with\nsimple geometries and homogeneous materials, single-finger sensing, and mixing\ntraining and testing on the same objects. SonicSense enables container\ninventory status differentiation, heterogeneous material prediction, 3D shape\nreconstruction, and object re-identification from a diverse set of 83\nreal-world objects. Our system employs a simple but effective heuristic\nexploration policy to interact with the objects as well as end-to-end\nlearning-based algorithms to fuse vibration signals to infer object properties.\nOur framework underscores the significance of in-hand acoustic vibration\nsensing in advancing robot tactile perception.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce SonicSense, a holistic design of hardware and software to enable\nrich robot object perception through in-hand acoustic vibration sensing. While\nprevious studies have shown promising results with acoustic sensing for object\nperception, current solutions are constrained to a handful of objects with\nsimple geometries and homogeneous materials, single-finger sensing, and mixing\ntraining and testing on the same objects. SonicSense enables container\ninventory status differentiation, heterogeneous material prediction, 3D shape\nreconstruction, and object re-identification from a diverse set of 83\nreal-world objects. Our system employs a simple but effective heuristic\nexploration policy to interact with the objects as well as end-to-end\nlearning-based algorithms to fuse vibration signals to infer object properties.\nOur framework underscores the significance of in-hand acoustic vibration\nsensing in advancing robot tactile perception."
                },
                "authors": [
                    {
                        "name": "Jiaxun Liu"
                    },
                    {
                        "name": "Boyuan Chen"
                    }
                ],
                "author_detail": {
                    "name": "Boyuan Chen"
                },
                "author": "Boyuan Chen",
                "arxiv_comment": "Our project website is at: http://generalroboticslab.com/SonicSense",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17932v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17932v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20242v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20242v3",
                "updated": "2024-10-03T14:31:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    31,
                    39,
                    3,
                    277,
                    0
                ],
                "published": "2024-07-16T13:13:16Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    13,
                    13,
                    16,
                    1,
                    198,
                    0
                ],
                "title": "BadRobot: Manipulating Embodied LLMs in the Physical World",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BadRobot: Manipulating Embodied LLMs in the Physical World"
                },
                "summary": "Embodied AI represents systems where AI is integrated into physical entities,\nenabling them to perceive and interact with their surroundings. Large Language\nModel (LLM), which exhibits powerful language understanding abilities, has been\nextensively employed in embodied AI by facilitating sophisticated task\nplanning. However, a critical safety issue remains overlooked: could these\nembodied LLMs perpetrate harmful behaviors? In response, we introduce BadRobot,\na novel attack paradigm aiming to make embodied LLMs violate safety and ethical\nconstraints through typical voice-based user-system interactions. Specifically,\nthree vulnerabilities are exploited to achieve this type of attack: (i)\nmanipulation of LLMs within robotic systems, (ii) misalignment between\nlinguistic outputs and physical actions, and (iii) unintentional hazardous\nbehaviors caused by world knowledge's flaws. Furthermore, we construct a\nbenchmark of various malicious physical action queries to evaluate BadRobot's\nattack performance. Based on this benchmark, extensive experiments against\nexisting prominent embodied LLM frameworks (e.g., Voxposer, Code as Policies,\nand ProgPrompt) demonstrate the effectiveness of our BadRobot. Warning: This\npaper contains harmful AI-generated language and aggressive actions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied AI represents systems where AI is integrated into physical entities,\nenabling them to perceive and interact with their surroundings. Large Language\nModel (LLM), which exhibits powerful language understanding abilities, has been\nextensively employed in embodied AI by facilitating sophisticated task\nplanning. However, a critical safety issue remains overlooked: could these\nembodied LLMs perpetrate harmful behaviors? In response, we introduce BadRobot,\na novel attack paradigm aiming to make embodied LLMs violate safety and ethical\nconstraints through typical voice-based user-system interactions. Specifically,\nthree vulnerabilities are exploited to achieve this type of attack: (i)\nmanipulation of LLMs within robotic systems, (ii) misalignment between\nlinguistic outputs and physical actions, and (iii) unintentional hazardous\nbehaviors caused by world knowledge's flaws. Furthermore, we construct a\nbenchmark of various malicious physical action queries to evaluate BadRobot's\nattack performance. Based on this benchmark, extensive experiments against\nexisting prominent embodied LLM frameworks (e.g., Voxposer, Code as Policies,\nand ProgPrompt) demonstrate the effectiveness of our BadRobot. Warning: This\npaper contains harmful AI-generated language and aggressive actions."
                },
                "authors": [
                    {
                        "name": "Hangtao Zhang"
                    },
                    {
                        "name": "Chenyu Zhu"
                    },
                    {
                        "name": "Xianlong Wang"
                    },
                    {
                        "name": "Ziqi Zhou"
                    },
                    {
                        "name": "Changgan Yin"
                    },
                    {
                        "name": "Minghui Li"
                    },
                    {
                        "name": "Lulu Xue"
                    },
                    {
                        "name": "Yichen Wang"
                    },
                    {
                        "name": "Shengshan Hu"
                    },
                    {
                        "name": "Aishan Liu"
                    },
                    {
                        "name": "Peijin Guo"
                    },
                    {
                        "name": "Leo Yu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Leo Yu Zhang"
                },
                "author": "Leo Yu Zhang",
                "arxiv_comment": "38 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20242v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20242v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02523v1",
                "updated": "2024-10-03T14:29:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    29,
                    46,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T14:29:46Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    29,
                    46,
                    3,
                    277,
                    0
                ],
                "title": "Med-TTT: Vision Test-Time Training model for Medical Image Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Med-TTT: Vision Test-Time Training model for Medical Image Segmentation"
                },
                "summary": "Medical image segmentation plays a crucial role in clinical diagnosis and\ntreatment planning. Although models based on convolutional neural networks\n(CNNs) and Transformers have achieved remarkable success in medical image\nsegmentation tasks, they still face challenges such as high computational\ncomplexity and the loss of local features when capturing long-range\ndependencies. To address these limitations, we propose Med-TTT, a visual\nbackbone network integrated with Test-Time Training (TTT) layers, which\nincorporates dynamic adjustment capabilities. Med-TTT introduces the Vision-TTT\nlayer, which enables effective modeling of long-range dependencies with linear\ncomputational complexity and adaptive parameter adjustment during inference.\nFurthermore, we designed a multi-resolution fusion mechanism to combine image\nfeatures at different scales, facilitating the identification of subtle lesion\ncharacteristics in complex backgrounds. At the same time, we adopt a frequency\ndomain feature enhancement strategy based on high pass filtering, which can\nbetter capture texture and fine-grained details in images. Experimental results\ndemonstrate that Med-TTT significantly outperforms existing methods on multiple\nmedical image datasets, exhibiting strong segmentation capabilities,\nparticularly in complex image backgrounds. The model achieves leading\nperformance in terms of accuracy, sensitivity, and Dice coefficient, providing\nan efficient and robust solution for the field of medical image\nsegmentation.The code is available at https://github.com/Jiashu-Xu/Med-TTT .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical image segmentation plays a crucial role in clinical diagnosis and\ntreatment planning. Although models based on convolutional neural networks\n(CNNs) and Transformers have achieved remarkable success in medical image\nsegmentation tasks, they still face challenges such as high computational\ncomplexity and the loss of local features when capturing long-range\ndependencies. To address these limitations, we propose Med-TTT, a visual\nbackbone network integrated with Test-Time Training (TTT) layers, which\nincorporates dynamic adjustment capabilities. Med-TTT introduces the Vision-TTT\nlayer, which enables effective modeling of long-range dependencies with linear\ncomputational complexity and adaptive parameter adjustment during inference.\nFurthermore, we designed a multi-resolution fusion mechanism to combine image\nfeatures at different scales, facilitating the identification of subtle lesion\ncharacteristics in complex backgrounds. At the same time, we adopt a frequency\ndomain feature enhancement strategy based on high pass filtering, which can\nbetter capture texture and fine-grained details in images. Experimental results\ndemonstrate that Med-TTT significantly outperforms existing methods on multiple\nmedical image datasets, exhibiting strong segmentation capabilities,\nparticularly in complex image backgrounds. The model achieves leading\nperformance in terms of accuracy, sensitivity, and Dice coefficient, providing\nan efficient and robust solution for the field of medical image\nsegmentation.The code is available at https://github.com/Jiashu-Xu/Med-TTT ."
                },
                "authors": [
                    {
                        "name": "Jiashu Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jiashu Xu"
                },
                "author": "Jiashu Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02519v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02519v1",
                "updated": "2024-10-03T14:28:05Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    28,
                    5,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T14:28:05Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    28,
                    5,
                    3,
                    277,
                    0
                ],
                "title": "Semantic-Guided RL for Interpretable Feature Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic-Guided RL for Interpretable Feature Engineering"
                },
                "summary": "The quality of Machine Learning (ML) models strongly depends on the input\ndata, as such generating high-quality features is often required to improve the\npredictive accuracy. This process is referred to as Feature Engineering (FE).\nHowever, since manual feature engineering is time-consuming and requires\ncase-by-case domain knowledge, Automated Feature Engineering (AutoFE) is\ncrucial. A major challenge that remains is to generate interpretable features.\nTo tackle this problem, we introduce SMART, a hybrid approach that uses\nsemantic technologies to guide the generation of interpretable features through\na two-step process: Exploitation and Exploration. The former uses Description\nLogics (DL) to reason on the semantics embedded in Knowledge Graphs (KG) to\ninfer domain-specific features, while the latter exploits the knowledge graph\nto conduct a guided exploration of the search space through Deep Reinforcement\nLearning (DRL). Our experiments on public datasets demonstrate that SMART\nsignificantly improves prediction accuracy while ensuring a high level of\ninterpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The quality of Machine Learning (ML) models strongly depends on the input\ndata, as such generating high-quality features is often required to improve the\npredictive accuracy. This process is referred to as Feature Engineering (FE).\nHowever, since manual feature engineering is time-consuming and requires\ncase-by-case domain knowledge, Automated Feature Engineering (AutoFE) is\ncrucial. A major challenge that remains is to generate interpretable features.\nTo tackle this problem, we introduce SMART, a hybrid approach that uses\nsemantic technologies to guide the generation of interpretable features through\na two-step process: Exploitation and Exploration. The former uses Description\nLogics (DL) to reason on the semantics embedded in Knowledge Graphs (KG) to\ninfer domain-specific features, while the latter exploits the knowledge graph\nto conduct a guided exploration of the search space through Deep Reinforcement\nLearning (DRL). Our experiments on public datasets demonstrate that SMART\nsignificantly improves prediction accuracy while ensuring a high level of\ninterpretability."
                },
                "authors": [
                    {
                        "name": "Mohamed Bouadi"
                    },
                    {
                        "name": "Arta Alavi"
                    },
                    {
                        "name": "Salima Benbernou"
                    },
                    {
                        "name": "Mourad Ouziri"
                    }
                ],
                "author_detail": {
                    "name": "Mourad Ouziri"
                },
                "author": "Mourad Ouziri",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2406.00544",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02519v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02519v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.15656v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.15656v2",
                "updated": "2024-10-03T14:27:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    27,
                    14,
                    3,
                    277,
                    0
                ],
                "published": "2023-09-27T13:45:38Z",
                "published_parsed": [
                    2023,
                    9,
                    27,
                    13,
                    45,
                    38,
                    2,
                    270,
                    0
                ],
                "title": "Conversational Feedback in Scripted versus Spontaneous Dialogues: A\n  Comparative Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational Feedback in Scripted versus Spontaneous Dialogues: A\n  Comparative Analysis"
                },
                "summary": "Scripted dialogues such as movie and TV subtitles constitute a widespread\nsource of training data for conversational NLP models. However, there are\nnotable linguistic differences between these dialogues and spontaneous\ninteractions, especially regarding the occurrence of communicative feedback\nsuch as backchannels, acknowledgments, or clarification requests. This paper\npresents a quantitative analysis of such feedback phenomena in both subtitles\nand spontaneous conversations. Based on conversational data spanning eight\nlanguages and multiple genres, we extract lexical statistics, classifications\nfrom a dialogue act tagger, expert annotations and labels derived from a\nfine-tuned Large Language Model (LLM). Our main empirical findings are that (1)\ncommunicative feedback is markedly less frequent in subtitles than in\nspontaneous dialogues and (2) subtitles contain a higher proportion of negative\nfeedback. We also show that dialogues generated by standard LLMs lie much\ncloser to scripted dialogues than spontaneous interactions in terms of\ncommunicative feedback.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scripted dialogues such as movie and TV subtitles constitute a widespread\nsource of training data for conversational NLP models. However, there are\nnotable linguistic differences between these dialogues and spontaneous\ninteractions, especially regarding the occurrence of communicative feedback\nsuch as backchannels, acknowledgments, or clarification requests. This paper\npresents a quantitative analysis of such feedback phenomena in both subtitles\nand spontaneous conversations. Based on conversational data spanning eight\nlanguages and multiple genres, we extract lexical statistics, classifications\nfrom a dialogue act tagger, expert annotations and labels derived from a\nfine-tuned Large Language Model (LLM). Our main empirical findings are that (1)\ncommunicative feedback is markedly less frequent in subtitles than in\nspontaneous dialogues and (2) subtitles contain a higher proportion of negative\nfeedback. We also show that dialogues generated by standard LLMs lie much\ncloser to scripted dialogues than spontaneous interactions in terms of\ncommunicative feedback."
                },
                "authors": [
                    {
                        "name": "Ildik Piln"
                    },
                    {
                        "name": "Laurent Prvot"
                    },
                    {
                        "name": "Hendrik Buschmeier"
                    },
                    {
                        "name": "Pierre Lison"
                    }
                ],
                "author_detail": {
                    "name": "Pierre Lison"
                },
                "author": "Pierre Lison",
                "arxiv_doi": "10.18653/v1/2024.sigdial-1.38",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.18653/v1/2024.sigdial-1.38",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2309.15656v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.15656v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Updated version for SIGdial 2024",
                "arxiv_journal_ref": "In Proceedings of SIGdial 2024, pp. 440-457. Kyoto, Japan (2024)",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02511v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02511v1",
                "updated": "2024-10-03T14:21:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    21,
                    23,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T14:21:23Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    21,
                    23,
                    3,
                    277,
                    0
                ],
                "title": "Choices are More Important than Efforts: LLM Enables Efficient\n  Multi-Agent Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Choices are More Important than Efforts: LLM Enables Efficient\n  Multi-Agent Exploration"
                },
                "summary": "With expansive state-action spaces, efficient multi-agent exploration remains\na longstanding challenge in reinforcement learning. Although pursuing novelty,\ndiversity, or uncertainty attracts increasing attention, redundant efforts\nbrought by exploration without proper guidance choices poses a practical issue\nfor the community. This paper introduces a systematic approach, termed LEMAE,\nchoosing to channel informative task-relevant guidance from a knowledgeable\nLarge Language Model (LLM) for Efficient Multi-Agent Exploration. Specifically,\nwe ground linguistic knowledge from LLM into symbolic key states, that are\ncritical for task fulfillment, in a discriminative manner at low LLM inference\ncosts. To unleash the power of key states, we design Subspace-based Hindsight\nIntrinsic Reward (SHIR) to guide agents toward key states by increasing reward\ndensity. Additionally, we build the Key State Memory Tree (KSMT) to track\ntransitions between key states in a specific task for organized exploration.\nBenefiting from diminishing redundant explorations, LEMAE outperforms existing\nSOTA approaches on the challenging benchmarks (e.g., SMAC and MPE) by a large\nmargin, achieving a 10x acceleration in certain scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With expansive state-action spaces, efficient multi-agent exploration remains\na longstanding challenge in reinforcement learning. Although pursuing novelty,\ndiversity, or uncertainty attracts increasing attention, redundant efforts\nbrought by exploration without proper guidance choices poses a practical issue\nfor the community. This paper introduces a systematic approach, termed LEMAE,\nchoosing to channel informative task-relevant guidance from a knowledgeable\nLarge Language Model (LLM) for Efficient Multi-Agent Exploration. Specifically,\nwe ground linguistic knowledge from LLM into symbolic key states, that are\ncritical for task fulfillment, in a discriminative manner at low LLM inference\ncosts. To unleash the power of key states, we design Subspace-based Hindsight\nIntrinsic Reward (SHIR) to guide agents toward key states by increasing reward\ndensity. Additionally, we build the Key State Memory Tree (KSMT) to track\ntransitions between key states in a specific task for organized exploration.\nBenefiting from diminishing redundant explorations, LEMAE outperforms existing\nSOTA approaches on the challenging benchmarks (e.g., SMAC and MPE) by a large\nmargin, achieving a 10x acceleration in certain scenarios."
                },
                "authors": [
                    {
                        "name": "Yun Qu"
                    },
                    {
                        "name": "Boyuan Wang"
                    },
                    {
                        "name": "Yuhang Jiang"
                    },
                    {
                        "name": "Jianzhun Shao"
                    },
                    {
                        "name": "Yixiu Mao"
                    },
                    {
                        "name": "Cheems Wang"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Xiangyang Ji"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyang Ji"
                },
                "author": "Xiangyang Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02511v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02511v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02507v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02507v1",
                "updated": "2024-10-03T14:15:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    15,
                    0,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T14:15:00Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    15,
                    0,
                    3,
                    277,
                    0
                ],
                "title": "Can Large Language Models Grasp Legal Theories? Enhance Legal Reasoning\n  with Insights from Multi-Agent Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Grasp Legal Theories? Enhance Legal Reasoning\n  with Insights from Multi-Agent Collaboration"
                },
                "summary": "Large Language Models (LLMs) could struggle to fully understand legal\ntheories and perform complex legal reasoning tasks. In this study, we introduce\na challenging task (confusing charge prediction) to better evaluate LLMs'\nunderstanding of legal theories and reasoning capabilities. We also propose a\nnovel framework: Multi-Agent framework for improving complex Legal Reasoning\ncapability (MALR). MALR employs non-parametric learning, encouraging LLMs to\nautomatically decompose complex legal tasks and mimic human learning process to\nextract insights from legal rules, helping LLMs better understand legal\ntheories and enhance their legal reasoning abilities. Extensive experiments on\nmultiple real-world datasets demonstrate that the proposed framework\neffectively addresses complex reasoning issues in practical scenarios, paving\nthe way for more reliable applications in the legal domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) could struggle to fully understand legal\ntheories and perform complex legal reasoning tasks. In this study, we introduce\na challenging task (confusing charge prediction) to better evaluate LLMs'\nunderstanding of legal theories and reasoning capabilities. We also propose a\nnovel framework: Multi-Agent framework for improving complex Legal Reasoning\ncapability (MALR). MALR employs non-parametric learning, encouraging LLMs to\nautomatically decompose complex legal tasks and mimic human learning process to\nextract insights from legal rules, helping LLMs better understand legal\ntheories and enhance their legal reasoning abilities. Extensive experiments on\nmultiple real-world datasets demonstrate that the proposed framework\neffectively addresses complex reasoning issues in practical scenarios, paving\nthe way for more reliable applications in the legal domain."
                },
                "authors": [
                    {
                        "name": "Weikang Yuan"
                    },
                    {
                        "name": "Junjie Cao"
                    },
                    {
                        "name": "Zhuoren Jiang"
                    },
                    {
                        "name": "Yangyang Kang"
                    },
                    {
                        "name": "Jun Lin"
                    },
                    {
                        "name": "Kaisong Song"
                    },
                    {
                        "name": "tianqianjin lin"
                    },
                    {
                        "name": "Pengwei Yan"
                    },
                    {
                        "name": "Changlong Sun"
                    },
                    {
                        "name": "Xiaozhong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaozhong Liu"
                },
                "author": "Xiaozhong Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02507v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02507v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02506v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02506v1",
                "updated": "2024-10-03T14:14:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    14,
                    31,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T14:14:31Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    14,
                    31,
                    3,
                    277,
                    0
                ],
                "title": "Cut the Crap: An Economical Communication Pipeline for LLM-based\n  Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cut the Crap: An Economical Communication Pipeline for LLM-based\n  Multi-Agent Systems"
                },
                "summary": "Recent advancements in large language model (LLM)-powered agents have shown\nthat collective intelligence can significantly outperform individual\ncapabilities, largely attributed to the meticulously designed inter-agent\ncommunication topologies. Though impressive in performance, existing\nmulti-agent pipelines inherently introduce substantial token overhead, as well\nas increased economic costs, which pose challenges for their large-scale\ndeployments. In response to this challenge, we propose an economical, simple,\nand robust multi-agent communication framework, termed $\\texttt{AgentPrune}$,\nwhich can seamlessly integrate into mainstream multi-agent systems and prunes\nredundant or even malicious communication messages. Technically,\n$\\texttt{AgentPrune}$ is the first to identify and formally define the\n\\textit{communication redundancy} issue present in current LLM-based\nmulti-agent pipelines, and efficiently performs one-shot pruning on the\nspatial-temporal message-passing graph, yielding a token-economic and\nhigh-performing communication topology. Extensive experiments across six\nbenchmarks demonstrate that $\\texttt{AgentPrune}$ \\textbf{(I)} achieves\ncomparable results as state-of-the-art topologies at merely $\\$5.6$ cost\ncompared to their $\\$43.7$, \\textbf{(II)} integrates seamlessly into existing\nmulti-agent frameworks with $28.1\\%\\sim72.8\\%\\downarrow$ token reduction, and\n\\textbf{(III)} successfully defend against two types of agent-based adversarial\nattacks with $3.5\\%\\sim10.8\\%\\uparrow$ performance boost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language model (LLM)-powered agents have shown\nthat collective intelligence can significantly outperform individual\ncapabilities, largely attributed to the meticulously designed inter-agent\ncommunication topologies. Though impressive in performance, existing\nmulti-agent pipelines inherently introduce substantial token overhead, as well\nas increased economic costs, which pose challenges for their large-scale\ndeployments. In response to this challenge, we propose an economical, simple,\nand robust multi-agent communication framework, termed $\\texttt{AgentPrune}$,\nwhich can seamlessly integrate into mainstream multi-agent systems and prunes\nredundant or even malicious communication messages. Technically,\n$\\texttt{AgentPrune}$ is the first to identify and formally define the\n\\textit{communication redundancy} issue present in current LLM-based\nmulti-agent pipelines, and efficiently performs one-shot pruning on the\nspatial-temporal message-passing graph, yielding a token-economic and\nhigh-performing communication topology. Extensive experiments across six\nbenchmarks demonstrate that $\\texttt{AgentPrune}$ \\textbf{(I)} achieves\ncomparable results as state-of-the-art topologies at merely $\\$5.6$ cost\ncompared to their $\\$43.7$, \\textbf{(II)} integrates seamlessly into existing\nmulti-agent frameworks with $28.1\\%\\sim72.8\\%\\downarrow$ token reduction, and\n\\textbf{(III)} successfully defend against two types of agent-based adversarial\nattacks with $3.5\\%\\sim10.8\\%\\uparrow$ performance boost."
                },
                "authors": [
                    {
                        "name": "Guibin Zhang"
                    },
                    {
                        "name": "Yanwei Yue"
                    },
                    {
                        "name": "Zhixun Li"
                    },
                    {
                        "name": "Sukwon Yun"
                    },
                    {
                        "name": "Guancheng Wan"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Dawei Cheng"
                    },
                    {
                        "name": "Jeffrey Xu Yu"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02506v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02506v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02505v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02505v1",
                "updated": "2024-10-03T14:14:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    14,
                    21,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T14:14:21Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    14,
                    21,
                    3,
                    277,
                    0
                ],
                "title": "Dog-IQA: Standard-guided Zero-shot MLLM for Mix-grained Image Quality\n  Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dog-IQA: Standard-guided Zero-shot MLLM for Mix-grained Image Quality\n  Assessment"
                },
                "summary": "Image quality assessment (IQA) serves as the golden standard for all models'\nperformance in nearly all computer vision fields. However, it still suffers\nfrom poor out-of-distribution generalization ability and expensive training\ncosts. To address these problems, we propose Dog-IQA, a standard-guided\nzero-shot mix-grained IQA method, which is training-free and utilizes the\nexceptional prior knowledge of multimodal large language models (MLLMs). To\nobtain accurate IQA scores, namely scores consistent with humans, we design an\nMLLM-based inference pipeline that imitates human experts. In detail, Dog-IQA\napplies two techniques. First, Dog-IQA objectively scores with specific\nstandards that utilize MLLM's behavior pattern and minimize the influence of\nsubjective factors. Second, Dog-IQA comprehensively takes local semantic\nobjects and the whole image as input and aggregates their scores, leveraging\nlocal and global information. Our proposed Dog-IQA achieves state-of-the-art\n(SOTA) performance compared with training-free methods, and competitive\nperformance compared with training-based methods in cross-dataset scenarios.\nOur code and models will be available at https://github.com/Kai-Liu001/Dog-IQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image quality assessment (IQA) serves as the golden standard for all models'\nperformance in nearly all computer vision fields. However, it still suffers\nfrom poor out-of-distribution generalization ability and expensive training\ncosts. To address these problems, we propose Dog-IQA, a standard-guided\nzero-shot mix-grained IQA method, which is training-free and utilizes the\nexceptional prior knowledge of multimodal large language models (MLLMs). To\nobtain accurate IQA scores, namely scores consistent with humans, we design an\nMLLM-based inference pipeline that imitates human experts. In detail, Dog-IQA\napplies two techniques. First, Dog-IQA objectively scores with specific\nstandards that utilize MLLM's behavior pattern and minimize the influence of\nsubjective factors. Second, Dog-IQA comprehensively takes local semantic\nobjects and the whole image as input and aggregates their scores, leveraging\nlocal and global information. Our proposed Dog-IQA achieves state-of-the-art\n(SOTA) performance compared with training-free methods, and competitive\nperformance compared with training-based methods in cross-dataset scenarios.\nOur code and models will be available at https://github.com/Kai-Liu001/Dog-IQA."
                },
                "authors": [
                    {
                        "name": "Kai Liu"
                    },
                    {
                        "name": "Ziqing Zhang"
                    },
                    {
                        "name": "Wenbo Li"
                    },
                    {
                        "name": "Renjing Pei"
                    },
                    {
                        "name": "Fenglong Song"
                    },
                    {
                        "name": "Xiaohong Liu"
                    },
                    {
                        "name": "Linghe Kong"
                    },
                    {
                        "name": "Yulun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yulun Zhang"
                },
                "author": "Yulun Zhang",
                "arxiv_comment": "10 pages, 5 figures. The code and models will be available at\n  https://github.com/Kai-Liu001/Dog-IQA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02505v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02505v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18028v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18028v2",
                "updated": "2024-10-03T14:11:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    11,
                    23,
                    3,
                    277,
                    0
                ],
                "published": "2024-09-26T16:34:35Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    16,
                    34,
                    35,
                    3,
                    270,
                    0
                ],
                "title": "Compositional Hardness of Code in Large Language Models -- A\n  Probabilistic Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compositional Hardness of Code in Large Language Models -- A\n  Probabilistic Perspective"
                },
                "summary": "A common practice in large language model (LLM) usage for complex analytical\ntasks such as code generation, is to sample a solution for the entire task\nwithin the model's context window. Previous works have shown that subtask\ndecomposition within the model's context (chain of thought), is beneficial for\nsolving such tasks. In this work, we point a limitation of LLMs' ability to\nperform several sub-tasks within the same context window - an in-context\nhardness of composition, pointing to an advantage for distributing a decomposed\nproblem in a multi-agent system of LLMs. The hardness of composition is\nquantified by a generation complexity metric, i.e., the number of LLM\ngenerations required to sample at least one correct solution. We find a gap\nbetween the generation complexity of solving a compositional problem within the\nsame context relative to distributing it among multiple agents, that increases\nexponentially with the solution's length. We prove our results theoretically\nand demonstrate them empirically.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A common practice in large language model (LLM) usage for complex analytical\ntasks such as code generation, is to sample a solution for the entire task\nwithin the model's context window. Previous works have shown that subtask\ndecomposition within the model's context (chain of thought), is beneficial for\nsolving such tasks. In this work, we point a limitation of LLMs' ability to\nperform several sub-tasks within the same context window - an in-context\nhardness of composition, pointing to an advantage for distributing a decomposed\nproblem in a multi-agent system of LLMs. The hardness of composition is\nquantified by a generation complexity metric, i.e., the number of LLM\ngenerations required to sample at least one correct solution. We find a gap\nbetween the generation complexity of solving a compositional problem within the\nsame context relative to distributing it among multiple agents, that increases\nexponentially with the solution's length. We prove our results theoretically\nand demonstrate them empirically."
                },
                "authors": [
                    {
                        "name": "Yotam Wolf"
                    },
                    {
                        "name": "Binyamin Rothberg"
                    },
                    {
                        "name": "Dorin Shteyman"
                    },
                    {
                        "name": "Amnon Shashua"
                    }
                ],
                "author_detail": {
                    "name": "Amnon Shashua"
                },
                "author": "Amnon Shashua",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18028v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18028v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02504v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02504v1",
                "updated": "2024-10-03T14:09:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    9,
                    58,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T14:09:58Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    9,
                    58,
                    3,
                    277,
                    0
                ],
                "title": "Dual Active Learning for Reinforcement Learning from Human Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dual Active Learning for Reinforcement Learning from Human Feedback"
                },
                "summary": "Aligning large language models (LLMs) with human preferences is critical to\nrecent advances in generative artificial intelligence. Reinforcement learning\nfrom human feedback (RLHF) is widely applied to achieve this objective. A key\nstep in RLHF is to learn the reward function from human feedback. However,\nhuman feedback is costly and time-consuming, making it essential to collect\nhigh-quality conversation data for human teachers to label. Additionally,\ndifferent human teachers have different levels of expertise. It is thus\ncritical to query the most appropriate teacher for their opinions. In this\npaper, we use offline reinforcement learning (RL) to formulate the alignment\nproblem. Motivated by the idea of $D$-optimal design, we first propose a dual\nactive reward learning algorithm for the simultaneous selection of\nconversations and teachers. Next, we apply pessimistic RL to solve the\nalignment problem, based on the learned reward estimator. Theoretically, we\nshow that the reward estimator obtained through our proposed adaptive selection\nstrategy achieves minimal generalized variance asymptotically, and prove that\nthe sub-optimality of our pessimistic policy scales as $O(1/\\sqrt{T})$ with a\ngiven sample budget $T$. Through simulations and experiments on LLMs, we\ndemonstrate the effectiveness of our algorithm and its superiority over\nstate-of-the-arts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning large language models (LLMs) with human preferences is critical to\nrecent advances in generative artificial intelligence. Reinforcement learning\nfrom human feedback (RLHF) is widely applied to achieve this objective. A key\nstep in RLHF is to learn the reward function from human feedback. However,\nhuman feedback is costly and time-consuming, making it essential to collect\nhigh-quality conversation data for human teachers to label. Additionally,\ndifferent human teachers have different levels of expertise. It is thus\ncritical to query the most appropriate teacher for their opinions. In this\npaper, we use offline reinforcement learning (RL) to formulate the alignment\nproblem. Motivated by the idea of $D$-optimal design, we first propose a dual\nactive reward learning algorithm for the simultaneous selection of\nconversations and teachers. Next, we apply pessimistic RL to solve the\nalignment problem, based on the learned reward estimator. Theoretically, we\nshow that the reward estimator obtained through our proposed adaptive selection\nstrategy achieves minimal generalized variance asymptotically, and prove that\nthe sub-optimality of our pessimistic policy scales as $O(1/\\sqrt{T})$ with a\ngiven sample budget $T$. Through simulations and experiments on LLMs, we\ndemonstrate the effectiveness of our algorithm and its superiority over\nstate-of-the-arts."
                },
                "authors": [
                    {
                        "name": "Pangpang Liu"
                    },
                    {
                        "name": "Chengchun Shi"
                    },
                    {
                        "name": "Will Wei Sun"
                    }
                ],
                "author_detail": {
                    "name": "Will Wei Sun"
                },
                "author": "Will Wei Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02504v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02504v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02499v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02499v1",
                "updated": "2024-10-03T14:01:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    1,
                    1,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T14:01:01Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    1,
                    1,
                    3,
                    277,
                    0
                ],
                "title": "Defining Knowledge: Bridging Epistemology and Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Defining Knowledge: Bridging Epistemology and Large Language Models"
                },
                "summary": "Knowledge claims are abundant in the literature on large language models\n(LLMs); but can we say that GPT-4 truly \"knows\" the Earth is round? To address\nthis question, we review standard definitions of knowledge in epistemology and\nwe formalize interpretations applicable to LLMs. In doing so, we identify\ninconsistencies and gaps in how current NLP research conceptualizes knowledge\nwith respect to epistemological frameworks. Additionally, we conduct a survey\nof 100 professional philosophers and computer scientists to compare their\npreferences in knowledge definitions and their views on whether LLMs can really\nbe said to know. Finally, we suggest evaluation protocols for testing knowledge\nin accordance to the most relevant definitions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge claims are abundant in the literature on large language models\n(LLMs); but can we say that GPT-4 truly \"knows\" the Earth is round? To address\nthis question, we review standard definitions of knowledge in epistemology and\nwe formalize interpretations applicable to LLMs. In doing so, we identify\ninconsistencies and gaps in how current NLP research conceptualizes knowledge\nwith respect to epistemological frameworks. Additionally, we conduct a survey\nof 100 professional philosophers and computer scientists to compare their\npreferences in knowledge definitions and their views on whether LLMs can really\nbe said to know. Finally, we suggest evaluation protocols for testing knowledge\nin accordance to the most relevant definitions."
                },
                "authors": [
                    {
                        "name": "Constanza Fierro"
                    },
                    {
                        "name": "Ruchira Dhar"
                    },
                    {
                        "name": "Filippos Stamatiou"
                    },
                    {
                        "name": "Nicolas Garneau"
                    },
                    {
                        "name": "Anders Sgaard"
                    }
                ],
                "author_detail": {
                    "name": "Anders Sgaard"
                },
                "author": "Anders Sgaard",
                "arxiv_comment": "EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02499v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02499v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02498v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02498v1",
                "updated": "2024-10-03T14:00:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    0,
                    44,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T14:00:44Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    0,
                    44,
                    3,
                    277,
                    0
                ],
                "title": "Dynamic Gradient Alignment for Online Data Mixing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Gradient Alignment for Online Data Mixing"
                },
                "summary": "The composition of training data mixtures is critical for effectively\ntraining large language models (LLMs), as it directly impacts their performance\non downstream tasks. Our goal is to identify an optimal data mixture to\nspecialize an LLM for a specific task with access to only a few examples.\nTraditional approaches to this problem include ad-hoc reweighting methods,\nimportance sampling, and gradient alignment techniques. This paper focuses on\ngradient alignment and introduces Dynamic Gradient Alignment (DGA), a scalable\nonline gradient alignment algorithm. DGA dynamically estimates the pre-training\ndata mixture on which the models' gradients align as well as possible with\nthose of the model on the specific task. DGA is the first gradient alignment\napproach that incurs minimal overhead compared to standard pre-training and\noutputs a competitive model, eliminating the need for retraining the model.\nExperimentally, we demonstrate significant improvements over importance\nsampling in two key scenarios: (i) when the pre-training set is small and\nimportance sampling overfits due to limited data; and (ii) when there is\ninsufficient specialized data, trapping importance sampling on narrow pockets\nof data. Our findings underscore the effectiveness of gradient alignment\nmethods in optimizing training data mixtures, particularly in data-constrained\nenvironments, and offer a practical solution for enhancing LLM performance on\nspecific tasks with limited data availability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The composition of training data mixtures is critical for effectively\ntraining large language models (LLMs), as it directly impacts their performance\non downstream tasks. Our goal is to identify an optimal data mixture to\nspecialize an LLM for a specific task with access to only a few examples.\nTraditional approaches to this problem include ad-hoc reweighting methods,\nimportance sampling, and gradient alignment techniques. This paper focuses on\ngradient alignment and introduces Dynamic Gradient Alignment (DGA), a scalable\nonline gradient alignment algorithm. DGA dynamically estimates the pre-training\ndata mixture on which the models' gradients align as well as possible with\nthose of the model on the specific task. DGA is the first gradient alignment\napproach that incurs minimal overhead compared to standard pre-training and\noutputs a competitive model, eliminating the need for retraining the model.\nExperimentally, we demonstrate significant improvements over importance\nsampling in two key scenarios: (i) when the pre-training set is small and\nimportance sampling overfits due to limited data; and (ii) when there is\ninsufficient specialized data, trapping importance sampling on narrow pockets\nof data. Our findings underscore the effectiveness of gradient alignment\nmethods in optimizing training data mixtures, particularly in data-constrained\nenvironments, and offer a practical solution for enhancing LLM performance on\nspecific tasks with limited data availability."
                },
                "authors": [
                    {
                        "name": "Simin Fan"
                    },
                    {
                        "name": "David Grangier"
                    },
                    {
                        "name": "Pierre Ablin"
                    }
                ],
                "author_detail": {
                    "name": "Pierre Ablin"
                },
                "author": "Pierre Ablin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02498v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02498v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2404.10917v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10917v2",
                "updated": "2024-10-03T17:59:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    59,
                    55,
                    3,
                    277,
                    0
                ],
                "published": "2024-04-16T21:33:05Z",
                "published_parsed": [
                    2024,
                    4,
                    16,
                    21,
                    33,
                    5,
                    1,
                    107,
                    0
                ],
                "title": "Which questions should I answer? Salience Prediction of Inquisitive\n  Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Which questions should I answer? Salience Prediction of Inquisitive\n  Questions"
                },
                "summary": "Inquisitive questions -- open-ended, curiosity-driven questions people ask as\nthey read -- are an integral part of discourse processing (Kehler and Rohde,\n2017; Onea, 2016) and comprehension (Prince, 2004). Recent work in NLP has\ntaken advantage of question generation capabilities of LLMs to enhance a wide\nrange of applications. But the space of inquisitive questions is vast: many\nquestions can be evoked from a given context. So which of those should be\nprioritized to find answers? Linguistic theories, unfortunately, have not yet\nprovided an answer to this question. This paper presents QSALIENCE, a salience\npredictor of inquisitive questions. QSALIENCE is instruction-tuned over our\ndataset of linguist-annotated salience scores of 1,766 (context, question)\npairs. A question scores high on salience if answering it would greatly enhance\nthe understanding of the text (Van Rooy, 2003). We show that highly salient\nquestions are empirically more likely to be answered in the same article,\nbridging potential questions (Onea, 2016) with Questions Under Discussion\n(Roberts, 2012). We further validate our findings by showing that answering\nsalient questions is an indicator of summarization quality in news.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inquisitive questions -- open-ended, curiosity-driven questions people ask as\nthey read -- are an integral part of discourse processing (Kehler and Rohde,\n2017; Onea, 2016) and comprehension (Prince, 2004). Recent work in NLP has\ntaken advantage of question generation capabilities of LLMs to enhance a wide\nrange of applications. But the space of inquisitive questions is vast: many\nquestions can be evoked from a given context. So which of those should be\nprioritized to find answers? Linguistic theories, unfortunately, have not yet\nprovided an answer to this question. This paper presents QSALIENCE, a salience\npredictor of inquisitive questions. QSALIENCE is instruction-tuned over our\ndataset of linguist-annotated salience scores of 1,766 (context, question)\npairs. A question scores high on salience if answering it would greatly enhance\nthe understanding of the text (Van Rooy, 2003). We show that highly salient\nquestions are empirically more likely to be answered in the same article,\nbridging potential questions (Onea, 2016) with Questions Under Discussion\n(Roberts, 2012). We further validate our findings by showing that answering\nsalient questions is an indicator of summarization quality in news."
                },
                "authors": [
                    {
                        "name": "Yating Wu"
                    },
                    {
                        "name": "Ritika Mangla"
                    },
                    {
                        "name": "Alexandros G. Dimakis"
                    },
                    {
                        "name": "Greg Durrett"
                    },
                    {
                        "name": "Junyi Jessy Li"
                    }
                ],
                "author_detail": {
                    "name": "Junyi Jessy Li"
                },
                "author": "Junyi Jessy Li",
                "arxiv_comment": "Camera Ready for EMNLP 2024 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.10917v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10917v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02757v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02757v1",
                "updated": "2024-10-03T17:59:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    59,
                    2,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T17:59:02Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    59,
                    2,
                    3,
                    277,
                    0
                ],
                "title": "Loong: Generating Minute-level Long Videos with Autoregressive Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Loong: Generating Minute-level Long Videos with Autoregressive Language\n  Models"
                },
                "summary": "It is desirable but challenging to generate content-rich long videos in the\nscale of minutes. Autoregressive large language models (LLMs) have achieved\ngreat success in generating coherent and long sequences of tokens in the domain\nof natural language processing, while the exploration of autoregressive LLMs\nfor video generation is limited to generating short videos of several seconds.\nIn this work, we conduct a deep analysis of the challenges that prevent\nautoregressive LLM-based video generators from generating long videos. Based on\nthe observations and analysis, we propose Loong, a new autoregressive LLM-based\nvideo generator that can generate minute-long videos. Specifically, we model\nthe text tokens and video tokens as a unified sequence for autoregressive LLMs\nand train the model from scratch. We propose progressive short-to-long training\nwith a loss re-weighting scheme to mitigate the loss imbalance problem for long\nvideo training. We further investigate inference strategies, including video\ntoken re-encoding and sampling strategies, to diminish error accumulation\nduring inference. Our proposed Loong can be trained on 10-second videos and be\nextended to generate minute-level long videos conditioned on text prompts, as\ndemonstrated by the results. More samples are available at:\nhttps://epiphqny.github.io/Loong-video.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is desirable but challenging to generate content-rich long videos in the\nscale of minutes. Autoregressive large language models (LLMs) have achieved\ngreat success in generating coherent and long sequences of tokens in the domain\nof natural language processing, while the exploration of autoregressive LLMs\nfor video generation is limited to generating short videos of several seconds.\nIn this work, we conduct a deep analysis of the challenges that prevent\nautoregressive LLM-based video generators from generating long videos. Based on\nthe observations and analysis, we propose Loong, a new autoregressive LLM-based\nvideo generator that can generate minute-long videos. Specifically, we model\nthe text tokens and video tokens as a unified sequence for autoregressive LLMs\nand train the model from scratch. We propose progressive short-to-long training\nwith a loss re-weighting scheme to mitigate the loss imbalance problem for long\nvideo training. We further investigate inference strategies, including video\ntoken re-encoding and sampling strategies, to diminish error accumulation\nduring inference. Our proposed Loong can be trained on 10-second videos and be\nextended to generate minute-level long videos conditioned on text prompts, as\ndemonstrated by the results. More samples are available at:\nhttps://epiphqny.github.io/Loong-video."
                },
                "authors": [
                    {
                        "name": "Yuqing Wang"
                    },
                    {
                        "name": "Tianwei Xiong"
                    },
                    {
                        "name": "Daquan Zhou"
                    },
                    {
                        "name": "Zhijie Lin"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Bingyi Kang"
                    },
                    {
                        "name": "Jiashi Feng"
                    },
                    {
                        "name": "Xihui Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xihui Liu"
                },
                "author": "Xihui Liu",
                "arxiv_comment": "Project page: https://epiphqny.github.io/Loong-video/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02757v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02757v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02755v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02755v1",
                "updated": "2024-10-03T17:58:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    58,
                    29,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T17:58:29Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    58,
                    29,
                    3,
                    277,
                    0
                ],
                "title": "SIEVE: General Purpose Data Filtering System Matching GPT-4o Accuracy at\n  1% the Cost",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SIEVE: General Purpose Data Filtering System Matching GPT-4o Accuracy at\n  1% the Cost"
                },
                "summary": "Creating specialized large language models requires vast amounts of clean,\nspecial purpose data for training and fine-tuning. With only a handful of\nexisting large-scale, domain-specific datasets, creation of new datasets is\nrequired in most applications. This requires the development of new\napplication-specific filtering of web-scale data. Filtering with a\nhigh-performance, general-purpose LLM such as GPT-4o can be highly effective,\nbut this is extremely expensive at web-scale. This paper proposes SIEVE, a\nlightweight alternative that matches GPT-4o accuracy at a fraction of the cost.\nSIEVE can perform up to 500 filtering operations for the cost of one GPT-4o\nfiltering call. The key to SIEVE is a seamless integration of GPT-4o and\nlightweight T5 models, using active learning to fine-tune T5 in the background\nwith a small number of calls to GPT-4o. Once trained, it performs as well as\nGPT-4o at a tiny fraction of the cost. We experimentally validate SIEVE on the\nOpenWebText dataset, using five highly customized filter tasks targeting high\nquality and domain-specific content. Our results demonstrate the effectiveness\nand efficiency of our method in curating large, high-quality datasets for\nlanguage model training at a substantially lower cost (1%) than existing\ntechniques. To further validate SIEVE, experiments show that SIEVE and GPT-4o\nachieve similar accuracy, with human evaluators preferring SIEVE's filtering\nresults to those of GPT-4o.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creating specialized large language models requires vast amounts of clean,\nspecial purpose data for training and fine-tuning. With only a handful of\nexisting large-scale, domain-specific datasets, creation of new datasets is\nrequired in most applications. This requires the development of new\napplication-specific filtering of web-scale data. Filtering with a\nhigh-performance, general-purpose LLM such as GPT-4o can be highly effective,\nbut this is extremely expensive at web-scale. This paper proposes SIEVE, a\nlightweight alternative that matches GPT-4o accuracy at a fraction of the cost.\nSIEVE can perform up to 500 filtering operations for the cost of one GPT-4o\nfiltering call. The key to SIEVE is a seamless integration of GPT-4o and\nlightweight T5 models, using active learning to fine-tune T5 in the background\nwith a small number of calls to GPT-4o. Once trained, it performs as well as\nGPT-4o at a tiny fraction of the cost. We experimentally validate SIEVE on the\nOpenWebText dataset, using five highly customized filter tasks targeting high\nquality and domain-specific content. Our results demonstrate the effectiveness\nand efficiency of our method in curating large, high-quality datasets for\nlanguage model training at a substantially lower cost (1%) than existing\ntechniques. To further validate SIEVE, experiments show that SIEVE and GPT-4o\nachieve similar accuracy, with human evaluators preferring SIEVE's filtering\nresults to those of GPT-4o."
                },
                "authors": [
                    {
                        "name": "Jifan Zhang"
                    },
                    {
                        "name": "Robert Nowak"
                    }
                ],
                "author_detail": {
                    "name": "Robert Nowak"
                },
                "author": "Robert Nowak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02755v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02755v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02749v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02749v1",
                "updated": "2024-10-03T17:57:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    57,
                    22,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T17:57:22Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    57,
                    22,
                    3,
                    277,
                    0
                ],
                "title": "Training Language Models on Synthetic Edit Sequences Improves Code\n  Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Language Models on Synthetic Edit Sequences Improves Code\n  Synthesis"
                },
                "summary": "Software engineers mainly write code by editing existing programs. In\ncontrast, large language models (LLMs) autoregressively synthesize programs in\na single pass. One explanation for this is the scarcity of open-sourced edit\ndata. While high-quality instruction data for code synthesis is already scarce,\nhigh-quality edit data is even scarcer. To fill this gap, we develop a\nsynthetic data generation algorithm called LintSeq. This algorithm refactors\nexisting code into a sequence of code edits by using a linter to procedurally\nsample across the error-free insertions that can be used to sequentially write\nprograms. It outputs edit sequences as text strings consisting of consecutive\nprogram diffs. To test LintSeq, we use it to refactor a dataset of instruction\n+ program pairs into instruction + program-diff-sequence tuples. Then, we\ninstruction finetune a series of smaller LLMs ranging from 2.6B to 14B\nparameters on both the re-factored and original versions of this dataset,\ncomparing zero-shot performance on code synthesis benchmarks. We show that\nduring repeated sampling, edit sequence finetuned models produce more diverse\nprograms than baselines. This results in better inference-time scaling for\nbenchmark coverage as a function of samples, i.e. the fraction of problems\n\"pass@k\" solved by any attempt given \"k\" tries. For example, on HumanEval\npass@50, small LLMs finetuned on synthetic edit sequences are competitive with\nGPT-4 and outperform models finetuned on the baseline dataset by +20% (+/-3%)\nin absolute score. Finally, we also pretrain our own tiny LMs for code\nunderstanding. We show that finetuning tiny models on synthetic code edits\nresults in state-of-the-art code synthesis for the on-device model class. Our\n150M parameter edit sequence LM matches or outperforms code models with twice\nas many parameters, both with and without repeated sampling, including Codex\nand AlphaCode.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software engineers mainly write code by editing existing programs. In\ncontrast, large language models (LLMs) autoregressively synthesize programs in\na single pass. One explanation for this is the scarcity of open-sourced edit\ndata. While high-quality instruction data for code synthesis is already scarce,\nhigh-quality edit data is even scarcer. To fill this gap, we develop a\nsynthetic data generation algorithm called LintSeq. This algorithm refactors\nexisting code into a sequence of code edits by using a linter to procedurally\nsample across the error-free insertions that can be used to sequentially write\nprograms. It outputs edit sequences as text strings consisting of consecutive\nprogram diffs. To test LintSeq, we use it to refactor a dataset of instruction\n+ program pairs into instruction + program-diff-sequence tuples. Then, we\ninstruction finetune a series of smaller LLMs ranging from 2.6B to 14B\nparameters on both the re-factored and original versions of this dataset,\ncomparing zero-shot performance on code synthesis benchmarks. We show that\nduring repeated sampling, edit sequence finetuned models produce more diverse\nprograms than baselines. This results in better inference-time scaling for\nbenchmark coverage as a function of samples, i.e. the fraction of problems\n\"pass@k\" solved by any attempt given \"k\" tries. For example, on HumanEval\npass@50, small LLMs finetuned on synthetic edit sequences are competitive with\nGPT-4 and outperform models finetuned on the baseline dataset by +20% (+/-3%)\nin absolute score. Finally, we also pretrain our own tiny LMs for code\nunderstanding. We show that finetuning tiny models on synthetic code edits\nresults in state-of-the-art code synthesis for the on-device model class. Our\n150M parameter edit sequence LM matches or outperforms code models with twice\nas many parameters, both with and without repeated sampling, including Codex\nand AlphaCode."
                },
                "authors": [
                    {
                        "name": "Ulyana Piterbarg"
                    },
                    {
                        "name": "Lerrel Pinto"
                    },
                    {
                        "name": "Rob Fergus"
                    }
                ],
                "author_detail": {
                    "name": "Rob Fergus"
                },
                "author": "Rob Fergus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02749v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02749v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18957v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18957v2",
                "updated": "2024-10-03T17:57:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    57,
                    7,
                    3,
                    277,
                    0
                ],
                "published": "2024-09-27T17:58:50Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    17,
                    58,
                    50,
                    4,
                    271,
                    0
                ],
                "title": "LML-DAP: Language Model Learning a Dataset for Data-Augmented Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LML-DAP: Language Model Learning a Dataset for Data-Augmented Prediction"
                },
                "summary": "Classification tasks are typically handled using Machine Learning (ML)\nmodels, which lack a balance between accuracy and interpretability. This paper\nintroduces a new approach to using Large Language Models (LLMs) for\nclassification tasks in an explainable way. Unlike ML models that rely heavily\non data cleaning and feature engineering, this method streamlines the process\nusing LLMs. This paper proposes a new concept called \"Language Model Learning\n(LML)\" powered by a new method called \"Data-Augmented Prediction (DAP)\". The\nclassification is performed by LLMs using a method similar to humans manually\nexploring and understanding the data and deciding classifications using data as\na reference. In the LML process, a dataset is summarized and evaluated to\ndetermine the features that lead to the classification of each label the most.\nIn the process of DAP, the system uses the data summary and a row of the\ntesting dataset to automatically generate a query, which is used to retrieve\nrelevant rows from the dataset. A classification is generated by the LLM using\ndata summary and relevant rows, ensuring satisfactory accuracy even with\ncomplex data using context-aware decision-making. LML and DAP unlock the\npossibilities of new applications. The proposed method uses the words \"Act as\nan Explainable Machine Learning Model\" in the prompt to enhance the\ninterpretability of the predictions by allowing users to review the logic\nbehind each prediction. In some test cases, the system scored an accuracy above\n90%, proving the effectiveness of the system and its potential to outperform\nconventional ML models in various scenarios. The code is available at\nhttps://github.com/Pro-GenAI/LML-DAP",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Classification tasks are typically handled using Machine Learning (ML)\nmodels, which lack a balance between accuracy and interpretability. This paper\nintroduces a new approach to using Large Language Models (LLMs) for\nclassification tasks in an explainable way. Unlike ML models that rely heavily\non data cleaning and feature engineering, this method streamlines the process\nusing LLMs. This paper proposes a new concept called \"Language Model Learning\n(LML)\" powered by a new method called \"Data-Augmented Prediction (DAP)\". The\nclassification is performed by LLMs using a method similar to humans manually\nexploring and understanding the data and deciding classifications using data as\na reference. In the LML process, a dataset is summarized and evaluated to\ndetermine the features that lead to the classification of each label the most.\nIn the process of DAP, the system uses the data summary and a row of the\ntesting dataset to automatically generate a query, which is used to retrieve\nrelevant rows from the dataset. A classification is generated by the LLM using\ndata summary and relevant rows, ensuring satisfactory accuracy even with\ncomplex data using context-aware decision-making. LML and DAP unlock the\npossibilities of new applications. The proposed method uses the words \"Act as\nan Explainable Machine Learning Model\" in the prompt to enhance the\ninterpretability of the predictions by allowing users to review the logic\nbehind each prediction. In some test cases, the system scored an accuracy above\n90%, proving the effectiveness of the system and its potential to outperform\nconventional ML models in various scenarios. The code is available at\nhttps://github.com/Pro-GenAI/LML-DAP"
                },
                "authors": [
                    {
                        "name": "Praneeth Vadlapati"
                    }
                ],
                "author_detail": {
                    "name": "Praneeth Vadlapati"
                },
                "author": "Praneeth Vadlapati",
                "arxiv_comment": "Updated title, abstract, and images",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18957v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18957v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02748v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02748v1",
                "updated": "2024-10-03T17:57:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    57,
                    1,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T17:57:01Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    57,
                    1,
                    3,
                    277,
                    0
                ],
                "title": "CriSPO: Multi-Aspect Critique-Suggestion-guided Automatic Prompt\n  Optimization for Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CriSPO: Multi-Aspect Critique-Suggestion-guided Automatic Prompt\n  Optimization for Text Generation"
                },
                "summary": "Large language models (LLMs) can generate fluent summaries across domains\nusing prompting techniques, reducing the need to train models for summarization\napplications. However, crafting effective prompts that guide LLMs to generate\nsummaries with the appropriate level of detail and writing style remains a\nchallenge. In this paper, we explore the use of salient information extracted\nfrom the source document to enhance summarization prompts. We show that adding\nkeyphrases in prompts can improve ROUGE F1 and recall, making the generated\nsummaries more similar to the reference and more complete. The number of\nkeyphrases can control the precision-recall trade-off. Furthermore, our\nanalysis reveals that incorporating phrase-level salient information is\nsuperior to word- or sentence-level. However, the impact on hallucination is\nnot universally positive across LLMs. To conduct this analysis, we introduce\nKeyphrase Signal Extractor (CriSPO), a lightweight model that can be finetuned\nto extract salient keyphrases. By using CriSPO, we achieve consistent ROUGE\nimprovements across datasets and open-weight and proprietary LLMs without any\nLLM customization. Our findings provide insights into leveraging salient\ninformation in building prompt-based summarization systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) can generate fluent summaries across domains\nusing prompting techniques, reducing the need to train models for summarization\napplications. However, crafting effective prompts that guide LLMs to generate\nsummaries with the appropriate level of detail and writing style remains a\nchallenge. In this paper, we explore the use of salient information extracted\nfrom the source document to enhance summarization prompts. We show that adding\nkeyphrases in prompts can improve ROUGE F1 and recall, making the generated\nsummaries more similar to the reference and more complete. The number of\nkeyphrases can control the precision-recall trade-off. Furthermore, our\nanalysis reveals that incorporating phrase-level salient information is\nsuperior to word- or sentence-level. However, the impact on hallucination is\nnot universally positive across LLMs. To conduct this analysis, we introduce\nKeyphrase Signal Extractor (CriSPO), a lightweight model that can be finetuned\nto extract salient keyphrases. By using CriSPO, we achieve consistent ROUGE\nimprovements across datasets and open-weight and proprietary LLMs without any\nLLM customization. Our findings provide insights into leveraging salient\ninformation in building prompt-based summarization systems."
                },
                "authors": [
                    {
                        "name": "Han He"
                    },
                    {
                        "name": "Qianchu Liu"
                    },
                    {
                        "name": "Lei Xu"
                    },
                    {
                        "name": "Chaitanya Shivade"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Sundararajan Srinivasan"
                    },
                    {
                        "name": "Katrin Kirchhoff"
                    }
                ],
                "author_detail": {
                    "name": "Katrin Kirchhoff"
                },
                "author": "Katrin Kirchhoff",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02748v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02748v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11687v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11687v2",
                "updated": "2024-10-03T17:56:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    56,
                    34,
                    3,
                    277,
                    0
                ],
                "published": "2024-06-17T16:05:32Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    16,
                    5,
                    32,
                    0,
                    169,
                    0
                ],
                "title": "Tokenization Falling Short: The Curse of Tokenization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokenization Falling Short: The Curse of Tokenization"
                },
                "summary": "Language models typically tokenize raw text into sequences of subword\nidentifiers from a predefined vocabulary, a process inherently sensitive to\ntypographical errors, length variations, and largely oblivious to the internal\nstructure of tokens--issues we term the curse of tokenization. In this study,\nwe delve into these drawbacks and demonstrate that large language models (LLMs)\nremain susceptible to these problems. This study systematically investigates\nthese challenges and their impact on LLMs through three critical research\nquestions: (1) complex problem solving, (2) token structure probing, and (3)\nresilience to typographical variation. Our findings reveal that scaling model\nparameters can mitigate the issue of tokenization; however, LLMs still suffer\nfrom biases induced by typos and other text format variations. Our experiments\nshow that subword regularization such as BPE-dropout can mitigate this issue.\nWe release our evaluation code and data at https://github.com/FloatAI/TKEval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models typically tokenize raw text into sequences of subword\nidentifiers from a predefined vocabulary, a process inherently sensitive to\ntypographical errors, length variations, and largely oblivious to the internal\nstructure of tokens--issues we term the curse of tokenization. In this study,\nwe delve into these drawbacks and demonstrate that large language models (LLMs)\nremain susceptible to these problems. This study systematically investigates\nthese challenges and their impact on LLMs through three critical research\nquestions: (1) complex problem solving, (2) token structure probing, and (3)\nresilience to typographical variation. Our findings reveal that scaling model\nparameters can mitigate the issue of tokenization; however, LLMs still suffer\nfrom biases induced by typos and other text format variations. Our experiments\nshow that subword regularization such as BPE-dropout can mitigate this issue.\nWe release our evaluation code and data at https://github.com/FloatAI/TKEval."
                },
                "authors": [
                    {
                        "name": "Yekun Chai"
                    },
                    {
                        "name": "Yewei Fang"
                    },
                    {
                        "name": "Qiwei Peng"
                    },
                    {
                        "name": "Xuhong Li"
                    }
                ],
                "author_detail": {
                    "name": "Xuhong Li"
                },
                "author": "Xuhong Li",
                "arxiv_comment": "EMNLP 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11687v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11687v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.10432v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.10432v3",
                "updated": "2024-10-03T17:55:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    55,
                    29,
                    3,
                    277,
                    0
                ],
                "published": "2023-07-19T19:40:34Z",
                "published_parsed": [
                    2023,
                    7,
                    19,
                    19,
                    40,
                    34,
                    2,
                    200,
                    0
                ],
                "title": "PharmacyGPT: The AI Pharmacist",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PharmacyGPT: The AI Pharmacist"
                },
                "summary": "In this study, we introduce PharmacyGPT, a novel framework to assess the\ncapabilities of large language models (LLMs) such as ChatGPT and GPT-4 in\nemulating the role of clinical pharmacists. Our methodology encompasses the\nutilization of LLMs to generate comprehensible patient clusters, formulate\nmedication plans, and forecast patient outcomes. We conduct our investigation\nusing real data acquired from the intensive care unit (ICU) at the University\nof North Carolina Chapel Hill (UNC) Hospital. Our analysis offers valuable\ninsights into the potential applications and limitations of LLMs in the field\nof clinical pharmacy, with implications for both patient care and the\ndevelopment of future AI-driven healthcare solutions. By evaluating the\nperformance of PharmacyGPT, we aim to contribute to the ongoing discourse\nsurrounding the integration of artificial intelligence in healthcare settings,\nultimately promoting the responsible and efficacious use of such technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we introduce PharmacyGPT, a novel framework to assess the\ncapabilities of large language models (LLMs) such as ChatGPT and GPT-4 in\nemulating the role of clinical pharmacists. Our methodology encompasses the\nutilization of LLMs to generate comprehensible patient clusters, formulate\nmedication plans, and forecast patient outcomes. We conduct our investigation\nusing real data acquired from the intensive care unit (ICU) at the University\nof North Carolina Chapel Hill (UNC) Hospital. Our analysis offers valuable\ninsights into the potential applications and limitations of LLMs in the field\nof clinical pharmacy, with implications for both patient care and the\ndevelopment of future AI-driven healthcare solutions. By evaluating the\nperformance of PharmacyGPT, we aim to contribute to the ongoing discourse\nsurrounding the integration of artificial intelligence in healthcare settings,\nultimately promoting the responsible and efficacious use of such technologies."
                },
                "authors": [
                    {
                        "name": "Zhengliang Liu"
                    },
                    {
                        "name": "Zihao Wu"
                    },
                    {
                        "name": "Mengxuan Hu"
                    },
                    {
                        "name": "Bokai Zhao"
                    },
                    {
                        "name": "Lin Zhao"
                    },
                    {
                        "name": "Tianyi Zhang"
                    },
                    {
                        "name": "Haixing Dai"
                    },
                    {
                        "name": "Xianyan Chen"
                    },
                    {
                        "name": "Ye Shen"
                    },
                    {
                        "name": "Sheng Li"
                    },
                    {
                        "name": "Quanzheng Li"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Brian Murray"
                    },
                    {
                        "name": "Tianming Liu"
                    },
                    {
                        "name": "Andrea Sikora"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Sikora"
                },
                "author": "Andrea Sikora",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2307.10432v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.10432v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02743v1",
                "updated": "2024-10-03T17:55:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    55,
                    13,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T17:55:13Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    55,
                    13,
                    3,
                    277,
                    0
                ],
                "title": "MA-RLHF: Reinforcement Learning from Human Feedback with Macro Actions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MA-RLHF: Reinforcement Learning from Human Feedback with Macro Actions"
                },
                "summary": "Reinforcement learning from human feedback (RLHF) has demonstrated\neffectiveness in aligning large language models (LLMs) with human preferences.\nHowever, token-level RLHF suffers from the credit assignment problem over long\nsequences, where delayed rewards make it challenging for the model to discern\nwhich actions contributed to successful outcomes. This hinders learning\nefficiency and slows convergence. In this paper, we propose MA-RLHF, a simple\nyet effective RLHF framework that incorporates macro actions -- sequences of\ntokens or higher-level language constructs -- into the learning process. By\noperating at this higher level of abstraction, our approach reduces the\ntemporal distance between actions and rewards, facilitating faster and more\naccurate credit assignment. This results in more stable policy gradient\nestimates and enhances learning efficiency within each episode, all without\nincreasing computational complexity during training or inference. We validate\nour approach through extensive experiments across various model sizes and\ntasks, including text summarization, dialogue generation, question answering,\nand program synthesis. Our method achieves substantial performance improvements\nover standard RLHF, with performance gains of up to 30% in text summarization\nand code generation, 18% in dialogue, and 8% in question answering tasks.\nNotably, our approach reaches parity with vanilla RLHF 1.7x to 2x faster in\nterms of training time and continues to outperform it with further training. We\nwill make our code and data publicly available at\nhttps://github.com/ernie-research/MA-RLHF .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning from human feedback (RLHF) has demonstrated\neffectiveness in aligning large language models (LLMs) with human preferences.\nHowever, token-level RLHF suffers from the credit assignment problem over long\nsequences, where delayed rewards make it challenging for the model to discern\nwhich actions contributed to successful outcomes. This hinders learning\nefficiency and slows convergence. In this paper, we propose MA-RLHF, a simple\nyet effective RLHF framework that incorporates macro actions -- sequences of\ntokens or higher-level language constructs -- into the learning process. By\noperating at this higher level of abstraction, our approach reduces the\ntemporal distance between actions and rewards, facilitating faster and more\naccurate credit assignment. This results in more stable policy gradient\nestimates and enhances learning efficiency within each episode, all without\nincreasing computational complexity during training or inference. We validate\nour approach through extensive experiments across various model sizes and\ntasks, including text summarization, dialogue generation, question answering,\nand program synthesis. Our method achieves substantial performance improvements\nover standard RLHF, with performance gains of up to 30% in text summarization\nand code generation, 18% in dialogue, and 8% in question answering tasks.\nNotably, our approach reaches parity with vanilla RLHF 1.7x to 2x faster in\nterms of training time and continues to outperform it with further training. We\nwill make our code and data publicly available at\nhttps://github.com/ernie-research/MA-RLHF ."
                },
                "authors": [
                    {
                        "name": "Yekun Chai"
                    },
                    {
                        "name": "Haoran Sun"
                    },
                    {
                        "name": "Huang Fang"
                    },
                    {
                        "name": "Shuohuan Wang"
                    },
                    {
                        "name": "Yu Sun"
                    },
                    {
                        "name": "Hua Wu"
                    }
                ],
                "author_detail": {
                    "name": "Hua Wu"
                },
                "author": "Hua Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02742v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02742v1",
                "updated": "2024-10-03T17:55:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    55,
                    9,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T17:55:09Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    55,
                    9,
                    3,
                    277,
                    0
                ],
                "title": "Grounding Large Language Models In Embodied Environment With Imperfect\n  World Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grounding Large Language Models In Embodied Environment With Imperfect\n  World Models"
                },
                "summary": "Despite a widespread success in various applications, large language models\n(LLMs) often stumble when tackling basic physical reasoning or executing\nrobotics tasks, due to a lack of direct experience with the physical nuances of\nthe real world. To address these issues, we propose a Grounding Large language\nmodel with Imperfect world MOdel (GLIMO), which utilizes proxy world models\nsuch as simulators to collect and synthesize trining data. GLIMO incorporates\nan LLM agent-based data generator to automatically create high-quality and\ndiverse instruction datasets. The generator includes an iterative self-refining\nmodule for temporally consistent experience sampling, a diverse set of\nquestion-answering instruction seeds, and a retrieval-augmented generation\nmodule for reflecting on prior experiences. Comprehensive experiments show that\nour approach improve the performance of strong open-source LLMs like LLaMA-3\nwith a performance boost of 2.04 $\\times$, 1.54 $\\times$, and 1.82 $\\times$\nacross three different benchmarks, respectively. The performance is able to\ncompete with or surpass their larger counterparts such as GPT-4.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite a widespread success in various applications, large language models\n(LLMs) often stumble when tackling basic physical reasoning or executing\nrobotics tasks, due to a lack of direct experience with the physical nuances of\nthe real world. To address these issues, we propose a Grounding Large language\nmodel with Imperfect world MOdel (GLIMO), which utilizes proxy world models\nsuch as simulators to collect and synthesize trining data. GLIMO incorporates\nan LLM agent-based data generator to automatically create high-quality and\ndiverse instruction datasets. The generator includes an iterative self-refining\nmodule for temporally consistent experience sampling, a diverse set of\nquestion-answering instruction seeds, and a retrieval-augmented generation\nmodule for reflecting on prior experiences. Comprehensive experiments show that\nour approach improve the performance of strong open-source LLMs like LLaMA-3\nwith a performance boost of 2.04 $\\times$, 1.54 $\\times$, and 1.82 $\\times$\nacross three different benchmarks, respectively. The performance is able to\ncompete with or surpass their larger counterparts such as GPT-4."
                },
                "authors": [
                    {
                        "name": "Haolan Liu"
                    },
                    {
                        "name": "Jishen Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jishen Zhao"
                },
                "author": "Jishen Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02742v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02742v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02741v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02741v1",
                "updated": "2024-10-03T17:54:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    54,
                    56,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T17:54:56Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    54,
                    56,
                    3,
                    277,
                    0
                ],
                "title": "Salient Information Prompting to Steer Content in Prompt-based\n  Abstractive Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Salient Information Prompting to Steer Content in Prompt-based\n  Abstractive Summarization"
                },
                "summary": "Large language models (LLMs) can generate fluent summaries across domains\nusing prompting techniques, reducing the need to train models for summarization\napplications. However, crafting effective prompts that guide LLMs to generate\nsummaries with the appropriate level of detail and writing style remains a\nchallenge. In this paper, we explore the use of salient information extracted\nfrom the source document to enhance summarization prompts. We show that adding\nkeyphrases in prompts can improve ROUGE F1 and recall, making the generated\nsummaries more similar to the reference and more complete. The number of\nkeyphrases can control the precision-recall trade-off. Furthermore, our\nanalysis reveals that incorporating phrase-level salient information is\nsuperior to word- or sentence-level. However, the impact on hallucination is\nnot universally positive across LLMs. To conduct this analysis, we introduce\nKeyphrase Signal Extractor (SigExt), a lightweight model that can be finetuned\nto extract salient keyphrases. By using SigExt, we achieve consistent ROUGE\nimprovements across datasets and open-weight and proprietary LLMs without any\nLLM customization. Our findings provide insights into leveraging salient\ninformation in building prompt-based summarization systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) can generate fluent summaries across domains\nusing prompting techniques, reducing the need to train models for summarization\napplications. However, crafting effective prompts that guide LLMs to generate\nsummaries with the appropriate level of detail and writing style remains a\nchallenge. In this paper, we explore the use of salient information extracted\nfrom the source document to enhance summarization prompts. We show that adding\nkeyphrases in prompts can improve ROUGE F1 and recall, making the generated\nsummaries more similar to the reference and more complete. The number of\nkeyphrases can control the precision-recall trade-off. Furthermore, our\nanalysis reveals that incorporating phrase-level salient information is\nsuperior to word- or sentence-level. However, the impact on hallucination is\nnot universally positive across LLMs. To conduct this analysis, we introduce\nKeyphrase Signal Extractor (SigExt), a lightweight model that can be finetuned\nto extract salient keyphrases. By using SigExt, we achieve consistent ROUGE\nimprovements across datasets and open-weight and proprietary LLMs without any\nLLM customization. Our findings provide insights into leveraging salient\ninformation in building prompt-based summarization systems."
                },
                "authors": [
                    {
                        "name": "Lei Xu"
                    },
                    {
                        "name": "Mohammed Asad Karim"
                    },
                    {
                        "name": "Saket Dingliwal"
                    },
                    {
                        "name": "Aparna Elangovan"
                    }
                ],
                "author_detail": {
                    "name": "Aparna Elangovan"
                },
                "author": "Aparna Elangovan",
                "arxiv_comment": "Accepted to EMNLP 2024 Industry Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02741v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02740v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02740v1",
                "updated": "2024-10-03T17:54:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    54,
                    52,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T17:54:52Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    54,
                    52,
                    3,
                    277,
                    0
                ],
                "title": "Revisit Large-Scale Image-Caption Data in Pre-training Multimodal\n  Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisit Large-Scale Image-Caption Data in Pre-training Multimodal\n  Foundation Models"
                },
                "summary": "Recent advancements in multimodal models highlight the value of rewritten\ncaptions for improving performance, yet key challenges remain. For example,\nwhile synthetic captions often provide superior quality and image-text\nalignment, it is not clear whether they can fully replace AltTexts: the role of\nsynthetic captions and their interaction with original web-crawled AltTexts in\npre-training is still not well understood. Moreover, different multimodal\nfoundation models may have unique preferences for specific caption formats, but\nefforts to identify the optimal captions for each model remain limited. In this\nwork, we propose a novel, controllable, and scalable captioning pipeline\ndesigned to generate diverse caption formats tailored to various multimodal\nmodels. By examining Short Synthetic Captions (SSC) towards Dense Synthetic\nCaptions (DSC+) as case studies, we systematically explore their effects and\ninteractions with AltTexts across models such as CLIP, multimodal LLMs, and\ndiffusion models. Our findings reveal that a hybrid approach that keeps both\nsynthetic captions and AltTexts can outperform the use of synthetic captions\nalone, improving both alignment and performance, with each model demonstrating\npreferences for particular caption formats. This comprehensive analysis\nprovides valuable insights into optimizing captioning strategies, thereby\nadvancing the pre-training of multimodal foundation models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in multimodal models highlight the value of rewritten\ncaptions for improving performance, yet key challenges remain. For example,\nwhile synthetic captions often provide superior quality and image-text\nalignment, it is not clear whether they can fully replace AltTexts: the role of\nsynthetic captions and their interaction with original web-crawled AltTexts in\npre-training is still not well understood. Moreover, different multimodal\nfoundation models may have unique preferences for specific caption formats, but\nefforts to identify the optimal captions for each model remain limited. In this\nwork, we propose a novel, controllable, and scalable captioning pipeline\ndesigned to generate diverse caption formats tailored to various multimodal\nmodels. By examining Short Synthetic Captions (SSC) towards Dense Synthetic\nCaptions (DSC+) as case studies, we systematically explore their effects and\ninteractions with AltTexts across models such as CLIP, multimodal LLMs, and\ndiffusion models. Our findings reveal that a hybrid approach that keeps both\nsynthetic captions and AltTexts can outperform the use of synthetic captions\nalone, improving both alignment and performance, with each model demonstrating\npreferences for particular caption formats. This comprehensive analysis\nprovides valuable insights into optimizing captioning strategies, thereby\nadvancing the pre-training of multimodal foundation models."
                },
                "authors": [
                    {
                        "name": "Zhengfeng Lai"
                    },
                    {
                        "name": "Vasileios Saveris"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Hong-You Chen"
                    },
                    {
                        "name": "Haotian Zhang"
                    },
                    {
                        "name": "Bowen Zhang"
                    },
                    {
                        "name": "Juan Lao Tebar"
                    },
                    {
                        "name": "Wenze Hu"
                    },
                    {
                        "name": "Zhe Gan"
                    },
                    {
                        "name": "Peter Grasch"
                    },
                    {
                        "name": "Meng Cao"
                    },
                    {
                        "name": "Yinfei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yinfei Yang"
                },
                "author": "Yinfei Yang",
                "arxiv_comment": "CV/ML",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02740v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02740v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02736v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02736v1",
                "updated": "2024-10-03T17:53:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    53,
                    30,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T17:53:30Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    53,
                    30,
                    3,
                    277,
                    0
                ],
                "title": "Justice or Prejudice? Quantifying Biases in LLM-as-a-Judge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Justice or Prejudice? Quantifying Biases in LLM-as-a-Judge"
                },
                "summary": "LLM-as-a-Judge has been widely utilized as an evaluation method in various\nbenchmarks and served as supervised rewards in model training. However, despite\ntheir excellence in many domains, potential issues are under-explored,\nundermining their reliability and the scope of their utility. Therefore, we\nidentify 12 key potential biases and propose a new automated bias\nquantification framework-CALM-which systematically quantifies and analyzes each\ntype of bias in LLM-as-a-Judge by using automated and principle-guided\nmodification. Our experiments cover multiple popular language models, and the\nresults indicate that while advanced models have achieved commendable overall\nperformance, significant biases persist in certain specific tasks. Empirical\nresults suggest that there remains room for improvement in the reliability of\nLLM-as-a-Judge. Moreover, we also discuss the explicit and implicit influence\nof these biases and give some suggestions for the reliable application of\nLLM-as-a-Judge. Our work highlights the need for stakeholders to address these\nissues and remind users to exercise caution in LLM-as-a-Judge applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-as-a-Judge has been widely utilized as an evaluation method in various\nbenchmarks and served as supervised rewards in model training. However, despite\ntheir excellence in many domains, potential issues are under-explored,\nundermining their reliability and the scope of their utility. Therefore, we\nidentify 12 key potential biases and propose a new automated bias\nquantification framework-CALM-which systematically quantifies and analyzes each\ntype of bias in LLM-as-a-Judge by using automated and principle-guided\nmodification. Our experiments cover multiple popular language models, and the\nresults indicate that while advanced models have achieved commendable overall\nperformance, significant biases persist in certain specific tasks. Empirical\nresults suggest that there remains room for improvement in the reliability of\nLLM-as-a-Judge. Moreover, we also discuss the explicit and implicit influence\nof these biases and give some suggestions for the reliable application of\nLLM-as-a-Judge. Our work highlights the need for stakeholders to address these\nissues and remind users to exercise caution in LLM-as-a-Judge applications."
                },
                "authors": [
                    {
                        "name": "Jiayi Ye"
                    },
                    {
                        "name": "Yanbo Wang"
                    },
                    {
                        "name": "Yue Huang"
                    },
                    {
                        "name": "Dongping Chen"
                    },
                    {
                        "name": "Qihui Zhang"
                    },
                    {
                        "name": "Nuno Moniz"
                    },
                    {
                        "name": "Tian Gao"
                    },
                    {
                        "name": "Werner Geyer"
                    },
                    {
                        "name": "Chao Huang"
                    },
                    {
                        "name": "Pin-Yu Chen"
                    },
                    {
                        "name": "Nitesh V Chawla"
                    },
                    {
                        "name": "Xiangliang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiangliang Zhang"
                },
                "author": "Xiangliang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02736v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02736v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00023v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00023v2",
                "updated": "2024-10-03T17:50:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    50,
                    33,
                    3,
                    277,
                    0
                ],
                "published": "2024-05-08T06:30:58Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    6,
                    30,
                    58,
                    2,
                    129,
                    0
                ],
                "title": "Preble: Efficient Distributed Prompt Scheduling for LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preble: Efficient Distributed Prompt Scheduling for LLM Serving"
                },
                "summary": "Prompts to large language models (LLMs) have evolved beyond simple user\nquestions. For LLMs to solve complex problems, today's practices are to include\ndomain-specific instructions, illustration of tool usages, and/or long context\nsuch as textbook chapters in prompts. As such, many parts of prompts are\nrepetitive across requests. Recent works propose to cache and reuse KV state of\nprompts. However, they are all confined to a single-GPU optimization, while\nproduction LLM serving systems are distributed by nature.\n  This paper proposes Preble, the first distributed LLM serving platform that\ntargets and optimizes for prompt sharing. We designed a distributed scheduling\nsystem that co-optimizes KV state reuse and computation load-balancing with a\nnew scheduling algorithm and a hierarchical scheduling mechanism. Our\nevaluation of Preble with real workloads and request arrival patterns on two\nopen-source LLMs shows that Preble outperforms the SOTA serving systems by 1.5X\nto 14.5X on average latency and 2X to 10X on p99 latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompts to large language models (LLMs) have evolved beyond simple user\nquestions. For LLMs to solve complex problems, today's practices are to include\ndomain-specific instructions, illustration of tool usages, and/or long context\nsuch as textbook chapters in prompts. As such, many parts of prompts are\nrepetitive across requests. Recent works propose to cache and reuse KV state of\nprompts. However, they are all confined to a single-GPU optimization, while\nproduction LLM serving systems are distributed by nature.\n  This paper proposes Preble, the first distributed LLM serving platform that\ntargets and optimizes for prompt sharing. We designed a distributed scheduling\nsystem that co-optimizes KV state reuse and computation load-balancing with a\nnew scheduling algorithm and a hierarchical scheduling mechanism. Our\nevaluation of Preble with real workloads and request arrival patterns on two\nopen-source LLMs shows that Preble outperforms the SOTA serving systems by 1.5X\nto 14.5X on average latency and 2X to 10X on p99 latency."
                },
                "authors": [
                    {
                        "name": "Vikranth Srivatsa"
                    },
                    {
                        "name": "Zijian He"
                    },
                    {
                        "name": "Reyna Abhyankar"
                    },
                    {
                        "name": "Dongming Li"
                    },
                    {
                        "name": "Yiying Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yiying Zhang"
                },
                "author": "Yiying Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00023v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00023v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02725v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02725v1",
                "updated": "2024-10-03T17:47:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    47,
                    29,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T17:47:29Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    47,
                    29,
                    3,
                    277,
                    0
                ],
                "title": "Adaptive Inference-Time Compute: LLMs Can Predict if They Can Do Better,\n  Even Mid-Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Inference-Time Compute: LLMs Can Predict if They Can Do Better,\n  Even Mid-Generation"
                },
                "summary": "Inference-time computation is a powerful paradigm to enhance the performance\nof large language models (LLMs), with Best-of-N sampling being a widely used\ntechnique. However, this method is computationally expensive, requiring both\n(1) an external reward model and (2) the generation of multiple samples. In\nthis work, we introduce a new generative self-evaluation scheme designed to\nadaptively reduce the number of generated samples while maintaining or even\nimproving performance. We use a generative reward model formulation, allowing\nthe LLM to predict mid-generation the probability that restarting the\ngeneration will yield a better response. These predictions are obtained without\nan external reward model and can be used to decide whether or not to generate\nmore samples, prune unpromising samples early on, or to pick the best sample.\nThis capability is very inexpensive as it involves generating a single\npredefined token. Trained using a dataset constructed with real unfiltered\nLMSYS user prompts, Llama 3.1 8B's win rate against GPT-4 on AlpacaEval\nincreases from 21% to 34% with 16 samples and math performance on GSM8K\nimproves from 84% to 91%. By sampling only when the LLM determines that it is\nbeneficial to do so and adaptively adjusting temperature annealing, we\ndemonstrate that 74% of the improvement from using 16 samples can be achieved\nwith only 1.2 samples on average. We further demonstrate that 50-75% of samples\ncan be pruned early in generation with minimal degradation in performance.\nOverall, our methods enable more efficient and scalable compute utilization\nduring inference for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-time computation is a powerful paradigm to enhance the performance\nof large language models (LLMs), with Best-of-N sampling being a widely used\ntechnique. However, this method is computationally expensive, requiring both\n(1) an external reward model and (2) the generation of multiple samples. In\nthis work, we introduce a new generative self-evaluation scheme designed to\nadaptively reduce the number of generated samples while maintaining or even\nimproving performance. We use a generative reward model formulation, allowing\nthe LLM to predict mid-generation the probability that restarting the\ngeneration will yield a better response. These predictions are obtained without\nan external reward model and can be used to decide whether or not to generate\nmore samples, prune unpromising samples early on, or to pick the best sample.\nThis capability is very inexpensive as it involves generating a single\npredefined token. Trained using a dataset constructed with real unfiltered\nLMSYS user prompts, Llama 3.1 8B's win rate against GPT-4 on AlpacaEval\nincreases from 21% to 34% with 16 samples and math performance on GSM8K\nimproves from 84% to 91%. By sampling only when the LLM determines that it is\nbeneficial to do so and adaptively adjusting temperature annealing, we\ndemonstrate that 74% of the improvement from using 16 samples can be achieved\nwith only 1.2 samples on average. We further demonstrate that 50-75% of samples\ncan be pruned early in generation with minimal degradation in performance.\nOverall, our methods enable more efficient and scalable compute utilization\nduring inference for LLMs."
                },
                "authors": [
                    {
                        "name": "Rohin Manvi"
                    },
                    {
                        "name": "Anikait Singh"
                    },
                    {
                        "name": "Stefano Ermon"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Ermon"
                },
                "author": "Stefano Ermon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02725v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02725v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02724v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02724v1",
                "updated": "2024-10-03T17:45:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    45,
                    31,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T17:45:31Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    45,
                    31,
                    3,
                    277,
                    0
                ],
                "title": "Large Language Models as Markov Chains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models as Markov Chains"
                },
                "summary": "Large language models (LLMs) have proven to be remarkably efficient, both\nacross a wide range of natural language processing tasks and well beyond them.\nHowever, a comprehensive theoretical analysis of the origins of their\nimpressive performance remains elusive. In this paper, we approach this\nchallenging task by drawing an equivalence between generic autoregressive\nlanguage models with vocabulary of size $T$ and context window of size $K$ and\nMarkov chains defined on a finite state space of size $\\mathcal{O}(T^K)$. We\nderive several surprising findings related to the existence of a stationary\ndistribution of Markov chains that capture the inference power of LLMs, their\nspeed of convergence to it, and the influence of the temperature on the latter.\nWe then prove pre-training and in-context generalization bounds and show how\nthe drawn equivalence allows us to enrich their interpretation. Finally, we\nillustrate our theoretical guarantees with experiments on several recent LLMs\nto highlight how they capture the behavior observed in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have proven to be remarkably efficient, both\nacross a wide range of natural language processing tasks and well beyond them.\nHowever, a comprehensive theoretical analysis of the origins of their\nimpressive performance remains elusive. In this paper, we approach this\nchallenging task by drawing an equivalence between generic autoregressive\nlanguage models with vocabulary of size $T$ and context window of size $K$ and\nMarkov chains defined on a finite state space of size $\\mathcal{O}(T^K)$. We\nderive several surprising findings related to the existence of a stationary\ndistribution of Markov chains that capture the inference power of LLMs, their\nspeed of convergence to it, and the influence of the temperature on the latter.\nWe then prove pre-training and in-context generalization bounds and show how\nthe drawn equivalence allows us to enrich their interpretation. Finally, we\nillustrate our theoretical guarantees with experiments on several recent LLMs\nto highlight how they capture the behavior observed in practice."
                },
                "authors": [
                    {
                        "name": "Oussama Zekri"
                    },
                    {
                        "name": "Ambroise Odonnat"
                    },
                    {
                        "name": "Abdelhakim Benechehab"
                    },
                    {
                        "name": "Linus Bleistein"
                    },
                    {
                        "name": "Nicolas Boull"
                    },
                    {
                        "name": "Ievgen Redko"
                    }
                ],
                "author_detail": {
                    "name": "Ievgen Redko"
                },
                "author": "Ievgen Redko",
                "arxiv_comment": "49 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02724v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02724v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02721v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02721v1",
                "updated": "2024-10-03T17:40:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    40,
                    55,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T17:40:55Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    40,
                    55,
                    3,
                    277,
                    0
                ],
                "title": "Domain-Specific Retrieval-Augmented Generation Using Vector Stores,\n  Knowledge Graphs, and Tensor Factorization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain-Specific Retrieval-Augmented Generation Using Vector Stores,\n  Knowledge Graphs, and Tensor Factorization"
                },
                "summary": "Large Language Models (LLMs) are pre-trained on large-scale corpora and excel\nin numerous general natural language processing (NLP) tasks, such as question\nanswering (QA). Despite their advanced language capabilities, when it comes to\ndomain-specific and knowledge-intensive tasks, LLMs suffer from hallucinations,\nknowledge cut-offs, and lack of knowledge attributions. Additionally, fine\ntuning LLMs' intrinsic knowledge to highly specific domains is an expensive and\ntime consuming process. The retrieval-augmented generation (RAG) process has\nrecently emerged as a method capable of optimization of LLM responses, by\nreferencing them to a predetermined ontology. It was shown that using a\nKnowledge Graph (KG) ontology for RAG improves the QA accuracy, by taking into\naccount relevant sub-graphs that preserve the information in a structured\nmanner. In this paper, we introduce SMART-SLIC, a highly domain-specific LLM\nframework, that integrates RAG with KG and a vector store (VS) that store\nfactual domain specific information. Importantly, to avoid hallucinations in\nthe KG, we build these highly domain-specific KGs and VSs without the use of\nLLMs, but via NLP, data mining, and nonnegative tensor factorization with\nautomatic model selection. Pairing our RAG with a domain-specific: (i) KG\n(containing structured information), and (ii) VS (containing unstructured\ninformation) enables the development of domain-specific chat-bots that\nattribute the source of information, mitigate hallucinations, lessen the need\nfor fine-tuning, and excel in highly domain-specific question answering tasks.\nWe pair SMART-SLIC with chain-of-thought prompting agents. The framework is\ndesigned to be generalizable to adapt to any specific or specialized domain. In\nthis paper, we demonstrate the question answering capabilities of our framework\non a corpus of scientific publications on malware analysis and anomaly\ndetection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are pre-trained on large-scale corpora and excel\nin numerous general natural language processing (NLP) tasks, such as question\nanswering (QA). Despite their advanced language capabilities, when it comes to\ndomain-specific and knowledge-intensive tasks, LLMs suffer from hallucinations,\nknowledge cut-offs, and lack of knowledge attributions. Additionally, fine\ntuning LLMs' intrinsic knowledge to highly specific domains is an expensive and\ntime consuming process. The retrieval-augmented generation (RAG) process has\nrecently emerged as a method capable of optimization of LLM responses, by\nreferencing them to a predetermined ontology. It was shown that using a\nKnowledge Graph (KG) ontology for RAG improves the QA accuracy, by taking into\naccount relevant sub-graphs that preserve the information in a structured\nmanner. In this paper, we introduce SMART-SLIC, a highly domain-specific LLM\nframework, that integrates RAG with KG and a vector store (VS) that store\nfactual domain specific information. Importantly, to avoid hallucinations in\nthe KG, we build these highly domain-specific KGs and VSs without the use of\nLLMs, but via NLP, data mining, and nonnegative tensor factorization with\nautomatic model selection. Pairing our RAG with a domain-specific: (i) KG\n(containing structured information), and (ii) VS (containing unstructured\ninformation) enables the development of domain-specific chat-bots that\nattribute the source of information, mitigate hallucinations, lessen the need\nfor fine-tuning, and excel in highly domain-specific question answering tasks.\nWe pair SMART-SLIC with chain-of-thought prompting agents. The framework is\ndesigned to be generalizable to adapt to any specific or specialized domain. In\nthis paper, we demonstrate the question answering capabilities of our framework\non a corpus of scientific publications on malware analysis and anomaly\ndetection."
                },
                "authors": [
                    {
                        "name": "Ryan C. Barron"
                    },
                    {
                        "name": "Ves Grantcharov"
                    },
                    {
                        "name": "Selma Wanna"
                    },
                    {
                        "name": "Maksim E. Eren"
                    },
                    {
                        "name": "Manish Bhattarai"
                    },
                    {
                        "name": "Nicholas Solovyev"
                    },
                    {
                        "name": "George Tompkins"
                    },
                    {
                        "name": "Charles Nicholas"
                    },
                    {
                        "name": "Kim . Rasmussen"
                    },
                    {
                        "name": "Cynthia Matuszek"
                    },
                    {
                        "name": "Boian S. Alexandrov"
                    }
                ],
                "author_detail": {
                    "name": "Boian S. Alexandrov"
                },
                "author": "Boian S. Alexandrov",
                "arxiv_comment": "9 pages 7 figures, 1 table, 1 cypher code Accepted to ICMLA 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02721v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02721v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02707v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02707v1",
                "updated": "2024-10-03T17:31:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    31,
                    31,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T17:31:31Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    31,
                    31,
                    3,
                    277,
                    0
                ],
                "title": "LLMs Know More Than They Show: On the Intrinsic Representation of LLM\n  Hallucinations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Know More Than They Show: On the Intrinsic Representation of LLM\n  Hallucinations"
                },
                "summary": "Large language models (LLMs) often produce errors, including factual\ninaccuracies, biases, and reasoning failures, collectively referred to as\n\"hallucinations\". Recent studies have demonstrated that LLMs' internal states\nencode information regarding the truthfulness of their outputs, and that this\ninformation can be utilized to detect errors. In this work, we show that the\ninternal representations of LLMs encode much more information about\ntruthfulness than previously recognized. We first discover that the\ntruthfulness information is concentrated in specific tokens, and leveraging\nthis property significantly enhances error detection performance. Yet, we show\nthat such error detectors fail to generalize across datasets, implying that --\ncontrary to prior claims -- truthfulness encoding is not universal but rather\nmultifaceted. Next, we show that internal representations can also be used for\npredicting the types of errors the model is likely to make, facilitating the\ndevelopment of tailored mitigation strategies. Lastly, we reveal a discrepancy\nbetween LLMs' internal encoding and external behavior: they may encode the\ncorrect answer, yet consistently generate an incorrect one. Taken together,\nthese insights deepen our understanding of LLM errors from the model's internal\nperspective, which can guide future research on enhancing error analysis and\nmitigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often produce errors, including factual\ninaccuracies, biases, and reasoning failures, collectively referred to as\n\"hallucinations\". Recent studies have demonstrated that LLMs' internal states\nencode information regarding the truthfulness of their outputs, and that this\ninformation can be utilized to detect errors. In this work, we show that the\ninternal representations of LLMs encode much more information about\ntruthfulness than previously recognized. We first discover that the\ntruthfulness information is concentrated in specific tokens, and leveraging\nthis property significantly enhances error detection performance. Yet, we show\nthat such error detectors fail to generalize across datasets, implying that --\ncontrary to prior claims -- truthfulness encoding is not universal but rather\nmultifaceted. Next, we show that internal representations can also be used for\npredicting the types of errors the model is likely to make, facilitating the\ndevelopment of tailored mitigation strategies. Lastly, we reveal a discrepancy\nbetween LLMs' internal encoding and external behavior: they may encode the\ncorrect answer, yet consistently generate an incorrect one. Taken together,\nthese insights deepen our understanding of LLM errors from the model's internal\nperspective, which can guide future research on enhancing error analysis and\nmitigation."
                },
                "authors": [
                    {
                        "name": "Hadas Orgad"
                    },
                    {
                        "name": "Michael Toker"
                    },
                    {
                        "name": "Zorik Gekhman"
                    },
                    {
                        "name": "Roi Reichart"
                    },
                    {
                        "name": "Idan Szpektor"
                    },
                    {
                        "name": "Hadas Kotek"
                    },
                    {
                        "name": "Yonatan Belinkov"
                    }
                ],
                "author_detail": {
                    "name": "Yonatan Belinkov"
                },
                "author": "Yonatan Belinkov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02707v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02707v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.12683v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.12683v2",
                "updated": "2024-10-03T17:27:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    27,
                    28,
                    3,
                    277,
                    0
                ],
                "published": "2023-12-20T00:49:52Z",
                "published_parsed": [
                    2023,
                    12,
                    20,
                    0,
                    49,
                    52,
                    2,
                    354,
                    0
                ],
                "title": "Turning English-centric LLMs Into Polyglots: How Much Multilinguality Is\n  Needed?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Turning English-centric LLMs Into Polyglots: How Much Multilinguality Is\n  Needed?"
                },
                "summary": "The vast majority of today's large language models (LLMs) are\nEnglish-centric, having been pretrained predominantly on English text. Yet, in\norder to meet user expectations, models need to be able to respond\nappropriately in multiple languages once deployed in downstream applications.\nThis requires strong cross-lingual transfer abilities. In this work, we\ninvestigate the minimal amount of multilinguality required during finetuning to\nelicit cross-lingual generalisation in English-centric LLMs. In experiments\nacross four LLMs, we find that multilingual instruction tuning with as few as\ntwo to three languages is both necessary and sufficient to elicit effective\ncross-lingual generalisation, with the limiting factor being the degree to\nwhich a target language is seen during pretraining. Evaluations on five\ndifferent tasks further reveal that multilingual instruction tuning is most\nbeneficial for generative tasks that assume input/output language agreement,\nsuch as in chat settings, while being of less importance for highly structured\nclassification-style tasks. Our code and data is available at\nhttps://github.com/ZurichNLP/multilingual-instruction-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The vast majority of today's large language models (LLMs) are\nEnglish-centric, having been pretrained predominantly on English text. Yet, in\norder to meet user expectations, models need to be able to respond\nappropriately in multiple languages once deployed in downstream applications.\nThis requires strong cross-lingual transfer abilities. In this work, we\ninvestigate the minimal amount of multilinguality required during finetuning to\nelicit cross-lingual generalisation in English-centric LLMs. In experiments\nacross four LLMs, we find that multilingual instruction tuning with as few as\ntwo to three languages is both necessary and sufficient to elicit effective\ncross-lingual generalisation, with the limiting factor being the degree to\nwhich a target language is seen during pretraining. Evaluations on five\ndifferent tasks further reveal that multilingual instruction tuning is most\nbeneficial for generative tasks that assume input/output language agreement,\nsuch as in chat settings, while being of less importance for highly structured\nclassification-style tasks. Our code and data is available at\nhttps://github.com/ZurichNLP/multilingual-instruction-tuning."
                },
                "authors": [
                    {
                        "name": "Tannon Kew"
                    },
                    {
                        "name": "Florian Schottmann"
                    },
                    {
                        "name": "Rico Sennrich"
                    }
                ],
                "author_detail": {
                    "name": "Rico Sennrich"
                },
                "author": "Rico Sennrich",
                "arxiv_comment": "Accepted at Findings of EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.12683v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.12683v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07071v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07071v2",
                "updated": "2024-10-03T17:26:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    26,
                    48,
                    3,
                    277,
                    0
                ],
                "published": "2024-07-09T17:44:34Z",
                "published_parsed": [
                    2024,
                    7,
                    9,
                    17,
                    44,
                    34,
                    1,
                    191,
                    0
                ],
                "title": "Lookback Lens: Detecting and Mitigating Contextual Hallucinations in\n  Large Language Models Using Only Attention Maps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lookback Lens: Detecting and Mitigating Contextual Hallucinations in\n  Large Language Models Using Only Attention Maps"
                },
                "summary": "When asked to summarize articles or answer questions given a passage, large\nlanguage models (LLMs) can hallucinate details and respond with unsubstantiated\nanswers that are inaccurate with respect to the input context. This paper\ndescribes a simple approach for detecting such contextual hallucinations. We\nhypothesize that contextual hallucinations are related to the extent to which\nan LLM attends to information in the provided context versus its own\ngenerations. Based on this intuition, we propose a simple hallucination\ndetection model whose input features are given by the ratio of attention\nweights on the context versus newly generated tokens (for each attention head).\nWe find that a linear classifier based on these lookback ratio features is as\neffective as a richer detector that utilizes the entire hidden states of an LLM\nor a text-based entailment model. The lookback ratio-based detector -- Lookback\nLens -- is found to transfer across tasks and even models, allowing a detector\nthat is trained on a 7B model to be applied (without retraining) to a larger\n13B model. We further apply this detector to mitigate contextual\nhallucinations, and find that a simple classifier-guided decoding approach is\nable to reduce the amount of hallucination, for example by 9.6% in the XSum\nsummarization task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When asked to summarize articles or answer questions given a passage, large\nlanguage models (LLMs) can hallucinate details and respond with unsubstantiated\nanswers that are inaccurate with respect to the input context. This paper\ndescribes a simple approach for detecting such contextual hallucinations. We\nhypothesize that contextual hallucinations are related to the extent to which\nan LLM attends to information in the provided context versus its own\ngenerations. Based on this intuition, we propose a simple hallucination\ndetection model whose input features are given by the ratio of attention\nweights on the context versus newly generated tokens (for each attention head).\nWe find that a linear classifier based on these lookback ratio features is as\neffective as a richer detector that utilizes the entire hidden states of an LLM\nor a text-based entailment model. The lookback ratio-based detector -- Lookback\nLens -- is found to transfer across tasks and even models, allowing a detector\nthat is trained on a 7B model to be applied (without retraining) to a larger\n13B model. We further apply this detector to mitigate contextual\nhallucinations, and find that a simple classifier-guided decoding approach is\nable to reduce the amount of hallucination, for example by 9.6% in the XSum\nsummarization task."
                },
                "authors": [
                    {
                        "name": "Yung-Sung Chuang"
                    },
                    {
                        "name": "Linlu Qiu"
                    },
                    {
                        "name": "Cheng-Yu Hsieh"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Yoon Kim"
                    },
                    {
                        "name": "James Glass"
                    }
                ],
                "author_detail": {
                    "name": "James Glass"
                },
                "author": "James Glass",
                "arxiv_comment": "EMNLP 2024 main conference long paper. The source code is available\n  at https://github.com/voidism/Lookback-Lens",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.07071v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07071v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.00237v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.00237v3",
                "updated": "2024-10-03T17:25:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    25,
                    2,
                    3,
                    277,
                    0
                ],
                "published": "2023-11-01T02:40:42Z",
                "published_parsed": [
                    2023,
                    11,
                    1,
                    2,
                    40,
                    42,
                    2,
                    305,
                    0
                ],
                "title": "The Mystery of In-Context Learning: A Comprehensive Survey on\n  Interpretation and Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mystery of In-Context Learning: A Comprehensive Survey on\n  Interpretation and Analysis"
                },
                "summary": "Understanding in-context learning (ICL) capability that enables large\nlanguage models (LLMs) to excel in proficiency through demonstration examples\nis of utmost importance. This importance stems not only from the better\nutilization of this capability across various tasks, but also from the\nproactive identification and mitigation of potential risks, including concerns\nregarding truthfulness, bias, and toxicity, that may arise alongside the\ncapability. In this paper, we present a thorough survey on the interpretation\nand analysis of in-context learning. First, we provide a concise introduction\nto the background and definition of in-context learning. Then, we give an\noverview of advancements from two perspectives: 1) a theoretical perspective,\nemphasizing studies on mechanistic interpretability and delving into the\nmathematical foundations behind ICL; and 2) an empirical perspective,\nconcerning studies that empirically analyze factors associated with ICL. We\nconclude by highlighting the challenges encountered and suggesting potential\navenues for future research. We believe that our work establishes the basis for\nfurther exploration into the interpretation of in-context learning.\nAdditionally, we have created a repository containing the resources referenced\nin our survey.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding in-context learning (ICL) capability that enables large\nlanguage models (LLMs) to excel in proficiency through demonstration examples\nis of utmost importance. This importance stems not only from the better\nutilization of this capability across various tasks, but also from the\nproactive identification and mitigation of potential risks, including concerns\nregarding truthfulness, bias, and toxicity, that may arise alongside the\ncapability. In this paper, we present a thorough survey on the interpretation\nand analysis of in-context learning. First, we provide a concise introduction\nto the background and definition of in-context learning. Then, we give an\noverview of advancements from two perspectives: 1) a theoretical perspective,\nemphasizing studies on mechanistic interpretability and delving into the\nmathematical foundations behind ICL; and 2) an empirical perspective,\nconcerning studies that empirically analyze factors associated with ICL. We\nconclude by highlighting the challenges encountered and suggesting potential\navenues for future research. We believe that our work establishes the basis for\nfurther exploration into the interpretation of in-context learning.\nAdditionally, we have created a repository containing the resources referenced\nin our survey."
                },
                "authors": [
                    {
                        "name": "Yuxiang Zhou"
                    },
                    {
                        "name": "Jiazheng Li"
                    },
                    {
                        "name": "Yanzheng Xiang"
                    },
                    {
                        "name": "Hanqi Yan"
                    },
                    {
                        "name": "Lin Gui"
                    },
                    {
                        "name": "Yulan He"
                    }
                ],
                "author_detail": {
                    "name": "Yulan He"
                },
                "author": "Yulan He",
                "arxiv_comment": "Accepted to the main conference of EMNLP 2024. Resources are\n  available at https://github.com/zyxnlp/ICL-Interpretation-Analysis-Resources",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.00237v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.00237v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02693v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02693v1",
                "updated": "2024-10-03T17:18:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    18,
                    37,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T17:18:37Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    18,
                    37,
                    3,
                    277,
                    0
                ],
                "title": "Discovering Clues of Spoofed LM Watermarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discovering Clues of Spoofed LM Watermarks"
                },
                "summary": "LLM watermarks stand out as a promising way to attribute ownership of\nLLM-generated text. One threat to watermark credibility comes from spoofing\nattacks, where an unauthorized third party forges the watermark, enabling it to\nfalsely attribute arbitrary texts to a particular LLM. While recent works have\ndemonstrated that state-of-the-art schemes are in fact vulnerable to spoofing,\nthey lack deeper qualitative analysis of the texts produced by spoofing\nmethods. In this work, we for the first time reveal that there are observable\ndifferences between genuine and spoofed watermark texts. Namely, we show that\nregardless of their underlying approach, all current spoofing methods\nconsistently leave observable artifacts in spoofed texts, indicative of\nwatermark forgery. We build upon these findings to propose rigorous statistical\ntests that reliably reveal the presence of such artifacts, effectively\ndiscovering that a watermark was spoofed. Our experimental evaluation shows\nhigh test power across all current spoofing methods, providing insights into\ntheir fundamental limitations, and suggesting a way to mitigate this threat.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM watermarks stand out as a promising way to attribute ownership of\nLLM-generated text. One threat to watermark credibility comes from spoofing\nattacks, where an unauthorized third party forges the watermark, enabling it to\nfalsely attribute arbitrary texts to a particular LLM. While recent works have\ndemonstrated that state-of-the-art schemes are in fact vulnerable to spoofing,\nthey lack deeper qualitative analysis of the texts produced by spoofing\nmethods. In this work, we for the first time reveal that there are observable\ndifferences between genuine and spoofed watermark texts. Namely, we show that\nregardless of their underlying approach, all current spoofing methods\nconsistently leave observable artifacts in spoofed texts, indicative of\nwatermark forgery. We build upon these findings to propose rigorous statistical\ntests that reliably reveal the presence of such artifacts, effectively\ndiscovering that a watermark was spoofed. Our experimental evaluation shows\nhigh test power across all current spoofing methods, providing insights into\ntheir fundamental limitations, and suggesting a way to mitigate this threat."
                },
                "authors": [
                    {
                        "name": "Thibaud Gloaguen"
                    },
                    {
                        "name": "Nikola Jovanovi"
                    },
                    {
                        "name": "Robin Staab"
                    },
                    {
                        "name": "Martin Vechev"
                    }
                ],
                "author_detail": {
                    "name": "Martin Vechev"
                },
                "author": "Martin Vechev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02693v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02693v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05514v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05514v3",
                "updated": "2024-10-03T17:15:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    15,
                    34,
                    3,
                    277,
                    0
                ],
                "published": "2024-06-08T16:24:24Z",
                "published_parsed": [
                    2024,
                    6,
                    8,
                    16,
                    24,
                    24,
                    5,
                    160,
                    0
                ],
                "title": "RAG-Enhanced Commit Message Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAG-Enhanced Commit Message Generation"
                },
                "summary": "Commit message is one of the most important textual information in software\ndevelopment and maintenance. However, it is time-consuming to write commit\nmessages manually. Commit Message Generation (CMG) has become a research\nhotspot. Recently, several pre-trained language models (PLMs) and large\nlanguage models (LLMs) with code capabilities have been introduced,\ndemonstrating impressive performance on code-related tasks. Meanwhile, prior\nstudies have explored the utilization of retrieval techniques for CMG, but it\nis still unclear what effects would emerge from combining advanced retrieval\ntechniques with various generation models. This paper proposed REACT, a\nREtrieval-Augmented framework for CommiT message generation. It integrates\nadvanced retrieval techniques with different PLMs and LLMs, to enhance the\nperformance of these models on the CMG task. Specifically, a hybrid retriever\nis designed and used to retrieve the most relevant code diff and commit message\npair as an exemplar. Then, the retrieved pair is utilized to guide and enhance\nthe CMG task by PLMs and LLMs through fine-tuning and in-context learning. The\nexperimental results show that REACT significantly enhances these models'\nperformance on the CMG task, improving the BLEU score of CodeT5 by up to 55%,\nboosting Llama 3's BLEU score by 102%, and substantially surpassing all\nbaselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Commit message is one of the most important textual information in software\ndevelopment and maintenance. However, it is time-consuming to write commit\nmessages manually. Commit Message Generation (CMG) has become a research\nhotspot. Recently, several pre-trained language models (PLMs) and large\nlanguage models (LLMs) with code capabilities have been introduced,\ndemonstrating impressive performance on code-related tasks. Meanwhile, prior\nstudies have explored the utilization of retrieval techniques for CMG, but it\nis still unclear what effects would emerge from combining advanced retrieval\ntechniques with various generation models. This paper proposed REACT, a\nREtrieval-Augmented framework for CommiT message generation. It integrates\nadvanced retrieval techniques with different PLMs and LLMs, to enhance the\nperformance of these models on the CMG task. Specifically, a hybrid retriever\nis designed and used to retrieve the most relevant code diff and commit message\npair as an exemplar. Then, the retrieved pair is utilized to guide and enhance\nthe CMG task by PLMs and LLMs through fine-tuning and in-context learning. The\nexperimental results show that REACT significantly enhances these models'\nperformance on the CMG task, improving the BLEU score of CodeT5 by up to 55%,\nboosting Llama 3's BLEU score by 102%, and substantially surpassing all\nbaselines."
                },
                "authors": [
                    {
                        "name": "Linghao Zhang"
                    },
                    {
                        "name": "Hongyi Zhang"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Peng Liang"
                    }
                ],
                "author_detail": {
                    "name": "Peng Liang"
                },
                "author": "Peng Liang",
                "arxiv_comment": "22 pages, 5 images, 6 tables, Manuscript submitted to a journal\n  (2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05514v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05514v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.03741v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.03741v2",
                "updated": "2024-10-03T17:15:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    15,
                    24,
                    3,
                    277,
                    0
                ],
                "published": "2024-01-08T09:01:29Z",
                "published_parsed": [
                    2024,
                    1,
                    8,
                    9,
                    1,
                    29,
                    0,
                    8,
                    0
                ],
                "title": "Enhanced Automated Code Vulnerability Repair using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhanced Automated Code Vulnerability Repair using Large Language Models"
                },
                "summary": "This research addresses the complex challenge of automated repair of code\nvulnerabilities, vital for enhancing digital security in an increasingly\ntechnology-driven world. The study introduces a novel and efficient format for\nthe representation of code modification, using advanced Large Language Models\n(LLMs) such as Code Llama and Mistral. These models, fine-tuned on datasets\nfeaturing C code vulnerabilities, significantly improve the accuracy and\nadaptability of automated code repair techniques. A key finding is the enhanced\nrepair accuracy of these models when compared to previous methods such as\nVulRepair, which underscores their practical utility and efficiency. The\nresearch also offers a critical assessment of current evaluation metrics, such\nas perfect predictions, and their limitations in reflecting the true\ncapabilities of automated repair models in real-world scenarios. Following\nthis, it underscores the importance of using test datasets devoid of train\nsamples, emphasizing the need for dataset integrity to enhance the\neffectiveness of LLMs in code repair tasks. The significance of this work is\nits contribution to digital security, setting new standards for automated code\nvulnerability repair and paving the way for future advancements in the fields\nof cybersecurity and artificial intelligence. The study does not only highlight\nthe potential of LLMs in enhancing code security but also fosters further\nexploration and research in these crucial areas.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research addresses the complex challenge of automated repair of code\nvulnerabilities, vital for enhancing digital security in an increasingly\ntechnology-driven world. The study introduces a novel and efficient format for\nthe representation of code modification, using advanced Large Language Models\n(LLMs) such as Code Llama and Mistral. These models, fine-tuned on datasets\nfeaturing C code vulnerabilities, significantly improve the accuracy and\nadaptability of automated code repair techniques. A key finding is the enhanced\nrepair accuracy of these models when compared to previous methods such as\nVulRepair, which underscores their practical utility and efficiency. The\nresearch also offers a critical assessment of current evaluation metrics, such\nas perfect predictions, and their limitations in reflecting the true\ncapabilities of automated repair models in real-world scenarios. Following\nthis, it underscores the importance of using test datasets devoid of train\nsamples, emphasizing the need for dataset integrity to enhance the\neffectiveness of LLMs in code repair tasks. The significance of this work is\nits contribution to digital security, setting new standards for automated code\nvulnerability repair and paving the way for future advancements in the fields\nof cybersecurity and artificial intelligence. The study does not only highlight\nthe potential of LLMs in enhancing code security but also fosters further\nexploration and research in these crucial areas."
                },
                "authors": [
                    {
                        "name": "David de-Fitero-Dominguez"
                    },
                    {
                        "name": "Eva Garcia-Lopez"
                    },
                    {
                        "name": "Antonio Garcia-Cabot"
                    },
                    {
                        "name": "Jose-Javier Martinez-Herraiz"
                    }
                ],
                "author_detail": {
                    "name": "Jose-Javier Martinez-Herraiz"
                },
                "author": "Jose-Javier Martinez-Herraiz",
                "arxiv_doi": "10.1016/j.engappai.2024.109291",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.engappai.2024.109291",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2401.03741v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.03741v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Engineering Applications of Artificial Intelligence. Volume 138,\n  Part A, December 2024, 109291",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03650v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03650v2",
                "updated": "2024-10-03T17:13:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    13,
                    4,
                    3,
                    277,
                    0
                ],
                "published": "2024-09-05T16:08:19Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    16,
                    8,
                    19,
                    3,
                    249,
                    0
                ],
                "title": "On the Limited Generalization Capability of the Implicit Reward Model\n  Induced by Direct Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Limited Generalization Capability of the Implicit Reward Model\n  Induced by Direct Preference Optimization"
                },
                "summary": "Reinforcement Learning from Human Feedback (RLHF) is an effective approach\nfor aligning language models to human preferences. Central to RLHF is learning\na reward function for scoring human preferences. Two main approaches for\nlearning a reward model are 1) training an EXplicit Reward Model (EXRM) as in\nRLHF, and 2) using an implicit reward learned from preference data through\nmethods such as Direct Preference Optimization (DPO). Prior work has shown that\nthe implicit reward model of DPO (denoted as DPORM) can approximate an EXRM in\nthe limit. DPORM's effectiveness directly implies the optimality of the learned\npolicy, and also has practical implication for LLM alignment methods including\niterative DPO. However, it is unclear how well DPORM empirically matches the\nperformance of EXRM. This work studies the accuracy at distinguishing preferred\nand rejected answers for both DPORM and EXRM. Our findings indicate that even\nthough DPORM fits the training dataset comparably, it generalizes less\neffectively than EXRM, especially when the validation datasets contain\ndistribution shifts. Across five out-of-distribution settings, DPORM has a mean\ndrop in accuracy of 3% and a maximum drop of 7%. These findings highlight that\nDPORM has limited generalization ability and substantiates the integration of\nan explicit reward model in iterative DPO approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning from Human Feedback (RLHF) is an effective approach\nfor aligning language models to human preferences. Central to RLHF is learning\na reward function for scoring human preferences. Two main approaches for\nlearning a reward model are 1) training an EXplicit Reward Model (EXRM) as in\nRLHF, and 2) using an implicit reward learned from preference data through\nmethods such as Direct Preference Optimization (DPO). Prior work has shown that\nthe implicit reward model of DPO (denoted as DPORM) can approximate an EXRM in\nthe limit. DPORM's effectiveness directly implies the optimality of the learned\npolicy, and also has practical implication for LLM alignment methods including\niterative DPO. However, it is unclear how well DPORM empirically matches the\nperformance of EXRM. This work studies the accuracy at distinguishing preferred\nand rejected answers for both DPORM and EXRM. Our findings indicate that even\nthough DPORM fits the training dataset comparably, it generalizes less\neffectively than EXRM, especially when the validation datasets contain\ndistribution shifts. Across five out-of-distribution settings, DPORM has a mean\ndrop in accuracy of 3% and a maximum drop of 7%. These findings highlight that\nDPORM has limited generalization ability and substantiates the integration of\nan explicit reward model in iterative DPO approaches."
                },
                "authors": [
                    {
                        "name": "Yong Lin"
                    },
                    {
                        "name": "Skyler Seto"
                    },
                    {
                        "name": "Maartje ter Hoeve"
                    },
                    {
                        "name": "Katherine Metcalf"
                    },
                    {
                        "name": "Barry-John Theobald"
                    },
                    {
                        "name": "Xuan Wang"
                    },
                    {
                        "name": "Yizhe Zhang"
                    },
                    {
                        "name": "Chen Huang"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "arxiv_comment": "12 pages, 8 tables, 3 figures; Paper Accepted at EMNLP Findings 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03650v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03650v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02684v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02684v1",
                "updated": "2024-10-03T17:10:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    10,
                    41,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T17:10:41Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    10,
                    41,
                    3,
                    277,
                    0
                ],
                "title": "HiddenGuard: Fine-Grained Safe Generation with Specialized\n  Representation Router",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HiddenGuard: Fine-Grained Safe Generation with Specialized\n  Representation Router"
                },
                "summary": "As Large Language Models (LLMs) grow increasingly powerful, ensuring their\nsafety and alignment with human values remains a critical challenge. Ideally,\nLLMs should provide informative responses while avoiding the disclosure of\nharmful or sensitive information. However, current alignment approaches, which\nrely heavily on refusal strategies, such as training models to completely\nreject harmful prompts or applying coarse filters are limited by their binary\nnature. These methods either fully deny access to information or grant it\nwithout sufficient nuance, leading to overly cautious responses or failures to\ndetect subtle harmful content. For example, LLMs may refuse to provide basic,\npublic information about medication due to misuse concerns. Moreover, these\nrefusal-based methods struggle to handle mixed-content scenarios and lack the\nability to adapt to context-dependent sensitivities, which can result in\nover-censorship of benign content. To overcome these challenges, we introduce\nHiddenGuard, a novel framework for fine-grained, safe generation in LLMs.\nHiddenGuard incorporates Prism (rePresentation Router for In-Stream\nModeration), which operates alongside the LLM to enable real-time, token-level\ndetection and redaction of harmful content by leveraging intermediate hidden\nstates. This fine-grained approach allows for more nuanced, context-aware\nmoderation, enabling the model to generate informative responses while\nselectively redacting or replacing sensitive information, rather than outright\nrefusal. We also contribute a comprehensive dataset with token-level\nfine-grained annotations of potentially harmful information across diverse\ncontexts. Our experiments demonstrate that HiddenGuard achieves over 90% in F1\nscore for detecting and redacting harmful content while preserving the overall\nutility and informativeness of the model's responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) grow increasingly powerful, ensuring their\nsafety and alignment with human values remains a critical challenge. Ideally,\nLLMs should provide informative responses while avoiding the disclosure of\nharmful or sensitive information. However, current alignment approaches, which\nrely heavily on refusal strategies, such as training models to completely\nreject harmful prompts or applying coarse filters are limited by their binary\nnature. These methods either fully deny access to information or grant it\nwithout sufficient nuance, leading to overly cautious responses or failures to\ndetect subtle harmful content. For example, LLMs may refuse to provide basic,\npublic information about medication due to misuse concerns. Moreover, these\nrefusal-based methods struggle to handle mixed-content scenarios and lack the\nability to adapt to context-dependent sensitivities, which can result in\nover-censorship of benign content. To overcome these challenges, we introduce\nHiddenGuard, a novel framework for fine-grained, safe generation in LLMs.\nHiddenGuard incorporates Prism (rePresentation Router for In-Stream\nModeration), which operates alongside the LLM to enable real-time, token-level\ndetection and redaction of harmful content by leveraging intermediate hidden\nstates. This fine-grained approach allows for more nuanced, context-aware\nmoderation, enabling the model to generate informative responses while\nselectively redacting or replacing sensitive information, rather than outright\nrefusal. We also contribute a comprehensive dataset with token-level\nfine-grained annotations of potentially harmful information across diverse\ncontexts. Our experiments demonstrate that HiddenGuard achieves over 90% in F1\nscore for detecting and redacting harmful content while preserving the overall\nutility and informativeness of the model's responses."
                },
                "authors": [
                    {
                        "name": "Lingrui Mei"
                    },
                    {
                        "name": "Shenghua Liu"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Baolong Bi"
                    },
                    {
                        "name": "Ruibin Yuan"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02684v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02684v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18725v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18725v2",
                "updated": "2024-10-03T17:10:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    10,
                    9,
                    3,
                    277,
                    0
                ],
                "published": "2024-06-26T19:48:48Z",
                "published_parsed": [
                    2024,
                    6,
                    26,
                    19,
                    48,
                    48,
                    2,
                    178,
                    0
                ],
                "title": "Jailbreaking LLMs with Arabic Transliteration and Arabizi",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreaking LLMs with Arabic Transliteration and Arabizi"
                },
                "summary": "This study identifies the potential vulnerabilities of Large Language Models\n(LLMs) to 'jailbreak' attacks, specifically focusing on the Arabic language and\nits various forms. While most research has concentrated on English-based prompt\nmanipulation, our investigation broadens the scope to investigate the Arabic\nlanguage. We initially tested the AdvBench benchmark in Standardized Arabic,\nfinding that even with prompt manipulation techniques like prefix injection, it\nwas insufficient to provoke LLMs into generating unsafe content. However, when\nusing Arabic transliteration and chatspeak (or arabizi), we found that unsafe\ncontent could be produced on platforms like OpenAI GPT-4 and Anthropic Claude 3\nSonnet. Our findings suggest that using Arabic and its various forms could\nexpose information that might remain hidden, potentially increasing the risk of\njailbreak attacks. We hypothesize that this exposure could be due to the\nmodel's learned connection to specific words, highlighting the need for more\ncomprehensive safety training across all language forms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study identifies the potential vulnerabilities of Large Language Models\n(LLMs) to 'jailbreak' attacks, specifically focusing on the Arabic language and\nits various forms. While most research has concentrated on English-based prompt\nmanipulation, our investigation broadens the scope to investigate the Arabic\nlanguage. We initially tested the AdvBench benchmark in Standardized Arabic,\nfinding that even with prompt manipulation techniques like prefix injection, it\nwas insufficient to provoke LLMs into generating unsafe content. However, when\nusing Arabic transliteration and chatspeak (or arabizi), we found that unsafe\ncontent could be produced on platforms like OpenAI GPT-4 and Anthropic Claude 3\nSonnet. Our findings suggest that using Arabic and its various forms could\nexpose information that might remain hidden, potentially increasing the risk of\njailbreak attacks. We hypothesize that this exposure could be due to the\nmodel's learned connection to specific words, highlighting the need for more\ncomprehensive safety training across all language forms."
                },
                "authors": [
                    {
                        "name": "Mansour Al Ghanim"
                    },
                    {
                        "name": "Saleh Almohaimeed"
                    },
                    {
                        "name": "Mengxin Zheng"
                    },
                    {
                        "name": "Yan Solihin"
                    },
                    {
                        "name": "Qian Lou"
                    }
                ],
                "author_detail": {
                    "name": "Qian Lou"
                },
                "author": "Qian Lou",
                "arxiv_comment": "Accepted by EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18725v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18725v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02683v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02683v1",
                "updated": "2024-10-03T17:08:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    8,
                    52,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T17:08:52Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    8,
                    52,
                    3,
                    277,
                    0
                ],
                "title": "DailyDilemmas: Revealing Value Preferences of LLMs with Quandaries of\n  Daily Life",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DailyDilemmas: Revealing Value Preferences of LLMs with Quandaries of\n  Daily Life"
                },
                "summary": "As we increasingly seek guidance from LLMs for decision-making in daily life,\nmany of these decisions are not clear-cut and depend significantly on the\npersonal values and ethical standards of the users. We present DailyDilemmas, a\ndataset of 1,360 moral dilemmas encountered in everyday life. Each dilemma\nincludes two possible actions and with each action, the affected parties and\nhuman values invoked. Based on these dilemmas, we consolidated a set of human\nvalues across everyday topics e.g., interpersonal relationships, workplace, and\nenvironmental issues. We evaluated LLMs on these dilemmas to determine what\naction they will take and the values represented by these actions. Then, we\nanalyzed these values through the lens of five popular theories inspired by\nsociology, psychology and philosophy. These theories are: World Value Survey,\nMoral Foundation Theory, Maslow's Hierarchy of Needs, Aristotle's Virtues, and\nPlutchik Wheel of Emotion. We find that LLMs are most aligned with the\nself-expression over survival values in terms of World Value Survey, care over\nloyalty in Moral Foundation Theory. Interestingly, we find large preferences\ndifferences in models for some core values such as truthfulness e.g.,\nMixtral-8x7B model tends to neglect it by 9.7% while GPT-4-turbo model tends to\nselect it by 9.4%. We also study the recent guidance released by OpenAI\n(ModelSpec), and Anthropic (Constitutional AI) to understand how their released\nprinciples reflect their actual value prioritization when facing nuanced moral\nreasoning in daily-life settings. We find that end users cannot effectively\nsteer such prioritization using system prompts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As we increasingly seek guidance from LLMs for decision-making in daily life,\nmany of these decisions are not clear-cut and depend significantly on the\npersonal values and ethical standards of the users. We present DailyDilemmas, a\ndataset of 1,360 moral dilemmas encountered in everyday life. Each dilemma\nincludes two possible actions and with each action, the affected parties and\nhuman values invoked. Based on these dilemmas, we consolidated a set of human\nvalues across everyday topics e.g., interpersonal relationships, workplace, and\nenvironmental issues. We evaluated LLMs on these dilemmas to determine what\naction they will take and the values represented by these actions. Then, we\nanalyzed these values through the lens of five popular theories inspired by\nsociology, psychology and philosophy. These theories are: World Value Survey,\nMoral Foundation Theory, Maslow's Hierarchy of Needs, Aristotle's Virtues, and\nPlutchik Wheel of Emotion. We find that LLMs are most aligned with the\nself-expression over survival values in terms of World Value Survey, care over\nloyalty in Moral Foundation Theory. Interestingly, we find large preferences\ndifferences in models for some core values such as truthfulness e.g.,\nMixtral-8x7B model tends to neglect it by 9.7% while GPT-4-turbo model tends to\nselect it by 9.4%. We also study the recent guidance released by OpenAI\n(ModelSpec), and Anthropic (Constitutional AI) to understand how their released\nprinciples reflect their actual value prioritization when facing nuanced moral\nreasoning in daily-life settings. We find that end users cannot effectively\nsteer such prioritization using system prompts."
                },
                "authors": [
                    {
                        "name": "Yu Ying Chiu"
                    },
                    {
                        "name": "Liwei Jiang"
                    },
                    {
                        "name": "Yejin Choi"
                    }
                ],
                "author_detail": {
                    "name": "Yejin Choi"
                },
                "author": "Yejin Choi",
                "arxiv_comment": "Preprint. Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02683v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02683v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02681v1",
                "updated": "2024-10-03T17:06:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    6,
                    21,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T17:06:21Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    6,
                    21,
                    3,
                    277,
                    0
                ],
                "title": "Understanding and Mitigating Miscalibration in Prompt Tuning for\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and Mitigating Miscalibration in Prompt Tuning for\n  Vision-Language Models"
                },
                "summary": "Confidence calibration is critical for the safe deployment of machine\nlearning models in the real world. However, such issue in vision-language\nmodels like CLIP, particularly after fine-tuning, has not been fully addressed.\nIn this work, we demonstrate that existing prompt tuning methods usually lead\nto a trade-off of calibration between base and new classes: the cross-entropy\nloss in CoOp causes overconfidence in new classes by increasing textual label\ndivergence, whereas the regularization of KgCoOp maintains the confidence level\nbut results in underconfidence in base classes due to the improved accuracy.\nInspired by the observations, we introduce Dynamic Outlier Regularization (DOR)\nto ensure the confidence calibration on both base and new classes after\nfine-tuning. In particular, we propose to minimize the feature deviation of\nnovel textual labels (instead of base classes) sampled from a large vocabulary.\nIn effect, DOR prevents the increase in textual divergence for new labels while\neasing restrictions on base classes. Extensive experiments demonstrate that DOR\ncan enhance the calibration performance of current fine-tuning methods on base\nand new classes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confidence calibration is critical for the safe deployment of machine\nlearning models in the real world. However, such issue in vision-language\nmodels like CLIP, particularly after fine-tuning, has not been fully addressed.\nIn this work, we demonstrate that existing prompt tuning methods usually lead\nto a trade-off of calibration between base and new classes: the cross-entropy\nloss in CoOp causes overconfidence in new classes by increasing textual label\ndivergence, whereas the regularization of KgCoOp maintains the confidence level\nbut results in underconfidence in base classes due to the improved accuracy.\nInspired by the observations, we introduce Dynamic Outlier Regularization (DOR)\nto ensure the confidence calibration on both base and new classes after\nfine-tuning. In particular, we propose to minimize the feature deviation of\nnovel textual labels (instead of base classes) sampled from a large vocabulary.\nIn effect, DOR prevents the increase in textual divergence for new labels while\neasing restrictions on base classes. Extensive experiments demonstrate that DOR\ncan enhance the calibration performance of current fine-tuning methods on base\nand new classes."
                },
                "authors": [
                    {
                        "name": "Shuoyuan Wang"
                    },
                    {
                        "name": "Yixuan Li"
                    },
                    {
                        "name": "Hongxin Wei"
                    }
                ],
                "author_detail": {
                    "name": "Hongxin Wei"
                },
                "author": "Hongxin Wei",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02678v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02678v1",
                "updated": "2024-10-03T17:04:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    4,
                    48,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T17:04:48Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    4,
                    48,
                    3,
                    277,
                    0
                ],
                "title": "Distilling an End-to-End Voice Assistant Without Instruction Training\n  Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilling an End-to-End Voice Assistant Without Instruction Training\n  Data"
                },
                "summary": "Voice assistants, such as Siri and Google Assistant, typically model audio\nand text separately, resulting in lost speech information and increased\ncomplexity. Recent efforts to address this with end-to-end Speech Large\nLanguage Models (LLMs) trained with supervised finetuning (SFT)\n  have led to models ``forgetting\" capabilities from text-only LLMs. Our work\nproposes an alternative paradigm for training Speech LLMs without instruction\ndata, using the response of a text-only LLM to transcripts as self-supervision.\nImportantly, this process can be performed without annotated responses. We show\nthat our Distilled Voice Assistant (DiVA) generalizes to Spoken Question\nAnswering, Classification, and Translation. Furthermore, we show that DiVA\nbetter meets user preferences, achieving a 72\\% win rate compared with\nstate-of-the-art models like Qwen 2 Audio, despite using $>$100x less training\ncompute.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Voice assistants, such as Siri and Google Assistant, typically model audio\nand text separately, resulting in lost speech information and increased\ncomplexity. Recent efforts to address this with end-to-end Speech Large\nLanguage Models (LLMs) trained with supervised finetuning (SFT)\n  have led to models ``forgetting\" capabilities from text-only LLMs. Our work\nproposes an alternative paradigm for training Speech LLMs without instruction\ndata, using the response of a text-only LLM to transcripts as self-supervision.\nImportantly, this process can be performed without annotated responses. We show\nthat our Distilled Voice Assistant (DiVA) generalizes to Spoken Question\nAnswering, Classification, and Translation. Furthermore, we show that DiVA\nbetter meets user preferences, achieving a 72\\% win rate compared with\nstate-of-the-art models like Qwen 2 Audio, despite using $>$100x less training\ncompute."
                },
                "authors": [
                    {
                        "name": "William Held"
                    },
                    {
                        "name": "Ella Li"
                    },
                    {
                        "name": "Michael Ryan"
                    },
                    {
                        "name": "Weiyan Shi"
                    },
                    {
                        "name": "Yanzhe Zhang"
                    },
                    {
                        "name": "Diyi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Diyi Yang"
                },
                "author": "Diyi Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02678v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02678v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02677v1",
                "updated": "2024-10-03T17:04:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    4,
                    31,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T17:04:31Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    4,
                    31,
                    3,
                    277,
                    0
                ],
                "title": "CulturalBench: a Robust, Diverse and Challenging Benchmark on Measuring\n  the (Lack of) Cultural Knowledge of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CulturalBench: a Robust, Diverse and Challenging Benchmark on Measuring\n  the (Lack of) Cultural Knowledge of LLMs"
                },
                "summary": "To make large language models (LLMs) more helpful across diverse cultures, it\nis essential to have effective cultural knowledge benchmarks to measure and\ntrack our progress. Effective benchmarks need to be robust, diverse, and\nchallenging. We introduce CulturalBench: a set of 1,227 human-written and\nhuman-verified questions for effectively assessing LLMs' cultural knowledge,\ncovering 45 global regions including the underrepresented ones like Bangladesh,\nZimbabwe, and Peru. Questions - each verified by five independent annotators -\nspan 17 diverse topics ranging from food preferences to greeting etiquettes. We\nevaluate models on two setups: CulturalBench-Easy and CulturalBench-Hard which\nshare the same questions but asked differently. We find that LLMs are sensitive\nto such difference in setups (e.g., GPT-4o with 27.3% difference). Compared to\nhuman performance (92.6% accuracy), CulturalBench-Hard is more challenging for\nfrontier LLMs with the best performing model (GPT-4o) at only 61.5% and the\nworst (Llama3-8b) at 21.4%. Moreover, we find that LLMs often struggle with\ntricky questions that have multiple correct answers (e.g., What utensils do the\nChinese usually use?), revealing a tendency to converge to a single answer. Our\nresults also indicate that OpenAI GPT-4o substantially outperform other\nproprietary and open source models in questions related to all but one region\n(Oceania). Nonetheless, all models consistently underperform on questions\nrelated to South America and the Middle East.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To make large language models (LLMs) more helpful across diverse cultures, it\nis essential to have effective cultural knowledge benchmarks to measure and\ntrack our progress. Effective benchmarks need to be robust, diverse, and\nchallenging. We introduce CulturalBench: a set of 1,227 human-written and\nhuman-verified questions for effectively assessing LLMs' cultural knowledge,\ncovering 45 global regions including the underrepresented ones like Bangladesh,\nZimbabwe, and Peru. Questions - each verified by five independent annotators -\nspan 17 diverse topics ranging from food preferences to greeting etiquettes. We\nevaluate models on two setups: CulturalBench-Easy and CulturalBench-Hard which\nshare the same questions but asked differently. We find that LLMs are sensitive\nto such difference in setups (e.g., GPT-4o with 27.3% difference). Compared to\nhuman performance (92.6% accuracy), CulturalBench-Hard is more challenging for\nfrontier LLMs with the best performing model (GPT-4o) at only 61.5% and the\nworst (Llama3-8b) at 21.4%. Moreover, we find that LLMs often struggle with\ntricky questions that have multiple correct answers (e.g., What utensils do the\nChinese usually use?), revealing a tendency to converge to a single answer. Our\nresults also indicate that OpenAI GPT-4o substantially outperform other\nproprietary and open source models in questions related to all but one region\n(Oceania). Nonetheless, all models consistently underperform on questions\nrelated to South America and the Middle East."
                },
                "authors": [
                    {
                        "name": "Yu Ying Chiu"
                    },
                    {
                        "name": "Liwei Jiang"
                    },
                    {
                        "name": "Bill Yuchen Lin"
                    },
                    {
                        "name": "Chan Young Park"
                    },
                    {
                        "name": "Shuyue Stella Li"
                    },
                    {
                        "name": "Sahithya Ravi"
                    },
                    {
                        "name": "Mehar Bhatia"
                    },
                    {
                        "name": "Maria Antoniak"
                    },
                    {
                        "name": "Yulia Tsvetkov"
                    },
                    {
                        "name": "Vered Shwartz"
                    },
                    {
                        "name": "Yejin Choi"
                    }
                ],
                "author_detail": {
                    "name": "Yejin Choi"
                },
                "author": "Yejin Choi",
                "arxiv_comment": "Preprint. Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07950v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07950v2",
                "updated": "2024-10-03T16:54:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    16,
                    54,
                    59,
                    3,
                    277,
                    0
                ],
                "published": "2024-07-10T18:00:05Z",
                "published_parsed": [
                    2024,
                    7,
                    10,
                    18,
                    0,
                    5,
                    2,
                    192,
                    0
                ],
                "title": "Rel-A.I.: An Interaction-Centered Approach To Measuring Human-LM\n  Reliance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rel-A.I.: An Interaction-Centered Approach To Measuring Human-LM\n  Reliance"
                },
                "summary": "The ability to communicate uncertainty, risk, and limitation is crucial for\nthe safety of large language models. However, current evaluations of these\nabilities rely on simple calibration, asking whether the language generated by\nthe model matches appropriate probabilities. Instead, evaluation of this aspect\nof LLM communication should focus on the behaviors of their human\ninterlocutors: how much do they rely on what the LLM says? Here we introduce an\ninteraction-centered evaluation framework called Rel-A.I. (pronounced \"rely\"})\nthat measures whether humans rely on LLM generations. We use this framework to\nstudy how reliance is affected by contextual features of the interaction (e.g,\nthe knowledge domain that is being discussed), or the use of greetings\ncommunicating warmth or competence (e.g., \"I'm happy to help!\"). We find that\ncontextual characteristics significantly affect human reliance behavior. For\nexample, people rely 10% more on LMs when responding to questions involving\ncalculations and rely 30% more on LMs that are perceived as more competent. Our\nresults show that calibration and language quality alone are insufficient in\nevaluating the risks of human-LM interactions, and illustrate the need to\nconsider features of the interactional context.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability to communicate uncertainty, risk, and limitation is crucial for\nthe safety of large language models. However, current evaluations of these\nabilities rely on simple calibration, asking whether the language generated by\nthe model matches appropriate probabilities. Instead, evaluation of this aspect\nof LLM communication should focus on the behaviors of their human\ninterlocutors: how much do they rely on what the LLM says? Here we introduce an\ninteraction-centered evaluation framework called Rel-A.I. (pronounced \"rely\"})\nthat measures whether humans rely on LLM generations. We use this framework to\nstudy how reliance is affected by contextual features of the interaction (e.g,\nthe knowledge domain that is being discussed), or the use of greetings\ncommunicating warmth or competence (e.g., \"I'm happy to help!\"). We find that\ncontextual characteristics significantly affect human reliance behavior. For\nexample, people rely 10% more on LMs when responding to questions involving\ncalculations and rely 30% more on LMs that are perceived as more competent. Our\nresults show that calibration and language quality alone are insufficient in\nevaluating the risks of human-LM interactions, and illustrate the need to\nconsider features of the interactional context."
                },
                "authors": [
                    {
                        "name": "Kaitlyn Zhou"
                    },
                    {
                        "name": "Jena D. Hwang"
                    },
                    {
                        "name": "Xiang Ren"
                    },
                    {
                        "name": "Nouha Dziri"
                    },
                    {
                        "name": "Dan Jurafsky"
                    },
                    {
                        "name": "Maarten Sap"
                    }
                ],
                "author_detail": {
                    "name": "Maarten Sap"
                },
                "author": "Maarten Sap",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.07950v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07950v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02666v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02666v1",
                "updated": "2024-10-03T16:50:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    16,
                    50,
                    30,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T16:50:30Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    16,
                    50,
                    30,
                    3,
                    277,
                    0
                ],
                "title": "AlphaIntegrator: Transformer Action Search for Symbolic Integration\n  Proofs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlphaIntegrator: Transformer Action Search for Symbolic Integration\n  Proofs"
                },
                "summary": "We present the first correct-by-construction learning-based system for\nstep-by-step mathematical integration. The key idea is to learn a policy,\nrepresented by a GPT transformer model, which guides the search for the right\nmathematical integration rule, to be carried out by a symbolic solver.\nConcretely, we introduce a symbolic engine with axiomatically correct actions\non mathematical expressions, as well as the first dataset for step-by-step\nintegration. Our GPT-style transformer model, trained on this synthetic data,\ndemonstrates strong generalization by surpassing its own data generator in\naccuracy and efficiency, using 50% fewer search steps. Our experimental results\nwith SoTA LLMs also demonstrate that the standard approach of fine-tuning LLMs\non a set of question-answer pairs is insufficient for solving this mathematical\ntask. This motivates the importance of discovering creative methods for\ncombining LLMs with symbolic reasoning engines, of which our work is an\ninstance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the first correct-by-construction learning-based system for\nstep-by-step mathematical integration. The key idea is to learn a policy,\nrepresented by a GPT transformer model, which guides the search for the right\nmathematical integration rule, to be carried out by a symbolic solver.\nConcretely, we introduce a symbolic engine with axiomatically correct actions\non mathematical expressions, as well as the first dataset for step-by-step\nintegration. Our GPT-style transformer model, trained on this synthetic data,\ndemonstrates strong generalization by surpassing its own data generator in\naccuracy and efficiency, using 50% fewer search steps. Our experimental results\nwith SoTA LLMs also demonstrate that the standard approach of fine-tuning LLMs\non a set of question-answer pairs is insufficient for solving this mathematical\ntask. This motivates the importance of discovering creative methods for\ncombining LLMs with symbolic reasoning engines, of which our work is an\ninstance."
                },
                "authors": [
                    {
                        "name": "Mert nsal"
                    },
                    {
                        "name": "Timon Gehr"
                    },
                    {
                        "name": "Martin Vechev"
                    }
                ],
                "author_detail": {
                    "name": "Martin Vechev"
                },
                "author": "Martin Vechev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02666v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02666v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11969v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11969v3",
                "updated": "2024-10-03T16:46:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    16,
                    46,
                    9,
                    3,
                    277,
                    0
                ],
                "published": "2024-07-16T17:59:55Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    17,
                    59,
                    55,
                    1,
                    198,
                    0
                ],
                "title": "Does Refusal Training in LLMs Generalize to the Past Tense?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does Refusal Training in LLMs Generalize to the Past Tense?"
                },
                "summary": "Refusal training is widely used to prevent LLMs from generating harmful,\nundesirable, or illegal outputs. We reveal a curious generalization gap in the\ncurrent refusal training approaches: simply reformulating a harmful request in\nthe past tense (e.g., \"How to make a Molotov cocktail?\" to \"How did people make\na Molotov cocktail?\") is often sufficient to jailbreak many state-of-the-art\nLLMs. We systematically evaluate this method on Llama-3 8B, Claude-3.5 Sonnet,\nGPT-3.5 Turbo, Gemma-2 9B, Phi-3-Mini, GPT-4o mini, GPT-4o, o1-mini,\no1-preview, and R2D2 models using GPT-3.5 Turbo as a reformulation model. For\nexample, the success rate of this simple attack on GPT-4o increases from 1%\nusing direct requests to 88% using 20 past tense reformulation attempts on\nharmful requests from JailbreakBench with GPT-4 as a jailbreak judge.\nInterestingly, we also find that reformulations in the future tense are less\neffective, suggesting that refusal guardrails tend to consider past historical\nquestions more benign than hypothetical future questions. Moreover, our\nexperiments on fine-tuning GPT-3.5 Turbo show that defending against past\nreformulations is feasible when past tense examples are explicitly included in\nthe fine-tuning data. Overall, our findings highlight that the widely used\nalignment techniques -- such as SFT, RLHF, and adversarial training -- employed\nto align the studied models can be brittle and do not always generalize as\nintended. We provide code and jailbreak artifacts at\nhttps://github.com/tml-epfl/llm-past-tense.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refusal training is widely used to prevent LLMs from generating harmful,\nundesirable, or illegal outputs. We reveal a curious generalization gap in the\ncurrent refusal training approaches: simply reformulating a harmful request in\nthe past tense (e.g., \"How to make a Molotov cocktail?\" to \"How did people make\na Molotov cocktail?\") is often sufficient to jailbreak many state-of-the-art\nLLMs. We systematically evaluate this method on Llama-3 8B, Claude-3.5 Sonnet,\nGPT-3.5 Turbo, Gemma-2 9B, Phi-3-Mini, GPT-4o mini, GPT-4o, o1-mini,\no1-preview, and R2D2 models using GPT-3.5 Turbo as a reformulation model. For\nexample, the success rate of this simple attack on GPT-4o increases from 1%\nusing direct requests to 88% using 20 past tense reformulation attempts on\nharmful requests from JailbreakBench with GPT-4 as a jailbreak judge.\nInterestingly, we also find that reformulations in the future tense are less\neffective, suggesting that refusal guardrails tend to consider past historical\nquestions more benign than hypothetical future questions. Moreover, our\nexperiments on fine-tuning GPT-3.5 Turbo show that defending against past\nreformulations is feasible when past tense examples are explicitly included in\nthe fine-tuning data. Overall, our findings highlight that the widely used\nalignment techniques -- such as SFT, RLHF, and adversarial training -- employed\nto align the studied models can be brittle and do not always generalize as\nintended. We provide code and jailbreak artifacts at\nhttps://github.com/tml-epfl/llm-past-tense."
                },
                "authors": [
                    {
                        "name": "Maksym Andriushchenko"
                    },
                    {
                        "name": "Nicolas Flammarion"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Flammarion"
                },
                "author": "Nicolas Flammarion",
                "arxiv_comment": "Update in v3: o1-mini and o1-preview results (on top of GPT-4o and\n  Claude 3.5 Sonnet added in v2). We provide code and jailbreak artifacts at\n  https://github.com/tml-epfl/llm-past-tense",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11969v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11969v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02657v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02657v1",
                "updated": "2024-10-03T16:43:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    16,
                    43,
                    17,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T16:43:17Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    16,
                    43,
                    17,
                    3,
                    277,
                    0
                ],
                "title": "Hate Personified: Investigating the role of LLMs in content moderation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hate Personified: Investigating the role of LLMs in content moderation"
                },
                "summary": "For subjective tasks such as hate detection, where people perceive hate\ndifferently, the Large Language Model's (LLM) ability to represent diverse\ngroups is unclear. By including additional context in prompts, we\ncomprehensively analyze LLM's sensitivity to geographical priming, persona\nattributes, and numerical information to assess how well the needs of various\ngroups are reflected. Our findings on two LLMs, five languages, and six\ndatasets reveal that mimicking persona-based attributes leads to annotation\nvariability. Meanwhile, incorporating geographical signals leads to better\nregional alignment. We also find that the LLMs are sensitive to numerical\nanchors, indicating the ability to leverage community-based flagging efforts\nand exposure to adversaries. Our work provides preliminary guidelines and\nhighlights the nuances of applying LLMs in culturally sensitive cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For subjective tasks such as hate detection, where people perceive hate\ndifferently, the Large Language Model's (LLM) ability to represent diverse\ngroups is unclear. By including additional context in prompts, we\ncomprehensively analyze LLM's sensitivity to geographical priming, persona\nattributes, and numerical information to assess how well the needs of various\ngroups are reflected. Our findings on two LLMs, five languages, and six\ndatasets reveal that mimicking persona-based attributes leads to annotation\nvariability. Meanwhile, incorporating geographical signals leads to better\nregional alignment. We also find that the LLMs are sensitive to numerical\nanchors, indicating the ability to leverage community-based flagging efforts\nand exposure to adversaries. Our work provides preliminary guidelines and\nhighlights the nuances of applying LLMs in culturally sensitive cases."
                },
                "authors": [
                    {
                        "name": "Sarah Masud"
                    },
                    {
                        "name": "Sahajpreet Singh"
                    },
                    {
                        "name": "Viktor Hangya"
                    },
                    {
                        "name": "Alexander Fraser"
                    },
                    {
                        "name": "Tanmoy Chakraborty"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Chakraborty"
                },
                "author": "Tanmoy Chakraborty",
                "arxiv_comment": "17 pages, 6 Figures, 13 Tables, EMNLP'24 Mains",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02657v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02657v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.16382v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.16382v2",
                "updated": "2024-10-03T16:39:32Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    16,
                    39,
                    32,
                    3,
                    277,
                    0
                ],
                "published": "2024-02-26T08:08:03Z",
                "published_parsed": [
                    2024,
                    2,
                    26,
                    8,
                    8,
                    3,
                    0,
                    57,
                    0
                ],
                "title": "Immunization against harmful fine-tuning attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Immunization against harmful fine-tuning attacks"
                },
                "summary": "Large Language Models (LLMs) are often trained with safety guards intended to\nprevent harmful text generation. However, such safety training can be removed\nby fine-tuning the LLM on harmful datasets. While this emerging threat (harmful\nfine-tuning attacks) has been characterized by previous work, there is little\nunderstanding of how we should proceed in constructing and validating defenses\nagainst these attacks especially in the case where defenders would not have\ncontrol of the fine-tuning process. We introduce a formal framework based on\nthe training budget of an attacker which we call \"Immunization\" conditions.\nUsing a formal characterisation of the harmful fine-tuning problem, we provide\na thorough description of what a successful defense must comprise of and\nestablish a set of guidelines on how rigorous defense research that gives us\nconfidence should proceed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are often trained with safety guards intended to\nprevent harmful text generation. However, such safety training can be removed\nby fine-tuning the LLM on harmful datasets. While this emerging threat (harmful\nfine-tuning attacks) has been characterized by previous work, there is little\nunderstanding of how we should proceed in constructing and validating defenses\nagainst these attacks especially in the case where defenders would not have\ncontrol of the fine-tuning process. We introduce a formal framework based on\nthe training budget of an attacker which we call \"Immunization\" conditions.\nUsing a formal characterisation of the harmful fine-tuning problem, we provide\na thorough description of what a successful defense must comprise of and\nestablish a set of guidelines on how rigorous defense research that gives us\nconfidence should proceed."
                },
                "authors": [
                    {
                        "name": "Domenic Rosati"
                    },
                    {
                        "name": "Jan Wehner"
                    },
                    {
                        "name": "Kai Williams"
                    },
                    {
                        "name": "ukasz Bartoszcze"
                    },
                    {
                        "name": "Jan Batzner"
                    },
                    {
                        "name": "Hassan Sajjad"
                    },
                    {
                        "name": "Frank Rudzicz"
                    }
                ],
                "author_detail": {
                    "name": "Frank Rudzicz"
                },
                "author": "Frank Rudzicz",
                "arxiv_comment": "Published in EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.16382v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.16382v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02653v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02653v1",
                "updated": "2024-10-03T16:36:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    16,
                    36,
                    35,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T16:36:35Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    16,
                    36,
                    35,
                    3,
                    277,
                    0
                ],
                "title": "Measuring and Improving Persuasiveness of Generative Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring and Improving Persuasiveness of Generative Models"
                },
                "summary": "LLMs are increasingly being used in workflows involving generating content to\nbe consumed by humans (e.g., marketing) and also in directly interacting with\nhumans (e.g., through chatbots). The development of such systems that are\ncapable of generating verifiably persuasive messages presents both\nopportunities and challenges for society. On the one hand, such systems could\npositively impact domains like advertising and social good, such as addressing\ndrug addiction, and on the other, they could be misused for spreading\nmisinformation and shaping political opinions. To channel LLMs' impact on\nsociety, we need to develop systems to measure and benchmark their\npersuasiveness. With this motivation, we introduce PersuasionBench and\nPersuasionArena, the first large-scale benchmark and arena containing a battery\nof tasks to measure the persuasion ability of generative models automatically.\nWe investigate to what extent LLMs know and leverage linguistic patterns that\ncan help them generate more persuasive language. Our findings indicate that the\npersuasiveness of LLMs correlates positively with model size, but smaller\nmodels can also be made to have a higher persuasiveness than much larger\nmodels. Notably, targeted training using synthetic and natural datasets\nsignificantly enhances smaller models' persuasive capabilities, challenging\nscale-dependent assumptions. Our findings carry key implications for both model\ndevelopers and policymakers. For instance, while the EU AI Act and California's\nSB-1047 aim to regulate AI models based on the number of floating point\noperations, we demonstrate that simple metrics like this alone fail to capture\nthe full scope of AI's societal impact. We invite the community to explore and\ncontribute to PersuasionArena and PersuasionBench, available at\nhttps://bit.ly/measure-persuasion, to advance our understanding of AI-driven\npersuasion and its societal implications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are increasingly being used in workflows involving generating content to\nbe consumed by humans (e.g., marketing) and also in directly interacting with\nhumans (e.g., through chatbots). The development of such systems that are\ncapable of generating verifiably persuasive messages presents both\nopportunities and challenges for society. On the one hand, such systems could\npositively impact domains like advertising and social good, such as addressing\ndrug addiction, and on the other, they could be misused for spreading\nmisinformation and shaping political opinions. To channel LLMs' impact on\nsociety, we need to develop systems to measure and benchmark their\npersuasiveness. With this motivation, we introduce PersuasionBench and\nPersuasionArena, the first large-scale benchmark and arena containing a battery\nof tasks to measure the persuasion ability of generative models automatically.\nWe investigate to what extent LLMs know and leverage linguistic patterns that\ncan help them generate more persuasive language. Our findings indicate that the\npersuasiveness of LLMs correlates positively with model size, but smaller\nmodels can also be made to have a higher persuasiveness than much larger\nmodels. Notably, targeted training using synthetic and natural datasets\nsignificantly enhances smaller models' persuasive capabilities, challenging\nscale-dependent assumptions. Our findings carry key implications for both model\ndevelopers and policymakers. For instance, while the EU AI Act and California's\nSB-1047 aim to regulate AI models based on the number of floating point\noperations, we demonstrate that simple metrics like this alone fail to capture\nthe full scope of AI's societal impact. We invite the community to explore and\ncontribute to PersuasionArena and PersuasionBench, available at\nhttps://bit.ly/measure-persuasion, to advance our understanding of AI-driven\npersuasion and its societal implications."
                },
                "authors": [
                    {
                        "name": "Somesh Singh"
                    },
                    {
                        "name": "Yaman K Singla"
                    },
                    {
                        "name": "Harini SI"
                    },
                    {
                        "name": "Balaji Krishnamurthy"
                    }
                ],
                "author_detail": {
                    "name": "Balaji Krishnamurthy"
                },
                "author": "Balaji Krishnamurthy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02653v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02653v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02650v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02650v1",
                "updated": "2024-10-03T16:34:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    16,
                    34,
                    46,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T16:34:46Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    16,
                    34,
                    46,
                    3,
                    277,
                    0
                ],
                "title": "Undesirable Memorization in Large Language Models: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Undesirable Memorization in Large Language Models: A Survey"
                },
                "summary": "While recent research increasingly showcases the remarkable capabilities of\nLarge Language Models (LLMs), it's vital to confront their hidden pitfalls.\nAmong these challenges, the issue of memorization stands out, posing\nsignificant ethical and legal risks. In this paper, we presents a\nSystematization of Knowledge (SoK) on the topic of memorization in LLMs.\nMemorization is the effect that a model tends to store and reproduce phrases or\npassages from the training data and has been shown to be the fundamental issue\nto various privacy and security attacks against LLMs.\n  We begin by providing an overview of the literature on the memorization,\nexploring it across five key dimensions: intentionality, degree,\nretrievability, abstraction, and transparency. Next, we discuss the metrics and\nmethods used to measure memorization, followed by an analysis of the factors\nthat contribute to memorization phenomenon. We then examine how memorization\nmanifests itself in specific model architectures and explore strategies for\nmitigating these effects. We conclude our overview by identifying potential\nresearch topics for the near future: to develop methods for balancing\nperformance and privacy in LLMs, and the analysis of memorization in specific\ncontexts, including conversational agents, retrieval-augmented generation,\nmultilingual language models, and diffusion language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While recent research increasingly showcases the remarkable capabilities of\nLarge Language Models (LLMs), it's vital to confront their hidden pitfalls.\nAmong these challenges, the issue of memorization stands out, posing\nsignificant ethical and legal risks. In this paper, we presents a\nSystematization of Knowledge (SoK) on the topic of memorization in LLMs.\nMemorization is the effect that a model tends to store and reproduce phrases or\npassages from the training data and has been shown to be the fundamental issue\nto various privacy and security attacks against LLMs.\n  We begin by providing an overview of the literature on the memorization,\nexploring it across five key dimensions: intentionality, degree,\nretrievability, abstraction, and transparency. Next, we discuss the metrics and\nmethods used to measure memorization, followed by an analysis of the factors\nthat contribute to memorization phenomenon. We then examine how memorization\nmanifests itself in specific model architectures and explore strategies for\nmitigating these effects. We conclude our overview by identifying potential\nresearch topics for the near future: to develop methods for balancing\nperformance and privacy in LLMs, and the analysis of memorization in specific\ncontexts, including conversational agents, retrieval-augmented generation,\nmultilingual language models, and diffusion language models."
                },
                "authors": [
                    {
                        "name": "Ali Satvaty"
                    },
                    {
                        "name": "Suzan Verberne"
                    },
                    {
                        "name": "Fatih Turkmen"
                    }
                ],
                "author_detail": {
                    "name": "Fatih Turkmen"
                },
                "author": "Fatih Turkmen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02650v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02650v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02026v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02026v2",
                "updated": "2024-10-03T16:31:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    16,
                    31,
                    59,
                    3,
                    277,
                    0
                ],
                "published": "2024-09-03T16:20:22Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    16,
                    20,
                    22,
                    1,
                    247,
                    0
                ],
                "title": "Foundations of Large Language Model Compression -- Part 1: Weight\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundations of Large Language Model Compression -- Part 1: Weight\n  Quantization"
                },
                "summary": "In recent years, compression of large language models (LLMs) has emerged as\nan important problem to enable language model deployment on\nresource-constrained devices, reduce computational costs, and mitigate the\nenvironmental footprint of large-scale AI infrastructure. In this paper, we lay\ndown the foundation for LLM quantization from a convex optimization perspective\nand propose a quantization technique that builds on this foundation for optimum\nquantization outcomes. Our quantization framework, CVXQ, scales to models\ncontaining hundreds of billions of weight parameters and provides users with\nthe flexibility to compress models to any specified model size, post-training.\nA reference implementation of CVXQ can be obtained from github.com/seannz/cvxq.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, compression of large language models (LLMs) has emerged as\nan important problem to enable language model deployment on\nresource-constrained devices, reduce computational costs, and mitigate the\nenvironmental footprint of large-scale AI infrastructure. In this paper, we lay\ndown the foundation for LLM quantization from a convex optimization perspective\nand propose a quantization technique that builds on this foundation for optimum\nquantization outcomes. Our quantization framework, CVXQ, scales to models\ncontaining hundreds of billions of weight parameters and provides users with\nthe flexibility to compress models to any specified model size, post-training.\nA reference implementation of CVXQ can be obtained from github.com/seannz/cvxq."
                },
                "authors": [
                    {
                        "name": "Sean I. Young"
                    }
                ],
                "author_detail": {
                    "name": "Sean I. Young"
                },
                "author": "Sean I. Young",
                "arxiv_comment": "Preprint. 17 pages, 4 figures, 5 appendices",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02026v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02026v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02644v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02644v1",
                "updated": "2024-10-03T16:30:47Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    16,
                    30,
                    47,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T16:30:47Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    16,
                    30,
                    47,
                    3,
                    277,
                    0
                ],
                "title": "Agent Security Bench (ASB): Formalizing and Benchmarking Attacks and\n  Defenses in LLM-based Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent Security Bench (ASB): Formalizing and Benchmarking Attacks and\n  Defenses in LLM-based Agents"
                },
                "summary": "Although LLM-based agents, powered by Large Language Models (LLMs), can use\nexternal tools and memory mechanisms to solve complex real-world tasks, they\nmay also introduce critical security vulnerabilities. However, the existing\nliterature does not comprehensively evaluate attacks and defenses against\nLLM-based agents. To address this, we introduce Agent Security Bench (ASB), a\ncomprehensive framework designed to formalize, benchmark, and evaluate the\nattacks and defenses of LLM-based agents, including 10 scenarios (e.g.,\ne-commerce, autonomous driving, finance), 10 agents targeting the scenarios,\nover 400 tools, 23 different types of attack/defense methods, and 8 evaluation\nmetrics. Based on ASB, we benchmark 10 prompt injection attacks, a memory\npoisoning attack, a novel Plan-of-Thought backdoor attack, a mixed attack, and\n10 corresponding defenses across 13 LLM backbones with nearly 90,000 testing\ncases in total. Our benchmark results reveal critical vulnerabilities in\ndifferent stages of agent operation, including system prompt, user prompt\nhandling, tool usage, and memory retrieval, with the highest average attack\nsuccess rate of 84.30\\%, but limited effectiveness shown in current defenses,\nunveiling important works to be done in terms of agent security for the\ncommunity. Our code can be found at https://github.com/agiresearch/ASB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although LLM-based agents, powered by Large Language Models (LLMs), can use\nexternal tools and memory mechanisms to solve complex real-world tasks, they\nmay also introduce critical security vulnerabilities. However, the existing\nliterature does not comprehensively evaluate attacks and defenses against\nLLM-based agents. To address this, we introduce Agent Security Bench (ASB), a\ncomprehensive framework designed to formalize, benchmark, and evaluate the\nattacks and defenses of LLM-based agents, including 10 scenarios (e.g.,\ne-commerce, autonomous driving, finance), 10 agents targeting the scenarios,\nover 400 tools, 23 different types of attack/defense methods, and 8 evaluation\nmetrics. Based on ASB, we benchmark 10 prompt injection attacks, a memory\npoisoning attack, a novel Plan-of-Thought backdoor attack, a mixed attack, and\n10 corresponding defenses across 13 LLM backbones with nearly 90,000 testing\ncases in total. Our benchmark results reveal critical vulnerabilities in\ndifferent stages of agent operation, including system prompt, user prompt\nhandling, tool usage, and memory retrieval, with the highest average attack\nsuccess rate of 84.30\\%, but limited effectiveness shown in current defenses,\nunveiling important works to be done in terms of agent security for the\ncommunity. Our code can be found at https://github.com/agiresearch/ASB."
                },
                "authors": [
                    {
                        "name": "Hanrong Zhang"
                    },
                    {
                        "name": "Jingyuan Huang"
                    },
                    {
                        "name": "Kai Mei"
                    },
                    {
                        "name": "Yifei Yao"
                    },
                    {
                        "name": "Zhenting Wang"
                    },
                    {
                        "name": "Chenlu Zhan"
                    },
                    {
                        "name": "Hongwei Wang"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02644v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02644v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11295v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11295v2",
                "updated": "2024-10-03T16:30:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    16,
                    30,
                    43,
                    3,
                    277,
                    0
                ],
                "published": "2024-09-17T15:49:44Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    15,
                    49,
                    44,
                    1,
                    261,
                    0
                ],
                "title": "EIA: Environmental Injection Attack on Generalist Web Agents for Privacy\n  Leakage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EIA: Environmental Injection Attack on Generalist Web Agents for Privacy\n  Leakage"
                },
                "summary": "Generalist web agents have demonstrated remarkable potential in autonomously\ncompleting a wide range of tasks on real websites, significantly boosting human\nproductivity. However, web tasks, such as booking flights, usually involve\nusers' PII, which may be exposed to potential privacy risks if web agents\naccidentally interact with compromised websites, a scenario that remains\nlargely unexplored in the literature. In this work, we narrow this gap by\nconducting the first study on the privacy risks of generalist web agents in\nadversarial environments. First, we present a realistic threat model for\nattacks on the website, where we consider two adversarial targets: stealing\nusers' specific PII or the entire user request. Then, we propose a novel attack\nmethod, termed Environmental Injection Attack (EIA). EIA injects malicious\ncontent designed to adapt well to environments where the agents operate and our\nwork instantiates EIA specifically for privacy scenarios in web environments.\nWe collect 177 action steps that involve diverse PII categories on realistic\nwebsites from the Mind2Web, and conduct experiments using one of the most\ncapable generalist web agent frameworks to date. The results demonstrate that\nEIA achieves up to 70% ASR in stealing specific PII and 16% ASR for full user\nrequest. Additionally, by accessing the stealthiness and experimenting with a\ndefensive system prompt, we indicate that EIA is hard to detect and mitigate.\nNotably, attacks that are not well adapted for a webpage can be detected via\nhuman inspection, leading to our discussion about the trade-off between\nsecurity and autonomy. However, extra attackers' efforts can make EIA\nseamlessly adapted, rendering such supervision ineffective. Thus, we further\ndiscuss the defenses at the pre- and post-deployment stages of the websites\nwithout relying on human supervision and call for more advanced defense\nstrategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalist web agents have demonstrated remarkable potential in autonomously\ncompleting a wide range of tasks on real websites, significantly boosting human\nproductivity. However, web tasks, such as booking flights, usually involve\nusers' PII, which may be exposed to potential privacy risks if web agents\naccidentally interact with compromised websites, a scenario that remains\nlargely unexplored in the literature. In this work, we narrow this gap by\nconducting the first study on the privacy risks of generalist web agents in\nadversarial environments. First, we present a realistic threat model for\nattacks on the website, where we consider two adversarial targets: stealing\nusers' specific PII or the entire user request. Then, we propose a novel attack\nmethod, termed Environmental Injection Attack (EIA). EIA injects malicious\ncontent designed to adapt well to environments where the agents operate and our\nwork instantiates EIA specifically for privacy scenarios in web environments.\nWe collect 177 action steps that involve diverse PII categories on realistic\nwebsites from the Mind2Web, and conduct experiments using one of the most\ncapable generalist web agent frameworks to date. The results demonstrate that\nEIA achieves up to 70% ASR in stealing specific PII and 16% ASR for full user\nrequest. Additionally, by accessing the stealthiness and experimenting with a\ndefensive system prompt, we indicate that EIA is hard to detect and mitigate.\nNotably, attacks that are not well adapted for a webpage can be detected via\nhuman inspection, leading to our discussion about the trade-off between\nsecurity and autonomy. However, extra attackers' efforts can make EIA\nseamlessly adapted, rendering such supervision ineffective. Thus, we further\ndiscuss the defenses at the pre- and post-deployment stages of the websites\nwithout relying on human supervision and call for more advanced defense\nstrategies."
                },
                "authors": [
                    {
                        "name": "Zeyi Liao"
                    },
                    {
                        "name": "Lingbo Mo"
                    },
                    {
                        "name": "Chejian Xu"
                    },
                    {
                        "name": "Mintong Kang"
                    },
                    {
                        "name": "Jiawei Zhang"
                    },
                    {
                        "name": "Chaowei Xiao"
                    },
                    {
                        "name": "Yuan Tian"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Huan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Huan Sun"
                },
                "author": "Huan Sun",
                "arxiv_comment": "29 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11295v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11295v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02643v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02643v1",
                "updated": "2024-10-03T16:29:47Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    16,
                    29,
                    47,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T16:29:47Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    16,
                    29,
                    47,
                    3,
                    277,
                    0
                ],
                "title": "Why Sample Space Matters: Keyframe Sampling Optimization for LiDAR-based\n  Place Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why Sample Space Matters: Keyframe Sampling Optimization for LiDAR-based\n  Place Recognition"
                },
                "summary": "Recent advances in robotics are pushing real-world autonomy, enabling robots\nto perform long-term and large-scale missions. A crucial component for\nsuccessful missions is the incorporation of loop closures through place\nrecognition, which effectively mitigates accumulated pose estimation drift.\nDespite computational advancements, optimizing performance for real-time\ndeployment remains challenging, especially in resource-constrained mobile\nrobots and multi-robot systems since, conventional keyframe sampling practices\nin place recognition often result in retaining redundant information or\noverlooking relevant data, as they rely on fixed sampling intervals or work\ndirectly in the 3D space instead of the feature space. To address these\nconcerns, we introduce the concept of sample space in place recognition and\ndemonstrate how different sampling techniques affect the query process and\noverall performance. We then present a novel keyframe sampling approach for\nLiDAR-based place recognition, which focuses on redundancy minimization and\ninformation preservation in the hyper-dimensional descriptor space. This\napproach is applicable to both learning-based and handcrafted descriptors, and\nthrough the experimental validation across multiple datasets and descriptor\nframeworks, we demonstrate the effectiveness of our proposed method, showing it\ncan jointly minimize redundancy and preserve essential information in\nreal-time. The proposed approach maintains robust performance across various\ndatasets without requiring parameter tuning, contributing to more efficient and\nreliable place recognition for a wide range of robotic applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in robotics are pushing real-world autonomy, enabling robots\nto perform long-term and large-scale missions. A crucial component for\nsuccessful missions is the incorporation of loop closures through place\nrecognition, which effectively mitigates accumulated pose estimation drift.\nDespite computational advancements, optimizing performance for real-time\ndeployment remains challenging, especially in resource-constrained mobile\nrobots and multi-robot systems since, conventional keyframe sampling practices\nin place recognition often result in retaining redundant information or\noverlooking relevant data, as they rely on fixed sampling intervals or work\ndirectly in the 3D space instead of the feature space. To address these\nconcerns, we introduce the concept of sample space in place recognition and\ndemonstrate how different sampling techniques affect the query process and\noverall performance. We then present a novel keyframe sampling approach for\nLiDAR-based place recognition, which focuses on redundancy minimization and\ninformation preservation in the hyper-dimensional descriptor space. This\napproach is applicable to both learning-based and handcrafted descriptors, and\nthrough the experimental validation across multiple datasets and descriptor\nframeworks, we demonstrate the effectiveness of our proposed method, showing it\ncan jointly minimize redundancy and preserve essential information in\nreal-time. The proposed approach maintains robust performance across various\ndatasets without requiring parameter tuning, contributing to more efficient and\nreliable place recognition for a wide range of robotic applications."
                },
                "authors": [
                    {
                        "name": "Nikolaos Stathoulopoulos"
                    },
                    {
                        "name": "Vidya Sumathy"
                    },
                    {
                        "name": "Christoforos Kanellakis"
                    },
                    {
                        "name": "George Nikolakopoulos"
                    }
                ],
                "author_detail": {
                    "name": "George Nikolakopoulos"
                },
                "author": "George Nikolakopoulos",
                "arxiv_comment": "20 pages, 15 figures. Submitted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02643v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02643v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02642v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02642v1",
                "updated": "2024-10-03T16:25:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    16,
                    25,
                    37,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T16:25:37Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    16,
                    25,
                    37,
                    3,
                    277,
                    0
                ],
                "title": "Attention in Large Language Models Yields Efficient Zero-Shot Re-Rankers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention in Large Language Models Yields Efficient Zero-Shot Re-Rankers"
                },
                "summary": "Information retrieval (IR) systems have played a vital role in modern digital\nlife and have cemented their continued usefulness in this new era of generative\nAI via retrieval-augmented generation. With strong language processing\ncapabilities and remarkable versatility, large language models (LLMs) have\nbecome popular choices for zero-shot re-ranking in IR systems. So far,\nLLM-based re-ranking methods rely on strong generative capabilities, which\nrestricts their use to either specialized or powerful proprietary models. Given\nthese restrictions, we ask: is autoregressive generation necessary and optimal\nfor LLMs to perform re-ranking? We hypothesize that there are abundant signals\nrelevant to re-ranking within LLMs that might not be used to their full\npotential via generation. To more directly leverage such signals, we propose\nin-context re-ranking (ICR), a novel method that leverages the change in\nattention pattern caused by the search query for accurate and efficient\nre-ranking. To mitigate the intrinsic biases in LLMs, we propose a calibration\nmethod using a content-free query. Due to the absence of generation, ICR only\nrequires two ($O(1)$) forward passes to re-rank $N$ documents, making it\nsubstantially more efficient than generative re-ranking methods that require at\nleast $O(N)$ forward passes. Our novel design also enables ICR to be applied to\nany LLM without specialized training while guaranteeing a well-formed ranking.\nExtensive experiments with two popular open-weight LLMs on standard single-hop\nand multi-hop information retrieval benchmarks show that ICR outperforms\nRankGPT while cutting the latency by more than 60% in practice. Through\ndetailed analyses, we show that ICR's performance is specially strong on tasks\nthat require more complex re-ranking signals. Our findings call for further\nexploration on novel ways of utilizing open-weight LLMs beyond text generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information retrieval (IR) systems have played a vital role in modern digital\nlife and have cemented their continued usefulness in this new era of generative\nAI via retrieval-augmented generation. With strong language processing\ncapabilities and remarkable versatility, large language models (LLMs) have\nbecome popular choices for zero-shot re-ranking in IR systems. So far,\nLLM-based re-ranking methods rely on strong generative capabilities, which\nrestricts their use to either specialized or powerful proprietary models. Given\nthese restrictions, we ask: is autoregressive generation necessary and optimal\nfor LLMs to perform re-ranking? We hypothesize that there are abundant signals\nrelevant to re-ranking within LLMs that might not be used to their full\npotential via generation. To more directly leverage such signals, we propose\nin-context re-ranking (ICR), a novel method that leverages the change in\nattention pattern caused by the search query for accurate and efficient\nre-ranking. To mitigate the intrinsic biases in LLMs, we propose a calibration\nmethod using a content-free query. Due to the absence of generation, ICR only\nrequires two ($O(1)$) forward passes to re-rank $N$ documents, making it\nsubstantially more efficient than generative re-ranking methods that require at\nleast $O(N)$ forward passes. Our novel design also enables ICR to be applied to\nany LLM without specialized training while guaranteeing a well-formed ranking.\nExtensive experiments with two popular open-weight LLMs on standard single-hop\nand multi-hop information retrieval benchmarks show that ICR outperforms\nRankGPT while cutting the latency by more than 60% in practice. Through\ndetailed analyses, we show that ICR's performance is specially strong on tasks\nthat require more complex re-ranking signals. Our findings call for further\nexploration on novel ways of utilizing open-weight LLMs beyond text generation."
                },
                "authors": [
                    {
                        "name": "Shijie Chen"
                    },
                    {
                        "name": "Bernal Jimnez Gutirrez"
                    },
                    {
                        "name": "Yu Su"
                    }
                ],
                "author_detail": {
                    "name": "Yu Su"
                },
                "author": "Yu Su",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02642v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02642v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02631v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02631v1",
                "updated": "2024-10-03T16:15:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    16,
                    15,
                    4,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T16:15:04Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    16,
                    15,
                    4,
                    3,
                    277,
                    0
                ],
                "title": "Large Language Model for Multi-Domain Translation: Benchmarking and\n  Domain CoT Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model for Multi-Domain Translation: Benchmarking and\n  Domain CoT Fine-tuning"
                },
                "summary": "Achieving consistent high-quality machine translation (MT) across diverse\ndomains remains a significant challenge, primarily due to the limited and\nimbalanced parallel training data available in various domains. While large\nlanguage models (LLMs) have demonstrated impressive general understanding and\ngeneration abilities, their potential in multi-domain MT is under-explored. We\nestablish a comprehensive benchmark for multi-domain translation, featuring 25\nGerman$\\Leftrightarrow$English and 22 Chinese$\\Leftrightarrow$English test sets\nrespectively covering 15 domains. Our evaluation of prominent LLMs reveals a\ndiscernible performance gap against traditional MT systems, highlighting domain\noverfitting and catastrophic forgetting issues after fine-tuning on\ndomain-limited corpora. To mitigate this, we propose a domain Chain of Thought\n(CoT) fine-tuning technique that utilizes the intrinsic multi-domain\nintelligence of LLMs to improve translation performance. This method inspires\nthe LLM to perceive domain information from the source text, which then serves\nas a helpful hint to guide the translation process. Despite being trained on a\nsmall dataset of four domains, our CoT fine-tune approach achieves notable\nenhancements in translation accuracy and domain robustness than traditional\nfine-tuning, as evidenced by an average 1.53 BLEU score increase in over 20\nGerman$\\rightarrow$English distinct out-of-domain tests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Achieving consistent high-quality machine translation (MT) across diverse\ndomains remains a significant challenge, primarily due to the limited and\nimbalanced parallel training data available in various domains. While large\nlanguage models (LLMs) have demonstrated impressive general understanding and\ngeneration abilities, their potential in multi-domain MT is under-explored. We\nestablish a comprehensive benchmark for multi-domain translation, featuring 25\nGerman$\\Leftrightarrow$English and 22 Chinese$\\Leftrightarrow$English test sets\nrespectively covering 15 domains. Our evaluation of prominent LLMs reveals a\ndiscernible performance gap against traditional MT systems, highlighting domain\noverfitting and catastrophic forgetting issues after fine-tuning on\ndomain-limited corpora. To mitigate this, we propose a domain Chain of Thought\n(CoT) fine-tuning technique that utilizes the intrinsic multi-domain\nintelligence of LLMs to improve translation performance. This method inspires\nthe LLM to perceive domain information from the source text, which then serves\nas a helpful hint to guide the translation process. Despite being trained on a\nsmall dataset of four domains, our CoT fine-tune approach achieves notable\nenhancements in translation accuracy and domain robustness than traditional\nfine-tuning, as evidenced by an average 1.53 BLEU score increase in over 20\nGerman$\\rightarrow$English distinct out-of-domain tests."
                },
                "authors": [
                    {
                        "name": "Tianxiang Hu"
                    },
                    {
                        "name": "Pei Zhang"
                    },
                    {
                        "name": "Baosong Yang"
                    },
                    {
                        "name": "Jun Xie"
                    },
                    {
                        "name": "Derek F. Wong"
                    },
                    {
                        "name": "Rui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Wang"
                },
                "author": "Rui Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02631v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02631v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.15284v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.15284v5",
                "updated": "2024-10-03T16:13:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    16,
                    13,
                    55,
                    3,
                    277,
                    0
                ],
                "published": "2024-01-27T03:53:25Z",
                "published_parsed": [
                    2024,
                    1,
                    27,
                    3,
                    53,
                    25,
                    5,
                    27,
                    0
                ],
                "title": "Beyond principlism: Practical strategies for ethical AI use in research\n  practices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond principlism: Practical strategies for ethical AI use in research\n  practices"
                },
                "summary": "The rapid adoption of generative artificial intelligence (AI) in scientific\nresearch, particularly large language models (LLMs), has outpaced the\ndevelopment of ethical guidelines, leading to a Triple-Too problem: too many\nhigh-level ethical initiatives, too abstract principles lacking contextual and\npractical relevance, and too much focus on restrictions and risks over benefits\nand utilities. Existing approaches, including principlism (reliance on abstract\nethical principles), formalism (rigid application of rules), and technical\nsolutionism (overemphasis on technological fixes), offer little practical\nguidance for addressing ethical challenges of AI in scientific research\npractices. To bridge the gap between abstract principles and day-to-day\nresearch practices, a user-centered, realism-inspired approach is proposed\nhere. It outlines five specific goals for ethical AI use: 1) understanding\nmodel training and output, including bias mitigation strategies; 2) respecting\nprivacy, confidentiality, and copyright; 3) avoiding plagiarism and policy\nviolations; 4) applying AI beneficially compared to alternatives; and 5) using\nAI transparently and reproducibly. Each goal is accompanied by actionable\nstrategies and realistic cases of misuse and corrective measures. I argue that\nethical AI application requires evaluating its utility against existing\nalternatives rather than isolated performance metrics. Additionally, I propose\ndocumentation guidelines to enhance transparency and reproducibility in\nAI-assisted research. Moving forward, we need targeted professional\ndevelopment, training programs, and balanced enforcement mechanisms to promote\nresponsible AI use while fostering innovation. By refining these ethical\nguidelines and adapting them to emerging AI capabilities, we can accelerate\nscientific progress without compromising research integrity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid adoption of generative artificial intelligence (AI) in scientific\nresearch, particularly large language models (LLMs), has outpaced the\ndevelopment of ethical guidelines, leading to a Triple-Too problem: too many\nhigh-level ethical initiatives, too abstract principles lacking contextual and\npractical relevance, and too much focus on restrictions and risks over benefits\nand utilities. Existing approaches, including principlism (reliance on abstract\nethical principles), formalism (rigid application of rules), and technical\nsolutionism (overemphasis on technological fixes), offer little practical\nguidance for addressing ethical challenges of AI in scientific research\npractices. To bridge the gap between abstract principles and day-to-day\nresearch practices, a user-centered, realism-inspired approach is proposed\nhere. It outlines five specific goals for ethical AI use: 1) understanding\nmodel training and output, including bias mitigation strategies; 2) respecting\nprivacy, confidentiality, and copyright; 3) avoiding plagiarism and policy\nviolations; 4) applying AI beneficially compared to alternatives; and 5) using\nAI transparently and reproducibly. Each goal is accompanied by actionable\nstrategies and realistic cases of misuse and corrective measures. I argue that\nethical AI application requires evaluating its utility against existing\nalternatives rather than isolated performance metrics. Additionally, I propose\ndocumentation guidelines to enhance transparency and reproducibility in\nAI-assisted research. Moving forward, we need targeted professional\ndevelopment, training programs, and balanced enforcement mechanisms to promote\nresponsible AI use while fostering innovation. By refining these ethical\nguidelines and adapting them to emerging AI capabilities, we can accelerate\nscientific progress without compromising research integrity."
                },
                "authors": [
                    {
                        "name": "Zhicheng Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Lin"
                },
                "author": "Zhicheng Lin",
                "arxiv_doi": "10.1007/s43681-024-00585-5",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s43681-024-00585-5",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2401.15284v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.15284v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted in: AI and Ethics. 20 pages, 1 figure, 3 tables, 2 boxes",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.08702v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.08702v4",
                "updated": "2024-10-03T16:11:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    16,
                    11,
                    43,
                    3,
                    277,
                    0
                ],
                "published": "2024-02-13T16:38:01Z",
                "published_parsed": [
                    2024,
                    2,
                    13,
                    16,
                    38,
                    1,
                    1,
                    44,
                    0
                ],
                "title": "PRompt Optimization in Multi-Step Tasks (PROMST): Integrating Human\n  Feedback and Heuristic-based Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRompt Optimization in Multi-Step Tasks (PROMST): Integrating Human\n  Feedback and Heuristic-based Sampling"
                },
                "summary": "Prompt optimization aims to find the best prompt to a large language model\n(LLM) for a given task. LLMs have been successfully used to help find and\nimprove prompt candidates for single-step tasks. However, realistic tasks for\nagents are multi-step and introduce new challenges: (1) Prompt content is\nlikely to be more extensive and complex, making it more difficult for LLMs to\nanalyze errors, (2) the impact of an individual step is difficult to evaluate,\nand (3) different people may have varied preferences about task execution.\nWhile humans struggle to optimize prompts, they are good at providing feedback\nabout LLM outputs; we therefore introduce a new LLM-driven discrete prompt\noptimization framework PRompt Optimization in Multi-Step Tasks (PROMST) that\nincorporates human-designed feedback rules to automatically offer direct\nsuggestions for improvement. We also use an extra learned heuristic model that\npredicts prompt performance to efficiently sample from prompt candidates. This\napproach significantly outperforms both human-engineered prompts and several\nother prompt optimization methods across 11 representative multi-step tasks (an\naverage 10.6\\%-29.3\\% improvement to current best methods on five LLMs\nrespectively). We believe our work can serve as a benchmark for automatic\nprompt optimization for LLM-driven multi-step tasks. Datasets and Codes are\navailable at https://github.com/yongchao98/PROMST. Project Page is available at\nhttps://yongchao98.github.io/MIT-REALM-PROMST.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt optimization aims to find the best prompt to a large language model\n(LLM) for a given task. LLMs have been successfully used to help find and\nimprove prompt candidates for single-step tasks. However, realistic tasks for\nagents are multi-step and introduce new challenges: (1) Prompt content is\nlikely to be more extensive and complex, making it more difficult for LLMs to\nanalyze errors, (2) the impact of an individual step is difficult to evaluate,\nand (3) different people may have varied preferences about task execution.\nWhile humans struggle to optimize prompts, they are good at providing feedback\nabout LLM outputs; we therefore introduce a new LLM-driven discrete prompt\noptimization framework PRompt Optimization in Multi-Step Tasks (PROMST) that\nincorporates human-designed feedback rules to automatically offer direct\nsuggestions for improvement. We also use an extra learned heuristic model that\npredicts prompt performance to efficiently sample from prompt candidates. This\napproach significantly outperforms both human-engineered prompts and several\nother prompt optimization methods across 11 representative multi-step tasks (an\naverage 10.6\\%-29.3\\% improvement to current best methods on five LLMs\nrespectively). We believe our work can serve as a benchmark for automatic\nprompt optimization for LLM-driven multi-step tasks. Datasets and Codes are\navailable at https://github.com/yongchao98/PROMST. Project Page is available at\nhttps://yongchao98.github.io/MIT-REALM-PROMST."
                },
                "authors": [
                    {
                        "name": "Yongchao Chen"
                    },
                    {
                        "name": "Jacob Arkin"
                    },
                    {
                        "name": "Yilun Hao"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Nicholas Roy"
                    },
                    {
                        "name": "Chuchu Fan"
                    }
                ],
                "author_detail": {
                    "name": "Chuchu Fan"
                },
                "author": "Chuchu Fan",
                "arxiv_comment": "62 pages, 14 figures, Published in EMNLP 2024 Main",
                "arxiv_journal_ref": "EMNLP 2024 Main (The 2024 Conference on Empirical Methods on\n  Natural Language Processing )",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.08702v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.08702v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.05900v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.05900v2",
                "updated": "2024-10-03T16:10:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    16,
                    10,
                    9,
                    3,
                    277,
                    0
                ],
                "published": "2024-01-11T13:22:30Z",
                "published_parsed": [
                    2024,
                    1,
                    11,
                    13,
                    22,
                    30,
                    3,
                    11,
                    0
                ],
                "title": "Near-Field Communications: A Comprehensive Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Near-Field Communications: A Comprehensive Survey"
                },
                "summary": "Multiple-antenna technologies are evolving towards larger aperture sizes,\nextremely high frequencies, and innovative antenna types. This evolution is\nfostering the emergence of near-field communications (NFC) in future wireless\nsystems. Considerable attention has been directed towards this cutting-edge\ntechnology due to its potential to enhance the capacity of wireless networks by\nintroducing increased spatial degrees of freedom (DoFs) in the range domain.\nWithin this context, a comprehensive review of the state of the art on NFC is\npresented, with a specific focus on its 1) fundamental operating principles, 2)\nchannel modeling, 3) performance analysis, 4) signal processing techniques, and\n5) integration with other emerging applications. Specifically, 1) the basic\nprinciples of NFC are characterized from both physics and communications\nperspectives, unveiling its unique properties in contrast to far-field\ncommunications. 2) Building on these principles, deterministic and stochastic\nnear-field channel models are explored for spatially-discrete (SPD) and\ncontinuous-aperture (CAP) arrays. 3) Based on these models, existing\ncontributions to near-field performance analysis are reviewed in terms of\nDoFs/effective DoFs (EDoFs), the power scaling law, and transmission rate. 4)\nExisting signal processing techniques for NFC are systematically surveyed,\nwhich include channel estimation, beamforming design, and low-complexity beam\ntraining. 5) Major issues and research opportunities in incorporating\nnear-field models into other promising technologies are identified to advance\nNFC's deployment in next-generation networks. Throughout this paper, promising\ndirections are highlighted to inspire future research endeavors in the realm of\nNFC, underscoring its significance in the advancement of wireless communication\ntechnologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiple-antenna technologies are evolving towards larger aperture sizes,\nextremely high frequencies, and innovative antenna types. This evolution is\nfostering the emergence of near-field communications (NFC) in future wireless\nsystems. Considerable attention has been directed towards this cutting-edge\ntechnology due to its potential to enhance the capacity of wireless networks by\nintroducing increased spatial degrees of freedom (DoFs) in the range domain.\nWithin this context, a comprehensive review of the state of the art on NFC is\npresented, with a specific focus on its 1) fundamental operating principles, 2)\nchannel modeling, 3) performance analysis, 4) signal processing techniques, and\n5) integration with other emerging applications. Specifically, 1) the basic\nprinciples of NFC are characterized from both physics and communications\nperspectives, unveiling its unique properties in contrast to far-field\ncommunications. 2) Building on these principles, deterministic and stochastic\nnear-field channel models are explored for spatially-discrete (SPD) and\ncontinuous-aperture (CAP) arrays. 3) Based on these models, existing\ncontributions to near-field performance analysis are reviewed in terms of\nDoFs/effective DoFs (EDoFs), the power scaling law, and transmission rate. 4)\nExisting signal processing techniques for NFC are systematically surveyed,\nwhich include channel estimation, beamforming design, and low-complexity beam\ntraining. 5) Major issues and research opportunities in incorporating\nnear-field models into other promising technologies are identified to advance\nNFC's deployment in next-generation networks. Throughout this paper, promising\ndirections are highlighted to inspire future research endeavors in the realm of\nNFC, underscoring its significance in the advancement of wireless communication\ntechnologies."
                },
                "authors": [
                    {
                        "name": "Yuanwei Liu"
                    },
                    {
                        "name": "Chongjun Ouyang"
                    },
                    {
                        "name": "Zhaolin Wang"
                    },
                    {
                        "name": "Jiaqi Xu"
                    },
                    {
                        "name": "Xidong Mu"
                    },
                    {
                        "name": "A. Lee Swindlehurst"
                    }
                ],
                "author_detail": {
                    "name": "A. Lee Swindlehurst"
                },
                "author": "A. Lee Swindlehurst",
                "arxiv_comment": "41 pages; to appear in IEEE Communications Surveys and Tutorials",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.05900v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.05900v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.08492v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.08492v2",
                "updated": "2024-10-03T16:04:36Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    16,
                    4,
                    36,
                    3,
                    277,
                    0
                ],
                "published": "2024-04-12T14:20:57Z",
                "published_parsed": [
                    2024,
                    4,
                    12,
                    14,
                    20,
                    57,
                    4,
                    103,
                    0
                ],
                "title": "Strategic Interactions between Large Language Models-based Agents in\n  Beauty Contests",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strategic Interactions between Large Language Models-based Agents in\n  Beauty Contests"
                },
                "summary": "The growing adoption of large language models (LLMs) presents potential for\ndeeper understanding of human behaviours within game theory frameworks.\nAddressing research gap on multi-player competitive games, this paper examines\nthe strategic interactions among multiple types of LLM-based agents in a\nclassical beauty contest game. LLM-based agents demonstrate varying depth of\nreasoning that fall within a range of level-0 to 1, which are lower than\nexperimental results conducted with human subjects, but they do display similar\nconvergence pattern towards Nash Equilibrium (NE) choice in repeated setting.\nFurther, through variation in group composition of agent types, I found\nenvironment with lower strategic uncertainty enhances convergence for LLM-based\nagents, and having a mixed environment comprises of LLM-based agents of\ndiffering strategic levels accelerates convergence for all. Higher average\npayoffs for the more intelligent agents are usually observed, albeit at the\nexpense of less intelligent agents. The results from game play with simulated\nagents not only convey insights on potential human behaviours under specified\nexperimental set-ups, they also offer valuable understanding of strategic\ninteractions among algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing adoption of large language models (LLMs) presents potential for\ndeeper understanding of human behaviours within game theory frameworks.\nAddressing research gap on multi-player competitive games, this paper examines\nthe strategic interactions among multiple types of LLM-based agents in a\nclassical beauty contest game. LLM-based agents demonstrate varying depth of\nreasoning that fall within a range of level-0 to 1, which are lower than\nexperimental results conducted with human subjects, but they do display similar\nconvergence pattern towards Nash Equilibrium (NE) choice in repeated setting.\nFurther, through variation in group composition of agent types, I found\nenvironment with lower strategic uncertainty enhances convergence for LLM-based\nagents, and having a mixed environment comprises of LLM-based agents of\ndiffering strategic levels accelerates convergence for all. Higher average\npayoffs for the more intelligent agents are usually observed, albeit at the\nexpense of less intelligent agents. The results from game play with simulated\nagents not only convey insights on potential human behaviours under specified\nexperimental set-ups, they also offer valuable understanding of strategic\ninteractions among algorithms."
                },
                "authors": [
                    {
                        "name": "Siting Estee Lu"
                    }
                ],
                "author_detail": {
                    "name": "Siting Estee Lu"
                },
                "author": "Siting Estee Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.08492v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.08492v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.13681v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.13681v2",
                "updated": "2024-10-03T16:01:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    16,
                    1,
                    1,
                    3,
                    277,
                    0
                ],
                "published": "2024-03-20T15:39:54Z",
                "published_parsed": [
                    2024,
                    3,
                    20,
                    15,
                    39,
                    54,
                    2,
                    80,
                    0
                ],
                "title": "PARAMANU-AYN: Pretrain from scratch or Continual Pretraining of LLMs for\n  Legal Domain Adaptation?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PARAMANU-AYN: Pretrain from scratch or Continual Pretraining of LLMs for\n  Legal Domain Adaptation?"
                },
                "summary": "In this paper, we present Paramanu-Ayn, a collection of legal language models\ntrained exclusively on Indian legal case documents. This 97-million-parameter\nAuto-Regressive (AR) decoder-only model was pretrained from scratch with a\ncontext size of 8192 on a single GPU for just 185 hours, achieving an efficient\nMFU of 41.35. We also developed a legal domain specialized BPE tokenizer. We\nevaluated our model using perplexity and zero-shot tasks: case judgment\nprediction with explanation and abstractive case summarization. Paramanu-Ayn\noutperformed Llama-2 7B and Gemini-Pro in case judgment prediction with\nexplanation task on test accuracy by nearly 2 percentage points, despite being\n72 times smaller. In zero-shot abstractive summarization, it surpassed\ndecoder-only LLMs generating fixed-length summaries (5000 tokens) by over 10\npercentage points in BLEU and METEOR metrics, and by nearly 4 percentage points\nin BERTScore. Further evaluations on zero-shot commonsense and mathematical\nbenchmarks showed that Paramanu-Ayn excelled despite being trained exclusively\non legal documents, outperforming Llama-1, Llama-2, and Falcon on\nAGIEVAL-AQuA-RAT and AGIEVAL-SAT-Math tasks. We also instruction-tuned our\nmodel on 10,763 diverse legal tasks, including legal clause generation, legal\ndrafting, case summarization, etc. The Paramanu-Ayn-instruct model scored above\n8 out of 10 in clarity, relevance, completeness, and legal reasoning metrics by\nGPT-3.5-Turbo. We found that our models, were able to learn drafting knowledge\nand generalize to draft legal contracts and legal clauses with limited\ninstruction-tuning. Hence, we conclude that for a strong domain-specialized\ngenerative language model (such as legal), domain specialized pretraining from\nscratch is more cost effective, environmentally friendly, and remains\ncompetitive with larger models or even better than adapting LLMs for legal\ndomain tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present Paramanu-Ayn, a collection of legal language models\ntrained exclusively on Indian legal case documents. This 97-million-parameter\nAuto-Regressive (AR) decoder-only model was pretrained from scratch with a\ncontext size of 8192 on a single GPU for just 185 hours, achieving an efficient\nMFU of 41.35. We also developed a legal domain specialized BPE tokenizer. We\nevaluated our model using perplexity and zero-shot tasks: case judgment\nprediction with explanation and abstractive case summarization. Paramanu-Ayn\noutperformed Llama-2 7B and Gemini-Pro in case judgment prediction with\nexplanation task on test accuracy by nearly 2 percentage points, despite being\n72 times smaller. In zero-shot abstractive summarization, it surpassed\ndecoder-only LLMs generating fixed-length summaries (5000 tokens) by over 10\npercentage points in BLEU and METEOR metrics, and by nearly 4 percentage points\nin BERTScore. Further evaluations on zero-shot commonsense and mathematical\nbenchmarks showed that Paramanu-Ayn excelled despite being trained exclusively\non legal documents, outperforming Llama-1, Llama-2, and Falcon on\nAGIEVAL-AQuA-RAT and AGIEVAL-SAT-Math tasks. We also instruction-tuned our\nmodel on 10,763 diverse legal tasks, including legal clause generation, legal\ndrafting, case summarization, etc. The Paramanu-Ayn-instruct model scored above\n8 out of 10 in clarity, relevance, completeness, and legal reasoning metrics by\nGPT-3.5-Turbo. We found that our models, were able to learn drafting knowledge\nand generalize to draft legal contracts and legal clauses with limited\ninstruction-tuning. Hence, we conclude that for a strong domain-specialized\ngenerative language model (such as legal), domain specialized pretraining from\nscratch is more cost effective, environmentally friendly, and remains\ncompetitive with larger models or even better than adapting LLMs for legal\ndomain tasks."
                },
                "authors": [
                    {
                        "name": "Mitodru Niyogi"
                    },
                    {
                        "name": "Arnab Bhattacharya"
                    }
                ],
                "author_detail": {
                    "name": "Arnab Bhattacharya"
                },
                "author": "Arnab Bhattacharya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.13681v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.13681v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05197v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05197v2",
                "updated": "2024-10-03T15:55:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    55,
                    40,
                    3,
                    277,
                    0
                ],
                "published": "2024-09-08T19:22:58Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    19,
                    22,
                    58,
                    6,
                    252,
                    0
                ],
                "title": "Seemingly Plausible Distractors in Multi-Hop Reasoning: Are Large\n  Language Models Attentive Readers?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seemingly Plausible Distractors in Multi-Hop Reasoning: Are Large\n  Language Models Attentive Readers?"
                },
                "summary": "State-of-the-art Large Language Models (LLMs) are accredited with an\nincreasing number of different capabilities, ranging from reading\ncomprehension, over advanced mathematical and reasoning skills to possessing\nscientific knowledge. In this paper we focus on their multi-hop reasoning\ncapability: the ability to identify and integrate information from multiple\ntextual sources.\n  Given the concerns with the presence of simplifying cues in existing\nmulti-hop reasoning benchmarks, which allow models to circumvent the reasoning\nrequirement, we set out to investigate, whether LLMs are prone to exploiting\nsuch simplifying cues. We find evidence that they indeed circumvent the\nrequirement to perform multi-hop reasoning, but they do so in more subtle ways\nthan what was reported about their fine-tuned pre-trained language model (PLM)\npredecessors. Motivated by this finding, we propose a challenging multi-hop\nreasoning benchmark, by generating seemingly plausible multi-hop reasoning\nchains, which ultimately lead to incorrect answers. We evaluate multiple open\nand proprietary state-of-the-art LLMs, and find that their performance to\nperform multi-hop reasoning is affected, as indicated by up to 45% relative\ndecrease in F1 score when presented with such seemingly plausible alternatives.\nWe conduct a deeper analysis and find evidence that while LLMs tend to ignore\nmisleading lexical cues, misleading reasoning paths indeed present a\nsignificant challenge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State-of-the-art Large Language Models (LLMs) are accredited with an\nincreasing number of different capabilities, ranging from reading\ncomprehension, over advanced mathematical and reasoning skills to possessing\nscientific knowledge. In this paper we focus on their multi-hop reasoning\ncapability: the ability to identify and integrate information from multiple\ntextual sources.\n  Given the concerns with the presence of simplifying cues in existing\nmulti-hop reasoning benchmarks, which allow models to circumvent the reasoning\nrequirement, we set out to investigate, whether LLMs are prone to exploiting\nsuch simplifying cues. We find evidence that they indeed circumvent the\nrequirement to perform multi-hop reasoning, but they do so in more subtle ways\nthan what was reported about their fine-tuned pre-trained language model (PLM)\npredecessors. Motivated by this finding, we propose a challenging multi-hop\nreasoning benchmark, by generating seemingly plausible multi-hop reasoning\nchains, which ultimately lead to incorrect answers. We evaluate multiple open\nand proprietary state-of-the-art LLMs, and find that their performance to\nperform multi-hop reasoning is affected, as indicated by up to 45% relative\ndecrease in F1 score when presented with such seemingly plausible alternatives.\nWe conduct a deeper analysis and find evidence that while LLMs tend to ignore\nmisleading lexical cues, misleading reasoning paths indeed present a\nsignificant challenge."
                },
                "authors": [
                    {
                        "name": "Neeladri Bhuiya"
                    },
                    {
                        "name": "Viktor Schlegel"
                    },
                    {
                        "name": "Stefan Winkler"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Winkler"
                },
                "author": "Stefan Winkler",
                "arxiv_comment": "15 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05197v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05197v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14662v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14662v2",
                "updated": "2024-10-03T15:52:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    52,
                    17,
                    3,
                    277,
                    0
                ],
                "published": "2024-06-20T18:30:09Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    18,
                    30,
                    9,
                    3,
                    172,
                    0
                ],
                "title": "Advantage Alignment Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advantage Alignment Algorithms"
                },
                "summary": "Artificially intelligent agents are increasingly being integrated into human\ndecision-making: from large language model (LLM) assistants to autonomous\nvehicles. These systems often optimize their individual objective, leading to\nconflicts, particularly in general-sum games where naive reinforcement learning\nagents empirically converge to Pareto-suboptimal Nash equilibria. To address\nthis issue, opponent shaping has emerged as a paradigm for finding socially\nbeneficial equilibria in general-sum games. In this work, we introduce\nAdvantage Alignment, a family of algorithms derived from first principles that\nperform opponent shaping efficiently and intuitively. We achieve this by\naligning the advantages of interacting agents, increasing the probability of\nmutually beneficial actions when their interaction has been positive. We prove\nthat existing opponent shaping methods implicitly perform Advantage Alignment.\nCompared to these methods, Advantage Alignment simplifies the mathematical\nformulation of opponent shaping, reduces the computational burden and extends\nto continuous action domains. We demonstrate the effectiveness of our\nalgorithms across a range of social dilemmas, achieving state-of-the-art\ncooperation and robustness against exploitation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificially intelligent agents are increasingly being integrated into human\ndecision-making: from large language model (LLM) assistants to autonomous\nvehicles. These systems often optimize their individual objective, leading to\nconflicts, particularly in general-sum games where naive reinforcement learning\nagents empirically converge to Pareto-suboptimal Nash equilibria. To address\nthis issue, opponent shaping has emerged as a paradigm for finding socially\nbeneficial equilibria in general-sum games. In this work, we introduce\nAdvantage Alignment, a family of algorithms derived from first principles that\nperform opponent shaping efficiently and intuitively. We achieve this by\naligning the advantages of interacting agents, increasing the probability of\nmutually beneficial actions when their interaction has been positive. We prove\nthat existing opponent shaping methods implicitly perform Advantage Alignment.\nCompared to these methods, Advantage Alignment simplifies the mathematical\nformulation of opponent shaping, reduces the computational burden and extends\nto continuous action domains. We demonstrate the effectiveness of our\nalgorithms across a range of social dilemmas, achieving state-of-the-art\ncooperation and robustness against exploitation."
                },
                "authors": [
                    {
                        "name": "Juan Agustin Duque"
                    },
                    {
                        "name": "Milad Aghajohari"
                    },
                    {
                        "name": "Tim Cooijmans"
                    },
                    {
                        "name": "Razvan Ciuca"
                    },
                    {
                        "name": "Tianyu Zhang"
                    },
                    {
                        "name": "Gauthier Gidel"
                    },
                    {
                        "name": "Aaron Courville"
                    }
                ],
                "author_detail": {
                    "name": "Aaron Courville"
                },
                "author": "Aaron Courville",
                "arxiv_comment": "25 Pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14662v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14662v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.05662v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.05662v4",
                "updated": "2024-10-03T15:50:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    50,
                    48,
                    3,
                    277,
                    0
                ],
                "published": "2024-04-08T16:46:25Z",
                "published_parsed": [
                    2024,
                    4,
                    8,
                    16,
                    46,
                    25,
                    0,
                    99,
                    0
                ],
                "title": "BinaryDM: Accurate Weight Binarization for Efficient Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BinaryDM: Accurate Weight Binarization for Efficient Diffusion Models"
                },
                "summary": "With the advancement of diffusion models (DMs) and the substantially\nincreased computational requirements, quantization emerges as a practical\nsolution to obtain compact and efficient low-bit DMs. However, the highly\ndiscrete representation leads to severe accuracy degradation, hindering the\nquantization of diffusion models to ultra-low bit-widths. This paper proposes a\nnovel weight binarization approach for DMs, namely BinaryDM, pushing binarized\nDMs to be accurate and efficient by improving the representation and\noptimization. From the representation perspective, we present an\nEvolvable-Basis Binarizer (EBB) to enable a smooth evolution of DMs from\nfull-precision to accurately binarized. EBB enhances information representation\nin the initial stage through the flexible combination of multiple binary bases\nand applies regularization to evolve into efficient single-basis binarization.\nThe evolution only occurs in the head and tail of the DM architecture to retain\nthe stability of training. From the optimization perspective, a Low-rank\nRepresentation Mimicking (LRM) is applied to assist the optimization of\nbinarized DMs. The LRM mimics the representations of full-precision DMs in\nlow-rank space, alleviating the direction ambiguity of the optimization process\ncaused by fine-grained alignment. Comprehensive experiments demonstrate that\nBinaryDM achieves significant accuracy and efficiency gains compared to SOTA\nquantization methods of DMs under ultra-low bit-widths. With 1-bit weight and\n4-bit activation (W1A4), BinaryDM achieves as low as 7.74 FID and saves the\nperformance from collapse (baseline FID 10.87). As the first binarization\nmethod for diffusion models, W1A4 BinaryDM achieves impressive 15.2x OPs and\n29.2x model size savings, showcasing its substantial potential for edge\ndeployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advancement of diffusion models (DMs) and the substantially\nincreased computational requirements, quantization emerges as a practical\nsolution to obtain compact and efficient low-bit DMs. However, the highly\ndiscrete representation leads to severe accuracy degradation, hindering the\nquantization of diffusion models to ultra-low bit-widths. This paper proposes a\nnovel weight binarization approach for DMs, namely BinaryDM, pushing binarized\nDMs to be accurate and efficient by improving the representation and\noptimization. From the representation perspective, we present an\nEvolvable-Basis Binarizer (EBB) to enable a smooth evolution of DMs from\nfull-precision to accurately binarized. EBB enhances information representation\nin the initial stage through the flexible combination of multiple binary bases\nand applies regularization to evolve into efficient single-basis binarization.\nThe evolution only occurs in the head and tail of the DM architecture to retain\nthe stability of training. From the optimization perspective, a Low-rank\nRepresentation Mimicking (LRM) is applied to assist the optimization of\nbinarized DMs. The LRM mimics the representations of full-precision DMs in\nlow-rank space, alleviating the direction ambiguity of the optimization process\ncaused by fine-grained alignment. Comprehensive experiments demonstrate that\nBinaryDM achieves significant accuracy and efficiency gains compared to SOTA\nquantization methods of DMs under ultra-low bit-widths. With 1-bit weight and\n4-bit activation (W1A4), BinaryDM achieves as low as 7.74 FID and saves the\nperformance from collapse (baseline FID 10.87). As the first binarization\nmethod for diffusion models, W1A4 BinaryDM achieves impressive 15.2x OPs and\n29.2x model size savings, showcasing its substantial potential for edge\ndeployment."
                },
                "authors": [
                    {
                        "name": "Xingyu Zheng"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Haotong Qin"
                    },
                    {
                        "name": "Xudong Ma"
                    },
                    {
                        "name": "Mingyuan Zhang"
                    },
                    {
                        "name": "Haojie Hao"
                    },
                    {
                        "name": "Jiakai Wang"
                    },
                    {
                        "name": "Zixiang Zhao"
                    },
                    {
                        "name": "Jinyang Guo"
                    },
                    {
                        "name": "Michele Magno"
                    }
                ],
                "author_detail": {
                    "name": "Michele Magno"
                },
                "author": "Michele Magno",
                "arxiv_comment": "The code is available at https://github.com/Xingyu-Zheng/BinaryDM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.05662v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.05662v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10960v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10960v3",
                "updated": "2024-10-03T15:48:45Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    48,
                    45,
                    3,
                    277,
                    0
                ],
                "published": "2024-07-15T17:55:42Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    17,
                    55,
                    42,
                    0,
                    197,
                    0
                ],
                "title": "Fast Matrix Multiplications for Lookup Table-Quantized LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Matrix Multiplications for Lookup Table-Quantized LLMs"
                },
                "summary": "The deployment of large language models (LLMs) is often constrained by memory\nbandwidth, where the primary bottleneck is the cost of transferring model\nparameters from the GPU's global memory to its registers. When coupled with\ncustom kernels that fuse the dequantization and matmul operations, weight-only\nquantization can thus enable faster inference by reducing the amount of memory\nmovement. However, developing high-performance kernels for weight-quantized\nLLMs presents substantial challenges, especially when the weights are\ncompressed to non-evenly-divisible bit widths (e.g., 3 bits) with non-uniform,\nlookup table (LUT) quantization. This paper describes FLUTE, a flexible lookup\ntable engine for LUT-quantized LLMs, which uses offline restructuring of the\nquantized weight matrix to minimize bit manipulations associated with\nunpacking, and vectorization and duplication of the lookup table to mitigate\nshared memory bandwidth constraints. At batch sizes < 32 and quantization group\nsize of 128 (typical in LLM inference), the FLUTE kernel can be 2-4x faster\nthan existing GEMM kernels. As an application of FLUTE, we explore a simple\nextension to lookup table-based NormalFloat quantization and apply it to\nquantize LLaMA3 to various configurations, obtaining competitive quantization\nperformance against strong baselines while obtaining an end-to-end throughput\nincrease of 1.5 to 2 times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of large language models (LLMs) is often constrained by memory\nbandwidth, where the primary bottleneck is the cost of transferring model\nparameters from the GPU's global memory to its registers. When coupled with\ncustom kernels that fuse the dequantization and matmul operations, weight-only\nquantization can thus enable faster inference by reducing the amount of memory\nmovement. However, developing high-performance kernels for weight-quantized\nLLMs presents substantial challenges, especially when the weights are\ncompressed to non-evenly-divisible bit widths (e.g., 3 bits) with non-uniform,\nlookup table (LUT) quantization. This paper describes FLUTE, a flexible lookup\ntable engine for LUT-quantized LLMs, which uses offline restructuring of the\nquantized weight matrix to minimize bit manipulations associated with\nunpacking, and vectorization and duplication of the lookup table to mitigate\nshared memory bandwidth constraints. At batch sizes < 32 and quantization group\nsize of 128 (typical in LLM inference), the FLUTE kernel can be 2-4x faster\nthan existing GEMM kernels. As an application of FLUTE, we explore a simple\nextension to lookup table-based NormalFloat quantization and apply it to\nquantize LLaMA3 to various configurations, obtaining competitive quantization\nperformance against strong baselines while obtaining an end-to-end throughput\nincrease of 1.5 to 2 times."
                },
                "authors": [
                    {
                        "name": "Han Guo"
                    },
                    {
                        "name": "William Brandon"
                    },
                    {
                        "name": "Radostin Cholakov"
                    },
                    {
                        "name": "Jonathan Ragan-Kelley"
                    },
                    {
                        "name": "Eric P. Xing"
                    },
                    {
                        "name": "Yoon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Yoon Kim"
                },
                "author": "Yoon Kim",
                "arxiv_comment": "EMNLP 2024 (Findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10960v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10960v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18256v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18256v3",
                "updated": "2024-10-03T15:48:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    48,
                    31,
                    3,
                    277,
                    0
                ],
                "published": "2024-06-26T11:08:17Z",
                "published_parsed": [
                    2024,
                    6,
                    26,
                    11,
                    8,
                    17,
                    2,
                    178,
                    0
                ],
                "title": "Llamipa: An Incremental Discourse Parser",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Llamipa: An Incremental Discourse Parser"
                },
                "summary": "This paper provides the first discourse parsing experiments with a large\nlanguage model(LLM) finetuned on corpora annotated in the style of SDRT\n(Segmented Discourse Representation Theory Asher, 1993; Asher and Lascarides,\n2003). The result is a discourse parser, Llamipa (Llama Incremental Parser),\nthat leverages discourse context, leading to substantial performance gains over\napproaches that use encoder-only models to provide local, context-sensitive\nrepresentations of discourse units. Furthermore, it can process discourse data\nincrementally, which is essential for the eventual use of discourse information\nin downstream tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides the first discourse parsing experiments with a large\nlanguage model(LLM) finetuned on corpora annotated in the style of SDRT\n(Segmented Discourse Representation Theory Asher, 1993; Asher and Lascarides,\n2003). The result is a discourse parser, Llamipa (Llama Incremental Parser),\nthat leverages discourse context, leading to substantial performance gains over\napproaches that use encoder-only models to provide local, context-sensitive\nrepresentations of discourse units. Furthermore, it can process discourse data\nincrementally, which is essential for the eventual use of discourse information\nin downstream tasks."
                },
                "authors": [
                    {
                        "name": "Kate Thompson"
                    },
                    {
                        "name": "Akshay Chaturvedi"
                    },
                    {
                        "name": "Julie Hunter"
                    },
                    {
                        "name": "Nicholas Asher"
                    }
                ],
                "author_detail": {
                    "name": "Nicholas Asher"
                },
                "author": "Nicholas Asher",
                "arxiv_comment": "EMNLP 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18256v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18256v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18164v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18164v3",
                "updated": "2024-10-03T15:46:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    46,
                    16,
                    3,
                    277,
                    0
                ],
                "published": "2024-06-26T08:24:44Z",
                "published_parsed": [
                    2024,
                    6,
                    26,
                    8,
                    24,
                    44,
                    2,
                    178,
                    0
                ],
                "title": "Nebula: A discourse aware Minecraft Builder",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nebula: A discourse aware Minecraft Builder"
                },
                "summary": "When engaging in collaborative tasks, humans efficiently exploit the semantic\nstructure of a conversation to optimize verbal and nonverbal interactions. But\nin recent \"language to code\" or \"language to action\" models, this information\nis lacking. We show how incorporating the prior discourse and nonlinguistic\ncontext of a conversation situated in a nonlinguistic environment can improve\nthe \"language to action\" component of such interactions. We finetune an LLM to\npredict actions based on prior context; our model, Nebula, doubles the\nnet-action F1 score over the baseline on this task of Jayannavar et al.(2020).\nWe also investigate our model's ability to construct shapes and understand\nlocation descriptions using a synthetic dataset",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When engaging in collaborative tasks, humans efficiently exploit the semantic\nstructure of a conversation to optimize verbal and nonverbal interactions. But\nin recent \"language to code\" or \"language to action\" models, this information\nis lacking. We show how incorporating the prior discourse and nonlinguistic\ncontext of a conversation situated in a nonlinguistic environment can improve\nthe \"language to action\" component of such interactions. We finetune an LLM to\npredict actions based on prior context; our model, Nebula, doubles the\nnet-action F1 score over the baseline on this task of Jayannavar et al.(2020).\nWe also investigate our model's ability to construct shapes and understand\nlocation descriptions using a synthetic dataset"
                },
                "authors": [
                    {
                        "name": "Akshay Chaturvedi"
                    },
                    {
                        "name": "Kate Thompson"
                    },
                    {
                        "name": "Nicholas Asher"
                    }
                ],
                "author_detail": {
                    "name": "Nicholas Asher"
                },
                "author": "Nicholas Asher",
                "arxiv_comment": "EMNLP 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18164v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18164v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.08460v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.08460v3",
                "updated": "2024-10-03T15:46:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    46,
                    13,
                    3,
                    277,
                    0
                ],
                "published": "2023-04-17T17:36:35Z",
                "published_parsed": [
                    2023,
                    4,
                    17,
                    17,
                    36,
                    35,
                    0,
                    107,
                    0
                ],
                "title": "LongForm: Effective Instruction Tuning with Reverse Instructions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongForm: Effective Instruction Tuning with Reverse Instructions"
                },
                "summary": "Instruction tuning enables language models to more effectively generalize and\nbetter follow user intent. However, obtaining instruction data is costly and\nchallenging. Prior work employs methods such as expensive human annotation,\ncrowd-sourced datasets with alignment issues, and generating noisy examples via\nLLMs. We introduce the LongForm-C dataset, which is created by reverse\ninstructions. We generate instructions via LLMs for human-written corpus\nexamples using reverse instructions. First we select a diverse set of\nhuman-written documents from corpora such as C4 and Wikipedia; then we generate\ninstructions for these documents via LLMs. This approach provides a cheaper and\ncleaner instruction-tuning dataset with natural output and one suitable for\nlong text generation. Our models outperform 10x larger language models without\ninstruction tuning on tasks such as story/recipe generation and long-form\nquestion answering. Moreover, LongForm models outperform prior\ninstruction-tuned models such as FLAN-T5 and Alpaca by a large margin, and\nimprove language understanding capabilities further. We publicly release our\ndata and models: https://github.com/akoksal/LongForm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction tuning enables language models to more effectively generalize and\nbetter follow user intent. However, obtaining instruction data is costly and\nchallenging. Prior work employs methods such as expensive human annotation,\ncrowd-sourced datasets with alignment issues, and generating noisy examples via\nLLMs. We introduce the LongForm-C dataset, which is created by reverse\ninstructions. We generate instructions via LLMs for human-written corpus\nexamples using reverse instructions. First we select a diverse set of\nhuman-written documents from corpora such as C4 and Wikipedia; then we generate\ninstructions for these documents via LLMs. This approach provides a cheaper and\ncleaner instruction-tuning dataset with natural output and one suitable for\nlong text generation. Our models outperform 10x larger language models without\ninstruction tuning on tasks such as story/recipe generation and long-form\nquestion answering. Moreover, LongForm models outperform prior\ninstruction-tuned models such as FLAN-T5 and Alpaca by a large margin, and\nimprove language understanding capabilities further. We publicly release our\ndata and models: https://github.com/akoksal/LongForm."
                },
                "authors": [
                    {
                        "name": "Abdullatif Kksal"
                    },
                    {
                        "name": "Timo Schick"
                    },
                    {
                        "name": "Anna Korhonen"
                    },
                    {
                        "name": "Hinrich Schtze"
                    }
                ],
                "author_detail": {
                    "name": "Hinrich Schtze"
                },
                "author": "Hinrich Schtze",
                "arxiv_comment": "EMNLP 2024 Findings. This version extends the training with recent\n  LLMs, evaluation with new metrics, and NLU tasks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.08460v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.08460v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12402v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12402v2",
                "updated": "2024-10-03T15:45:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    45,
                    52,
                    3,
                    277,
                    0
                ],
                "published": "2024-07-17T08:28:55Z",
                "published_parsed": [
                    2024,
                    7,
                    17,
                    8,
                    28,
                    55,
                    2,
                    199,
                    0
                ],
                "title": "TurkishMMLU: Measuring Massive Multitask Language Understanding in\n  Turkish",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TurkishMMLU: Measuring Massive Multitask Language Understanding in\n  Turkish"
                },
                "summary": "Multiple choice question answering tasks evaluate the reasoning,\ncomprehension, and mathematical abilities of Large Language Models (LLMs).\nWhile existing benchmarks employ automatic translation for multilingual\nevaluation, this approach is error-prone and potentially introduces culturally\nbiased questions, especially in social sciences. We introduce the first\nmultitask, multiple-choice Turkish QA benchmark, TurkishMMLU, to evaluate LLMs'\nunderstanding of the Turkish language. TurkishMMLU includes over 10,000\nquestions, covering 9 different subjects from Turkish high-school education\ncurricula. These questions are written by curriculum experts, suitable for the\nhigh-school curricula in Turkey, covering subjects ranging from natural\nsciences and math questions to more culturally representative topics such as\nTurkish Literature and the history of the Turkish Republic. We evaluate over 20\nLLMs, including multilingual open-source (e.g., Gemma, Llama, MT5),\nclosed-source (GPT 4o, Claude, Gemini), and Turkish-adapted (e.g., Trendyol)\nmodels. We provide an extensive evaluation, including zero-shot and few-shot\nevaluation of LLMs, chain-of-thought reasoning, and question difficulty\nanalysis along with model performance. We provide an in-depth analysis of the\nTurkish capabilities and limitations of current LLMs to provide insights for\nfuture LLMs for the Turkish language. We publicly release our code for the\ndataset and evaluation: https://github.com/ArdaYueksel/TurkishMMLU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiple choice question answering tasks evaluate the reasoning,\ncomprehension, and mathematical abilities of Large Language Models (LLMs).\nWhile existing benchmarks employ automatic translation for multilingual\nevaluation, this approach is error-prone and potentially introduces culturally\nbiased questions, especially in social sciences. We introduce the first\nmultitask, multiple-choice Turkish QA benchmark, TurkishMMLU, to evaluate LLMs'\nunderstanding of the Turkish language. TurkishMMLU includes over 10,000\nquestions, covering 9 different subjects from Turkish high-school education\ncurricula. These questions are written by curriculum experts, suitable for the\nhigh-school curricula in Turkey, covering subjects ranging from natural\nsciences and math questions to more culturally representative topics such as\nTurkish Literature and the history of the Turkish Republic. We evaluate over 20\nLLMs, including multilingual open-source (e.g., Gemma, Llama, MT5),\nclosed-source (GPT 4o, Claude, Gemini), and Turkish-adapted (e.g., Trendyol)\nmodels. We provide an extensive evaluation, including zero-shot and few-shot\nevaluation of LLMs, chain-of-thought reasoning, and question difficulty\nanalysis along with model performance. We provide an in-depth analysis of the\nTurkish capabilities and limitations of current LLMs to provide insights for\nfuture LLMs for the Turkish language. We publicly release our code for the\ndataset and evaluation: https://github.com/ArdaYueksel/TurkishMMLU."
                },
                "authors": [
                    {
                        "name": "Arda Yksel"
                    },
                    {
                        "name": "Abdullatif Kksal"
                    },
                    {
                        "name": "Ltfi Kerem enel"
                    },
                    {
                        "name": "Anna Korhonen"
                    },
                    {
                        "name": "Hinrich Schtze"
                    }
                ],
                "author_detail": {
                    "name": "Hinrich Schtze"
                },
                "author": "Hinrich Schtze",
                "arxiv_comment": "EMNLP 2024 - Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12402v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12402v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.14741v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.14741v2",
                "updated": "2024-10-03T15:44:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    44,
                    59,
                    3,
                    277,
                    0
                ],
                "published": "2024-04-23T04:47:22Z",
                "published_parsed": [
                    2024,
                    4,
                    23,
                    4,
                    47,
                    22,
                    1,
                    114,
                    0
                ],
                "title": "Generate-on-Graph: Treat LLM as both Agent and KG in Incomplete\n  Knowledge Graph Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generate-on-Graph: Treat LLM as both Agent and KG in Incomplete\n  Knowledge Graph Question Answering"
                },
                "summary": "To address the issues of insufficient knowledge and hallucination in Large\nLanguage Models (LLMs), numerous studies have explored integrating LLMs with\nKnowledge Graphs (KGs). However, these methods are typically evaluated on\nconventional Knowledge Graph Question Answering (KGQA) with complete KGs, where\nall factual triples required for each question are entirely covered by the\ngiven KG. In such cases, LLMs primarily act as an agent to find answer entities\nwithin the KG, rather than effectively integrating the internal knowledge of\nLLMs and external knowledge sources such as KGs. In fact, KGs are often\nincomplete to cover all the knowledge required to answer questions. To simulate\nthese real-world scenarios and evaluate the ability of LLMs to integrate\ninternal and external knowledge, we propose leveraging LLMs for QA under\nIncomplete Knowledge Graph (IKGQA), where the provided KG lacks some of the\nfactual triples for each question, and construct corresponding datasets. To\nhandle IKGQA, we propose a training-free method called Generate-on-Graph (GoG),\nwhich can generate new factual triples while exploring KGs. Specifically, GoG\nperforms reasoning through a Thinking-Searching-Generating framework, which\ntreats LLM as both Agent and KG in IKGQA. Experimental results on two datasets\ndemonstrate that our GoG outperforms all previous methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To address the issues of insufficient knowledge and hallucination in Large\nLanguage Models (LLMs), numerous studies have explored integrating LLMs with\nKnowledge Graphs (KGs). However, these methods are typically evaluated on\nconventional Knowledge Graph Question Answering (KGQA) with complete KGs, where\nall factual triples required for each question are entirely covered by the\ngiven KG. In such cases, LLMs primarily act as an agent to find answer entities\nwithin the KG, rather than effectively integrating the internal knowledge of\nLLMs and external knowledge sources such as KGs. In fact, KGs are often\nincomplete to cover all the knowledge required to answer questions. To simulate\nthese real-world scenarios and evaluate the ability of LLMs to integrate\ninternal and external knowledge, we propose leveraging LLMs for QA under\nIncomplete Knowledge Graph (IKGQA), where the provided KG lacks some of the\nfactual triples for each question, and construct corresponding datasets. To\nhandle IKGQA, we propose a training-free method called Generate-on-Graph (GoG),\nwhich can generate new factual triples while exploring KGs. Specifically, GoG\nperforms reasoning through a Thinking-Searching-Generating framework, which\ntreats LLM as both Agent and KG in IKGQA. Experimental results on two datasets\ndemonstrate that our GoG outperforms all previous methods."
                },
                "authors": [
                    {
                        "name": "Yao Xu"
                    },
                    {
                        "name": "Shizhu He"
                    },
                    {
                        "name": "Jiabei Chen"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Yangqiu Song"
                    },
                    {
                        "name": "Hanghang Tong"
                    },
                    {
                        "name": "Guang Liu"
                    },
                    {
                        "name": "Kang Liu"
                    },
                    {
                        "name": "Jun Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhao"
                },
                "author": "Jun Zhao",
                "arxiv_comment": "Accepted by EMNLP 2024 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.14741v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.14741v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02603v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02603v1",
                "updated": "2024-10-03T15:44:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    44,
                    42,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T15:44:42Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    44,
                    42,
                    3,
                    277,
                    0
                ],
                "title": "Agents' Room: Narrative Generation through Multi-step Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agents' Room: Narrative Generation through Multi-step Collaboration"
                },
                "summary": "Writing compelling fiction is a multifaceted process combining elements such\nas crafting a plot, developing interesting characters, and using evocative\nlanguage. While large language models (LLMs) show promise for story writing,\nthey currently rely heavily on intricate prompting, which limits their use. We\npropose Agents' Room, a generation framework inspired by narrative theory, that\ndecomposes narrative writing into subtasks tackled by specialized agents. To\nillustrate our method, we introduce Tell Me A Story, a high-quality dataset of\ncomplex writing prompts and human-written stories, and a novel evaluation\nframework designed specifically for assessing long narratives. We show that\nAgents' Room generates stories that are preferred by expert evaluators over\nthose produced by baseline systems by leveraging collaboration and\nspecialization to decompose the complex story writing task into tractable\ncomponents. We provide extensive analysis with automated and human-based\nmetrics of the generated output.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Writing compelling fiction is a multifaceted process combining elements such\nas crafting a plot, developing interesting characters, and using evocative\nlanguage. While large language models (LLMs) show promise for story writing,\nthey currently rely heavily on intricate prompting, which limits their use. We\npropose Agents' Room, a generation framework inspired by narrative theory, that\ndecomposes narrative writing into subtasks tackled by specialized agents. To\nillustrate our method, we introduce Tell Me A Story, a high-quality dataset of\ncomplex writing prompts and human-written stories, and a novel evaluation\nframework designed specifically for assessing long narratives. We show that\nAgents' Room generates stories that are preferred by expert evaluators over\nthose produced by baseline systems by leveraging collaboration and\nspecialization to decompose the complex story writing task into tractable\ncomponents. We provide extensive analysis with automated and human-based\nmetrics of the generated output."
                },
                "authors": [
                    {
                        "name": "Fantine Huot"
                    },
                    {
                        "name": "Reinald Kim Amplayo"
                    },
                    {
                        "name": "Jennimaria Palomaki"
                    },
                    {
                        "name": "Alice Shoshana Jakobovits"
                    },
                    {
                        "name": "Elizabeth Clark"
                    },
                    {
                        "name": "Mirella Lapata"
                    }
                ],
                "author_detail": {
                    "name": "Mirella Lapata"
                },
                "author": "Mirella Lapata",
                "arxiv_comment": "Under review as a conference paper at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02603v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02603v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01769v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01769v2",
                "updated": "2024-10-03T15:30:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    30,
                    12,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-02T17:25:37Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    17,
                    25,
                    37,
                    2,
                    276,
                    0
                ],
                "title": "Quantifying Generalization Complexity for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying Generalization Complexity for Large Language Models"
                },
                "summary": "While large language models (LLMs) have shown exceptional capabilities in\nunderstanding complex queries and performing sophisticated tasks, their\ngeneralization abilities are often deeply entangled with memorization,\nnecessitating more precise evaluation. To address this challenge, we introduce\nScylla, a dynamic evaluation framework that quantitatively measures the\ngeneralization abilities of LLMs. Scylla disentangles generalization from\nmemorization via assessing model performance on both in-distribution (ID) and\nout-of-distribution (OOD) data through 20 tasks across 5 levels of complexity.\nThrough extensive experiments, we uncover a non-monotonic relationship between\ntask complexity and the performance gap between ID and OOD data, which we term\nthe generalization valley. Specifically, this phenomenon reveals a critical\nthreshold - referred to as critical complexity - where reliance on\nnon-generalizable behavior peaks, indicating the upper bound of LLMs'\ngeneralization capabilities. As model size increases, the critical complexity\nshifts toward higher levels of task complexity, suggesting that larger models\ncan handle more complex reasoning tasks before over-relying on memorization.\nLeveraging Scylla and the concept of critical complexity, we benchmark 28LLMs\nincluding both open-sourced models such as LLaMA and Qwen families, and\nclose-sourced models like Claude and GPT, providing a more robust evaluation\nand establishing a clearer understanding of LLMs' generalization capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have shown exceptional capabilities in\nunderstanding complex queries and performing sophisticated tasks, their\ngeneralization abilities are often deeply entangled with memorization,\nnecessitating more precise evaluation. To address this challenge, we introduce\nScylla, a dynamic evaluation framework that quantitatively measures the\ngeneralization abilities of LLMs. Scylla disentangles generalization from\nmemorization via assessing model performance on both in-distribution (ID) and\nout-of-distribution (OOD) data through 20 tasks across 5 levels of complexity.\nThrough extensive experiments, we uncover a non-monotonic relationship between\ntask complexity and the performance gap between ID and OOD data, which we term\nthe generalization valley. Specifically, this phenomenon reveals a critical\nthreshold - referred to as critical complexity - where reliance on\nnon-generalizable behavior peaks, indicating the upper bound of LLMs'\ngeneralization capabilities. As model size increases, the critical complexity\nshifts toward higher levels of task complexity, suggesting that larger models\ncan handle more complex reasoning tasks before over-relying on memorization.\nLeveraging Scylla and the concept of critical complexity, we benchmark 28LLMs\nincluding both open-sourced models such as LLaMA and Qwen families, and\nclose-sourced models like Claude and GPT, providing a more robust evaluation\nand establishing a clearer understanding of LLMs' generalization capabilities."
                },
                "authors": [
                    {
                        "name": "Zhenting Qi"
                    },
                    {
                        "name": "Hongyin Luo"
                    },
                    {
                        "name": "Xuliang Huang"
                    },
                    {
                        "name": "Zhuokai Zhao"
                    },
                    {
                        "name": "Yibo Jiang"
                    },
                    {
                        "name": "Xiangjun Fan"
                    },
                    {
                        "name": "Himabindu Lakkaraju"
                    },
                    {
                        "name": "James Glass"
                    }
                ],
                "author_detail": {
                    "name": "James Glass"
                },
                "author": "James Glass",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01769v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01769v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02584v1",
                "updated": "2024-10-03T15:28:05Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    28,
                    5,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T15:28:05Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    28,
                    5,
                    3,
                    277,
                    0
                ],
                "title": "Towards Implicit Bias Detection and Mitigation in Multi-Agent LLM\n  Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Implicit Bias Detection and Mitigation in Multi-Agent LLM\n  Interactions"
                },
                "summary": "As Large Language Models (LLMs) continue to evolve, they are increasingly\nbeing employed in numerous studies to simulate societies and execute diverse\nsocial tasks. However, LLMs are susceptible to societal biases due to their\nexposure to human-generated data. Given that LLMs are being used to gain\ninsights into various societal aspects, it is essential to mitigate these\nbiases. To that end, our study investigates the presence of implicit gender\nbiases in multi-agent LLM interactions and proposes two strategies to mitigate\nthese biases. We begin by creating a dataset of scenarios where implicit gender\nbiases might arise, and subsequently develop a metric to assess the presence of\nbiases. Our empirical analysis reveals that LLMs generate outputs characterized\nby strong implicit bias associations (>= 50\\% of the time). Furthermore, these\nbiases tend to escalate following multi-agent interactions. To mitigate them,\nwe propose two strategies: self-reflection with in-context examples (ICE); and\nsupervised fine-tuning. Our research demonstrates that both methods effectively\nmitigate implicit biases, with the ensemble of fine-tuning and self-reflection\nproving to be the most successful.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) continue to evolve, they are increasingly\nbeing employed in numerous studies to simulate societies and execute diverse\nsocial tasks. However, LLMs are susceptible to societal biases due to their\nexposure to human-generated data. Given that LLMs are being used to gain\ninsights into various societal aspects, it is essential to mitigate these\nbiases. To that end, our study investigates the presence of implicit gender\nbiases in multi-agent LLM interactions and proposes two strategies to mitigate\nthese biases. We begin by creating a dataset of scenarios where implicit gender\nbiases might arise, and subsequently develop a metric to assess the presence of\nbiases. Our empirical analysis reveals that LLMs generate outputs characterized\nby strong implicit bias associations (>= 50\\% of the time). Furthermore, these\nbiases tend to escalate following multi-agent interactions. To mitigate them,\nwe propose two strategies: self-reflection with in-context examples (ICE); and\nsupervised fine-tuning. Our research demonstrates that both methods effectively\nmitigate implicit biases, with the ensemble of fine-tuning and self-reflection\nproving to be the most successful."
                },
                "authors": [
                    {
                        "name": "Angana Borah"
                    },
                    {
                        "name": "Rada Mihalcea"
                    }
                ],
                "author_detail": {
                    "name": "Rada Mihalcea"
                },
                "author": "Rada Mihalcea",
                "arxiv_comment": "Accepted to EMNLP Findings 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.04484v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.04484v3",
                "updated": "2024-10-03T15:20:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    20,
                    17,
                    3,
                    277,
                    0
                ],
                "published": "2023-10-06T13:28:04Z",
                "published_parsed": [
                    2023,
                    10,
                    6,
                    13,
                    28,
                    4,
                    4,
                    279,
                    0
                ],
                "title": "Ada-Instruct: Adapting Instruction Generators for Complex Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ada-Instruct: Adapting Instruction Generators for Complex Reasoning"
                },
                "summary": "Instructions augmentation is a crucial step for unleashing the full potential\nof large language models (LLMs) in downstream tasks. Existing Self-Instruct\nmethods primarily simulate new instructions from a few initial instructions\nwith in-context learning. However, our study identifies a critical flaw in this\napproach: even with GPT4o, Self-Instruct cannot generate complex instructions\nof length $\\ge 100$, which is necessary in complex tasks such as code\ncompletion.\n  To address this issue, our key insight is that fine-tuning open source LLMs\nwith only ten examples can produce complex instructions that maintain\ndistributional consistency for complex reasoning tasks. We introduce\nAda-Instruct, an adaptive instruction generator developed through fine-tuning.\nWe empirically validated Ada-Instruct's efficacy across different applications.\nThe results highlight Ada-Instruct's capacity to generate long, intricate, and\ndistributionally consistent instructions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instructions augmentation is a crucial step for unleashing the full potential\nof large language models (LLMs) in downstream tasks. Existing Self-Instruct\nmethods primarily simulate new instructions from a few initial instructions\nwith in-context learning. However, our study identifies a critical flaw in this\napproach: even with GPT4o, Self-Instruct cannot generate complex instructions\nof length $\\ge 100$, which is necessary in complex tasks such as code\ncompletion.\n  To address this issue, our key insight is that fine-tuning open source LLMs\nwith only ten examples can produce complex instructions that maintain\ndistributional consistency for complex reasoning tasks. We introduce\nAda-Instruct, an adaptive instruction generator developed through fine-tuning.\nWe empirically validated Ada-Instruct's efficacy across different applications.\nThe results highlight Ada-Instruct's capacity to generate long, intricate, and\ndistributionally consistent instructions."
                },
                "authors": [
                    {
                        "name": "Wanyun Cui"
                    },
                    {
                        "name": "Qianle Wang"
                    }
                ],
                "author_detail": {
                    "name": "Qianle Wang"
                },
                "author": "Qianle Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.04484v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.04484v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.18952v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.18952v4",
                "updated": "2024-10-03T15:08:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    15,
                    8,
                    7,
                    3,
                    277,
                    0
                ],
                "published": "2023-05-27T16:05:00Z",
                "published_parsed": [
                    2023,
                    5,
                    27,
                    16,
                    5,
                    0,
                    5,
                    147,
                    0
                ],
                "title": "Exploring the Practicality of Generative Retrieval on Dynamic Corpora",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Practicality of Generative Retrieval on Dynamic Corpora"
                },
                "summary": "Benchmarking the performance of information retrieval (IR) is mostly\nconducted with a fixed set of documents (static corpora). However, in realistic\nscenarios, this is rarely the case and the documents to be retrieved are\nconstantly updated and added. In this paper, we focus on Generative Retrievals\n(GR), which apply autoregressive language models to IR problems, and explore\ntheir adaptability and robustness in dynamic scenarios. We also conduct an\nextensive evaluation of computational and memory efficiency, crucial factors\nfor real-world deployment of IR systems handling vast and ever-changing\ndocument collections. Our results on the StreamingQA benchmark demonstrate that\nGR is more adaptable to evolving knowledge (4 -- 11%), robust in learning\nknowledge with temporal information, and efficient in terms of inference FLOPs\n(x 2), indexing time (x 6), and storage footprint (x 4) compared to Dual\nEncoders (DE), which are commonly used in retrieval systems. Our paper\nhighlights the potential of GR for future use in practical IR systems within\ndynamic environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking the performance of information retrieval (IR) is mostly\nconducted with a fixed set of documents (static corpora). However, in realistic\nscenarios, this is rarely the case and the documents to be retrieved are\nconstantly updated and added. In this paper, we focus on Generative Retrievals\n(GR), which apply autoregressive language models to IR problems, and explore\ntheir adaptability and robustness in dynamic scenarios. We also conduct an\nextensive evaluation of computational and memory efficiency, crucial factors\nfor real-world deployment of IR systems handling vast and ever-changing\ndocument collections. Our results on the StreamingQA benchmark demonstrate that\nGR is more adaptable to evolving knowledge (4 -- 11%), robust in learning\nknowledge with temporal information, and efficient in terms of inference FLOPs\n(x 2), indexing time (x 6), and storage footprint (x 4) compared to Dual\nEncoders (DE), which are commonly used in retrieval systems. Our paper\nhighlights the potential of GR for future use in practical IR systems within\ndynamic environments."
                },
                "authors": [
                    {
                        "name": "Chaeeun Kim"
                    },
                    {
                        "name": "Soyoung Yoon"
                    },
                    {
                        "name": "Hyunji Lee"
                    },
                    {
                        "name": "Joel Jang"
                    },
                    {
                        "name": "Sohee Yang"
                    },
                    {
                        "name": "Minjoon Seo"
                    }
                ],
                "author_detail": {
                    "name": "Minjoon Seo"
                },
                "author": "Minjoon Seo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.18952v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.18952v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19700v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19700v2",
                "updated": "2024-10-03T14:56:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    56,
                    2,
                    3,
                    277,
                    0
                ],
                "published": "2024-09-29T13:16:37Z",
                "published_parsed": [
                    2024,
                    9,
                    29,
                    13,
                    16,
                    37,
                    6,
                    273,
                    0
                ],
                "title": "2D-TPE: Two-Dimensional Positional Encoding Enhances Table Understanding\n  for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "2D-TPE: Two-Dimensional Positional Encoding Enhances Table Understanding\n  for Large Language Models"
                },
                "summary": "Tables are ubiquitous across various domains for concisely representing\nstructured information. Empowering large language models (LLMs) to reason over\ntabular data represents an actively explored direction. However, since typical\nLLMs only support one-dimensional~(1D) inputs, existing methods often flatten\nthe two-dimensional~(2D) table structure into a sequence of tokens, which can\nseverely disrupt the spatial relationships and result in an inevitable loss of\nvital contextual information. In this paper, we first empirically demonstrate\nthe detrimental impact of such flattening operations on the performance of LLMs\nin capturing the spatial information of tables through two elaborate proxy\ntasks. Subsequently, we introduce a simple yet effective positional encoding\nmethod, termed ``2D-TPE'' (Two-Dimensional Table Positional Encoding), to\naddress this challenge. 2D-TPE enables each attention head to dynamically\nselect a permutation order of tokens within the context for attending to them,\nwhere each permutation represents a distinct traversal mode for the table, such\nas column-wise or row-wise traversal. 2D-TPE effectively mitigates the risk of\nlosing essential spatial information while preserving computational efficiency,\nthus better preserving the table structure. Extensive experiments across five\nbenchmarks demonstrate that 2D-TPE outperforms strong baselines, underscoring\nthe importance of preserving the table structure for accurate table\ncomprehension. Comprehensive analysis further reveals the substantially better\nscalability of 2D-TPE to large tables than baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tables are ubiquitous across various domains for concisely representing\nstructured information. Empowering large language models (LLMs) to reason over\ntabular data represents an actively explored direction. However, since typical\nLLMs only support one-dimensional~(1D) inputs, existing methods often flatten\nthe two-dimensional~(2D) table structure into a sequence of tokens, which can\nseverely disrupt the spatial relationships and result in an inevitable loss of\nvital contextual information. In this paper, we first empirically demonstrate\nthe detrimental impact of such flattening operations on the performance of LLMs\nin capturing the spatial information of tables through two elaborate proxy\ntasks. Subsequently, we introduce a simple yet effective positional encoding\nmethod, termed ``2D-TPE'' (Two-Dimensional Table Positional Encoding), to\naddress this challenge. 2D-TPE enables each attention head to dynamically\nselect a permutation order of tokens within the context for attending to them,\nwhere each permutation represents a distinct traversal mode for the table, such\nas column-wise or row-wise traversal. 2D-TPE effectively mitigates the risk of\nlosing essential spatial information while preserving computational efficiency,\nthus better preserving the table structure. Extensive experiments across five\nbenchmarks demonstrate that 2D-TPE outperforms strong baselines, underscoring\nthe importance of preserving the table structure for accurate table\ncomprehension. Comprehensive analysis further reveals the substantially better\nscalability of 2D-TPE to large tables than baselines."
                },
                "authors": [
                    {
                        "name": "Jia-Nan Li"
                    },
                    {
                        "name": "Jian Guan"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Zhengtao Yu"
                    },
                    {
                        "name": "Rui Yan"
                    }
                ],
                "author_detail": {
                    "name": "Rui Yan"
                },
                "author": "Rui Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19700v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19700v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02551v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02551v1",
                "updated": "2024-10-03T14:55:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    55,
                    22,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T14:55:22Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    55,
                    22,
                    3,
                    277,
                    0
                ],
                "title": "ColaCare: Enhancing Electronic Health Record Modeling through Large\n  Language Model-Driven Multi-Agent Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ColaCare: Enhancing Electronic Health Record Modeling through Large\n  Language Model-Driven Multi-Agent Collaboration"
                },
                "summary": "We introduce ColaCare, a framework that enhances Electronic Health Record\n(EHR) modeling through multi-agent collaboration driven by Large Language\nModels (LLMs). Our approach seamlessly integrates domain-specific expert models\nwith LLMs to bridge the gap between structured EHR data and text-based\nreasoning. Inspired by clinical consultations, ColaCare employs two types of\nagents: DoctorAgent and MetaAgent, which collaboratively analyze patient data.\nExpert models process and generate predictions from numerical EHR data, while\nLLM agents produce reasoning references and decision-making reports within the\ncollaborative consultation framework. We additionally incorporate the Merck\nManual of Diagnosis and Therapy (MSD) medical guideline within a\nretrieval-augmented generation (RAG) module for authoritative evidence support.\nExtensive experiments conducted on four distinct EHR datasets demonstrate\nColaCare's superior performance in mortality prediction tasks, underscoring its\npotential to revolutionize clinical decision support systems and advance\npersonalized precision medicine. The code, complete prompt templates, more case\nstudies, etc. are publicly available at the anonymous link:\nhttps://colacare.netlify.app.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce ColaCare, a framework that enhances Electronic Health Record\n(EHR) modeling through multi-agent collaboration driven by Large Language\nModels (LLMs). Our approach seamlessly integrates domain-specific expert models\nwith LLMs to bridge the gap between structured EHR data and text-based\nreasoning. Inspired by clinical consultations, ColaCare employs two types of\nagents: DoctorAgent and MetaAgent, which collaboratively analyze patient data.\nExpert models process and generate predictions from numerical EHR data, while\nLLM agents produce reasoning references and decision-making reports within the\ncollaborative consultation framework. We additionally incorporate the Merck\nManual of Diagnosis and Therapy (MSD) medical guideline within a\nretrieval-augmented generation (RAG) module for authoritative evidence support.\nExtensive experiments conducted on four distinct EHR datasets demonstrate\nColaCare's superior performance in mortality prediction tasks, underscoring its\npotential to revolutionize clinical decision support systems and advance\npersonalized precision medicine. The code, complete prompt templates, more case\nstudies, etc. are publicly available at the anonymous link:\nhttps://colacare.netlify.app."
                },
                "authors": [
                    {
                        "name": "Zixiang Wang"
                    },
                    {
                        "name": "Yinghao Zhu"
                    },
                    {
                        "name": "Huiya Zhao"
                    },
                    {
                        "name": "Xiaochen Zheng"
                    },
                    {
                        "name": "Tianlong Wang"
                    },
                    {
                        "name": "Wen Tang"
                    },
                    {
                        "name": "Yasha Wang"
                    },
                    {
                        "name": "Chengwei Pan"
                    },
                    {
                        "name": "Ewen M. Harrison"
                    },
                    {
                        "name": "Junyi Gao"
                    },
                    {
                        "name": "Liantao Ma"
                    }
                ],
                "author_detail": {
                    "name": "Liantao Ma"
                },
                "author": "Liantao Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02551v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02551v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02458v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02458v1",
                "updated": "2024-10-03T14:50:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    50,
                    33,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T14:50:33Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    50,
                    33,
                    3,
                    277,
                    0
                ],
                "title": "MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to\n  Enhance Medical Image Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to\n  Enhance Medical Image Segmentation"
                },
                "summary": "Large Language Models (LLMs), known for their versatility in textual data,\nare increasingly being explored for their potential to enhance medical image\nsegmentation, a crucial task for accurate diagnostic imaging. This study\nexplores enhancing Vision Transformers (ViTs) for medical image segmentation by\nintegrating pre-trained LLM transformer blocks. Our approach, which\nincorporates a frozen LLM transformer block into the encoder of a ViT-based\nmodel, leads to substantial improvements in segmentation performance across\nvarious medical imaging modalities. We propose a Hybrid Attention Mechanism\nthat combines global and local feature learning with a Multi-Scale Fusion Block\nfor aggregating features across different scales. The enhanced model shows\nsignificant performance gains, including an average Dice score increase from\n0.74 to 0.79 and improvements in accuracy, precision, and the Jaccard Index.\nThese results demonstrate the effectiveness of LLM-based transformers in\nrefining medical image segmentation, highlighting their potential to\nsignificantly boost model accuracy and robustness. The source code and our\nimplementation are available at: https://bit.ly/3zf2CVs",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), known for their versatility in textual data,\nare increasingly being explored for their potential to enhance medical image\nsegmentation, a crucial task for accurate diagnostic imaging. This study\nexplores enhancing Vision Transformers (ViTs) for medical image segmentation by\nintegrating pre-trained LLM transformer blocks. Our approach, which\nincorporates a frozen LLM transformer block into the encoder of a ViT-based\nmodel, leads to substantial improvements in segmentation performance across\nvarious medical imaging modalities. We propose a Hybrid Attention Mechanism\nthat combines global and local feature learning with a Multi-Scale Fusion Block\nfor aggregating features across different scales. The enhanced model shows\nsignificant performance gains, including an average Dice score increase from\n0.74 to 0.79 and improvements in accuracy, precision, and the Jaccard Index.\nThese results demonstrate the effectiveness of LLM-based transformers in\nrefining medical image segmentation, highlighting their potential to\nsignificantly boost model accuracy and robustness. The source code and our\nimplementation are available at: https://bit.ly/3zf2CVs"
                },
                "authors": [
                    {
                        "name": "Gurucharan Marthi Krishna Kumar"
                    },
                    {
                        "name": "Aman Chadha"
                    },
                    {
                        "name": "Janine Mendola"
                    },
                    {
                        "name": "Amir Shmuel"
                    }
                ],
                "author_detail": {
                    "name": "Amir Shmuel"
                },
                "author": "Amir Shmuel",
                "arxiv_comment": "Submitted to IEEE/CVF Winter Conference on Applications of Computer\n  Vision (WACV) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02458v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02458v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.18045v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.18045v3",
                "updated": "2024-10-03T14:44:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    44,
                    44,
                    3,
                    277,
                    0
                ],
                "published": "2024-02-28T04:43:46Z",
                "published_parsed": [
                    2024,
                    2,
                    28,
                    4,
                    43,
                    46,
                    2,
                    59,
                    0
                ],
                "title": "Multi-FAct: Assessing Factuality of Multilingual LLMs using FActScore",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-FAct: Assessing Factuality of Multilingual LLMs using FActScore"
                },
                "summary": "Evaluating the factuality of long-form large language model (LLM)-generated\ntext is an important challenge. Recently there has been a surge of interest in\nfactuality evaluation for English, but little is known about the factuality\nevaluation of multilingual LLMs, specially when it comes to long-form\ngeneration. %This paper systematically evaluates multilingual LLMs' factual\naccuracy across languages and geographic regions. We introduce a simple\npipeline for multilingual factuality evaluation, by applying FActScore (Min et\nal., 2023) for diverse languages. In addition to evaluating multilingual\nfactual generation, we evaluate the factual accuracy of long-form text\ngeneration in topics that reflect regional diversity. We also examine the\nfeasibility of running the FActScore pipeline using non-English Wikipedia and\nprovide comprehensive guidelines on multilingual factual evaluation for\nregionally diverse topics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the factuality of long-form large language model (LLM)-generated\ntext is an important challenge. Recently there has been a surge of interest in\nfactuality evaluation for English, but little is known about the factuality\nevaluation of multilingual LLMs, specially when it comes to long-form\ngeneration. %This paper systematically evaluates multilingual LLMs' factual\naccuracy across languages and geographic regions. We introduce a simple\npipeline for multilingual factuality evaluation, by applying FActScore (Min et\nal., 2023) for diverse languages. In addition to evaluating multilingual\nfactual generation, we evaluate the factual accuracy of long-form text\ngeneration in topics that reflect regional diversity. We also examine the\nfeasibility of running the FActScore pipeline using non-English Wikipedia and\nprovide comprehensive guidelines on multilingual factual evaluation for\nregionally diverse topics."
                },
                "authors": [
                    {
                        "name": "Sheikh Shafayat"
                    },
                    {
                        "name": "Eunsu Kim"
                    },
                    {
                        "name": "Juhyun Oh"
                    },
                    {
                        "name": "Alice Oh"
                    }
                ],
                "author_detail": {
                    "name": "Alice Oh"
                },
                "author": "Alice Oh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.18045v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.18045v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02536v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02536v1",
                "updated": "2024-10-03T14:42:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    42,
                    34,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T14:42:34Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    42,
                    34,
                    3,
                    277,
                    0
                ],
                "title": "Intelligence at the Edge of Chaos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligence at the Edge of Chaos"
                },
                "summary": "We explore the emergence of intelligent behavior in artificial systems by\ninvestigating how the complexity of rule-based systems influences the\ncapabilities of models trained to predict these rules. Our study focuses on\nelementary cellular automata (ECA), simple yet powerful one-dimensional systems\nthat generate behaviors ranging from trivial to highly complex. By training\ndistinct Large Language Models (LLMs) on different ECAs, we evaluated the\nrelationship between the complexity of the rules' behavior and the intelligence\nexhibited by the LLMs, as reflected in their performance on downstream tasks.\nOur findings reveal that rules with higher complexity lead to models exhibiting\ngreater intelligence, as demonstrated by their performance on reasoning and\nchess move prediction tasks. Both uniform and periodic systems, and often also\nhighly chaotic systems, resulted in poorer downstream performance, highlighting\na sweet spot of complexity conducive to intelligence. We conjecture that\nintelligence arises from the ability to predict complexity and that creating\nintelligence may require only exposure to complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore the emergence of intelligent behavior in artificial systems by\ninvestigating how the complexity of rule-based systems influences the\ncapabilities of models trained to predict these rules. Our study focuses on\nelementary cellular automata (ECA), simple yet powerful one-dimensional systems\nthat generate behaviors ranging from trivial to highly complex. By training\ndistinct Large Language Models (LLMs) on different ECAs, we evaluated the\nrelationship between the complexity of the rules' behavior and the intelligence\nexhibited by the LLMs, as reflected in their performance on downstream tasks.\nOur findings reveal that rules with higher complexity lead to models exhibiting\ngreater intelligence, as demonstrated by their performance on reasoning and\nchess move prediction tasks. Both uniform and periodic systems, and often also\nhighly chaotic systems, resulted in poorer downstream performance, highlighting\na sweet spot of complexity conducive to intelligence. We conjecture that\nintelligence arises from the ability to predict complexity and that creating\nintelligence may require only exposure to complexity."
                },
                "authors": [
                    {
                        "name": "Shiyang Zhang"
                    },
                    {
                        "name": "Aakash Patel"
                    },
                    {
                        "name": "Syed A Rizvi"
                    },
                    {
                        "name": "Nianchen Liu"
                    },
                    {
                        "name": "Sizhuang He"
                    },
                    {
                        "name": "Amin Karbasi"
                    },
                    {
                        "name": "Emanuele Zappala"
                    },
                    {
                        "name": "David van Dijk"
                    }
                ],
                "author_detail": {
                    "name": "David van Dijk"
                },
                "author": "David van Dijk",
                "arxiv_comment": "15 pages,8 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02536v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02536v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20242v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20242v3",
                "updated": "2024-10-03T14:31:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    31,
                    39,
                    3,
                    277,
                    0
                ],
                "published": "2024-07-16T13:13:16Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    13,
                    13,
                    16,
                    1,
                    198,
                    0
                ],
                "title": "BadRobot: Manipulating Embodied LLMs in the Physical World",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BadRobot: Manipulating Embodied LLMs in the Physical World"
                },
                "summary": "Embodied AI represents systems where AI is integrated into physical entities,\nenabling them to perceive and interact with their surroundings. Large Language\nModel (LLM), which exhibits powerful language understanding abilities, has been\nextensively employed in embodied AI by facilitating sophisticated task\nplanning. However, a critical safety issue remains overlooked: could these\nembodied LLMs perpetrate harmful behaviors? In response, we introduce BadRobot,\na novel attack paradigm aiming to make embodied LLMs violate safety and ethical\nconstraints through typical voice-based user-system interactions. Specifically,\nthree vulnerabilities are exploited to achieve this type of attack: (i)\nmanipulation of LLMs within robotic systems, (ii) misalignment between\nlinguistic outputs and physical actions, and (iii) unintentional hazardous\nbehaviors caused by world knowledge's flaws. Furthermore, we construct a\nbenchmark of various malicious physical action queries to evaluate BadRobot's\nattack performance. Based on this benchmark, extensive experiments against\nexisting prominent embodied LLM frameworks (e.g., Voxposer, Code as Policies,\nand ProgPrompt) demonstrate the effectiveness of our BadRobot. Warning: This\npaper contains harmful AI-generated language and aggressive actions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied AI represents systems where AI is integrated into physical entities,\nenabling them to perceive and interact with their surroundings. Large Language\nModel (LLM), which exhibits powerful language understanding abilities, has been\nextensively employed in embodied AI by facilitating sophisticated task\nplanning. However, a critical safety issue remains overlooked: could these\nembodied LLMs perpetrate harmful behaviors? In response, we introduce BadRobot,\na novel attack paradigm aiming to make embodied LLMs violate safety and ethical\nconstraints through typical voice-based user-system interactions. Specifically,\nthree vulnerabilities are exploited to achieve this type of attack: (i)\nmanipulation of LLMs within robotic systems, (ii) misalignment between\nlinguistic outputs and physical actions, and (iii) unintentional hazardous\nbehaviors caused by world knowledge's flaws. Furthermore, we construct a\nbenchmark of various malicious physical action queries to evaluate BadRobot's\nattack performance. Based on this benchmark, extensive experiments against\nexisting prominent embodied LLM frameworks (e.g., Voxposer, Code as Policies,\nand ProgPrompt) demonstrate the effectiveness of our BadRobot. Warning: This\npaper contains harmful AI-generated language and aggressive actions."
                },
                "authors": [
                    {
                        "name": "Hangtao Zhang"
                    },
                    {
                        "name": "Chenyu Zhu"
                    },
                    {
                        "name": "Xianlong Wang"
                    },
                    {
                        "name": "Ziqi Zhou"
                    },
                    {
                        "name": "Changgan Yin"
                    },
                    {
                        "name": "Minghui Li"
                    },
                    {
                        "name": "Lulu Xue"
                    },
                    {
                        "name": "Yichen Wang"
                    },
                    {
                        "name": "Shengshan Hu"
                    },
                    {
                        "name": "Aishan Liu"
                    },
                    {
                        "name": "Peijin Guo"
                    },
                    {
                        "name": "Leo Yu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Leo Yu Zhang"
                },
                "author": "Leo Yu Zhang",
                "arxiv_comment": "38 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20242v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20242v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.15656v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.15656v2",
                "updated": "2024-10-03T14:27:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    27,
                    14,
                    3,
                    277,
                    0
                ],
                "published": "2023-09-27T13:45:38Z",
                "published_parsed": [
                    2023,
                    9,
                    27,
                    13,
                    45,
                    38,
                    2,
                    270,
                    0
                ],
                "title": "Conversational Feedback in Scripted versus Spontaneous Dialogues: A\n  Comparative Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational Feedback in Scripted versus Spontaneous Dialogues: A\n  Comparative Analysis"
                },
                "summary": "Scripted dialogues such as movie and TV subtitles constitute a widespread\nsource of training data for conversational NLP models. However, there are\nnotable linguistic differences between these dialogues and spontaneous\ninteractions, especially regarding the occurrence of communicative feedback\nsuch as backchannels, acknowledgments, or clarification requests. This paper\npresents a quantitative analysis of such feedback phenomena in both subtitles\nand spontaneous conversations. Based on conversational data spanning eight\nlanguages and multiple genres, we extract lexical statistics, classifications\nfrom a dialogue act tagger, expert annotations and labels derived from a\nfine-tuned Large Language Model (LLM). Our main empirical findings are that (1)\ncommunicative feedback is markedly less frequent in subtitles than in\nspontaneous dialogues and (2) subtitles contain a higher proportion of negative\nfeedback. We also show that dialogues generated by standard LLMs lie much\ncloser to scripted dialogues than spontaneous interactions in terms of\ncommunicative feedback.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scripted dialogues such as movie and TV subtitles constitute a widespread\nsource of training data for conversational NLP models. However, there are\nnotable linguistic differences between these dialogues and spontaneous\ninteractions, especially regarding the occurrence of communicative feedback\nsuch as backchannels, acknowledgments, or clarification requests. This paper\npresents a quantitative analysis of such feedback phenomena in both subtitles\nand spontaneous conversations. Based on conversational data spanning eight\nlanguages and multiple genres, we extract lexical statistics, classifications\nfrom a dialogue act tagger, expert annotations and labels derived from a\nfine-tuned Large Language Model (LLM). Our main empirical findings are that (1)\ncommunicative feedback is markedly less frequent in subtitles than in\nspontaneous dialogues and (2) subtitles contain a higher proportion of negative\nfeedback. We also show that dialogues generated by standard LLMs lie much\ncloser to scripted dialogues than spontaneous interactions in terms of\ncommunicative feedback."
                },
                "authors": [
                    {
                        "name": "Ildik Piln"
                    },
                    {
                        "name": "Laurent Prvot"
                    },
                    {
                        "name": "Hendrik Buschmeier"
                    },
                    {
                        "name": "Pierre Lison"
                    }
                ],
                "author_detail": {
                    "name": "Pierre Lison"
                },
                "author": "Pierre Lison",
                "arxiv_doi": "10.18653/v1/2024.sigdial-1.38",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.18653/v1/2024.sigdial-1.38",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2309.15656v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.15656v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Updated version for SIGdial 2024",
                "arxiv_journal_ref": "In Proceedings of SIGdial 2024, pp. 440-457. Kyoto, Japan (2024)",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02511v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02511v1",
                "updated": "2024-10-03T14:21:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    21,
                    23,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T14:21:23Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    21,
                    23,
                    3,
                    277,
                    0
                ],
                "title": "Choices are More Important than Efforts: LLM Enables Efficient\n  Multi-Agent Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Choices are More Important than Efforts: LLM Enables Efficient\n  Multi-Agent Exploration"
                },
                "summary": "With expansive state-action spaces, efficient multi-agent exploration remains\na longstanding challenge in reinforcement learning. Although pursuing novelty,\ndiversity, or uncertainty attracts increasing attention, redundant efforts\nbrought by exploration without proper guidance choices poses a practical issue\nfor the community. This paper introduces a systematic approach, termed LEMAE,\nchoosing to channel informative task-relevant guidance from a knowledgeable\nLarge Language Model (LLM) for Efficient Multi-Agent Exploration. Specifically,\nwe ground linguistic knowledge from LLM into symbolic key states, that are\ncritical for task fulfillment, in a discriminative manner at low LLM inference\ncosts. To unleash the power of key states, we design Subspace-based Hindsight\nIntrinsic Reward (SHIR) to guide agents toward key states by increasing reward\ndensity. Additionally, we build the Key State Memory Tree (KSMT) to track\ntransitions between key states in a specific task for organized exploration.\nBenefiting from diminishing redundant explorations, LEMAE outperforms existing\nSOTA approaches on the challenging benchmarks (e.g., SMAC and MPE) by a large\nmargin, achieving a 10x acceleration in certain scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With expansive state-action spaces, efficient multi-agent exploration remains\na longstanding challenge in reinforcement learning. Although pursuing novelty,\ndiversity, or uncertainty attracts increasing attention, redundant efforts\nbrought by exploration without proper guidance choices poses a practical issue\nfor the community. This paper introduces a systematic approach, termed LEMAE,\nchoosing to channel informative task-relevant guidance from a knowledgeable\nLarge Language Model (LLM) for Efficient Multi-Agent Exploration. Specifically,\nwe ground linguistic knowledge from LLM into symbolic key states, that are\ncritical for task fulfillment, in a discriminative manner at low LLM inference\ncosts. To unleash the power of key states, we design Subspace-based Hindsight\nIntrinsic Reward (SHIR) to guide agents toward key states by increasing reward\ndensity. Additionally, we build the Key State Memory Tree (KSMT) to track\ntransitions between key states in a specific task for organized exploration.\nBenefiting from diminishing redundant explorations, LEMAE outperforms existing\nSOTA approaches on the challenging benchmarks (e.g., SMAC and MPE) by a large\nmargin, achieving a 10x acceleration in certain scenarios."
                },
                "authors": [
                    {
                        "name": "Yun Qu"
                    },
                    {
                        "name": "Boyuan Wang"
                    },
                    {
                        "name": "Yuhang Jiang"
                    },
                    {
                        "name": "Jianzhun Shao"
                    },
                    {
                        "name": "Yixiu Mao"
                    },
                    {
                        "name": "Cheems Wang"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Xiangyang Ji"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyang Ji"
                },
                "author": "Xiangyang Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02511v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02511v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02507v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02507v1",
                "updated": "2024-10-03T14:15:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    15,
                    0,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T14:15:00Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    15,
                    0,
                    3,
                    277,
                    0
                ],
                "title": "Can Large Language Models Grasp Legal Theories? Enhance Legal Reasoning\n  with Insights from Multi-Agent Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Grasp Legal Theories? Enhance Legal Reasoning\n  with Insights from Multi-Agent Collaboration"
                },
                "summary": "Large Language Models (LLMs) could struggle to fully understand legal\ntheories and perform complex legal reasoning tasks. In this study, we introduce\na challenging task (confusing charge prediction) to better evaluate LLMs'\nunderstanding of legal theories and reasoning capabilities. We also propose a\nnovel framework: Multi-Agent framework for improving complex Legal Reasoning\ncapability (MALR). MALR employs non-parametric learning, encouraging LLMs to\nautomatically decompose complex legal tasks and mimic human learning process to\nextract insights from legal rules, helping LLMs better understand legal\ntheories and enhance their legal reasoning abilities. Extensive experiments on\nmultiple real-world datasets demonstrate that the proposed framework\neffectively addresses complex reasoning issues in practical scenarios, paving\nthe way for more reliable applications in the legal domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) could struggle to fully understand legal\ntheories and perform complex legal reasoning tasks. In this study, we introduce\na challenging task (confusing charge prediction) to better evaluate LLMs'\nunderstanding of legal theories and reasoning capabilities. We also propose a\nnovel framework: Multi-Agent framework for improving complex Legal Reasoning\ncapability (MALR). MALR employs non-parametric learning, encouraging LLMs to\nautomatically decompose complex legal tasks and mimic human learning process to\nextract insights from legal rules, helping LLMs better understand legal\ntheories and enhance their legal reasoning abilities. Extensive experiments on\nmultiple real-world datasets demonstrate that the proposed framework\neffectively addresses complex reasoning issues in practical scenarios, paving\nthe way for more reliable applications in the legal domain."
                },
                "authors": [
                    {
                        "name": "Weikang Yuan"
                    },
                    {
                        "name": "Junjie Cao"
                    },
                    {
                        "name": "Zhuoren Jiang"
                    },
                    {
                        "name": "Yangyang Kang"
                    },
                    {
                        "name": "Jun Lin"
                    },
                    {
                        "name": "Kaisong Song"
                    },
                    {
                        "name": "tianqianjin lin"
                    },
                    {
                        "name": "Pengwei Yan"
                    },
                    {
                        "name": "Changlong Sun"
                    },
                    {
                        "name": "Xiaozhong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaozhong Liu"
                },
                "author": "Xiaozhong Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02507v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02507v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02506v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02506v1",
                "updated": "2024-10-03T14:14:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    14,
                    31,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T14:14:31Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    14,
                    31,
                    3,
                    277,
                    0
                ],
                "title": "Cut the Crap: An Economical Communication Pipeline for LLM-based\n  Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cut the Crap: An Economical Communication Pipeline for LLM-based\n  Multi-Agent Systems"
                },
                "summary": "Recent advancements in large language model (LLM)-powered agents have shown\nthat collective intelligence can significantly outperform individual\ncapabilities, largely attributed to the meticulously designed inter-agent\ncommunication topologies. Though impressive in performance, existing\nmulti-agent pipelines inherently introduce substantial token overhead, as well\nas increased economic costs, which pose challenges for their large-scale\ndeployments. In response to this challenge, we propose an economical, simple,\nand robust multi-agent communication framework, termed $\\texttt{AgentPrune}$,\nwhich can seamlessly integrate into mainstream multi-agent systems and prunes\nredundant or even malicious communication messages. Technically,\n$\\texttt{AgentPrune}$ is the first to identify and formally define the\n\\textit{communication redundancy} issue present in current LLM-based\nmulti-agent pipelines, and efficiently performs one-shot pruning on the\nspatial-temporal message-passing graph, yielding a token-economic and\nhigh-performing communication topology. Extensive experiments across six\nbenchmarks demonstrate that $\\texttt{AgentPrune}$ \\textbf{(I)} achieves\ncomparable results as state-of-the-art topologies at merely $\\$5.6$ cost\ncompared to their $\\$43.7$, \\textbf{(II)} integrates seamlessly into existing\nmulti-agent frameworks with $28.1\\%\\sim72.8\\%\\downarrow$ token reduction, and\n\\textbf{(III)} successfully defend against two types of agent-based adversarial\nattacks with $3.5\\%\\sim10.8\\%\\uparrow$ performance boost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language model (LLM)-powered agents have shown\nthat collective intelligence can significantly outperform individual\ncapabilities, largely attributed to the meticulously designed inter-agent\ncommunication topologies. Though impressive in performance, existing\nmulti-agent pipelines inherently introduce substantial token overhead, as well\nas increased economic costs, which pose challenges for their large-scale\ndeployments. In response to this challenge, we propose an economical, simple,\nand robust multi-agent communication framework, termed $\\texttt{AgentPrune}$,\nwhich can seamlessly integrate into mainstream multi-agent systems and prunes\nredundant or even malicious communication messages. Technically,\n$\\texttt{AgentPrune}$ is the first to identify and formally define the\n\\textit{communication redundancy} issue present in current LLM-based\nmulti-agent pipelines, and efficiently performs one-shot pruning on the\nspatial-temporal message-passing graph, yielding a token-economic and\nhigh-performing communication topology. Extensive experiments across six\nbenchmarks demonstrate that $\\texttt{AgentPrune}$ \\textbf{(I)} achieves\ncomparable results as state-of-the-art topologies at merely $\\$5.6$ cost\ncompared to their $\\$43.7$, \\textbf{(II)} integrates seamlessly into existing\nmulti-agent frameworks with $28.1\\%\\sim72.8\\%\\downarrow$ token reduction, and\n\\textbf{(III)} successfully defend against two types of agent-based adversarial\nattacks with $3.5\\%\\sim10.8\\%\\uparrow$ performance boost."
                },
                "authors": [
                    {
                        "name": "Guibin Zhang"
                    },
                    {
                        "name": "Yanwei Yue"
                    },
                    {
                        "name": "Zhixun Li"
                    },
                    {
                        "name": "Sukwon Yun"
                    },
                    {
                        "name": "Guancheng Wan"
                    },
                    {
                        "name": "Kun Wang"
                    },
                    {
                        "name": "Dawei Cheng"
                    },
                    {
                        "name": "Jeffrey Xu Yu"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02506v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02506v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18028v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18028v2",
                "updated": "2024-10-03T14:11:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    11,
                    23,
                    3,
                    277,
                    0
                ],
                "published": "2024-09-26T16:34:35Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    16,
                    34,
                    35,
                    3,
                    270,
                    0
                ],
                "title": "Compositional Hardness of Code in Large Language Models -- A\n  Probabilistic Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compositional Hardness of Code in Large Language Models -- A\n  Probabilistic Perspective"
                },
                "summary": "A common practice in large language model (LLM) usage for complex analytical\ntasks such as code generation, is to sample a solution for the entire task\nwithin the model's context window. Previous works have shown that subtask\ndecomposition within the model's context (chain of thought), is beneficial for\nsolving such tasks. In this work, we point a limitation of LLMs' ability to\nperform several sub-tasks within the same context window - an in-context\nhardness of composition, pointing to an advantage for distributing a decomposed\nproblem in a multi-agent system of LLMs. The hardness of composition is\nquantified by a generation complexity metric, i.e., the number of LLM\ngenerations required to sample at least one correct solution. We find a gap\nbetween the generation complexity of solving a compositional problem within the\nsame context relative to distributing it among multiple agents, that increases\nexponentially with the solution's length. We prove our results theoretically\nand demonstrate them empirically.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A common practice in large language model (LLM) usage for complex analytical\ntasks such as code generation, is to sample a solution for the entire task\nwithin the model's context window. Previous works have shown that subtask\ndecomposition within the model's context (chain of thought), is beneficial for\nsolving such tasks. In this work, we point a limitation of LLMs' ability to\nperform several sub-tasks within the same context window - an in-context\nhardness of composition, pointing to an advantage for distributing a decomposed\nproblem in a multi-agent system of LLMs. The hardness of composition is\nquantified by a generation complexity metric, i.e., the number of LLM\ngenerations required to sample at least one correct solution. We find a gap\nbetween the generation complexity of solving a compositional problem within the\nsame context relative to distributing it among multiple agents, that increases\nexponentially with the solution's length. We prove our results theoretically\nand demonstrate them empirically."
                },
                "authors": [
                    {
                        "name": "Yotam Wolf"
                    },
                    {
                        "name": "Binyamin Rothberg"
                    },
                    {
                        "name": "Dorin Shteyman"
                    },
                    {
                        "name": "Amnon Shashua"
                    }
                ],
                "author_detail": {
                    "name": "Amnon Shashua"
                },
                "author": "Amnon Shashua",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18028v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18028v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02504v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02504v1",
                "updated": "2024-10-03T14:09:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    9,
                    58,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T14:09:58Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    9,
                    58,
                    3,
                    277,
                    0
                ],
                "title": "Dual Active Learning for Reinforcement Learning from Human Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dual Active Learning for Reinforcement Learning from Human Feedback"
                },
                "summary": "Aligning large language models (LLMs) with human preferences is critical to\nrecent advances in generative artificial intelligence. Reinforcement learning\nfrom human feedback (RLHF) is widely applied to achieve this objective. A key\nstep in RLHF is to learn the reward function from human feedback. However,\nhuman feedback is costly and time-consuming, making it essential to collect\nhigh-quality conversation data for human teachers to label. Additionally,\ndifferent human teachers have different levels of expertise. It is thus\ncritical to query the most appropriate teacher for their opinions. In this\npaper, we use offline reinforcement learning (RL) to formulate the alignment\nproblem. Motivated by the idea of $D$-optimal design, we first propose a dual\nactive reward learning algorithm for the simultaneous selection of\nconversations and teachers. Next, we apply pessimistic RL to solve the\nalignment problem, based on the learned reward estimator. Theoretically, we\nshow that the reward estimator obtained through our proposed adaptive selection\nstrategy achieves minimal generalized variance asymptotically, and prove that\nthe sub-optimality of our pessimistic policy scales as $O(1/\\sqrt{T})$ with a\ngiven sample budget $T$. Through simulations and experiments on LLMs, we\ndemonstrate the effectiveness of our algorithm and its superiority over\nstate-of-the-arts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning large language models (LLMs) with human preferences is critical to\nrecent advances in generative artificial intelligence. Reinforcement learning\nfrom human feedback (RLHF) is widely applied to achieve this objective. A key\nstep in RLHF is to learn the reward function from human feedback. However,\nhuman feedback is costly and time-consuming, making it essential to collect\nhigh-quality conversation data for human teachers to label. Additionally,\ndifferent human teachers have different levels of expertise. It is thus\ncritical to query the most appropriate teacher for their opinions. In this\npaper, we use offline reinforcement learning (RL) to formulate the alignment\nproblem. Motivated by the idea of $D$-optimal design, we first propose a dual\nactive reward learning algorithm for the simultaneous selection of\nconversations and teachers. Next, we apply pessimistic RL to solve the\nalignment problem, based on the learned reward estimator. Theoretically, we\nshow that the reward estimator obtained through our proposed adaptive selection\nstrategy achieves minimal generalized variance asymptotically, and prove that\nthe sub-optimality of our pessimistic policy scales as $O(1/\\sqrt{T})$ with a\ngiven sample budget $T$. Through simulations and experiments on LLMs, we\ndemonstrate the effectiveness of our algorithm and its superiority over\nstate-of-the-arts."
                },
                "authors": [
                    {
                        "name": "Pangpang Liu"
                    },
                    {
                        "name": "Chengchun Shi"
                    },
                    {
                        "name": "Will Wei Sun"
                    }
                ],
                "author_detail": {
                    "name": "Will Wei Sun"
                },
                "author": "Will Wei Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02504v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02504v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02499v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02499v1",
                "updated": "2024-10-03T14:01:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    1,
                    1,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T14:01:01Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    1,
                    1,
                    3,
                    277,
                    0
                ],
                "title": "Defining Knowledge: Bridging Epistemology and Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Defining Knowledge: Bridging Epistemology and Large Language Models"
                },
                "summary": "Knowledge claims are abundant in the literature on large language models\n(LLMs); but can we say that GPT-4 truly \"knows\" the Earth is round? To address\nthis question, we review standard definitions of knowledge in epistemology and\nwe formalize interpretations applicable to LLMs. In doing so, we identify\ninconsistencies and gaps in how current NLP research conceptualizes knowledge\nwith respect to epistemological frameworks. Additionally, we conduct a survey\nof 100 professional philosophers and computer scientists to compare their\npreferences in knowledge definitions and their views on whether LLMs can really\nbe said to know. Finally, we suggest evaluation protocols for testing knowledge\nin accordance to the most relevant definitions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge claims are abundant in the literature on large language models\n(LLMs); but can we say that GPT-4 truly \"knows\" the Earth is round? To address\nthis question, we review standard definitions of knowledge in epistemology and\nwe formalize interpretations applicable to LLMs. In doing so, we identify\ninconsistencies and gaps in how current NLP research conceptualizes knowledge\nwith respect to epistemological frameworks. Additionally, we conduct a survey\nof 100 professional philosophers and computer scientists to compare their\npreferences in knowledge definitions and their views on whether LLMs can really\nbe said to know. Finally, we suggest evaluation protocols for testing knowledge\nin accordance to the most relevant definitions."
                },
                "authors": [
                    {
                        "name": "Constanza Fierro"
                    },
                    {
                        "name": "Ruchira Dhar"
                    },
                    {
                        "name": "Filippos Stamatiou"
                    },
                    {
                        "name": "Nicolas Garneau"
                    },
                    {
                        "name": "Anders Sgaard"
                    }
                ],
                "author_detail": {
                    "name": "Anders Sgaard"
                },
                "author": "Anders Sgaard",
                "arxiv_comment": "EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02499v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02499v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02498v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02498v1",
                "updated": "2024-10-03T14:00:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    0,
                    44,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T14:00:44Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    14,
                    0,
                    44,
                    3,
                    277,
                    0
                ],
                "title": "Dynamic Gradient Alignment for Online Data Mixing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Gradient Alignment for Online Data Mixing"
                },
                "summary": "The composition of training data mixtures is critical for effectively\ntraining large language models (LLMs), as it directly impacts their performance\non downstream tasks. Our goal is to identify an optimal data mixture to\nspecialize an LLM for a specific task with access to only a few examples.\nTraditional approaches to this problem include ad-hoc reweighting methods,\nimportance sampling, and gradient alignment techniques. This paper focuses on\ngradient alignment and introduces Dynamic Gradient Alignment (DGA), a scalable\nonline gradient alignment algorithm. DGA dynamically estimates the pre-training\ndata mixture on which the models' gradients align as well as possible with\nthose of the model on the specific task. DGA is the first gradient alignment\napproach that incurs minimal overhead compared to standard pre-training and\noutputs a competitive model, eliminating the need for retraining the model.\nExperimentally, we demonstrate significant improvements over importance\nsampling in two key scenarios: (i) when the pre-training set is small and\nimportance sampling overfits due to limited data; and (ii) when there is\ninsufficient specialized data, trapping importance sampling on narrow pockets\nof data. Our findings underscore the effectiveness of gradient alignment\nmethods in optimizing training data mixtures, particularly in data-constrained\nenvironments, and offer a practical solution for enhancing LLM performance on\nspecific tasks with limited data availability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The composition of training data mixtures is critical for effectively\ntraining large language models (LLMs), as it directly impacts their performance\non downstream tasks. Our goal is to identify an optimal data mixture to\nspecialize an LLM for a specific task with access to only a few examples.\nTraditional approaches to this problem include ad-hoc reweighting methods,\nimportance sampling, and gradient alignment techniques. This paper focuses on\ngradient alignment and introduces Dynamic Gradient Alignment (DGA), a scalable\nonline gradient alignment algorithm. DGA dynamically estimates the pre-training\ndata mixture on which the models' gradients align as well as possible with\nthose of the model on the specific task. DGA is the first gradient alignment\napproach that incurs minimal overhead compared to standard pre-training and\noutputs a competitive model, eliminating the need for retraining the model.\nExperimentally, we demonstrate significant improvements over importance\nsampling in two key scenarios: (i) when the pre-training set is small and\nimportance sampling overfits due to limited data; and (ii) when there is\ninsufficient specialized data, trapping importance sampling on narrow pockets\nof data. Our findings underscore the effectiveness of gradient alignment\nmethods in optimizing training data mixtures, particularly in data-constrained\nenvironments, and offer a practical solution for enhancing LLM performance on\nspecific tasks with limited data availability."
                },
                "authors": [
                    {
                        "name": "Simin Fan"
                    },
                    {
                        "name": "David Grangier"
                    },
                    {
                        "name": "Pierre Ablin"
                    }
                ],
                "author_detail": {
                    "name": "Pierre Ablin"
                },
                "author": "Pierre Ablin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02498v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02498v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02492v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02492v1",
                "updated": "2024-10-03T13:57:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    13,
                    57,
                    7,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T13:57:07Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    13,
                    57,
                    7,
                    3,
                    277,
                    0
                ],
                "title": "DTVLT: A Multi-modal Diverse Text Benchmark for Visual Language Tracking\n  Based on LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DTVLT: A Multi-modal Diverse Text Benchmark for Visual Language Tracking\n  Based on LLM"
                },
                "summary": "Visual language tracking (VLT) has emerged as a cutting-edge research area,\nharnessing linguistic data to enhance algorithms with multi-modal inputs and\nbroadening the scope of traditional single object tracking (SOT) to encompass\nvideo understanding applications. Despite this, most VLT benchmarks still\ndepend on succinct, human-annotated text descriptions for each video. These\ndescriptions often fall short in capturing the nuances of video content\ndynamics and lack stylistic variety in language, constrained by their uniform\nlevel of detail and a fixed annotation frequency. As a result, algorithms tend\nto default to a \"memorize the answer\" strategy, diverging from the core\nobjective of achieving a deeper understanding of video content. Fortunately,\nthe emergence of large language models (LLMs) has enabled the generation of\ndiverse text. This work utilizes LLMs to generate varied semantic annotations\n(in terms of text lengths and granularities) for representative SOT benchmarks,\nthereby establishing a novel multi-modal benchmark. Specifically, we (1)\npropose a new visual language tracking benchmark with diverse texts, named\nDTVLT, based on five prominent VLT and SOT benchmarks, including three\nsub-tasks: short-term tracking, long-term tracking, and global instance\ntracking. (2) We offer four granularity texts in our benchmark, considering the\nextent and density of semantic information. We expect this multi-granular\ngeneration strategy to foster a favorable environment for VLT and video\nunderstanding research. (3) We conduct comprehensive experimental analyses on\nDTVLT, evaluating the impact of diverse text on tracking performance and hope\nthe identified performance bottlenecks of existing algorithms can support\nfurther research in VLT and video understanding. The proposed benchmark,\nexperimental results and toolkit will be released gradually on\nhttp://videocube.aitestunion.com/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual language tracking (VLT) has emerged as a cutting-edge research area,\nharnessing linguistic data to enhance algorithms with multi-modal inputs and\nbroadening the scope of traditional single object tracking (SOT) to encompass\nvideo understanding applications. Despite this, most VLT benchmarks still\ndepend on succinct, human-annotated text descriptions for each video. These\ndescriptions often fall short in capturing the nuances of video content\ndynamics and lack stylistic variety in language, constrained by their uniform\nlevel of detail and a fixed annotation frequency. As a result, algorithms tend\nto default to a \"memorize the answer\" strategy, diverging from the core\nobjective of achieving a deeper understanding of video content. Fortunately,\nthe emergence of large language models (LLMs) has enabled the generation of\ndiverse text. This work utilizes LLMs to generate varied semantic annotations\n(in terms of text lengths and granularities) for representative SOT benchmarks,\nthereby establishing a novel multi-modal benchmark. Specifically, we (1)\npropose a new visual language tracking benchmark with diverse texts, named\nDTVLT, based on five prominent VLT and SOT benchmarks, including three\nsub-tasks: short-term tracking, long-term tracking, and global instance\ntracking. (2) We offer four granularity texts in our benchmark, considering the\nextent and density of semantic information. We expect this multi-granular\ngeneration strategy to foster a favorable environment for VLT and video\nunderstanding research. (3) We conduct comprehensive experimental analyses on\nDTVLT, evaluating the impact of diverse text on tracking performance and hope\nthe identified performance bottlenecks of existing algorithms can support\nfurther research in VLT and video understanding. The proposed benchmark,\nexperimental results and toolkit will be released gradually on\nhttp://videocube.aitestunion.com/."
                },
                "authors": [
                    {
                        "name": "Xuchen Li"
                    },
                    {
                        "name": "Shiyu Hu"
                    },
                    {
                        "name": "Xiaokun Feng"
                    },
                    {
                        "name": "Dailing Zhang"
                    },
                    {
                        "name": "Meiqi Wu"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Kaiqi Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kaiqi Huang"
                },
                "author": "Kaiqi Huang",
                "arxiv_comment": "Preprint, Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02492v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02492v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.07103v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.07103v3",
                "updated": "2024-10-03T13:55:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    13,
                    55,
                    8,
                    3,
                    277,
                    0
                ],
                "published": "2024-04-10T15:41:53Z",
                "published_parsed": [
                    2024,
                    4,
                    10,
                    15,
                    41,
                    53,
                    2,
                    101,
                    0
                ],
                "title": "Graph Chain-of-Thought: Augmenting Large Language Models by Reasoning on\n  Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Chain-of-Thought: Augmenting Large Language Models by Reasoning on\n  Graphs"
                },
                "summary": "Large language models (LLMs), while exhibiting exceptional performance,\nsuffer from hallucinations, especially on knowledge-intensive tasks. Existing\nworks propose to augment LLMs with individual text units retrieved from\nexternal knowledge corpora to alleviate the issue. However, in many domains,\ntexts are interconnected (e.g., academic papers in a bibliographic graph are\nlinked by citations and co-authorships) which form a (text-attributed) graph.\nThe knowledge in such graphs is encoded not only in single texts/nodes but also\nin their associated connections. To facilitate the research of augmenting LLMs\nwith graphs, we manually construct a Graph Reasoning Benchmark dataset called\nGRBench, containing 1,740 questions that can be answered with the knowledge\nfrom 10 domain graphs. Then, we propose a simple and effective framework called\nGraph Chain-of-thought (Graph-CoT) to augment LLMs with graphs by encouraging\nLLMs to reason on the graph iteratively. Each Graph-CoT iteration consists of\nthree sub-steps: LLM reasoning, LLM-graph interaction, and graph execution. We\nconduct systematic experiments with three LLM backbones on GRBench, where\nGraph-CoT outperforms the baselines consistently. The code is available at\nhttps://github.com/PeterGriffinJin/Graph-CoT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), while exhibiting exceptional performance,\nsuffer from hallucinations, especially on knowledge-intensive tasks. Existing\nworks propose to augment LLMs with individual text units retrieved from\nexternal knowledge corpora to alleviate the issue. However, in many domains,\ntexts are interconnected (e.g., academic papers in a bibliographic graph are\nlinked by citations and co-authorships) which form a (text-attributed) graph.\nThe knowledge in such graphs is encoded not only in single texts/nodes but also\nin their associated connections. To facilitate the research of augmenting LLMs\nwith graphs, we manually construct a Graph Reasoning Benchmark dataset called\nGRBench, containing 1,740 questions that can be answered with the knowledge\nfrom 10 domain graphs. Then, we propose a simple and effective framework called\nGraph Chain-of-thought (Graph-CoT) to augment LLMs with graphs by encouraging\nLLMs to reason on the graph iteratively. Each Graph-CoT iteration consists of\nthree sub-steps: LLM reasoning, LLM-graph interaction, and graph execution. We\nconduct systematic experiments with three LLM backbones on GRBench, where\nGraph-CoT outperforms the baselines consistently. The code is available at\nhttps://github.com/PeterGriffinJin/Graph-CoT."
                },
                "authors": [
                    {
                        "name": "Bowen Jin"
                    },
                    {
                        "name": "Chulin Xie"
                    },
                    {
                        "name": "Jiawei Zhang"
                    },
                    {
                        "name": "Kashob Kumar Roy"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Ruirui Li"
                    },
                    {
                        "name": "Xianfeng Tang"
                    },
                    {
                        "name": "Suhang Wang"
                    },
                    {
                        "name": "Yu Meng"
                    },
                    {
                        "name": "Jiawei Han"
                    }
                ],
                "author_detail": {
                    "name": "Jiawei Han"
                },
                "author": "Jiawei Han",
                "arxiv_comment": "21 pages. Code: https://github.com/PeterGriffinJin/Graph-CoT",
                "arxiv_journal_ref": "ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.07103v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.07103v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.13448v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.13448v2",
                "updated": "2024-10-03T13:53:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    13,
                    53,
                    59,
                    3,
                    277,
                    0
                ],
                "published": "2024-05-22T08:38:26Z",
                "published_parsed": [
                    2024,
                    5,
                    22,
                    8,
                    38,
                    26,
                    2,
                    143,
                    0
                ],
                "title": "Distilling Instruction-following Abilities of Large Language Models with\n  Task-aware Curriculum Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilling Instruction-following Abilities of Large Language Models with\n  Task-aware Curriculum Planning"
                },
                "summary": "Instruction tuning aims to align large language models (LLMs) with\nopen-domain instructions and human-preferred responses. While several studies\nhave explored autonomous approaches to distilling and annotating instructions\nfrom powerful proprietary LLMs, such as ChatGPT, they often neglect the impact\nof the distributions and characteristics of tasks, together with the varying\ndifficulty of instructions in training sets. This oversight can lead to\nimbalanced knowledge capabilities and poor generalization powers of student\nLLMs. To address these challenges, we introduce Task-Aware Curriculum Planning\nfor Instruction Refinement (TAPIR), a multi-round distillation framework that\nutilizes an oracle LLM to select instructions that are difficult for a student\nLLM to follow. To balance the student's capabilities, task distributions in\ntraining sets are adjusted with responses automatically refined according to\ntheir corresponding tasks. In addition, by incorporating curriculum planning,\nour approach systematically escalates the difficulty levels of tasks,\nprogressively enhancing the student LLM's capabilities. We rigorously evaluate\nTAPIR using several widely recognized benchmarks (such as AlpacaEval 2.0,\nMT-Bench, etc.) and multiple student LLMs. Empirical results demonstrate that\nstudent LLMs, trained with our method and less training data, outperform larger\ninstruction-tuned models and strong distillation baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction tuning aims to align large language models (LLMs) with\nopen-domain instructions and human-preferred responses. While several studies\nhave explored autonomous approaches to distilling and annotating instructions\nfrom powerful proprietary LLMs, such as ChatGPT, they often neglect the impact\nof the distributions and characteristics of tasks, together with the varying\ndifficulty of instructions in training sets. This oversight can lead to\nimbalanced knowledge capabilities and poor generalization powers of student\nLLMs. To address these challenges, we introduce Task-Aware Curriculum Planning\nfor Instruction Refinement (TAPIR), a multi-round distillation framework that\nutilizes an oracle LLM to select instructions that are difficult for a student\nLLM to follow. To balance the student's capabilities, task distributions in\ntraining sets are adjusted with responses automatically refined according to\ntheir corresponding tasks. In addition, by incorporating curriculum planning,\nour approach systematically escalates the difficulty levels of tasks,\nprogressively enhancing the student LLM's capabilities. We rigorously evaluate\nTAPIR using several widely recognized benchmarks (such as AlpacaEval 2.0,\nMT-Bench, etc.) and multiple student LLMs. Empirical results demonstrate that\nstudent LLMs, trained with our method and less training data, outperform larger\ninstruction-tuned models and strong distillation baselines."
                },
                "authors": [
                    {
                        "name": "Yuanhao Yue"
                    },
                    {
                        "name": "Chengyu Wang"
                    },
                    {
                        "name": "Jun Huang"
                    },
                    {
                        "name": "Peng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Peng Wang"
                },
                "author": "Peng Wang",
                "arxiv_comment": "emnlp 2024 findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.13448v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.13448v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04069v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04069v2",
                "updated": "2024-10-03T13:51:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    13,
                    51,
                    53,
                    3,
                    277,
                    0
                ],
                "published": "2024-07-04T17:15:37Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    17,
                    15,
                    37,
                    3,
                    186,
                    0
                ],
                "title": "A Systematic Survey and Critical Review on Evaluating Large Language\n  Models: Challenges, Limitations, and Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Systematic Survey and Critical Review on Evaluating Large Language\n  Models: Challenges, Limitations, and Recommendations"
                },
                "summary": "Large Language Models (LLMs) have recently gained significant attention due\nto their remarkable capabilities in performing diverse tasks across various\ndomains. However, a thorough evaluation of these models is crucial before\ndeploying them in real-world applications to ensure they produce reliable\nperformance. Despite the well-established importance of evaluating LLMs in the\ncommunity, the complexity of the evaluation process has led to varied\nevaluation setups, causing inconsistencies in findings and interpretations. To\naddress this, we systematically review the primary challenges and limitations\ncausing these inconsistencies and unreliable evaluations in various steps of\nLLM evaluation. Based on our critical review, we present our perspectives and\nrecommendations to ensure LLM evaluations are reproducible, reliable, and\nrobust.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently gained significant attention due\nto their remarkable capabilities in performing diverse tasks across various\ndomains. However, a thorough evaluation of these models is crucial before\ndeploying them in real-world applications to ensure they produce reliable\nperformance. Despite the well-established importance of evaluating LLMs in the\ncommunity, the complexity of the evaluation process has led to varied\nevaluation setups, causing inconsistencies in findings and interpretations. To\naddress this, we systematically review the primary challenges and limitations\ncausing these inconsistencies and unreliable evaluations in various steps of\nLLM evaluation. Based on our critical review, we present our perspectives and\nrecommendations to ensure LLM evaluations are reproducible, reliable, and\nrobust."
                },
                "authors": [
                    {
                        "name": "Md Tahmid Rahman Laskar"
                    },
                    {
                        "name": "Sawsan Alqahtani"
                    },
                    {
                        "name": "M Saiful Bari"
                    },
                    {
                        "name": "Mizanur Rahman"
                    },
                    {
                        "name": "Mohammad Abdullah Matin Khan"
                    },
                    {
                        "name": "Haidar Khan"
                    },
                    {
                        "name": "Israt Jahan"
                    },
                    {
                        "name": "Amran Bhuiyan"
                    },
                    {
                        "name": "Chee Wei Tan"
                    },
                    {
                        "name": "Md Rizwan Parvez"
                    },
                    {
                        "name": "Enamul Hoque"
                    },
                    {
                        "name": "Shafiq Joty"
                    },
                    {
                        "name": "Jimmy Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jimmy Huang"
                },
                "author": "Jimmy Huang",
                "arxiv_comment": "Accepted at EMNLP 2024 (Main Conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.04069v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04069v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02486v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02486v1",
                "updated": "2024-10-03T13:48:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    13,
                    48,
                    35,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T13:48:35Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    13,
                    48,
                    35,
                    3,
                    277,
                    0
                ],
                "title": "Encryption-Friendly LLM Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Encryption-Friendly LLM Architecture"
                },
                "summary": "Large language models (LLMs) offer personalized responses based on user\ninteractions, but this use case raises serious privacy concerns. Homomorphic\nencryption (HE) is a cryptographic protocol supporting arithmetic computations\nin encrypted states and provides a potential solution for privacy-preserving\nmachine learning (PPML). However, the computational intensity of transformers\nposes challenges for applying HE to LLMs. In this work, we propose a modified\nHE-friendly transformer architecture with an emphasis on inference following\npersonalized (private) fine-tuning. Utilizing LoRA fine-tuning and Gaussian\nkernels, we achieve significant computational speedups -- 6.94x for fine-tuning\nand 2.3x for inference -- while maintaining performance comparable to plaintext\nmodels. Our findings provide a viable proof of concept for offering\nprivacy-preserving LLM services in areas where data protection is crucial.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) offer personalized responses based on user\ninteractions, but this use case raises serious privacy concerns. Homomorphic\nencryption (HE) is a cryptographic protocol supporting arithmetic computations\nin encrypted states and provides a potential solution for privacy-preserving\nmachine learning (PPML). However, the computational intensity of transformers\nposes challenges for applying HE to LLMs. In this work, we propose a modified\nHE-friendly transformer architecture with an emphasis on inference following\npersonalized (private) fine-tuning. Utilizing LoRA fine-tuning and Gaussian\nkernels, we achieve significant computational speedups -- 6.94x for fine-tuning\nand 2.3x for inference -- while maintaining performance comparable to plaintext\nmodels. Our findings provide a viable proof of concept for offering\nprivacy-preserving LLM services in areas where data protection is crucial."
                },
                "authors": [
                    {
                        "name": "Donghwan Rho"
                    },
                    {
                        "name": "Taeseong Kim"
                    },
                    {
                        "name": "Minje Park"
                    },
                    {
                        "name": "Jung Woo Kim"
                    },
                    {
                        "name": "Hyunsik Chae"
                    },
                    {
                        "name": "Jung Hee Cheon"
                    },
                    {
                        "name": "Ernest K. Ryu"
                    }
                ],
                "author_detail": {
                    "name": "Ernest K. Ryu"
                },
                "author": "Ernest K. Ryu",
                "arxiv_comment": "27 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02486v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02486v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.02783v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.02783v3",
                "updated": "2024-10-03T13:47:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    13,
                    47,
                    2,
                    3,
                    277,
                    0
                ],
                "published": "2023-12-05T14:14:27Z",
                "published_parsed": [
                    2023,
                    12,
                    5,
                    14,
                    14,
                    27,
                    1,
                    339,
                    0
                ],
                "title": "Large Language Models on Graphs: A Comprehensive Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models on Graphs: A Comprehensive Survey"
                },
                "summary": "Large language models (LLMs), such as GPT4 and LLaMA, are creating\nsignificant advancements in natural language processing, due to their strong\ntext encoding/decoding ability and newly found emergent capability (e.g.,\nreasoning). While LLMs are mainly designed to process pure texts, there are\nmany real-world scenarios where text data is associated with rich structure\ninformation in the form of graphs (e.g., academic networks, and e-commerce\nnetworks) or scenarios where graph data is paired with rich textual information\n(e.g., molecules with descriptions). Besides, although LLMs have shown their\npure text-based reasoning ability, it is underexplored whether such ability can\nbe generalized to graphs (i.e., graph-based reasoning). In this paper, we\nprovide a systematic review of scenarios and techniques related to large\nlanguage models on graphs. We first summarize potential scenarios of adopting\nLLMs on graphs into three categories, namely pure graphs, text-attributed\ngraphs, and text-paired graphs. We then discuss detailed techniques for\nutilizing LLMs on graphs, including LLM as Predictor, LLM as Encoder, and LLM\nas Aligner, and compare the advantages and disadvantages of different schools\nof models. Furthermore, we discuss the real-world applications of such methods\nand summarize open-source codes and benchmark datasets. Finally, we conclude\nwith potential future research directions in this fast-growing field. The\nrelated source can be found at\nhttps://github.com/PeterGriffinJin/Awesome-Language-Model-on-Graphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), such as GPT4 and LLaMA, are creating\nsignificant advancements in natural language processing, due to their strong\ntext encoding/decoding ability and newly found emergent capability (e.g.,\nreasoning). While LLMs are mainly designed to process pure texts, there are\nmany real-world scenarios where text data is associated with rich structure\ninformation in the form of graphs (e.g., academic networks, and e-commerce\nnetworks) or scenarios where graph data is paired with rich textual information\n(e.g., molecules with descriptions). Besides, although LLMs have shown their\npure text-based reasoning ability, it is underexplored whether such ability can\nbe generalized to graphs (i.e., graph-based reasoning). In this paper, we\nprovide a systematic review of scenarios and techniques related to large\nlanguage models on graphs. We first summarize potential scenarios of adopting\nLLMs on graphs into three categories, namely pure graphs, text-attributed\ngraphs, and text-paired graphs. We then discuss detailed techniques for\nutilizing LLMs on graphs, including LLM as Predictor, LLM as Encoder, and LLM\nas Aligner, and compare the advantages and disadvantages of different schools\nof models. Furthermore, we discuss the real-world applications of such methods\nand summarize open-source codes and benchmark datasets. Finally, we conclude\nwith potential future research directions in this fast-growing field. The\nrelated source can be found at\nhttps://github.com/PeterGriffinJin/Awesome-Language-Model-on-Graphs."
                },
                "authors": [
                    {
                        "name": "Bowen Jin"
                    },
                    {
                        "name": "Gang Liu"
                    },
                    {
                        "name": "Chi Han"
                    },
                    {
                        "name": "Meng Jiang"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Jiawei Han"
                    }
                ],
                "author_detail": {
                    "name": "Jiawei Han"
                },
                "author": "Jiawei Han",
                "arxiv_comment": "25 pages",
                "arxiv_journal_ref": "Transactions on Knowledge and Data Engineering (TKDE) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.02783v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.02783v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.16332v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.16332v4",
                "updated": "2024-10-03T13:40:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    13,
                    40,
                    39,
                    3,
                    277,
                    0
                ],
                "published": "2024-01-29T17:38:14Z",
                "published_parsed": [
                    2024,
                    1,
                    29,
                    17,
                    38,
                    14,
                    0,
                    29,
                    0
                ],
                "title": "Tradeoffs Between Alignment and Helpfulness in Language Models with\n  Representation Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tradeoffs Between Alignment and Helpfulness in Language Models with\n  Representation Engineering"
                },
                "summary": "Language model alignment has become an important component of AI safety,\nallowing safe interactions between humans and language models, by enhancing\ndesired behaviors and inhibiting undesired ones. It is often done by tuning the\nmodel or inserting preset aligning prompts. Recently, representation\nengineering, a method which alters the model's behavior via changing its\nrepresentations post-training, was shown to be effective in aligning LLMs (Zou\net al., 2023a). Representation engineering yields gains in alignment oriented\ntasks such as resistance to adversarial attacks and reduction of social biases,\nbut was also shown to cause a decrease in the ability of the model to perform\nbasic tasks. In this paper we study the tradeoff between the increase in\nalignment and decrease in helpfulness of the model. We propose a theoretical\nframework which provides bounds for these two quantities, and demonstrate their\nrelevance empirically. First, we find that under the conditions of our\nframework, alignment can be guaranteed with representation engineering, and at\nthe same time that helpfulness is harmed in the process. Second, we show that\nhelpfulness is harmed quadratically with the norm of the representation\nengineering vector, while the alignment increases linearly with it, indicating\na regime in which it is efficient to use representation engineering. We\nvalidate our findings empirically, and chart the boundaries to the usefulness\nof representation engineering for alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language model alignment has become an important component of AI safety,\nallowing safe interactions between humans and language models, by enhancing\ndesired behaviors and inhibiting undesired ones. It is often done by tuning the\nmodel or inserting preset aligning prompts. Recently, representation\nengineering, a method which alters the model's behavior via changing its\nrepresentations post-training, was shown to be effective in aligning LLMs (Zou\net al., 2023a). Representation engineering yields gains in alignment oriented\ntasks such as resistance to adversarial attacks and reduction of social biases,\nbut was also shown to cause a decrease in the ability of the model to perform\nbasic tasks. In this paper we study the tradeoff between the increase in\nalignment and decrease in helpfulness of the model. We propose a theoretical\nframework which provides bounds for these two quantities, and demonstrate their\nrelevance empirically. First, we find that under the conditions of our\nframework, alignment can be guaranteed with representation engineering, and at\nthe same time that helpfulness is harmed in the process. Second, we show that\nhelpfulness is harmed quadratically with the norm of the representation\nengineering vector, while the alignment increases linearly with it, indicating\na regime in which it is efficient to use representation engineering. We\nvalidate our findings empirically, and chart the boundaries to the usefulness\nof representation engineering for alignment."
                },
                "authors": [
                    {
                        "name": "Yotam Wolf"
                    },
                    {
                        "name": "Noam Wies"
                    },
                    {
                        "name": "Dorin Shteyman"
                    },
                    {
                        "name": "Binyamin Rothberg"
                    },
                    {
                        "name": "Yoav Levine"
                    },
                    {
                        "name": "Amnon Shashua"
                    }
                ],
                "author_detail": {
                    "name": "Amnon Shashua"
                },
                "author": "Amnon Shashua",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.16332v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.16332v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.17041v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.17041v4",
                "updated": "2024-10-03T13:31:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    13,
                    31,
                    39,
                    3,
                    277,
                    0
                ],
                "published": "2023-11-28T18:53:06Z",
                "published_parsed": [
                    2023,
                    11,
                    28,
                    18,
                    53,
                    6,
                    1,
                    332,
                    0
                ],
                "title": "Eliciting In-Context Learning in Vision-Language Models for Videos\n  Through Curated Data Distributional Properties",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eliciting In-Context Learning in Vision-Language Models for Videos\n  Through Curated Data Distributional Properties"
                },
                "summary": "A major reason behind the recent success of large language models (LLMs) is\ntheir \\textit{in-context learning} capability, which makes it possible to\nrapidly adapt them to downstream text-based tasks by prompting them with a\nsmall number of relevant demonstrations. While large vision-language models\n(VLMs) have recently been developed for tasks requiring both text and images,\nthey largely lack in-context learning over visual information, especially in\nunderstanding and generating text about videos. In this work, we implement\n\\textbf{E}mergent \\textbf{I}n-context \\textbf{Le}arning on \\textbf{V}ideos\n(\\eilev{}), a novel training paradigm that induces in-context learning over\nvideo and text by capturing key properties of pre-training data found by prior\nwork to be essential for in-context learning in transformers. In our\nexperiments, we show that \\eilev-trained models outperform other off-the-shelf\nVLMs in few-shot video narration for novel, rare actions. Furthermore, we\ndemonstrate that these key properties of bursty distributions, skewed marginal\ndistributions, and dynamic meaning each contribute to varying degrees to VLMs'\nin-context learning capability in narrating procedural videos. Our results,\nanalysis, and \\eilev{}-trained models yield numerous insights about the\nemergence of in-context learning over video and text, creating a foundation for\nfuture work to optimize and scale VLMs for open-domain video understanding and\nreasoning. Our code and demo are available at\n\\url{https://github.com/yukw777/EILEV}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A major reason behind the recent success of large language models (LLMs) is\ntheir \\textit{in-context learning} capability, which makes it possible to\nrapidly adapt them to downstream text-based tasks by prompting them with a\nsmall number of relevant demonstrations. While large vision-language models\n(VLMs) have recently been developed for tasks requiring both text and images,\nthey largely lack in-context learning over visual information, especially in\nunderstanding and generating text about videos. In this work, we implement\n\\textbf{E}mergent \\textbf{I}n-context \\textbf{Le}arning on \\textbf{V}ideos\n(\\eilev{}), a novel training paradigm that induces in-context learning over\nvideo and text by capturing key properties of pre-training data found by prior\nwork to be essential for in-context learning in transformers. In our\nexperiments, we show that \\eilev-trained models outperform other off-the-shelf\nVLMs in few-shot video narration for novel, rare actions. Furthermore, we\ndemonstrate that these key properties of bursty distributions, skewed marginal\ndistributions, and dynamic meaning each contribute to varying degrees to VLMs'\nin-context learning capability in narrating procedural videos. Our results,\nanalysis, and \\eilev{}-trained models yield numerous insights about the\nemergence of in-context learning over video and text, creating a foundation for\nfuture work to optimize and scale VLMs for open-domain video understanding and\nreasoning. Our code and demo are available at\n\\url{https://github.com/yukw777/EILEV}."
                },
                "authors": [
                    {
                        "name": "Keunwoo Peter Yu"
                    },
                    {
                        "name": "Zheyuan Zhang"
                    },
                    {
                        "name": "Fengyuan Hu"
                    },
                    {
                        "name": "Shane Storks"
                    },
                    {
                        "name": "Joyce Chai"
                    }
                ],
                "author_detail": {
                    "name": "Joyce Chai"
                },
                "author": "Joyce Chai",
                "arxiv_comment": "16 pages, LaTeX; Accepted to EMNLP 2024 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.17041v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.17041v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02472v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02472v1",
                "updated": "2024-10-03T13:25:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    13,
                    25,
                    15,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T13:25:15Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    13,
                    25,
                    15,
                    3,
                    277,
                    0
                ],
                "title": "Meta-Models: An Architecture for Decoding LLM Behaviors Through\n  Interpreted Embeddings and Natural Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meta-Models: An Architecture for Decoding LLM Behaviors Through\n  Interpreted Embeddings and Natural Language"
                },
                "summary": "As Large Language Models (LLMs) become increasingly integrated into our daily\nlives, the potential harms from deceptive behavior underlie the need for\nfaithfully interpreting their decision-making. While traditional probing\nmethods have shown some effectiveness, they remain best for narrowly scoped\ntasks while more comprehensive explanations are still necessary. To this end,\nwe investigate meta-models-an architecture using a \"meta-model\" that takes\nactivations from an \"input-model\" and answers natural language questions about\nthe input-model's behaviors. We evaluate the meta-model's ability to generalize\nby training them on selected task types and assessing their out-of-distribution\nperformance in deceptive scenarios. Our findings show that meta-models\ngeneralize well to out-of-distribution tasks and point towards opportunities\nfor future research in this area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) become increasingly integrated into our daily\nlives, the potential harms from deceptive behavior underlie the need for\nfaithfully interpreting their decision-making. While traditional probing\nmethods have shown some effectiveness, they remain best for narrowly scoped\ntasks while more comprehensive explanations are still necessary. To this end,\nwe investigate meta-models-an architecture using a \"meta-model\" that takes\nactivations from an \"input-model\" and answers natural language questions about\nthe input-model's behaviors. We evaluate the meta-model's ability to generalize\nby training them on selected task types and assessing their out-of-distribution\nperformance in deceptive scenarios. Our findings show that meta-models\ngeneralize well to out-of-distribution tasks and point towards opportunities\nfor future research in this area."
                },
                "authors": [
                    {
                        "name": "Anthony Costarelli"
                    },
                    {
                        "name": "Mat Allen"
                    },
                    {
                        "name": "Severin Field"
                    },
                    {
                        "name": "Joshua Clymer"
                    }
                ],
                "author_detail": {
                    "name": "Joshua Clymer"
                },
                "author": "Joshua Clymer",
                "arxiv_comment": "11 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02472v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02472v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.15206v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.15206v3",
                "updated": "2024-10-03T13:23:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    13,
                    23,
                    59,
                    3,
                    277,
                    0
                ],
                "published": "2024-04-23T16:39:03Z",
                "published_parsed": [
                    2024,
                    4,
                    23,
                    16,
                    39,
                    3,
                    1,
                    114,
                    0
                ],
                "title": "Does Instruction Tuning Make LLMs More Consistent?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does Instruction Tuning Make LLMs More Consistent?"
                },
                "summary": "The purpose of instruction tuning is enabling zero-shot performance, but\ninstruction tuning has also been shown to improve chain-of-thought reasoning\nand value alignment (Si et al., 2023). Here we consider the impact on\n$\\textit{consistency}$, i.e., the sensitivity of language models to small\nperturbations in the input. We compare 10 instruction-tuned LLaMA models to the\noriginal LLaMA-7b model and show that almost across-the-board they become more\nconsistent, both in terms of their representations and their predictions in\nzero-shot and downstream tasks. We explain these improvements through\nmechanistic analyses of factual recall.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The purpose of instruction tuning is enabling zero-shot performance, but\ninstruction tuning has also been shown to improve chain-of-thought reasoning\nand value alignment (Si et al., 2023). Here we consider the impact on\n$\\textit{consistency}$, i.e., the sensitivity of language models to small\nperturbations in the input. We compare 10 instruction-tuned LLaMA models to the\noriginal LLaMA-7b model and show that almost across-the-board they become more\nconsistent, both in terms of their representations and their predictions in\nzero-shot and downstream tasks. We explain these improvements through\nmechanistic analyses of factual recall."
                },
                "authors": [
                    {
                        "name": "Constanza Fierro"
                    },
                    {
                        "name": "Jiaang Li"
                    },
                    {
                        "name": "Anders Sgaard"
                    }
                ],
                "author_detail": {
                    "name": "Anders Sgaard"
                },
                "author": "Anders Sgaard",
                "arxiv_comment": "We need to run extra experiments to ensure some of the claims in the\n  paper are fully correct",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.15206v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.15206v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02465v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02465v1",
                "updated": "2024-10-03T13:15:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    13,
                    15,
                    19,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T13:15:19Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    13,
                    15,
                    19,
                    3,
                    277,
                    0
                ],
                "title": "Response Tuning: Aligning Large Language Models without Instruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Response Tuning: Aligning Large Language Models without Instruction"
                },
                "summary": "Instruction tuning-supervised fine-tuning using instruction-response pairs-is\na foundational step in transitioning pre-trained Large Language Models (LLMs)\ninto helpful and safe chat assistants. Our hypothesis is that establishing an\nadequate output space can enable such a transition given the capabilities\ninherent in pre-trained LLMs. To verify this, we propose Response Tuning (RT),\nwhich eliminates the instruction-conditioning step in instruction tuning and\nsolely focuses on response space supervision. Our experiments demonstrate that\nRT models, trained only using responses, can effectively respond to a wide\nrange of instructions and exhibit helpfulness comparable to that of their\ninstruction-tuned counterparts. Furthermore, we observe that controlling the\ntraining response distribution can significantly improve their user preference\nor elicit target behaviors such as refusing assistance for unsafe queries. Our\nfindings illuminate the role of establishing an adequate output space in\nalignment, highlighting the potential of the extensive inherent capabilities of\npre-trained LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction tuning-supervised fine-tuning using instruction-response pairs-is\na foundational step in transitioning pre-trained Large Language Models (LLMs)\ninto helpful and safe chat assistants. Our hypothesis is that establishing an\nadequate output space can enable such a transition given the capabilities\ninherent in pre-trained LLMs. To verify this, we propose Response Tuning (RT),\nwhich eliminates the instruction-conditioning step in instruction tuning and\nsolely focuses on response space supervision. Our experiments demonstrate that\nRT models, trained only using responses, can effectively respond to a wide\nrange of instructions and exhibit helpfulness comparable to that of their\ninstruction-tuned counterparts. Furthermore, we observe that controlling the\ntraining response distribution can significantly improve their user preference\nor elicit target behaviors such as refusing assistance for unsafe queries. Our\nfindings illuminate the role of establishing an adequate output space in\nalignment, highlighting the potential of the extensive inherent capabilities of\npre-trained LLMs."
                },
                "authors": [
                    {
                        "name": "Seokhyun An"
                    },
                    {
                        "name": "Hyounghun Kim"
                    }
                ],
                "author_detail": {
                    "name": "Hyounghun Kim"
                },
                "author": "Hyounghun Kim",
                "arxiv_comment": "34 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02465v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02465v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01242v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01242v2",
                "updated": "2024-10-03T13:12:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    13,
                    12,
                    24,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-02T05:07:02Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    5,
                    7,
                    2,
                    2,
                    276,
                    0
                ],
                "title": "RGD: Multi-LLM Based Agent Debugger via Refinement and Generation\n  Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RGD: Multi-LLM Based Agent Debugger via Refinement and Generation\n  Guidance"
                },
                "summary": "Large Language Models (LLMs) have shown incredible potential in code\ngeneration tasks, and recent research in prompt engineering have enhanced LLMs'\nunderstanding of textual information. However, ensuring the accuracy of\ngenerated code often requires extensive testing and validation by programmers.\nWhile LLMs can typically generate code based on task descriptions, their\naccuracy remains limited, especially for complex tasks that require a deeper\nunderstanding of both the problem statement and the code generation process.\nThis limitation is primarily due to the LLMs' need to simultaneously comprehend\ntext and generate syntactically and semantically correct code, without having\nthe capability to automatically refine the code. In real-world software\ndevelopment, programmers rarely produce flawless code in a single attempt based\non the task description alone, they rely on iterative feedback and debugging to\nrefine their programs. Inspired by this process, we introduce a novel\narchitecture of LLM-based agents for code generation and automatic debugging:\nRefinement and Guidance Debugging (RGD). The RGD framework is a multi-LLM-based\nagent debugger that leverages three distinct LLM agents-Guide Agent, Debug\nAgent, and Feedback Agent. RGD decomposes the code generation task into\nmultiple steps, ensuring a clearer workflow and enabling iterative code\nrefinement based on self-reflection and feedback. Experimental results\ndemonstrate that RGD exhibits remarkable code generation capabilities,\nachieving state-of-the-art performance with a 9.8% improvement on the HumanEval\ndataset and a 16.2% improvement on the MBPP dataset compared to the\nstate-of-the-art approaches and traditional direct prompting approaches. We\nhighlight the effectiveness of the RGD framework in enhancing LLMs' ability to\ngenerate and refine code autonomously.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown incredible potential in code\ngeneration tasks, and recent research in prompt engineering have enhanced LLMs'\nunderstanding of textual information. However, ensuring the accuracy of\ngenerated code often requires extensive testing and validation by programmers.\nWhile LLMs can typically generate code based on task descriptions, their\naccuracy remains limited, especially for complex tasks that require a deeper\nunderstanding of both the problem statement and the code generation process.\nThis limitation is primarily due to the LLMs' need to simultaneously comprehend\ntext and generate syntactically and semantically correct code, without having\nthe capability to automatically refine the code. In real-world software\ndevelopment, programmers rarely produce flawless code in a single attempt based\non the task description alone, they rely on iterative feedback and debugging to\nrefine their programs. Inspired by this process, we introduce a novel\narchitecture of LLM-based agents for code generation and automatic debugging:\nRefinement and Guidance Debugging (RGD). The RGD framework is a multi-LLM-based\nagent debugger that leverages three distinct LLM agents-Guide Agent, Debug\nAgent, and Feedback Agent. RGD decomposes the code generation task into\nmultiple steps, ensuring a clearer workflow and enabling iterative code\nrefinement based on self-reflection and feedback. Experimental results\ndemonstrate that RGD exhibits remarkable code generation capabilities,\nachieving state-of-the-art performance with a 9.8% improvement on the HumanEval\ndataset and a 16.2% improvement on the MBPP dataset compared to the\nstate-of-the-art approaches and traditional direct prompting approaches. We\nhighlight the effectiveness of the RGD framework in enhancing LLMs' ability to\ngenerate and refine code autonomously."
                },
                "authors": [
                    {
                        "name": "Haolin Jin"
                    },
                    {
                        "name": "Zechao Sun"
                    },
                    {
                        "name": "Huaming Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huaming Chen"
                },
                "author": "Huaming Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01242v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01242v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.19999v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.19999v2",
                "updated": "2024-10-03T13:02:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    13,
                    2,
                    11,
                    3,
                    277,
                    0
                ],
                "published": "2024-06-28T15:34:26Z",
                "published_parsed": [
                    2024,
                    6,
                    28,
                    15,
                    34,
                    26,
                    4,
                    180,
                    0
                ],
                "title": "The SIFo Benchmark: Investigating the Sequential Instruction Following\n  Ability of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The SIFo Benchmark: Investigating the Sequential Instruction Following\n  Ability of Large Language Models"
                },
                "summary": "Following multiple instructions is a crucial ability for large language\nmodels (LLMs). Evaluating this ability comes with significant challenges: (i)\nlimited coherence between multiple instructions, (ii) positional bias where the\norder of instructions affects model performance, and (iii) a lack of\nobjectively verifiable tasks. To address these issues, we introduce a benchmark\ndesigned to evaluate models' abilities to follow multiple instructions through\nsequential instruction following (SIFo) tasks. In SIFo, the successful\ncompletion of multiple instructions is verifiable by examining only the final\ninstruction. Our benchmark evaluates instruction following using four tasks\n(text modification, question answering, mathematics, and security rules), each\nassessing different aspects of sequential instruction following. Our evaluation\nof popular LLMs, both closed-source and open-source, shows that more recent and\nlarger models significantly outperform their older and smaller counterparts on\nthe SIFo tasks, validating the benchmark's effectiveness. All models struggle\nwith following sequences of instructions, hinting at an important lack of\nrobustness of today's language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Following multiple instructions is a crucial ability for large language\nmodels (LLMs). Evaluating this ability comes with significant challenges: (i)\nlimited coherence between multiple instructions, (ii) positional bias where the\norder of instructions affects model performance, and (iii) a lack of\nobjectively verifiable tasks. To address these issues, we introduce a benchmark\ndesigned to evaluate models' abilities to follow multiple instructions through\nsequential instruction following (SIFo) tasks. In SIFo, the successful\ncompletion of multiple instructions is verifiable by examining only the final\ninstruction. Our benchmark evaluates instruction following using four tasks\n(text modification, question answering, mathematics, and security rules), each\nassessing different aspects of sequential instruction following. Our evaluation\nof popular LLMs, both closed-source and open-source, shows that more recent and\nlarger models significantly outperform their older and smaller counterparts on\nthe SIFo tasks, validating the benchmark's effectiveness. All models struggle\nwith following sequences of instructions, hinting at an important lack of\nrobustness of today's language models."
                },
                "authors": [
                    {
                        "name": "Xinyi Chen"
                    },
                    {
                        "name": "Baohao Liao"
                    },
                    {
                        "name": "Jirui Qi"
                    },
                    {
                        "name": "Panagiotis Eustratiadis"
                    },
                    {
                        "name": "Christof Monz"
                    },
                    {
                        "name": "Arianna Bisazza"
                    },
                    {
                        "name": "Maarten de Rijke"
                    }
                ],
                "author_detail": {
                    "name": "Maarten de Rijke"
                },
                "author": "Maarten de Rijke",
                "arxiv_comment": "EMNLP 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.19999v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.19999v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02451v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02451v1",
                "updated": "2024-10-03T12:53:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    12,
                    53,
                    43,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T12:53:43Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    12,
                    53,
                    43,
                    3,
                    277,
                    0
                ],
                "title": "Strong Preferences Affect the Robustness of Value Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strong Preferences Affect the Robustness of Value Alignment"
                },
                "summary": "Value alignment, which aims to ensure that large language models (LLMs) and\nother AI agents behave in accordance with human values, is critical for\nensuring safety and trustworthiness of these systems. A key component of value\nalignment is the modeling of human preferences as a representation of human\nvalues. In this paper, we investigate the robustness of value alignment by\nexamining the sensitivity of preference models. Specifically, we ask: how do\nchanges in the probabilities of some preferences affect the predictions of\nthese models for other preferences? To answer this question, we theoretically\nanalyze the robustness of widely used preference models by examining their\nsensitivities to minor changes in preferences they model. Our findings reveal\nthat, in the Bradley-Terry and the Placket-Luce model, the probability of a\npreference can change significantly as other preferences change, especially\nwhen these preferences are dominant (i.e., with probabilities near 0 or 1). We\nidentify specific conditions where this sensitivity becomes significant for\nthese models and discuss the practical implications for the robustness and\nsafety of value alignment in AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value alignment, which aims to ensure that large language models (LLMs) and\nother AI agents behave in accordance with human values, is critical for\nensuring safety and trustworthiness of these systems. A key component of value\nalignment is the modeling of human preferences as a representation of human\nvalues. In this paper, we investigate the robustness of value alignment by\nexamining the sensitivity of preference models. Specifically, we ask: how do\nchanges in the probabilities of some preferences affect the predictions of\nthese models for other preferences? To answer this question, we theoretically\nanalyze the robustness of widely used preference models by examining their\nsensitivities to minor changes in preferences they model. Our findings reveal\nthat, in the Bradley-Terry and the Placket-Luce model, the probability of a\npreference can change significantly as other preferences change, especially\nwhen these preferences are dominant (i.e., with probabilities near 0 or 1). We\nidentify specific conditions where this sensitivity becomes significant for\nthese models and discuss the practical implications for the robustness and\nsafety of value alignment in AI systems."
                },
                "authors": [
                    {
                        "name": "Ziwei Xu"
                    },
                    {
                        "name": "Mohan Kankanhalli"
                    }
                ],
                "author_detail": {
                    "name": "Mohan Kankanhalli"
                },
                "author": "Mohan Kankanhalli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02451v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02451v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02440v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02440v1",
                "updated": "2024-10-03T12:37:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    12,
                    37,
                    39,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T12:37:39Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    12,
                    37,
                    39,
                    3,
                    277,
                    0
                ],
                "title": "Optimizing Adaptive Attacks against Content Watermarks for Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Adaptive Attacks against Content Watermarks for Language\n  Models"
                },
                "summary": "Large Language Models (LLMs) can be \\emph{misused} to spread online spam and\nmisinformation. Content watermarking deters misuse by hiding a message in\nmodel-generated outputs, enabling their detection using a secret watermarking\nkey. Robustness is a core security property, stating that evading detection\nrequires (significant) degradation of the content's quality. Many LLM\nwatermarking methods have been proposed, but robustness is tested only against\n\\emph{non-adaptive} attackers who lack knowledge of the watermarking method and\ncan find only suboptimal attacks. We formulate the robustness of LLM\nwatermarking as an objective function and propose preference-based optimization\nto tune \\emph{adaptive} attacks against the specific watermarking method. Our\nevaluation shows that (i) adaptive attacks substantially outperform\nnon-adaptive baselines. (ii) Even in a non-adaptive setting, adaptive attacks\noptimized against a few known watermarks remain highly effective when tested\nagainst other unseen watermarks, and (iii) optimization-based attacks are\npractical and require less than seven GPU hours. Our findings underscore the\nneed to test robustness against adaptive attackers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can be \\emph{misused} to spread online spam and\nmisinformation. Content watermarking deters misuse by hiding a message in\nmodel-generated outputs, enabling their detection using a secret watermarking\nkey. Robustness is a core security property, stating that evading detection\nrequires (significant) degradation of the content's quality. Many LLM\nwatermarking methods have been proposed, but robustness is tested only against\n\\emph{non-adaptive} attackers who lack knowledge of the watermarking method and\ncan find only suboptimal attacks. We formulate the robustness of LLM\nwatermarking as an objective function and propose preference-based optimization\nto tune \\emph{adaptive} attacks against the specific watermarking method. Our\nevaluation shows that (i) adaptive attacks substantially outperform\nnon-adaptive baselines. (ii) Even in a non-adaptive setting, adaptive attacks\noptimized against a few known watermarks remain highly effective when tested\nagainst other unseen watermarks, and (iii) optimization-based attacks are\npractical and require less than seven GPU hours. Our findings underscore the\nneed to test robustness against adaptive attackers."
                },
                "authors": [
                    {
                        "name": "Abdulrahman Diaa"
                    },
                    {
                        "name": "Toluwani Aremu"
                    },
                    {
                        "name": "Nils Lukas"
                    }
                ],
                "author_detail": {
                    "name": "Nils Lukas"
                },
                "author": "Nils Lukas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02440v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02440v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02429v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02429v1",
                "updated": "2024-10-03T12:24:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    12,
                    24,
                    18,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T12:24:18Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    12,
                    24,
                    18,
                    3,
                    277,
                    0
                ],
                "title": "IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language\n  Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\ntextual and visual domains but often generate outputs that violate physical\nlaws, revealing a gap in their understanding of the physical world. Inspired by\nhuman cognition, where perception is fundamental to reasoning, we explore\naugmenting LLMs with enhanced perception abilities using Internet of Things\n(IoT) sensor data and pertinent knowledge for IoT task reasoning in the\nphysical world. In this work, we systematically study LLMs capability to\naddress real-world IoT tasks by augmenting their perception and knowledge base,\nand then propose a unified framework, IoT-LLM, to enhance such capability. In\nIoT-LLM, we customize three steps for LLMs: preprocessing IoT data into formats\namenable to LLMs, activating their commonsense knowledge through\nchain-of-thought prompting and specialized role definitions, and expanding\ntheir understanding via IoT-oriented retrieval-augmented generation based on\nin-context learning. To evaluate the performance, We design a new benchmark\nwith five real-world IoT tasks with different data types and reasoning\ndifficulties and provide the benchmarking results on six open-source and\nclose-source LLMs. Experimental results demonstrate the limitations of existing\nLLMs with naive textual inputs that cannot perform these tasks effectively. We\nshow that IoT-LLM significantly enhances the performance of IoT tasks reasoning\nof LLM, such as GPT-4, achieving an average improvement of 65% across various\ntasks against previous methods. The results also showcase LLMs ability to\ncomprehend IoT data and the physical law behind data by providing a reasoning\nprocess. Limitations of our work are claimed to inspire future research in this\nnew era.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\ntextual and visual domains but often generate outputs that violate physical\nlaws, revealing a gap in their understanding of the physical world. Inspired by\nhuman cognition, where perception is fundamental to reasoning, we explore\naugmenting LLMs with enhanced perception abilities using Internet of Things\n(IoT) sensor data and pertinent knowledge for IoT task reasoning in the\nphysical world. In this work, we systematically study LLMs capability to\naddress real-world IoT tasks by augmenting their perception and knowledge base,\nand then propose a unified framework, IoT-LLM, to enhance such capability. In\nIoT-LLM, we customize three steps for LLMs: preprocessing IoT data into formats\namenable to LLMs, activating their commonsense knowledge through\nchain-of-thought prompting and specialized role definitions, and expanding\ntheir understanding via IoT-oriented retrieval-augmented generation based on\nin-context learning. To evaluate the performance, We design a new benchmark\nwith five real-world IoT tasks with different data types and reasoning\ndifficulties and provide the benchmarking results on six open-source and\nclose-source LLMs. Experimental results demonstrate the limitations of existing\nLLMs with naive textual inputs that cannot perform these tasks effectively. We\nshow that IoT-LLM significantly enhances the performance of IoT tasks reasoning\nof LLM, such as GPT-4, achieving an average improvement of 65% across various\ntasks against previous methods. The results also showcase LLMs ability to\ncomprehend IoT data and the physical law behind data by providing a reasoning\nprocess. Limitations of our work are claimed to inspire future research in this\nnew era."
                },
                "authors": [
                    {
                        "name": "Tuo An"
                    },
                    {
                        "name": "Yunjiao Zhou"
                    },
                    {
                        "name": "Han Zou"
                    },
                    {
                        "name": "Jianfei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jianfei Yang"
                },
                "author": "Jianfei Yang",
                "arxiv_comment": "21 pages, 10 figures, submitted to ICLR 2025 Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02429v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02429v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02428v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02428v1",
                "updated": "2024-10-03T12:21:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    12,
                    21,
                    17,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T12:21:17Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    12,
                    21,
                    17,
                    3,
                    277,
                    0
                ],
                "title": "Collective Critics for Creative Story Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collective Critics for Creative Story Generation"
                },
                "summary": "Generating a long story of several thousand words with narrative coherence\nusing Large Language Models (LLMs) has been a challenging task. Previous\nresearch has addressed this challenge by proposing different frameworks that\ncreate a story plan and generate a long story based on that plan. However,\nthese frameworks have been mainly focusing on maintaining narrative coherence\nin stories, often overlooking creativity in story planning and the\nexpressiveness of the stories generated from those plans, which are desirable\nproperties to captivate readers' interest. In this paper, we propose Collective\nCritics for Creative Story Generation framework (CritiCS), which is composed of\nplan refining stage (CrPlan) and story generation stage (CrText), to integrate\na collective revision mechanism that promotes those properties into long-form\nstory generation process. Specifically, in each stage, a group of LLM critics\nand one leader collaborate to incrementally refine drafts of plan and story\nthroughout multiple rounds. Extensive human evaluation shows that the CritiCS\ncan significantly enhance story creativity and reader engagement, while also\nmaintaining narrative coherence. Furthermore, the design of the framework\nallows active participation from human writers in any role within the critique\nprocess, enabling interactive human-machine collaboration in story writing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating a long story of several thousand words with narrative coherence\nusing Large Language Models (LLMs) has been a challenging task. Previous\nresearch has addressed this challenge by proposing different frameworks that\ncreate a story plan and generate a long story based on that plan. However,\nthese frameworks have been mainly focusing on maintaining narrative coherence\nin stories, often overlooking creativity in story planning and the\nexpressiveness of the stories generated from those plans, which are desirable\nproperties to captivate readers' interest. In this paper, we propose Collective\nCritics for Creative Story Generation framework (CritiCS), which is composed of\nplan refining stage (CrPlan) and story generation stage (CrText), to integrate\na collective revision mechanism that promotes those properties into long-form\nstory generation process. Specifically, in each stage, a group of LLM critics\nand one leader collaborate to incrementally refine drafts of plan and story\nthroughout multiple rounds. Extensive human evaluation shows that the CritiCS\ncan significantly enhance story creativity and reader engagement, while also\nmaintaining narrative coherence. Furthermore, the design of the framework\nallows active participation from human writers in any role within the critique\nprocess, enabling interactive human-machine collaboration in story writing."
                },
                "authors": [
                    {
                        "name": "Minwook Bae"
                    },
                    {
                        "name": "Hyounghun Kim"
                    }
                ],
                "author_detail": {
                    "name": "Hyounghun Kim"
                },
                "author": "Hyounghun Kim",
                "arxiv_comment": "EMNLP 2024 (36 pages)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02428v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02428v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02425v1",
                "updated": "2024-10-03T12:19:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    12,
                    19,
                    6,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T12:19:06Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    12,
                    19,
                    6,
                    3,
                    277,
                    0
                ],
                "title": "LLM-Pilot: Characterize and Optimize Performance of your LLM Inference\n  Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Pilot: Characterize and Optimize Performance of your LLM Inference\n  Services"
                },
                "summary": "As Large Language Models (LLMs) are rapidly growing in popularity, LLM\ninference services must be able to serve requests from thousands of users while\nsatisfying performance requirements. The performance of an LLM inference\nservice is largely determined by the hardware onto which it is deployed, but\nunderstanding of which hardware will deliver on performance requirements\nremains challenging. In this work we present LLM-Pilot - a first-of-its-kind\nsystem for characterizing and predicting performance of LLM inference services.\nLLM-Pilot performs benchmarking of LLM inference services, under a realistic\nworkload, across a variety of GPUs, and optimizes the service configuration for\neach considered GPU to maximize performance. Finally, using this\ncharacterization data, LLM-Pilot learns a predictive model, which can be used\nto recommend the most cost-effective hardware for a previously unseen LLM.\nCompared to existing methods, LLM-Pilot can deliver on performance requirements\n33% more frequently, whilst reducing costs by 60% on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) are rapidly growing in popularity, LLM\ninference services must be able to serve requests from thousands of users while\nsatisfying performance requirements. The performance of an LLM inference\nservice is largely determined by the hardware onto which it is deployed, but\nunderstanding of which hardware will deliver on performance requirements\nremains challenging. In this work we present LLM-Pilot - a first-of-its-kind\nsystem for characterizing and predicting performance of LLM inference services.\nLLM-Pilot performs benchmarking of LLM inference services, under a realistic\nworkload, across a variety of GPUs, and optimizes the service configuration for\neach considered GPU to maximize performance. Finally, using this\ncharacterization data, LLM-Pilot learns a predictive model, which can be used\nto recommend the most cost-effective hardware for a previously unseen LLM.\nCompared to existing methods, LLM-Pilot can deliver on performance requirements\n33% more frequently, whilst reducing costs by 60% on average."
                },
                "authors": [
                    {
                        "name": "Magorzata azuka"
                    },
                    {
                        "name": "Andreea Anghel"
                    },
                    {
                        "name": "Thomas Parnell"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Parnell"
                },
                "author": "Thomas Parnell",
                "arxiv_comment": "Accepted to the International Conference for High Performance\n  Computing, Networking, Storage and Analysis (SC '24)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02415v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02415v1",
                "updated": "2024-10-03T12:03:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    12,
                    3,
                    56,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T12:03:56Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    12,
                    3,
                    56,
                    3,
                    277,
                    0
                ],
                "title": "Cellular Network Densification: a System-level Analysis with IAB, NCR\n  and RIS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cellular Network Densification: a System-level Analysis with IAB, NCR\n  and RIS"
                },
                "summary": "As the number of user equipments increases in fifth generation (5G) and\nbeyond, it is desired to densify the cellular network with auxiliary nodes\nassisting the base stations. Examples of these nodes are integrated access and\nbackhaul (IAB) nodes, network-controlled repeaters (NCRs) and reconfigurable\nintelligent surfaces (RISs). In this context, this work presents a system level\noverview of these three nodes. Moreover, this work evaluates through\nsimulations the impact of network planning aiming at enhancing the performance\nof a network used to cover an outdoor sport event. We show that, in the\nconsidered scenario, in general, IAB nodes provide an improved signal to\ninterference-plus-noise ratio and throughput, compared to NCRs and RISs.\nHowever, there are situations where NCR outperforms IAB due to higher level of\ninterference caused by the latter. Finally, we show that the deployment of\nthese nodes in unmanned aerial vehicles (UAVs) also achieves performance gains\ndue to their aerial mobility. However, UAV constraints related to aerial\ndeployment may prevent these nodes from reaching results as good as the ones\nachieved by their stationary deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the number of user equipments increases in fifth generation (5G) and\nbeyond, it is desired to densify the cellular network with auxiliary nodes\nassisting the base stations. Examples of these nodes are integrated access and\nbackhaul (IAB) nodes, network-controlled repeaters (NCRs) and reconfigurable\nintelligent surfaces (RISs). In this context, this work presents a system level\noverview of these three nodes. Moreover, this work evaluates through\nsimulations the impact of network planning aiming at enhancing the performance\nof a network used to cover an outdoor sport event. We show that, in the\nconsidered scenario, in general, IAB nodes provide an improved signal to\ninterference-plus-noise ratio and throughput, compared to NCRs and RISs.\nHowever, there are situations where NCR outperforms IAB due to higher level of\ninterference caused by the latter. Finally, we show that the deployment of\nthese nodes in unmanned aerial vehicles (UAVs) also achieves performance gains\ndue to their aerial mobility. However, UAV constraints related to aerial\ndeployment may prevent these nodes from reaching results as good as the ones\nachieved by their stationary deployment."
                },
                "authors": [
                    {
                        "name": "Gabriel C. M. da Silva"
                    },
                    {
                        "name": "Victor F. Monteiro"
                    },
                    {
                        "name": "Diego A. Sousa"
                    },
                    {
                        "name": "Darlan C. Moreira"
                    },
                    {
                        "name": "Tarcisio F. Maciel"
                    },
                    {
                        "name": "Fco. Rafael M. Lima"
                    },
                    {
                        "name": "Behrooz Makki"
                    }
                ],
                "author_detail": {
                    "name": "Behrooz Makki"
                },
                "author": "Behrooz Makki",
                "arxiv_comment": "Paper submitted to IEEE Systems Journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02415v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02415v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05301v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05301v2",
                "updated": "2024-10-03T11:59:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    11,
                    59,
                    20,
                    3,
                    277,
                    0
                ],
                "published": "2024-03-08T13:31:43Z",
                "published_parsed": [
                    2024,
                    3,
                    8,
                    13,
                    31,
                    43,
                    4,
                    68,
                    0
                ],
                "title": "Wykorzystanie Rekonfigurowalnych Iinteligentnych Matryc Antenowych w\n  czu Dosyowym Sieci 5G/6G Wykorzystujcej Bezzaogowe\n  Statki Powietrzne",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wykorzystanie Rekonfigurowalnych Iinteligentnych Matryc Antenowych w\n  czu Dosyowym Sieci 5G/6G Wykorzystujcej Bezzaogowe\n  Statki Powietrzne"
                },
                "summary": "Drony, dzi\\k{e}ki mo\\.zliwo\\'sci ich szybkiego rozmieszczenia w trudnym\nterenie, uwa\\.zane s\\k{a} za jeden z kluczowych element\\'ow system\\'ow\nbezprzewodowych 6G. Jednak w celu wykorzystania ich jako punkty dost\\k{e}powe\nsieci konieczne jest zapewnienie {\\l}\\k{a}cza dosy{\\l}owego o odpowiedniej\nprzepustowo\\'sci. Dlatego w niniejszym artykule rozwa\\.zane jest\nzwi\\k{e}kszenie zasi\\k{e}gu sieci bezprzewodowej przez zapewnienie {\\l}\\k{a}cza\ndosy{\\l}owego dla ko\\'ncowego punktu dost\\k{e}powego z wykorzystaniem\nokre\\'slonej liczby dron\\'ow-przeka\\'znik\\'ow oraz rekonfigurowalnych\ninteligentnych matryc antenowych (RIS). Zaprezentowane wyniki bada\\'n\nsymulacyjnych pokazuj\\k{a}, \\.ze u\\.zycie RIS pozwala na znacz\\k{a}ce\nzwi\\k{e}kszenie zasi\\k{e}gu sieci bez konieczno\\'sci stosowania dodatkowych\nprzeka\\'znik\\'ow.\n  --\n  Unmanned Aerial Vehicles, due to the possibility of their fast deployment,\nare considered an essential element of the future wireless 6G communication\nsystems. However, an essential enabler for their use as access points is to\nprovide a sufficient throughput wireless backhaul link. Thus, in this paper we\nconsider the aspect of extension of network coverage with the use of\ndrone-based relaying and reconfigurable intelligent surfaces (RIS) for\nbackhauling. Presented results of simulation experiments indicate that the use\nof RIS allows for significant improvement of network coverage without the need\nto use additional relays.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Drony, dzi\\k{e}ki mo\\.zliwo\\'sci ich szybkiego rozmieszczenia w trudnym\nterenie, uwa\\.zane s\\k{a} za jeden z kluczowych element\\'ow system\\'ow\nbezprzewodowych 6G. Jednak w celu wykorzystania ich jako punkty dost\\k{e}powe\nsieci konieczne jest zapewnienie {\\l}\\k{a}cza dosy{\\l}owego o odpowiedniej\nprzepustowo\\'sci. Dlatego w niniejszym artykule rozwa\\.zane jest\nzwi\\k{e}kszenie zasi\\k{e}gu sieci bezprzewodowej przez zapewnienie {\\l}\\k{a}cza\ndosy{\\l}owego dla ko\\'ncowego punktu dost\\k{e}powego z wykorzystaniem\nokre\\'slonej liczby dron\\'ow-przeka\\'znik\\'ow oraz rekonfigurowalnych\ninteligentnych matryc antenowych (RIS). Zaprezentowane wyniki bada\\'n\nsymulacyjnych pokazuj\\k{a}, \\.ze u\\.zycie RIS pozwala na znacz\\k{a}ce\nzwi\\k{e}kszenie zasi\\k{e}gu sieci bez konieczno\\'sci stosowania dodatkowych\nprzeka\\'znik\\'ow.\n  --\n  Unmanned Aerial Vehicles, due to the possibility of their fast deployment,\nare considered an essential element of the future wireless 6G communication\nsystems. However, an essential enabler for their use as access points is to\nprovide a sufficient throughput wireless backhaul link. Thus, in this paper we\nconsider the aspect of extension of network coverage with the use of\ndrone-based relaying and reconfigurable intelligent surfaces (RIS) for\nbackhauling. Presented results of simulation experiments indicate that the use\nof RIS allows for significant improvement of network coverage without the need\nto use additional relays."
                },
                "authors": [
                    {
                        "name": "Salim Janji"
                    },
                    {
                        "name": "Pawe Sroka"
                    },
                    {
                        "name": "Adrian Kliks"
                    }
                ],
                "author_detail": {
                    "name": "Adrian Kliks"
                },
                "author": "Adrian Kliks",
                "arxiv_doi": "10.15199/59.2023.4.15",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.15199/59.2023.4.15",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.05301v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05301v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "in Polish language",
                "arxiv_journal_ref": "Przeglad Telekomunikacyjny - Wiadomosci Telekomunikacyjne, no. 4\n  (2023), pp. 85-88",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11096v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11096v3",
                "updated": "2024-10-03T11:57:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    11,
                    57,
                    0,
                    3,
                    277,
                    0
                ],
                "published": "2024-06-16T22:59:18Z",
                "published_parsed": [
                    2024,
                    6,
                    16,
                    22,
                    59,
                    18,
                    6,
                    168,
                    0
                ],
                "title": "The Potential and Challenges of Evaluating Attitudes, Opinions, and\n  Values in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Potential and Challenges of Evaluating Attitudes, Opinions, and\n  Values in Large Language Models"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have sparked wide interest in\nvalidating and comprehending the human-like cognitive-behavioral traits LLMs\nmay capture and convey. These cognitive-behavioral traits include typically\nAttitudes, Opinions, Values (AOVs). However, measuring AOVs embedded within\nLLMs remains opaque, and different evaluation methods may yield different\nresults. This has led to a lack of clarity on how different studies are related\nto each other and how they can be interpreted. This paper aims to bridge this\ngap by providing a comprehensive overview of recent works on the evaluation of\nAOVs in LLMs. Moreover, we survey related approaches in different stages of the\nevaluation pipeline in these works. By doing so, we address the potential and\nchallenges with respect to understanding the model, human-AI alignment, and\ndownstream application in social sciences. Finally, we provide practical\ninsights into evaluation methods, model enhancement, and interdisciplinary\ncollaboration, thereby contributing to the evolving landscape of evaluating\nAOVs in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have sparked wide interest in\nvalidating and comprehending the human-like cognitive-behavioral traits LLMs\nmay capture and convey. These cognitive-behavioral traits include typically\nAttitudes, Opinions, Values (AOVs). However, measuring AOVs embedded within\nLLMs remains opaque, and different evaluation methods may yield different\nresults. This has led to a lack of clarity on how different studies are related\nto each other and how they can be interpreted. This paper aims to bridge this\ngap by providing a comprehensive overview of recent works on the evaluation of\nAOVs in LLMs. Moreover, we survey related approaches in different stages of the\nevaluation pipeline in these works. By doing so, we address the potential and\nchallenges with respect to understanding the model, human-AI alignment, and\ndownstream application in social sciences. Finally, we provide practical\ninsights into evaluation methods, model enhancement, and interdisciplinary\ncollaboration, thereby contributing to the evolving landscape of evaluating\nAOVs in LLMs."
                },
                "authors": [
                    {
                        "name": "Bolei Ma"
                    },
                    {
                        "name": "Xinpeng Wang"
                    },
                    {
                        "name": "Tiancheng Hu"
                    },
                    {
                        "name": "Anna-Carolina Haensch"
                    },
                    {
                        "name": "Michael A. Hedderich"
                    },
                    {
                        "name": "Barbara Plank"
                    },
                    {
                        "name": "Frauke Kreuter"
                    }
                ],
                "author_detail": {
                    "name": "Frauke Kreuter"
                },
                "author": "Frauke Kreuter",
                "arxiv_comment": "EMNLP 2024 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11096v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11096v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02406v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02406v1",
                "updated": "2024-10-03T11:32:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    11,
                    32,
                    53,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T11:32:53Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    11,
                    32,
                    53,
                    3,
                    277,
                    0
                ],
                "title": "ELLMA-T: an Embodied LLM-agent for Supporting English Language Learning\n  in Social VR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ELLMA-T: an Embodied LLM-agent for Supporting English Language Learning\n  in Social VR"
                },
                "summary": "Many people struggle with learning a new language, with traditional tools\nfalling short in providing contextualized learning tailored to each learner's\nneeds. The recent development of large language models (LLMs) and embodied\nconversational agents (ECAs) in social virtual reality (VR) provide new\nopportunities to practice language learning in a contextualized and\nnaturalistic way that takes into account the learner's language level and\nneeds. To explore this opportunity, we developed ELLMA-T, an ECA that leverages\nan LLM (GPT-4) and situated learning framework for supporting learning English\nlanguage in social VR (VRChat). Drawing on qualitative interviews (N=12), we\nreveal the potential of ELLMA-T to generate realistic, believable and\ncontext-specific role plays for agent-learner interaction in VR, and LLM's\ncapability to provide initial language assessment and continuous feedback to\nlearners. We provide five design implications for the future development of\nLLM-based language agents in social VR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many people struggle with learning a new language, with traditional tools\nfalling short in providing contextualized learning tailored to each learner's\nneeds. The recent development of large language models (LLMs) and embodied\nconversational agents (ECAs) in social virtual reality (VR) provide new\nopportunities to practice language learning in a contextualized and\nnaturalistic way that takes into account the learner's language level and\nneeds. To explore this opportunity, we developed ELLMA-T, an ECA that leverages\nan LLM (GPT-4) and situated learning framework for supporting learning English\nlanguage in social VR (VRChat). Drawing on qualitative interviews (N=12), we\nreveal the potential of ELLMA-T to generate realistic, believable and\ncontext-specific role plays for agent-learner interaction in VR, and LLM's\ncapability to provide initial language assessment and continuous feedback to\nlearners. We provide five design implications for the future development of\nLLM-based language agents in social VR."
                },
                "authors": [
                    {
                        "name": "Mengxu Pan"
                    },
                    {
                        "name": "Alexandra Kitson"
                    },
                    {
                        "name": "Hongyu Wan"
                    },
                    {
                        "name": "Mirjana Prpa"
                    }
                ],
                "author_detail": {
                    "name": "Mirjana Prpa"
                },
                "author": "Mirjana Prpa",
                "arxiv_comment": "20 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02406v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02406v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02389v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02389v1",
                "updated": "2024-10-03T11:10:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    3,
                    11,
                    10,
                    37,
                    3,
                    277,
                    0
                ],
                "published": "2024-10-03T11:10:37Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    11,
                    10,
                    37,
                    3,
                    277,
                    0
                ],
                "title": "Diffusion Meets Options: Hierarchical Generative Skill Composition for\n  Temporally-Extended Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Meets Options: Hierarchical Generative Skill Composition for\n  Temporally-Extended Tasks"
                },
                "summary": "Safe and successful deployment of robots requires not only the ability to\ngenerate complex plans but also the capacity to frequently replan and correct\nexecution errors. This paper addresses the challenge of long-horizon trajectory\nplanning under temporally extended objectives in a receding horizon manner. To\nthis end, we propose DOPPLER, a data-driven hierarchical framework that\ngenerates and updates plans based on instruction specified by linear temporal\nlogic (LTL). Our method decomposes temporal tasks into chain of options with\nhierarchical reinforcement learning from offline non-expert datasets. It\nleverages diffusion models to generate options with low-level actions. We\ndevise a determinantal-guided posterior sampling technique during batch\ngeneration, which improves the speed and diversity of diffusion generated\noptions, leading to more efficient querying. Experiments on robot navigation\nand manipulation tasks demonstrate that DOPPLER can generate sequences of\ntrajectories that progressively satisfy the specified formulae for obstacle\navoidance and sequential visitation. Demonstration videos are available online\nat: https://philiptheother.github.io/doppler/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safe and successful deployment of robots requires not only the ability to\ngenerate complex plans but also the capacity to frequently replan and correct\nexecution errors. This paper addresses the challenge of long-horizon trajectory\nplanning under temporally extended objectives in a receding horizon manner. To\nthis end, we propose DOPPLER, a data-driven hierarchical framework that\ngenerates and updates plans based on instruction specified by linear temporal\nlogic (LTL). Our method decomposes temporal tasks into chain of options with\nhierarchical reinforcement learning from offline non-expert datasets. It\nleverages diffusion models to generate options with low-level actions. We\ndevise a determinantal-guided posterior sampling technique during batch\ngeneration, which improves the speed and diversity of diffusion generated\noptions, leading to more efficient querying. Experiments on robot navigation\nand manipulation tasks demonstrate that DOPPLER can generate sequences of\ntrajectories that progressively satisfy the specified formulae for obstacle\navoidance and sequential visitation. Demonstration videos are available online\nat: https://philiptheother.github.io/doppler/."
                },
                "authors": [
                    {
                        "name": "Zeyu Feng"
                    },
                    {
                        "name": "Hao Luan"
                    },
                    {
                        "name": "Kevin Yuchen Ma"
                    },
                    {
                        "name": "Harold Soh"
                    }
                ],
                "author_detail": {
                    "name": "Harold Soh"
                },
                "author": "Harold Soh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02389v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02389v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]