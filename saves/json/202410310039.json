[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2410.19274v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19274v2",
                "updated": "2024-10-29T17:33:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    33,
                    19,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-25T03:01:19Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    3,
                    1,
                    19,
                    4,
                    299,
                    0
                ],
                "title": "Ripple: Accelerating LLM Inference on Smartphones with Correlation-Aware\n  Neuron Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ripple: Accelerating LLM Inference on Smartphones with Correlation-Aware\n  Neuron Management"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains, yet deploying them on mobile devices remains an arduous challenge due\nto their extensive computational and memory demands. While lightweight LLMs\nhave been developed to fit mobile environments, they suffer from degraded model\naccuracy. In contrast, sparsity-based techniques minimize DRAM usage by\nselectively transferring only relevant neurons to DRAM while retaining the full\nmodel in external storage, such as flash. However, such approaches are\ncritically limited by numerous I/O operations, particularly on smartphones with\nsevere IOPS constraints.\n  In this paper, we propose Ripple, a novel approach that accelerates LLM\ninference on smartphones by optimizing neuron placement in flash memory. Ripple\nleverages the concept of Neuron Co-Activation, where neurons frequently\nactivated together are linked to facilitate continuous read access and optimize\ndata transfer efficiency. Our approach incorporates a two-stage solution: an\noffline stage that reorganizes neuron placement based on co-activation\npatterns, and an online stage that employs tailored data access and caching\nstrategies to align well with hardware characteristics. Evaluations conducted\non a variety of smartphones and LLMs demonstrate that Ripple achieves up to\n5.93x improvements in I/O latency compared to the state-of-the-art. As the\nfirst solution to optimize storage placement under sparsity, Ripple explores a\nnew optimization space at the intersection of sparsity-driven algorithm and\nstorage-level system co-design in LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains, yet deploying them on mobile devices remains an arduous challenge due\nto their extensive computational and memory demands. While lightweight LLMs\nhave been developed to fit mobile environments, they suffer from degraded model\naccuracy. In contrast, sparsity-based techniques minimize DRAM usage by\nselectively transferring only relevant neurons to DRAM while retaining the full\nmodel in external storage, such as flash. However, such approaches are\ncritically limited by numerous I/O operations, particularly on smartphones with\nsevere IOPS constraints.\n  In this paper, we propose Ripple, a novel approach that accelerates LLM\ninference on smartphones by optimizing neuron placement in flash memory. Ripple\nleverages the concept of Neuron Co-Activation, where neurons frequently\nactivated together are linked to facilitate continuous read access and optimize\ndata transfer efficiency. Our approach incorporates a two-stage solution: an\noffline stage that reorganizes neuron placement based on co-activation\npatterns, and an online stage that employs tailored data access and caching\nstrategies to align well with hardware characteristics. Evaluations conducted\non a variety of smartphones and LLMs demonstrate that Ripple achieves up to\n5.93x improvements in I/O latency compared to the state-of-the-art. As the\nfirst solution to optimize storage placement under sparsity, Ripple explores a\nnew optimization space at the intersection of sparsity-driven algorithm and\nstorage-level system co-design in LLM inference."
                },
                "authors": [
                    {
                        "name": "Tuowei Wang"
                    },
                    {
                        "name": "Ruwen Fan"
                    },
                    {
                        "name": "Minxing Huang"
                    },
                    {
                        "name": "Zixu Hao"
                    },
                    {
                        "name": "Kun Li"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Youyou Lu"
                    },
                    {
                        "name": "Yaoxue Zhang"
                    },
                    {
                        "name": "Ju Ren"
                    }
                ],
                "author_detail": {
                    "name": "Ju Ren"
                },
                "author": "Ju Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19274v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19274v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21142v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21142v2",
                "updated": "2024-10-29T16:55:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    16,
                    55,
                    23,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-28T15:43:33Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    15,
                    43,
                    33,
                    0,
                    302,
                    0
                ],
                "title": "Modeling and Monitoring of Indoor Populations using Sparse Positioning\n  Data (Extension)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling and Monitoring of Indoor Populations using Sparse Positioning\n  Data (Extension)"
                },
                "summary": "In large venues like shopping malls and airports, knowledge on the indoor\npopulations fuels applications such as business analytics, venue management,\nand safety control. In this work, we provide means of modeling populations in\npartitions of indoor space offline and of monitoring indoor populations\ncontinuously, by using indoor positioning data. However, the low-sampling rates\nof indoor positioning render the data temporally and spatially sparse, which in\nturn renders the offline capture of indoor populations challenging. It is even\nmore challenging to continuously monitor indoor populations, as positioning\ndata may be missing or not ready yet at the current moment. To address these\nchallenges, we first enable probabilistic modeling of populations in indoor\nspace partitions as Normal distributions. Based on that, we propose two\nlearning-based estimators for on-the-fly prediction of population\ndistributions. Leveraging the prediction-based schemes, we provide a unified\ncontinuous query processing framework for a type of query that enables\ncontinuous monitoring of populated partitions. The framework encompasses\ncaching and result validity mechanisms to reduce cost and maintain monitoring\neffectiveness. Extensive experiments on two real data sets show that the\nproposed estimators are able to outperform the state-of-the-art alternatives\nand that the query processing framework is effective and efficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large venues like shopping malls and airports, knowledge on the indoor\npopulations fuels applications such as business analytics, venue management,\nand safety control. In this work, we provide means of modeling populations in\npartitions of indoor space offline and of monitoring indoor populations\ncontinuously, by using indoor positioning data. However, the low-sampling rates\nof indoor positioning render the data temporally and spatially sparse, which in\nturn renders the offline capture of indoor populations challenging. It is even\nmore challenging to continuously monitor indoor populations, as positioning\ndata may be missing or not ready yet at the current moment. To address these\nchallenges, we first enable probabilistic modeling of populations in indoor\nspace partitions as Normal distributions. Based on that, we propose two\nlearning-based estimators for on-the-fly prediction of population\ndistributions. Leveraging the prediction-based schemes, we provide a unified\ncontinuous query processing framework for a type of query that enables\ncontinuous monitoring of populated partitions. The framework encompasses\ncaching and result validity mechanisms to reduce cost and maintain monitoring\neffectiveness. Extensive experiments on two real data sets show that the\nproposed estimators are able to outperform the state-of-the-art alternatives\nand that the query processing framework is effective and efficient."
                },
                "authors": [
                    {
                        "name": "Xiao Li"
                    },
                    {
                        "name": "Huan Li"
                    },
                    {
                        "name": "Hua Lu"
                    },
                    {
                        "name": "Christian S. Jensen"
                    }
                ],
                "author_detail": {
                    "name": "Christian S. Jensen"
                },
                "author": "Christian S. Jensen",
                "arxiv_comment": "Accepted at TKDE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21142v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21142v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22134v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22134v1",
                "updated": "2024-10-29T15:31:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    31,
                    27,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T15:31:27Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    31,
                    27,
                    1,
                    303,
                    0
                ],
                "title": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching"
                },
                "summary": "The promising applications of large language models are often constrained by\nthe limited GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help mitigate this issue by activating only a subset of the\nmodel's parameters during computation, allowing the unused parameters to be\noffloaded to host memory and reducing overall GPU memory demand. However,\nexisting cache-based offloading solutions handle cache misses reactively and\nsignificantly impact system performance. In this paper, we propose ProMoE, a\nnovel proactive caching system that leverages intermediate model results to\npredict subsequent parameter usage. By proactively fetching experts in advance,\nProMoE removes the loading time from the critical path and diminishes the\nperformance overhead of offloading. Our evaluations demonstrate that ProMoE\nachieves an average speedup of 2.13x and 2.84x in the prefill and decode stages\nrespectively, compared to existing offloading solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The promising applications of large language models are often constrained by\nthe limited GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help mitigate this issue by activating only a subset of the\nmodel's parameters during computation, allowing the unused parameters to be\noffloaded to host memory and reducing overall GPU memory demand. However,\nexisting cache-based offloading solutions handle cache misses reactively and\nsignificantly impact system performance. In this paper, we propose ProMoE, a\nnovel proactive caching system that leverages intermediate model results to\npredict subsequent parameter usage. By proactively fetching experts in advance,\nProMoE removes the loading time from the critical path and diminishes the\nperformance overhead of offloading. Our evaluations demonstrate that ProMoE\nachieves an average speedup of 2.13x and 2.84x in the prefill and decode stages\nrespectively, compared to existing offloading solutions."
                },
                "authors": [
                    {
                        "name": "Xiaoniu Song"
                    },
                    {
                        "name": "Zihang Zhong"
                    },
                    {
                        "name": "Rong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Rong Chen"
                },
                "author": "Rong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22134v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22134v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22118v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22118v1",
                "updated": "2024-10-29T15:19:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    19,
                    13,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T15:19:13Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    19,
                    13,
                    1,
                    303,
                    0
                ],
                "title": "The Impact of Inference Acceleration Strategies on Bias of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Inference Acceleration Strategies on Bias of LLMs"
                },
                "summary": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to deeply benefit a vast\narray of application domains. However, due to their immense size, performing\ninference with LLMs is both costly and slow. Consequently, a plethora of recent\nwork has proposed strategies to enhance inference efficiency, e.g.,\nquantization, pruning, and caching. These acceleration strategies reduce the\ninference cost and latency, often by several factors, while maintaining much of\nthe predictive performance measured via common benchmarks. In this work, we\nexplore another critical aspect of LLM performance: demographic bias in model\ngenerations due to inference acceleration optimizations. Using a wide range of\nmetrics, we probe bias in model outputs from a number of angles. Analysis of\noutputs before and after inference acceleration shows significant change in\nbias. Worryingly, these bias effects are complex and unpredictable. A\ncombination of an acceleration strategy and bias type may show little bias\nchange in one model but may lead to a large effect in another. Our results\nhighlight a need for in-depth and case-by-case evaluation of model bias after\nit has been modified to accelerate inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to deeply benefit a vast\narray of application domains. However, due to their immense size, performing\ninference with LLMs is both costly and slow. Consequently, a plethora of recent\nwork has proposed strategies to enhance inference efficiency, e.g.,\nquantization, pruning, and caching. These acceleration strategies reduce the\ninference cost and latency, often by several factors, while maintaining much of\nthe predictive performance measured via common benchmarks. In this work, we\nexplore another critical aspect of LLM performance: demographic bias in model\ngenerations due to inference acceleration optimizations. Using a wide range of\nmetrics, we probe bias in model outputs from a number of angles. Analysis of\noutputs before and after inference acceleration shows significant change in\nbias. Worryingly, these bias effects are complex and unpredictable. A\ncombination of an acceleration strategy and bias type may show little bias\nchange in one model but may lead to a large effect in another. Our results\nhighlight a need for in-depth and case-by-case evaluation of model bias after\nit has been modified to accelerate inference."
                },
                "authors": [
                    {
                        "name": "Elisabeth Kirsten"
                    },
                    {
                        "name": "Ivan Habernal"
                    },
                    {
                        "name": "Vedant Nanda"
                    },
                    {
                        "name": "Muhammad Bilal Zafar"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Bilal Zafar"
                },
                "author": "Muhammad Bilal Zafar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22118v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.09526v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.09526v2",
                "updated": "2024-10-29T13:04:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    13,
                    4,
                    42,
                    1,
                    303,
                    0
                ],
                "published": "2024-04-15T07:45:04Z",
                "published_parsed": [
                    2024,
                    4,
                    15,
                    7,
                    45,
                    4,
                    0,
                    106,
                    0
                ],
                "title": "LoongServe: Efficiently Serving Long-Context Large Language Models with\n  Elastic Sequence Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoongServe: Efficiently Serving Long-Context Large Language Models with\n  Elastic Sequence Parallelism"
                },
                "summary": "The context window of large language models (LLMs) is rapidly increasing,\nleading to a huge variance in resource usage between different requests as well\nas between different phases of the same request. Restricted by static\nparallelism strategies, existing LLM serving systems cannot efficiently utilize\nthe underlying resources to serve variable-length requests in different phases.\nTo address this problem, we propose a new parallelism paradigm, elastic\nsequence parallelism (ESP), to elastically adapt to the variance between\ndifferent requests and phases. Based on ESP, we design and build LoongServe, an\nLLM serving system that (1) improves computation efficiency by elastically\nadjusting the degree of parallelism in real-time, (2) improves communication\nefficiency by reducing key-value cache migration overhead and overlapping\npartial decoding communication with computation, and (3) improves GPU memory\nefficiency by reducing key-value cache fragmentation across instances. Our\nevaluation under diverse real-world datasets shows that LoongServe improves the\nmaximum throughput by up to 3.85$\\times$ compared to the chunked prefill and\n5.81$\\times$ compared to the prefill-decoding disaggregation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The context window of large language models (LLMs) is rapidly increasing,\nleading to a huge variance in resource usage between different requests as well\nas between different phases of the same request. Restricted by static\nparallelism strategies, existing LLM serving systems cannot efficiently utilize\nthe underlying resources to serve variable-length requests in different phases.\nTo address this problem, we propose a new parallelism paradigm, elastic\nsequence parallelism (ESP), to elastically adapt to the variance between\ndifferent requests and phases. Based on ESP, we design and build LoongServe, an\nLLM serving system that (1) improves computation efficiency by elastically\nadjusting the degree of parallelism in real-time, (2) improves communication\nefficiency by reducing key-value cache migration overhead and overlapping\npartial decoding communication with computation, and (3) improves GPU memory\nefficiency by reducing key-value cache fragmentation across instances. Our\nevaluation under diverse real-world datasets shows that LoongServe improves the\nmaximum throughput by up to 3.85$\\times$ compared to the chunked prefill and\n5.81$\\times$ compared to the prefill-decoding disaggregation."
                },
                "authors": [
                    {
                        "name": "Bingyang Wu"
                    },
                    {
                        "name": "Shengyu Liu"
                    },
                    {
                        "name": "Yinmin Zhong"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Xuanzhe Liu"
                    },
                    {
                        "name": "Xin Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xin Jin"
                },
                "author": "Xin Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.09526v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.09526v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05821v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05821v4",
                "updated": "2024-10-29T12:28:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    12,
                    28,
                    58,
                    1,
                    303,
                    0
                ],
                "published": "2023-12-10T08:41:24Z",
                "published_parsed": [
                    2023,
                    12,
                    10,
                    8,
                    41,
                    24,
                    6,
                    344,
                    0
                ],
                "title": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models"
                },
                "summary": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank decomposition, and find that the challenges of this task\nstem from the distribution variance in the LLM activations and the sensitivity\ndifference among various kinds of layers. To address these issues, we propose a\ntraining-free approach called Activation-aware Singular Value Decomposition\n(ASVD). Specifically, ASVD manages activation outliers by transforming the\nweight matrix based on the activation distribution. This transformation allows\nthe outliers in the activation matrix to be absorbed into the transformed\nweight matrix, thereby enhancing decomposition accuracy. Additionally, we\npropose an efficient iterative calibration process to optimize layer-specific\ndecomposition by addressing the varying sensitivity of different LLM layers. In\nthis way, ASVD can compress a network by 10%-30%. Based on the success of the\nlow-rank decomposition of projection matrices in the self-attention module, we\nfurther introduce ASVD to compress the KV cache. By reducing the channel\ndimension of KV activations, memory requirements for KV cache can be largely\nreduced. ASVD can further achieve 50% KV cache reductions without performance\ndrop in a training-free manner. Code is anonymously available in supplementary\nmaterials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank decomposition, and find that the challenges of this task\nstem from the distribution variance in the LLM activations and the sensitivity\ndifference among various kinds of layers. To address these issues, we propose a\ntraining-free approach called Activation-aware Singular Value Decomposition\n(ASVD). Specifically, ASVD manages activation outliers by transforming the\nweight matrix based on the activation distribution. This transformation allows\nthe outliers in the activation matrix to be absorbed into the transformed\nweight matrix, thereby enhancing decomposition accuracy. Additionally, we\npropose an efficient iterative calibration process to optimize layer-specific\ndecomposition by addressing the varying sensitivity of different LLM layers. In\nthis way, ASVD can compress a network by 10%-30%. Based on the success of the\nlow-rank decomposition of projection matrices in the self-attention module, we\nfurther introduce ASVD to compress the KV cache. By reducing the channel\ndimension of KV activations, memory requirements for KV cache can be largely\nreduced. ASVD can further achieve 50% KV cache reductions without performance\ndrop in a training-free manner. Code is anonymously available in supplementary\nmaterials."
                },
                "authors": [
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Yuzhang Shang"
                    },
                    {
                        "name": "Yue Song"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Yan Yan"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.05821v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05821v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18627v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18627v2",
                "updated": "2024-10-29T12:03:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    12,
                    3,
                    14,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-24T10:36:16Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    10,
                    36,
                    16,
                    3,
                    298,
                    0
                ],
                "title": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits"
                },
                "summary": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the greedy policy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the greedy policy."
                },
                "authors": [
                    {
                        "name": "Ankita Koley"
                    },
                    {
                        "name": "Chandramani Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Singh"
                },
                "author": "Chandramani Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18627v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18627v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00456v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00456v2",
                "updated": "2024-10-29T11:09:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    11,
                    9,
                    12,
                    1,
                    303,
                    0
                ],
                "published": "2024-03-30T19:20:06Z",
                "published_parsed": [
                    2024,
                    3,
                    30,
                    19,
                    20,
                    6,
                    5,
                    90,
                    0
                ],
                "title": "QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs"
                },
                "summary": "We introduce QuaRot, a new Quantization scheme based on Rotations, which is\nable to quantize LLMs end-to-end, including all weights, activations, and KV\ncache in 4 bits. QuaRot rotates LLMs in a way that removes outliers from the\nhidden state without changing the output, making quantization easier. This\ncomputational invariance is applied to the hidden state (residual) of the LLM,\nas well as to the activations of the feed-forward components, aspects of the\nattention mechanism, and to the KV cache. The result is a quantized model where\nall matrix multiplications are performed in 4 bits, without any channels\nidentified for retention in higher precision. Our 4-bit quantized LLaMa2-70B\nmodel has losses of at most 0.47 WikiText-2 perplexity and retains 99% of the\nzero-shot performance. We also show that QuaRot can provide lossless 6 and 8\nbit LLaMa2 models without any calibration data using round-to-nearest\nquantization. Code is available at: https://github.com/spcl/QuaRot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce QuaRot, a new Quantization scheme based on Rotations, which is\nable to quantize LLMs end-to-end, including all weights, activations, and KV\ncache in 4 bits. QuaRot rotates LLMs in a way that removes outliers from the\nhidden state without changing the output, making quantization easier. This\ncomputational invariance is applied to the hidden state (residual) of the LLM,\nas well as to the activations of the feed-forward components, aspects of the\nattention mechanism, and to the KV cache. The result is a quantized model where\nall matrix multiplications are performed in 4 bits, without any channels\nidentified for retention in higher precision. Our 4-bit quantized LLaMa2-70B\nmodel has losses of at most 0.47 WikiText-2 perplexity and retains 99% of the\nzero-shot performance. We also show that QuaRot can provide lossless 6 and 8\nbit LLaMa2 models without any calibration data using round-to-nearest\nquantization. Code is available at: https://github.com/spcl/QuaRot."
                },
                "authors": [
                    {
                        "name": "Saleh Ashkboos"
                    },
                    {
                        "name": "Amirkeivan Mohtashami"
                    },
                    {
                        "name": "Maximilian L. Croci"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Pashmina Cameron"
                    },
                    {
                        "name": "Martin Jaggi"
                    },
                    {
                        "name": "Dan Alistarh"
                    },
                    {
                        "name": "Torsten Hoefler"
                    },
                    {
                        "name": "James Hensman"
                    }
                ],
                "author_detail": {
                    "name": "James Hensman"
                },
                "author": "James Hensman",
                "arxiv_comment": "21 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00456v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00456v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11430v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11430v3",
                "updated": "2024-10-29T10:52:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    10,
                    52,
                    35,
                    1,
                    303,
                    0
                ],
                "published": "2024-06-17T11:35:16Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    11,
                    35,
                    16,
                    0,
                    169,
                    0
                ],
                "title": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression"
                },
                "summary": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability."
                },
                "authors": [
                    {
                        "name": "Alessio Devoto"
                    },
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Simone Scardapane"
                    },
                    {
                        "name": "Pasquale Minervini"
                    }
                ],
                "author_detail": {
                    "name": "Pasquale Minervini"
                },
                "author": "Pasquale Minervini",
                "arxiv_comment": "This is an extended version of a paper published in the proceedings\n  of the 2024 Conference on Empirical Methods in Natural Language Processing\n  (EMNLP 2024); this version was presented at the 4th NeurIPS Workshop on\n  Efficient Natural Language and Speech Processing (ENLSP-IV)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11430v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11430v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02369v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02369v3",
                "updated": "2024-10-29T04:21:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    4,
                    21,
                    30,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-03T10:33:49Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    10,
                    33,
                    49,
                    3,
                    277,
                    0
                ],
                "title": "Unleashing the Potential of the Diffusion Model in Few-shot Semantic\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unleashing the Potential of the Diffusion Model in Few-shot Semantic\n  Segmentation"
                },
                "summary": "The Diffusion Model has not only garnered noteworthy achievements in the\nrealm of image generation but has also demonstrated its potential as an\neffective pretraining method utilizing unlabeled data. Drawing from the\nextensive potential unveiled by the Diffusion Model in both semantic\ncorrespondence and open vocabulary segmentation, our work initiates an\ninvestigation into employing the Latent Diffusion Model for Few-shot Semantic\nSegmentation. Recently, inspired by the in-context learning ability of large\nlanguage models, Few-shot Semantic Segmentation has evolved into In-context\nSegmentation tasks, morphing into a crucial element in assessing generalist\nsegmentation models. In this context, we concentrate on Few-shot Semantic\nSegmentation, establishing a solid foundation for the future development of a\nDiffusion-based generalist model for segmentation. Our initial focus lies in\nunderstanding how to facilitate interaction between the query image and the\nsupport image, resulting in the proposal of a KV fusion method within the\nself-attention framework. Subsequently, we delve deeper into optimizing the\ninfusion of information from the support mask and simultaneously re-evaluating\nhow to provide reasonable supervision from the query mask. Based on our\nanalysis, we establish a simple and effective framework named DiffewS,\nmaximally retaining the original Latent Diffusion Model's generative framework\nand effectively utilizing the pre-training prior. Experimental results\ndemonstrate that our method significantly outperforms the previous SOTA models\nin multiple settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion Model has not only garnered noteworthy achievements in the\nrealm of image generation but has also demonstrated its potential as an\neffective pretraining method utilizing unlabeled data. Drawing from the\nextensive potential unveiled by the Diffusion Model in both semantic\ncorrespondence and open vocabulary segmentation, our work initiates an\ninvestigation into employing the Latent Diffusion Model for Few-shot Semantic\nSegmentation. Recently, inspired by the in-context learning ability of large\nlanguage models, Few-shot Semantic Segmentation has evolved into In-context\nSegmentation tasks, morphing into a crucial element in assessing generalist\nsegmentation models. In this context, we concentrate on Few-shot Semantic\nSegmentation, establishing a solid foundation for the future development of a\nDiffusion-based generalist model for segmentation. Our initial focus lies in\nunderstanding how to facilitate interaction between the query image and the\nsupport image, resulting in the proposal of a KV fusion method within the\nself-attention framework. Subsequently, we delve deeper into optimizing the\ninfusion of information from the support mask and simultaneously re-evaluating\nhow to provide reasonable supervision from the query mask. Based on our\nanalysis, we establish a simple and effective framework named DiffewS,\nmaximally retaining the original Latent Diffusion Model's generative framework\nand effectively utilizing the pre-training prior. Experimental results\ndemonstrate that our method significantly outperforms the previous SOTA models\nin multiple settings."
                },
                "authors": [
                    {
                        "name": "Muzhi Zhu"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Zekai Luo"
                    },
                    {
                        "name": "Chenchen Jing"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Guangkai Xu"
                    },
                    {
                        "name": "Xinlong Wang"
                    },
                    {
                        "name": "Chunhua Shen"
                    }
                ],
                "author_detail": {
                    "name": "Chunhua Shen"
                },
                "author": "Chunhua Shen",
                "arxiv_comment": "Accepted to Proc. Annual Conference on Neural Information Processing\n  Systems (NeurIPS) 2024. Webpage: https://github.com/aim-uofa/DiffewS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02369v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02369v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19291v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19291v3",
                "updated": "2024-10-29T02:52:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    2,
                    52,
                    24,
                    1,
                    303,
                    0
                ],
                "published": "2024-07-27T16:20:21Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    16,
                    20,
                    21,
                    5,
                    209,
                    0
                ],
                "title": "Symmetric Locality: Definition and Initial Results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Locality: Definition and Initial Results"
                },
                "summary": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs."
                },
                "authors": [
                    {
                        "name": "Giordan Escalona"
                    },
                    {
                        "name": "Dylan McKellips"
                    },
                    {
                        "name": "Chen Ding"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ding"
                },
                "author": "Chen Ding",
                "arxiv_comment": "6 pages, 2nd ver",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19291v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19291v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19258v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19258v2",
                "updated": "2024-10-28T19:32:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    19,
                    32,
                    23,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-25T02:22:00Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    2,
                    22,
                    0,
                    4,
                    299,
                    0
                ],
                "title": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning"
                },
                "summary": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark."
                },
                "authors": [
                    {
                        "name": "Yu Fu"
                    },
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Abedelkadir Asi"
                    },
                    {
                        "name": "Wayne Xiong"
                    },
                    {
                        "name": "Yue Dong"
                    },
                    {
                        "name": "Wen Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Xiao"
                },
                "author": "Wen Xiao",
                "arxiv_comment": "18pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19258v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19258v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21465v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21465v1",
                "updated": "2024-10-28T19:08:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    19,
                    8,
                    12,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T19:08:12Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    19,
                    8,
                    12,
                    0,
                    302,
                    0
                ],
                "title": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference"
                },
                "summary": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV."
                },
                "authors": [
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Li-Wen Chang"
                    },
                    {
                        "name": "Wenlei Bao"
                    },
                    {
                        "name": "Size Zheng"
                    },
                    {
                        "name": "Ningxin Zheng"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Harry Dong"
                    },
                    {
                        "name": "Yuejie Chi"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21465v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21465v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21266v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21266v1",
                "updated": "2024-10-28T17:57:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    57,
                    40,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T17:57:40Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    17,
                    57,
                    40,
                    0,
                    302,
                    0
                ],
                "title": "Online Weighted Paging with Unknown Weights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Weighted Paging with Unknown Weights"
                },
                "summary": "Online paging is a fundamental problem in the field of online algorithms, in\nwhich one maintains a cache of $k$ slots as requests for fetching pages arrive\nonline. In the weighted variant of this problem, each page has its own fetching\ncost; a substantial line of work on this problem culminated in an (optimal)\n$O(\\log k)$-competitive randomized algorithm, due to Bansal, Buchbinder and\nNaor (FOCS'07).\n  Existing work for weighted paging assumes that page weights are known in\nadvance, which is not always the case in practice. For example, in multi-level\ncaching architectures, the expected cost of fetching a memory block is a\nfunction of its probability of being in a mid-level cache rather than the main\nmemory. This complex property cannot be predicted in advance; over time,\nhowever, one may glean information about page weights through sampling their\nfetching cost multiple times.\n  We present the first algorithm for online weighted paging that does not know\npage weights in advance, but rather learns from weight samples. In terms of\ntechniques, this requires providing (integral) samples to a fractional solver,\nrequiring a delicate interface between this solver and the randomized rounding\nscheme; we believe that our work can inspire online algorithms to other\nproblems that involve cost sampling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online paging is a fundamental problem in the field of online algorithms, in\nwhich one maintains a cache of $k$ slots as requests for fetching pages arrive\nonline. In the weighted variant of this problem, each page has its own fetching\ncost; a substantial line of work on this problem culminated in an (optimal)\n$O(\\log k)$-competitive randomized algorithm, due to Bansal, Buchbinder and\nNaor (FOCS'07).\n  Existing work for weighted paging assumes that page weights are known in\nadvance, which is not always the case in practice. For example, in multi-level\ncaching architectures, the expected cost of fetching a memory block is a\nfunction of its probability of being in a mid-level cache rather than the main\nmemory. This complex property cannot be predicted in advance; over time,\nhowever, one may glean information about page weights through sampling their\nfetching cost multiple times.\n  We present the first algorithm for online weighted paging that does not know\npage weights in advance, but rather learns from weight samples. In terms of\ntechniques, this requires providing (integral) samples to a fractional solver,\nrequiring a delicate interface between this solver and the randomized rounding\nscheme; we believe that our work can inspire online algorithms to other\nproblems that involve cost sampling."
                },
                "authors": [
                    {
                        "name": "Orin Levy"
                    },
                    {
                        "name": "Noam Touitou"
                    },
                    {
                        "name": "Aviv Rosenberg"
                    }
                ],
                "author_detail": {
                    "name": "Aviv Rosenberg"
                },
                "author": "Aviv Rosenberg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21266v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21266v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08141v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08141v2",
                "updated": "2024-10-28T16:42:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    16,
                    42,
                    11,
                    0,
                    302,
                    0
                ],
                "published": "2024-09-12T15:34:23Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    34,
                    23,
                    3,
                    256,
                    0
                ],
                "title": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects"
                },
                "summary": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should be based on Direct Memory\nAccess (DMA), descriptor rings, and interrupts: DMA offloads transfers from the\nCPU, descriptor rings provide buffering and queuing, and interrupts facilitate\nasynchronous interaction between cores and device with a lightweight\nnotification mechanism. In this paper we question this wisdom in the light of\nmodern hardware and workloads, particularly in cloud servers. Like others\nbefore us, we argue that the assumptions that led to this model are obsolete,\nand in many use-cases use of Programmed I/O (PIO), where the CPU explicitly\ntransfers data and control information to and from a device via loads and\nstores, actually results in a more efficient system. However, unlike others to\ndate, we push this idea further and show, in a real implementation, the gains\nin average and tail latency for fine-grained communication achievable using an\nopen cache-coherence protocol which exposes cache transitions to a smart\ndevice. We show this using three use-cases: fine-grained RPC-style invocation\nof functions on an accelerator, offloading of operators in a streaming dataflow\nengine, and a network interface targeting for serverless functions, comparing\nour use of coherence with both traditional DMA-style interaction and a\nhighly-optimized implementation using PIO over PCI Express (PCIe).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should be based on Direct Memory\nAccess (DMA), descriptor rings, and interrupts: DMA offloads transfers from the\nCPU, descriptor rings provide buffering and queuing, and interrupts facilitate\nasynchronous interaction between cores and device with a lightweight\nnotification mechanism. In this paper we question this wisdom in the light of\nmodern hardware and workloads, particularly in cloud servers. Like others\nbefore us, we argue that the assumptions that led to this model are obsolete,\nand in many use-cases use of Programmed I/O (PIO), where the CPU explicitly\ntransfers data and control information to and from a device via loads and\nstores, actually results in a more efficient system. However, unlike others to\ndate, we push this idea further and show, in a real implementation, the gains\nin average and tail latency for fine-grained communication achievable using an\nopen cache-coherence protocol which exposes cache transitions to a smart\ndevice. We show this using three use-cases: fine-grained RPC-style invocation\nof functions on an accelerator, offloading of operators in a streaming dataflow\nengine, and a network interface targeting for serverless functions, comparing\nour use of coherence with both traditional DMA-style interaction and a\nhighly-optimized implementation using PIO over PCI Express (PCIe)."
                },
                "authors": [
                    {
                        "name": "Anastasiia Ruzhanskaia"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "David Cock"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08141v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08141v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16179v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16179v2",
                "updated": "2024-10-28T14:44:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    44,
                    22,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-21T16:44:51Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    16,
                    44,
                    51,
                    0,
                    295,
                    0
                ],
                "title": "MagicPIG: LSH Sampling for Efficient LLM Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicPIG: LSH Sampling for Efficient LLM Generation"
                },
                "summary": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by $1.9\\sim3.9\\times$ across various GPU hardware and achieve 110ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\n\\url{https://github.com/Infini-AI-Lab/MagicPIG}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by $1.9\\sim3.9\\times$ across various GPU hardware and achieve 110ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\n\\url{https://github.com/Infini-AI-Lab/MagicPIG}."
                },
                "authors": [
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Jianyu Zhang"
                    },
                    {
                        "name": "Niklas Nolte"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Matthijs Douze"
                    },
                    {
                        "name": "Leon Bottou"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16179v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16179v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21073v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21073v1",
                "updated": "2024-10-28T14:35:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    35,
                    12,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T14:35:12Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    14,
                    35,
                    12,
                    0,
                    302,
                    0
                ],
                "title": "Skip2-LoRA: A Lightweight On-device DNN Fine-tuning Method for Low-cost\n  Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skip2-LoRA: A Lightweight On-device DNN Fine-tuning Method for Low-cost\n  Edge Devices"
                },
                "summary": "This paper proposes Skip2-LoRA as a lightweight fine-tuning method for deep\nneural networks to address the gap between pre-trained and deployed models. In\nour approach, trainable LoRA (low-rank adaptation) adapters are inserted\nbetween the last layer and every other layer to enhance the network expressive\npower while keeping the backward computation cost low. This architecture is\nwell-suited to cache intermediate computation results of the forward pass and\nthen can skip the forward computation of seen samples as training epochs\nprogress. We implemented the combination of the proposed architecture and\ncache, denoted as Skip2-LoRA, and tested it on a $15 single board computer. Our\nresults show that Skip2-LoRA reduces the fine-tuning time by 90.0% on average\ncompared to the counterpart that has the same number of trainable parameters\nwhile preserving the accuracy, while taking only a few seconds on the\nmicrocontroller board.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes Skip2-LoRA as a lightweight fine-tuning method for deep\nneural networks to address the gap between pre-trained and deployed models. In\nour approach, trainable LoRA (low-rank adaptation) adapters are inserted\nbetween the last layer and every other layer to enhance the network expressive\npower while keeping the backward computation cost low. This architecture is\nwell-suited to cache intermediate computation results of the forward pass and\nthen can skip the forward computation of seen samples as training epochs\nprogress. We implemented the combination of the proposed architecture and\ncache, denoted as Skip2-LoRA, and tested it on a $15 single board computer. Our\nresults show that Skip2-LoRA reduces the fine-tuning time by 90.0% on average\ncompared to the counterpart that has the same number of trainable parameters\nwhile preserving the accuracy, while taking only a few seconds on the\nmicrocontroller board."
                },
                "authors": [
                    {
                        "name": "Hiroki Matsutani"
                    },
                    {
                        "name": "Masaaki Kondo"
                    },
                    {
                        "name": "Kazuki Sunaga"
                    },
                    {
                        "name": "Radu Marculescu"
                    }
                ],
                "author_detail": {
                    "name": "Radu Marculescu"
                },
                "author": "Radu Marculescu",
                "arxiv_comment": "ASP-DAC 2025 (accepted)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21073v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21073v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21035v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21035v1",
                "updated": "2024-10-28T13:56:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    56,
                    30,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T13:56:30Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    56,
                    30,
                    0,
                    302,
                    0
                ],
                "title": "Beyond Autoregression: Fast LLMs via Self-Distillation Through Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Autoregression: Fast LLMs via Self-Distillation Through Time"
                },
                "summary": "Autoregressive (AR) Large Language Models (LLMs) have demonstrated\nsignificant success across numerous tasks. However, the AR modeling paradigm\npresents certain limitations; for instance, contemporary autoregressive LLMs\nare trained to generate one token at a time, which can result in noticeable\nlatency. Recent advances have indicated that search and repeated sampling can\nenhance performance in various applications, such as theorem proving, code\ngeneration, and alignment, by utilizing greater computational resources during\ninference. In this study, we demonstrate that diffusion language models are\ncapable of generating at least 32 tokens simultaneously, while exceeding the\nperformance of AR models in text quality and on the LAMBADA natural language\nunderstanding benchmark. This outcome is achieved through a novel distillation\nmethod for discrete diffusion models, which reduces the number of inference\nsteps by a factor of 32-64. Practically, our models, even without caching, can\ngenerate tokens at a rate that is up to 8 times faster than AR models employing\nKV caching, and we anticipate further improvements with the inclusion of\ncaching. Moreover, we demonstrate the efficacy of our approach for diffusion\nlanguage models with up to 860M parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) Large Language Models (LLMs) have demonstrated\nsignificant success across numerous tasks. However, the AR modeling paradigm\npresents certain limitations; for instance, contemporary autoregressive LLMs\nare trained to generate one token at a time, which can result in noticeable\nlatency. Recent advances have indicated that search and repeated sampling can\nenhance performance in various applications, such as theorem proving, code\ngeneration, and alignment, by utilizing greater computational resources during\ninference. In this study, we demonstrate that diffusion language models are\ncapable of generating at least 32 tokens simultaneously, while exceeding the\nperformance of AR models in text quality and on the LAMBADA natural language\nunderstanding benchmark. This outcome is achieved through a novel distillation\nmethod for discrete diffusion models, which reduces the number of inference\nsteps by a factor of 32-64. Practically, our models, even without caching, can\ngenerate tokens at a rate that is up to 8 times faster than AR models employing\nKV caching, and we anticipate further improvements with the inclusion of\ncaching. Moreover, we demonstrate the efficacy of our approach for diffusion\nlanguage models with up to 860M parameters."
                },
                "authors": [
                    {
                        "name": "Justin Deschenaux"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    }
                ],
                "author_detail": {
                    "name": "Caglar Gulcehre"
                },
                "author": "Caglar Gulcehre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21035v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21035v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20790v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20790v1",
                "updated": "2024-10-28T07:13:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    28,
                    7,
                    13,
                    25,
                    0,
                    302,
                    0
                ],
                "published": "2024-10-28T07:13:25Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    7,
                    13,
                    25,
                    0,
                    302,
                    0
                ],
                "title": "SparseTem: Boosting the Efficiency of CNN-Based Video Encoders by\n  Exploiting Temporal Continuity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseTem: Boosting the Efficiency of CNN-Based Video Encoders by\n  Exploiting Temporal Continuity"
                },
                "summary": "Deep learning models have become pivotal in the field of video processing and\nis increasingly critical in practical applications such as autonomous driving\nand object detection. Although Vision Transformers (ViTs) have demonstrated\ntheir power, Convolutional Neural Networks (CNNs) remain a highly efficient and\nhigh-performance choice for feature extraction and encoding. However, the\nintensive computational demands of convolution operations hinder its broader\nadoption as a video encoder. Given the inherent temporal continuity in video\nframes, changes between consecutive frames are minimal, allowing for the\nskipping of redundant computations. This technique, which we term as Diff\nComputation, presents two primary challenges. First, Diff Computation requires\nto cache intermediate feature maps to ensure the correctness of non-linear\ncomputations, leading to significant memory consumption. Second, the imbalance\nof sparsity among layers, introduced by Diff Computation, incurs accuracy\ndegradation. To address these issues, we propose a memory-efficient scheduling\nmethod to eliminate memory overhead and an online adjustment mechanism to\nminimize accuracy degradation. We integrate these techniques into our\nframework, SparseTem, to seamlessly support various CNN-based video encoders.\nSparseTem achieves speedup of 1.79x for EfficientDet and 4.72x for CRNN, with\nminimal accuracy drop and no additional memory overhead. Extensive experimental\nresults demonstrate that SparseTem sets a new state-of-the-art by effectively\nutilizing temporal continuity to accelerate CNN-based video encoders.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning models have become pivotal in the field of video processing and\nis increasingly critical in practical applications such as autonomous driving\nand object detection. Although Vision Transformers (ViTs) have demonstrated\ntheir power, Convolutional Neural Networks (CNNs) remain a highly efficient and\nhigh-performance choice for feature extraction and encoding. However, the\nintensive computational demands of convolution operations hinder its broader\nadoption as a video encoder. Given the inherent temporal continuity in video\nframes, changes between consecutive frames are minimal, allowing for the\nskipping of redundant computations. This technique, which we term as Diff\nComputation, presents two primary challenges. First, Diff Computation requires\nto cache intermediate feature maps to ensure the correctness of non-linear\ncomputations, leading to significant memory consumption. Second, the imbalance\nof sparsity among layers, introduced by Diff Computation, incurs accuracy\ndegradation. To address these issues, we propose a memory-efficient scheduling\nmethod to eliminate memory overhead and an online adjustment mechanism to\nminimize accuracy degradation. We integrate these techniques into our\nframework, SparseTem, to seamlessly support various CNN-based video encoders.\nSparseTem achieves speedup of 1.79x for EfficientDet and 4.72x for CRNN, with\nminimal accuracy drop and no additional memory overhead. Extensive experimental\nresults demonstrate that SparseTem sets a new state-of-the-art by effectively\nutilizing temporal continuity to accelerate CNN-based video encoders."
                },
                "authors": [
                    {
                        "name": "Kunyun Wang"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Wenchao Ding"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "arxiv_comment": "9 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20790v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20790v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01847v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01847v3",
                "updated": "2024-10-27T14:40:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    27,
                    14,
                    40,
                    8,
                    6,
                    301,
                    0
                ],
                "published": "2024-04-02T11:12:42Z",
                "published_parsed": [
                    2024,
                    4,
                    2,
                    11,
                    12,
                    42,
                    1,
                    93,
                    0
                ],
                "title": "Accelerating Transformer Pre-training with 2:4 Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Transformer Pre-training with 2:4 Sparsity"
                },
                "summary": "Training large transformers is slow, but recent innovations on GPU\narchitecture give us an advantage. NVIDIA Ampere GPUs can execute a\nfine-grained 2:4 sparse matrix multiplication twice as fast as its dense\nequivalent. In the light of this property, we comprehensively investigate the\nfeasibility of accelerating feed-forward networks (FFNs) of transformers in\npre-training. First, we define a ``flip rate'' to monitor the stability of a\n2:4 training process. Utilizing this metric, we propose three techniques to\npreserve accuracy: to modify the sparse-refined straight-through estimator by\napplying the masked decay term on gradients, to determine a feasible decay\nfactor in warm-up stage, and to enhance the model's quality by a dense\nfine-tuning procedure near the end of pre-training. Besides, we devise two\ntechniques to practically accelerate training: to calculate transposable 2:4\nmasks by convolution, and to accelerate gated activation functions by reducing\nGPU L2 cache miss. Experiments show that our 2:4 sparse training algorithm\nachieves similar convergence to dense training algorithms on several\ntransformer pre-training tasks, while actual acceleration can be observed on\ndifferent shapes of transformer block apparently. Our toolkit is available at\nhttps://github.com/huyz2023/2by4-pretrain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large transformers is slow, but recent innovations on GPU\narchitecture give us an advantage. NVIDIA Ampere GPUs can execute a\nfine-grained 2:4 sparse matrix multiplication twice as fast as its dense\nequivalent. In the light of this property, we comprehensively investigate the\nfeasibility of accelerating feed-forward networks (FFNs) of transformers in\npre-training. First, we define a ``flip rate'' to monitor the stability of a\n2:4 training process. Utilizing this metric, we propose three techniques to\npreserve accuracy: to modify the sparse-refined straight-through estimator by\napplying the masked decay term on gradients, to determine a feasible decay\nfactor in warm-up stage, and to enhance the model's quality by a dense\nfine-tuning procedure near the end of pre-training. Besides, we devise two\ntechniques to practically accelerate training: to calculate transposable 2:4\nmasks by convolution, and to accelerate gated activation functions by reducing\nGPU L2 cache miss. Experiments show that our 2:4 sparse training algorithm\nachieves similar convergence to dense training algorithms on several\ntransformer pre-training tasks, while actual acceleration can be observed on\ndifferent shapes of transformer block apparently. Our toolkit is available at\nhttps://github.com/huyz2023/2by4-pretrain."
                },
                "authors": [
                    {
                        "name": "Yuezhou Hu"
                    },
                    {
                        "name": "Kang Zhao"
                    },
                    {
                        "name": "Weiyu Huang"
                    },
                    {
                        "name": "Jianfei Chen"
                    },
                    {
                        "name": "Jun Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhu"
                },
                "author": "Jun Zhu",
                "arxiv_journal_ref": "Proceedings of the 41st International Conference on Machine\n  Learning (2024), in Proceedings of Machine Learning Research 235:19531-19543",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01847v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01847v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20337v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20337v1",
                "updated": "2024-10-27T04:31:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    27,
                    4,
                    31,
                    35,
                    6,
                    301,
                    0
                ],
                "published": "2024-10-27T04:31:35Z",
                "published_parsed": [
                    2024,
                    10,
                    27,
                    4,
                    31,
                    35,
                    6,
                    301,
                    0
                ],
                "title": "On the I/O Complexity of the CYK Algorithm and of a Family of Related DP\n  Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the I/O Complexity of the CYK Algorithm and of a Family of Related DP\n  Algorithms"
                },
                "summary": "Asymptotically tight lower bounds are derived for the Input/Output (I/O)\ncomplexity of a class of dynamic programming algorithms including matrix chain\nmultiplication, optimal polygon triangulation, and the construction of optimal\nbinary search trees. Assuming no recomputation of intermediate values, we\nestablish an $\\Omega\\left(\\frac{n^3}{\\sqrt{M}B}\\right)$ I/O lower bound, where\n$n$ denotes the size of the input and $M$ denotes the size of the available\nfast memory (cache). When recomputation is allowed, we show the same bound\nholds for $M < cn$, where $c$ is a positive constant. In the case where $M \\ge\n2n$, we show an $\\Omega\\left(n/B\\right)$ I/O lower bound. We also discuss\nalgorithms for which the number of executed I/O operations matches\nasymptotically each of the presented lower bounds, which are thus\nasymptotically tight.\n  Additionally, we refine our general method to obtain a lower bound for the\nI/O complexity of the Cocke-Younger-Kasami algorithm, where the size of the\ngrammar impacts the I/O complexity. An upper bound with asymptotically matching\nperformance in many cases is also provided.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asymptotically tight lower bounds are derived for the Input/Output (I/O)\ncomplexity of a class of dynamic programming algorithms including matrix chain\nmultiplication, optimal polygon triangulation, and the construction of optimal\nbinary search trees. Assuming no recomputation of intermediate values, we\nestablish an $\\Omega\\left(\\frac{n^3}{\\sqrt{M}B}\\right)$ I/O lower bound, where\n$n$ denotes the size of the input and $M$ denotes the size of the available\nfast memory (cache). When recomputation is allowed, we show the same bound\nholds for $M < cn$, where $c$ is a positive constant. In the case where $M \\ge\n2n$, we show an $\\Omega\\left(n/B\\right)$ I/O lower bound. We also discuss\nalgorithms for which the number of executed I/O operations matches\nasymptotically each of the presented lower bounds, which are thus\nasymptotically tight.\n  Additionally, we refine our general method to obtain a lower bound for the\nI/O complexity of the Cocke-Younger-Kasami algorithm, where the size of the\ngrammar impacts the I/O complexity. An upper bound with asymptotically matching\nperformance in many cases is also provided."
                },
                "authors": [
                    {
                        "name": "Lorenzo De Stefani"
                    },
                    {
                        "name": "Vedant Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Vedant Gupta"
                },
                "author": "Vedant Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20337v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20337v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04216v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04216v3",
                "updated": "2024-10-26T22:19:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    26,
                    22,
                    19,
                    4,
                    5,
                    300,
                    0
                ],
                "published": "2024-02-06T18:17:02Z",
                "published_parsed": [
                    2024,
                    2,
                    6,
                    18,
                    17,
                    2,
                    1,
                    37,
                    0
                ],
                "title": "Resource-Aware Hierarchical Federated Learning in Wireless Video Caching\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource-Aware Hierarchical Federated Learning in Wireless Video Caching\n  Networks"
                },
                "summary": "Backhaul traffic congestion caused by the video traffic of a few popular\nfiles can be alleviated by storing the to-be-requested content at various\nlevels in wireless video caching networks. Typically, content service providers\n(CSPs) own the content, and the users request their preferred content from the\nCSPs using their (wireless) internet service providers (ISPs). As these parties\ndo not reveal their private information and business secrets, traditional\ntechniques may not be readily used to predict the dynamic changes in users'\nfuture demands. Motivated by this, we propose a novel resource-aware\nhierarchical federated learning (RawHFL) solution for predicting user's future\ncontent requests. A practical data acquisition technique is used that allows\nthe user to update its local training dataset based on its requested content.\nBesides, since networking and other computational resources are limited,\nconsidering that only a subset of the users participate in the model training,\nwe derive the convergence bound of the proposed algorithm. Based on this bound,\nwe minimize a weighted utility function for jointly configuring the\ncontrollable parameters to train the RawHFL energy efficiently under practical\nresource constraints. Our extensive simulation results validate the proposed\nalgorithm's superiority, in terms of test accuracy and energy cost, over\nexisting baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backhaul traffic congestion caused by the video traffic of a few popular\nfiles can be alleviated by storing the to-be-requested content at various\nlevels in wireless video caching networks. Typically, content service providers\n(CSPs) own the content, and the users request their preferred content from the\nCSPs using their (wireless) internet service providers (ISPs). As these parties\ndo not reveal their private information and business secrets, traditional\ntechniques may not be readily used to predict the dynamic changes in users'\nfuture demands. Motivated by this, we propose a novel resource-aware\nhierarchical federated learning (RawHFL) solution for predicting user's future\ncontent requests. A practical data acquisition technique is used that allows\nthe user to update its local training dataset based on its requested content.\nBesides, since networking and other computational resources are limited,\nconsidering that only a subset of the users participate in the model training,\nwe derive the convergence bound of the proposed algorithm. Based on this bound,\nwe minimize a weighted utility function for jointly configuring the\ncontrollable parameters to train the RawHFL energy efficiently under practical\nresource constraints. Our extensive simulation results validate the proposed\nalgorithm's superiority, in terms of test accuracy and energy cost, over\nexisting baselines."
                },
                "authors": [
                    {
                        "name": "Md Ferdous Pervej"
                    },
                    {
                        "name": "Andreas F. Molisch"
                    }
                ],
                "author_detail": {
                    "name": "Andreas F. Molisch"
                },
                "author": "Andreas F. Molisch",
                "arxiv_comment": "Under review for possible publication in IEEE Transactions on\n  Wireless Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04216v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04216v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20149v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20149v1",
                "updated": "2024-10-26T11:20:02Z",
                "updated_parsed": [
                    2024,
                    10,
                    26,
                    11,
                    20,
                    2,
                    5,
                    300,
                    0
                ],
                "published": "2024-10-26T11:20:02Z",
                "published_parsed": [
                    2024,
                    10,
                    26,
                    11,
                    20,
                    2,
                    5,
                    300,
                    0
                ],
                "title": "AdaNeg: Adaptive Negative Proxy Guided OOD Detection with\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaNeg: Adaptive Negative Proxy Guided OOD Detection with\n  Vision-Language Models"
                },
                "summary": "Recent research has shown that pre-trained vision-language models are\neffective at identifying out-of-distribution (OOD) samples by using negative\nlabels as guidance. However, employing consistent negative labels across\ndifferent OOD datasets often results in semantic misalignments, as these text\nlabels may not accurately reflect the actual space of OOD images. To overcome\nthis issue, we introduce \\textit{adaptive negative proxies}, which are\ndynamically generated during testing by exploring actual OOD images, to align\nmore closely with the underlying OOD label space and enhance the efficacy of\nnegative proxy guidance. Specifically, our approach utilizes a feature memory\nbank to selectively cache discriminative features from test images,\nrepresenting the targeted OOD distribution. This facilitates the creation of\nproxies that can better align with specific OOD datasets. While task-adaptive\nproxies average features to reflect the unique characteristics of each dataset,\nthe sample-adaptive proxies weight features based on their similarity to\nindividual test samples, exploring detailed sample-level nuances. The final\nscore for identifying OOD samples integrates static negative labels with our\nproposed adaptive proxies, effectively combining textual and visual knowledge\nfor enhanced performance. Our method is training-free and annotation-free, and\nit maintains fast testing speed. Extensive experiments across various\nbenchmarks demonstrate the effectiveness of our approach, abbreviated as\nAdaNeg. Notably, on the large-scale ImageNet benchmark, our AdaNeg\nsignificantly outperforms existing methods, with a 2.45\\% increase in AUROC and\na 6.48\\% reduction in FPR95. Codes are available at\n\\url{https://github.com/YBZh/OpenOOD-VLM}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research has shown that pre-trained vision-language models are\neffective at identifying out-of-distribution (OOD) samples by using negative\nlabels as guidance. However, employing consistent negative labels across\ndifferent OOD datasets often results in semantic misalignments, as these text\nlabels may not accurately reflect the actual space of OOD images. To overcome\nthis issue, we introduce \\textit{adaptive negative proxies}, which are\ndynamically generated during testing by exploring actual OOD images, to align\nmore closely with the underlying OOD label space and enhance the efficacy of\nnegative proxy guidance. Specifically, our approach utilizes a feature memory\nbank to selectively cache discriminative features from test images,\nrepresenting the targeted OOD distribution. This facilitates the creation of\nproxies that can better align with specific OOD datasets. While task-adaptive\nproxies average features to reflect the unique characteristics of each dataset,\nthe sample-adaptive proxies weight features based on their similarity to\nindividual test samples, exploring detailed sample-level nuances. The final\nscore for identifying OOD samples integrates static negative labels with our\nproposed adaptive proxies, effectively combining textual and visual knowledge\nfor enhanced performance. Our method is training-free and annotation-free, and\nit maintains fast testing speed. Extensive experiments across various\nbenchmarks demonstrate the effectiveness of our approach, abbreviated as\nAdaNeg. Notably, on the large-scale ImageNet benchmark, our AdaNeg\nsignificantly outperforms existing methods, with a 2.45\\% increase in AUROC and\na 6.48\\% reduction in FPR95. Codes are available at\n\\url{https://github.com/YBZh/OpenOOD-VLM}."
                },
                "authors": [
                    {
                        "name": "Yabin Zhang"
                    },
                    {
                        "name": "Lei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lei Zhang"
                },
                "author": "Lei Zhang",
                "arxiv_comment": "NIPS 2024 Camera Ready, Codes are available at\n  \\url{https://github.com/YBZh/OpenOOD-VLM}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20149v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20149v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20004v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20004v1",
                "updated": "2024-10-25T23:17:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    23,
                    17,
                    56,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T23:17:56Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    23,
                    17,
                    56,
                    4,
                    299,
                    0
                ],
                "title": "Lightweight, Secure and Stateful Serverless Computing with PSL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightweight, Secure and Stateful Serverless Computing with PSL"
                },
                "summary": "We present PSL, a lightweight, secure and stateful Function-as-a-Serivce\n(FaaS) framework for Trusted Execution Environments (TEEs). The framework\nprovides rich programming language support on heterogeneous TEE hardware for\nstatically compiled binaries and/or WebAssembly (WASM) bytecodes, with a\nfamiliar Key-Value Store (KVS) interface to secure, performant,\nnetwork-embedded storage. It achieves near-native execution speeds by utilizing\nthe dynamic memory mapping capabilities of Intel SGX2 to create an in-enclave\nWASM runtime with Just-In-Time (JIT) compilation. PSL is designed to\nefficiently operate within an asynchronous environment with a distributed\ntamper-proof confidential storage system, assuming minority failures. The\nsystem exchanges eventually-consistent state updates across nodes while\nutilizing release-consistent locking mechanisms to enhance transactional\ncapabilities. The execution of PSL is up to 3.7x faster than the\nstate-of-the-art SGX WASM runtime. PSL reaches 95k ops/s with YCSB 100% read\nworkload and 89k ops/s with 50% read/write workload. We demonstrate the\nscalability and adaptivity of PSL through a case study of secure and\ndistributed training of deep neural networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present PSL, a lightweight, secure and stateful Function-as-a-Serivce\n(FaaS) framework for Trusted Execution Environments (TEEs). The framework\nprovides rich programming language support on heterogeneous TEE hardware for\nstatically compiled binaries and/or WebAssembly (WASM) bytecodes, with a\nfamiliar Key-Value Store (KVS) interface to secure, performant,\nnetwork-embedded storage. It achieves near-native execution speeds by utilizing\nthe dynamic memory mapping capabilities of Intel SGX2 to create an in-enclave\nWASM runtime with Just-In-Time (JIT) compilation. PSL is designed to\nefficiently operate within an asynchronous environment with a distributed\ntamper-proof confidential storage system, assuming minority failures. The\nsystem exchanges eventually-consistent state updates across nodes while\nutilizing release-consistent locking mechanisms to enhance transactional\ncapabilities. The execution of PSL is up to 3.7x faster than the\nstate-of-the-art SGX WASM runtime. PSL reaches 95k ops/s with YCSB 100% read\nworkload and 89k ops/s with 50% read/write workload. We demonstrate the\nscalability and adaptivity of PSL through a case study of secure and\ndistributed training of deep neural networks."
                },
                "authors": [
                    {
                        "name": "Alexander Thomas"
                    },
                    {
                        "name": "Shubham Mishra"
                    },
                    {
                        "name": "Kaiyuan Chen"
                    },
                    {
                        "name": "John Kubiatowicz"
                    }
                ],
                "author_detail": {
                    "name": "John Kubiatowicz"
                },
                "author": "John Kubiatowicz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20004v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20004v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05317v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05317v2",
                "updated": "2024-10-25T21:09:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    21,
                    9,
                    59,
                    4,
                    299,
                    0
                ],
                "published": "2024-06-08T01:35:11Z",
                "published_parsed": [
                    2024,
                    6,
                    8,
                    1,
                    35,
                    11,
                    5,
                    160,
                    0
                ],
                "title": "LoCoCo: Dropping In Convolutions for Long Context Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoCoCo: Dropping In Convolutions for Long Context Compression"
                },
                "summary": "This paper tackles the memory hurdle of processing long context sequences in\nLarge Language Models (LLMs), by presenting a novel approach, Dropping In\nConvolutions for Long Context Compression (LoCoCo). LoCoCo employs only a\nfixed-size Key-Value (KV) cache, and can enhance efficiency in both inference\nand fine-tuning stages. Diverging from prior methods that selectively drop KV\npairs based on heuristics, LoCoCo leverages a data-driven adaptive fusion\ntechnique, blending previous KV pairs with incoming tokens to minimize the loss\nof contextual information and ensure accurate attention modeling. This token\nintegration is achieved through injecting one-dimensional convolutional kernels\nthat dynamically calculate mixing weights for each KV cache slot. Designed for\nbroad compatibility with existing LLM frameworks, LoCoCo allows for\nstraightforward \"drop-in\" integration without needing architectural\nmodifications, while incurring minimal tuning overhead. Experiments demonstrate\nthat LoCoCo maintains consistently outstanding performance across various\ncontext lengths and can achieve a high context compression rate during both\ninference and fine-tuning phases. During inference, we successfully compressed\nup to 3482 tokens into a 128-size KV cache, while retaining comparable\nperformance to the full sequence - an accuracy improvement of up to 0.2791\ncompared to baselines at the same cache size. During post-training tuning, we\nalso effectively extended the context length from 4K to 32K using a KV cache of\nfixed size 512, achieving performance similar to fine-tuning with entire\nsequences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper tackles the memory hurdle of processing long context sequences in\nLarge Language Models (LLMs), by presenting a novel approach, Dropping In\nConvolutions for Long Context Compression (LoCoCo). LoCoCo employs only a\nfixed-size Key-Value (KV) cache, and can enhance efficiency in both inference\nand fine-tuning stages. Diverging from prior methods that selectively drop KV\npairs based on heuristics, LoCoCo leverages a data-driven adaptive fusion\ntechnique, blending previous KV pairs with incoming tokens to minimize the loss\nof contextual information and ensure accurate attention modeling. This token\nintegration is achieved through injecting one-dimensional convolutional kernels\nthat dynamically calculate mixing weights for each KV cache slot. Designed for\nbroad compatibility with existing LLM frameworks, LoCoCo allows for\nstraightforward \"drop-in\" integration without needing architectural\nmodifications, while incurring minimal tuning overhead. Experiments demonstrate\nthat LoCoCo maintains consistently outstanding performance across various\ncontext lengths and can achieve a high context compression rate during both\ninference and fine-tuning phases. During inference, we successfully compressed\nup to 3482 tokens into a 128-size KV cache, while retaining comparable\nperformance to the full sequence - an accuracy improvement of up to 0.2791\ncompared to baselines at the same cache size. During post-training tuning, we\nalso effectively extended the context length from 4K to 32K using a KV cache of\nfixed size 512, achieving performance similar to fine-tuning with entire\nsequences."
                },
                "authors": [
                    {
                        "name": "Ruisi Cai"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05317v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05317v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03766v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03766v2",
                "updated": "2024-10-25T19:45:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    19,
                    45,
                    33,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-02T15:22:08Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    15,
                    22,
                    8,
                    2,
                    276,
                    0
                ],
                "title": "FutureFill: Fast Generation from Convolutional Sequence Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FutureFill: Fast Generation from Convolutional Sequence Models"
                },
                "summary": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill - a method for fast generation that\napplies to any sequence prediction algorithm based on convolutional operators.\nOur approach reduces the generation time requirement from quadratic to\nquasilinear relative to the context length. Additionally, FutureFill requires a\nprefill cache sized only by the number of tokens generated, which is smaller\nthan the cache requirements for standard convolutional and attention-based\nmodels. We validate our theoretical findings with experimental evidence\ndemonstrating correctness and efficiency gains in a synthetic generation task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the challenge of efficient auto-regressive generation in sequence\nprediction models by introducing FutureFill - a method for fast generation that\napplies to any sequence prediction algorithm based on convolutional operators.\nOur approach reduces the generation time requirement from quadratic to\nquasilinear relative to the context length. Additionally, FutureFill requires a\nprefill cache sized only by the number of tokens generated, which is smaller\nthan the cache requirements for standard convolutional and attention-based\nmodels. We validate our theoretical findings with experimental evidence\ndemonstrating correctness and efficiency gains in a synthetic generation task."
                },
                "authors": [
                    {
                        "name": "Naman Agarwal"
                    },
                    {
                        "name": "Xinyi Chen"
                    },
                    {
                        "name": "Evan Dogariu"
                    },
                    {
                        "name": "Vlad Feinberg"
                    },
                    {
                        "name": "Daniel Suo"
                    },
                    {
                        "name": "Peter Bartlett"
                    },
                    {
                        "name": "Elad Hazan"
                    }
                ],
                "author_detail": {
                    "name": "Elad Hazan"
                },
                "author": "Elad Hazan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03766v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03766v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19937v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19937v1",
                "updated": "2024-10-25T19:18:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    19,
                    18,
                    22,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T19:18:22Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    19,
                    18,
                    22,
                    4,
                    299,
                    0
                ],
                "title": "RobustKV: Defending Large Language Models against Jailbreak Attacks via\n  KV Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RobustKV: Defending Large Language Models against Jailbreak Attacks via\n  KV Eviction"
                },
                "summary": "Jailbreak attacks circumvent LLMs' built-in safeguards by concealing harmful\nqueries within jailbreak prompts. While existing defenses primarily focus on\nmitigating the effects of jailbreak prompts, they often prove inadequate as\njailbreak prompts can take arbitrary, adaptive forms. This paper presents\nRobustKV, a novel defense that adopts a fundamentally different approach by\nselectively removing critical tokens of harmful queries from key-value (KV)\ncaches. Intuitively, for a jailbreak prompt to be effective, its tokens must\nachieve sufficient `importance' (as measured by attention scores), which\ninevitably lowers the importance of tokens in the concealed harmful query.\nThus, by strategically evicting the KVs of the lowest-ranked tokens, RobustKV\ndiminishes the presence of the harmful query in the KV cache, thus preventing\nthe LLM from generating malicious responses. Extensive evaluation using\nbenchmark datasets and models demonstrates that RobustKV effectively counters\nstate-of-the-art jailbreak attacks while maintaining the LLM's general\nperformance on benign queries. Moreover, RobustKV creates an intriguing\nevasiveness dilemma for adversaries, forcing them to balance between evading\nRobustKV and bypassing the LLM's built-in safeguards. This trade-off\ncontributes to RobustKV's robustness against adaptive attacks. (warning: this\npaper contains potentially harmful content generated by LLMs.)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jailbreak attacks circumvent LLMs' built-in safeguards by concealing harmful\nqueries within jailbreak prompts. While existing defenses primarily focus on\nmitigating the effects of jailbreak prompts, they often prove inadequate as\njailbreak prompts can take arbitrary, adaptive forms. This paper presents\nRobustKV, a novel defense that adopts a fundamentally different approach by\nselectively removing critical tokens of harmful queries from key-value (KV)\ncaches. Intuitively, for a jailbreak prompt to be effective, its tokens must\nachieve sufficient `importance' (as measured by attention scores), which\ninevitably lowers the importance of tokens in the concealed harmful query.\nThus, by strategically evicting the KVs of the lowest-ranked tokens, RobustKV\ndiminishes the presence of the harmful query in the KV cache, thus preventing\nthe LLM from generating malicious responses. Extensive evaluation using\nbenchmark datasets and models demonstrates that RobustKV effectively counters\nstate-of-the-art jailbreak attacks while maintaining the LLM's general\nperformance on benign queries. Moreover, RobustKV creates an intriguing\nevasiveness dilemma for adversaries, forcing them to balance between evading\nRobustKV and bypassing the LLM's built-in safeguards. This trade-off\ncontributes to RobustKV's robustness against adaptive attacks. (warning: this\npaper contains potentially harmful content generated by LLMs.)"
                },
                "authors": [
                    {
                        "name": "Tanqiu Jiang"
                    },
                    {
                        "name": "Zian Wang"
                    },
                    {
                        "name": "Jiacheng Liang"
                    },
                    {
                        "name": "Changjiang Li"
                    },
                    {
                        "name": "Yuhui Wang"
                    },
                    {
                        "name": "Ting Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ting Wang"
                },
                "author": "Ting Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19937v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19937v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18248v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18248v2",
                "updated": "2024-10-25T19:18:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    19,
                    18,
                    0,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-23T19:53:30Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    19,
                    53,
                    30,
                    2,
                    297,
                    0
                ],
                "title": "Fast Inference for Augmented Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Inference for Augmented Large Language Models"
                },
                "summary": "Augmented Large Language Models (LLMs) enhance the capabilities of standalone\nLLMs by integrating external data sources through API calls. In interactive LLM\napplications, efficient scheduling is crucial for maintaining low request\ncompletion times, directly impacting user engagement. However, these\naugmentations introduce scheduling challenges due to the need to manage limited\nmemory for cached information (KV caches). As a result, traditional size-based\nscheduling algorithms, such as Shortest Job First (SJF), become less effective\nat minimizing completion times. Existing work focuses only on handling requests\nduring API calls by preserving, discarding, or swapping memory without\nconsidering how to schedule requests with API calls. In this paper, we propose\nLAMPS, a novel LLM inference framework for augmented LLMs. LAMPS minimizes\nrequest completion time through a unified scheduling approach that considers\nthe total length of requests and their handling strategies during API calls.\nRecognizing that LLM inference is memory-bound, our approach ranks requests\nbased on their consumption of memory over time, which depends on both the\noutput sizes and how a request is managed during its API calls. To implement\nour scheduling, LAMPS predicts the strategy that minimizes memory waste of a\nrequest during its API calls, aligning with but improving upon existing\napproaches. We also propose starvation prevention techniques and optimizations\nto mitigate the overhead of our scheduling. We implement LAMPS on top of vLLM\nand evaluate its performance against baseline LLM inference systems,\ndemonstrating improvements in end-to-end latency by 27%-85% and reductions in\nTTFT by 4%-96% compared to the existing augmented-LLM system, with even greater\ngains over vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmented Large Language Models (LLMs) enhance the capabilities of standalone\nLLMs by integrating external data sources through API calls. In interactive LLM\napplications, efficient scheduling is crucial for maintaining low request\ncompletion times, directly impacting user engagement. However, these\naugmentations introduce scheduling challenges due to the need to manage limited\nmemory for cached information (KV caches). As a result, traditional size-based\nscheduling algorithms, such as Shortest Job First (SJF), become less effective\nat minimizing completion times. Existing work focuses only on handling requests\nduring API calls by preserving, discarding, or swapping memory without\nconsidering how to schedule requests with API calls. In this paper, we propose\nLAMPS, a novel LLM inference framework for augmented LLMs. LAMPS minimizes\nrequest completion time through a unified scheduling approach that considers\nthe total length of requests and their handling strategies during API calls.\nRecognizing that LLM inference is memory-bound, our approach ranks requests\nbased on their consumption of memory over time, which depends on both the\noutput sizes and how a request is managed during its API calls. To implement\nour scheduling, LAMPS predicts the strategy that minimizes memory waste of a\nrequest during its API calls, aligning with but improving upon existing\napproaches. We also propose starvation prevention techniques and optimizations\nto mitigate the overhead of our scheduling. We implement LAMPS on top of vLLM\nand evaluate its performance against baseline LLM inference systems,\ndemonstrating improvements in end-to-end latency by 27%-85% and reductions in\nTTFT by 4%-96% compared to the existing augmented-LLM system, with even greater\ngains over vLLM."
                },
                "authors": [
                    {
                        "name": "Rana Shahout"
                    },
                    {
                        "name": "Cong Liang"
                    },
                    {
                        "name": "Shiji Xin"
                    },
                    {
                        "name": "Qianru Lao"
                    },
                    {
                        "name": "Yong Cui"
                    },
                    {
                        "name": "Minlan Yu"
                    },
                    {
                        "name": "Michael Mitzenmacher"
                    }
                ],
                "author_detail": {
                    "name": "Michael Mitzenmacher"
                },
                "author": "Michael Mitzenmacher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18248v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18248v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.18079v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.18079v5",
                "updated": "2024-10-25T18:29:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    18,
                    29,
                    43,
                    4,
                    299,
                    0
                ],
                "published": "2024-01-31T18:58:14Z",
                "published_parsed": [
                    2024,
                    1,
                    31,
                    18,
                    58,
                    14,
                    2,
                    31,
                    0
                ],
                "title": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache\n  Quantization"
                },
                "summary": "LLMs are seeing growing use for applications which require large context\nwindows, and with these large context windows KV cache activations surface as\nthe dominant contributor to memory consumption during inference. Quantization\nis a promising approach for compressing KV cache activations; however, existing\nsolutions fail to represent activations accurately in sub-4-bit precision. Our\nwork, KVQuant, facilitates low precision KV cache quantization by incorporating\nseveral novel methods: (i) Per-Channel Key Quantization, where we adjust the\ndimension along which we quantize the Key activations to better match the\ndistribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations\nbefore the rotary positional embedding to mitigate its impact on quantization;\n(iii) Non-Uniform KV Cache Quantization, where we derive per-layer\nsensitivity-weighted non-uniform datatypes that better represent the\ndistributions; and (iv) Per-Vector Dense-and-Sparse Quantization, where we\nisolate outliers separately for each vector to minimize skews in quantization\nranges. By applying our method to the LLaMA, Llama-2, Llama-3, and Mistral\nmodels, we achieve < 0.1 perplexity degradation with 3-bit quantization on both\nWikitext-2 and C4, outperforming existing approaches. Our method enables\nserving LLaMA-7B with a context length of up to 1 million on a single A100-80GB\nGPU and up to 10 million on an 8-GPU system. We develop custom CUDA kernels for\nKVQuant, showing that we can achieve up to ~1.7x speedups, compared to baseline\nfp16 matrix-vector multiplications, for the LLaMA-7B model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are seeing growing use for applications which require large context\nwindows, and with these large context windows KV cache activations surface as\nthe dominant contributor to memory consumption during inference. Quantization\nis a promising approach for compressing KV cache activations; however, existing\nsolutions fail to represent activations accurately in sub-4-bit precision. Our\nwork, KVQuant, facilitates low precision KV cache quantization by incorporating\nseveral novel methods: (i) Per-Channel Key Quantization, where we adjust the\ndimension along which we quantize the Key activations to better match the\ndistribution; (ii) Pre-RoPE Key Quantization, where we quantize Key activations\nbefore the rotary positional embedding to mitigate its impact on quantization;\n(iii) Non-Uniform KV Cache Quantization, where we derive per-layer\nsensitivity-weighted non-uniform datatypes that better represent the\ndistributions; and (iv) Per-Vector Dense-and-Sparse Quantization, where we\nisolate outliers separately for each vector to minimize skews in quantization\nranges. By applying our method to the LLaMA, Llama-2, Llama-3, and Mistral\nmodels, we achieve < 0.1 perplexity degradation with 3-bit quantization on both\nWikitext-2 and C4, outperforming existing approaches. Our method enables\nserving LLaMA-7B with a context length of up to 1 million on a single A100-80GB\nGPU and up to 10 million on an 8-GPU system. We develop custom CUDA kernels for\nKVQuant, showing that we can achieve up to ~1.7x speedups, compared to baseline\nfp16 matrix-vector multiplications, for the LLaMA-7B model."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Hiva Mohammadzadeh"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Yakun Sophia Shao"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.18079v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.18079v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19355v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19355v1",
                "updated": "2024-10-25T07:24:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    25,
                    7,
                    24,
                    38,
                    4,
                    299,
                    0
                ],
                "published": "2024-10-25T07:24:38Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    7,
                    24,
                    38,
                    4,
                    299,
                    0
                ],
                "title": "FasterCache: Training-Free Video Diffusion Model Acceleration with High\n  Quality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FasterCache: Training-Free Video Diffusion Model Acceleration with High\n  Quality"
                },
                "summary": "In this paper, we present \\textbf{\\textit{FasterCache}}, a novel\ntraining-free strategy designed to accelerate the inference of video diffusion\nmodels with high-quality generation. By analyzing existing cache-based methods,\nwe observe that \\textit{directly reusing adjacent-step features degrades video\nquality due to the loss of subtle variations}. We further perform a pioneering\ninvestigation of the acceleration potential of classifier-free guidance (CFG)\nand reveal significant redundancy between conditional and unconditional\nfeatures within the same timestep. Capitalizing on these observations, we\nintroduce FasterCache to substantially accelerate diffusion-based video\ngeneration. Our key contributions include a dynamic feature reuse strategy that\npreserves both feature distinction and temporal continuity, and CFG-Cache which\noptimizes the reuse of conditional and unconditional outputs to further enhance\ninference speed without compromising video quality. We empirically evaluate\nFasterCache on recent video diffusion models. Experimental results show that\nFasterCache can significantly accelerate video generation (\\eg 1.67$\\times$\nspeedup on Vchitect-2.0) while keeping video quality comparable to the\nbaseline, and consistently outperform existing methods in both inference speed\nand video quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present \\textbf{\\textit{FasterCache}}, a novel\ntraining-free strategy designed to accelerate the inference of video diffusion\nmodels with high-quality generation. By analyzing existing cache-based methods,\nwe observe that \\textit{directly reusing adjacent-step features degrades video\nquality due to the loss of subtle variations}. We further perform a pioneering\ninvestigation of the acceleration potential of classifier-free guidance (CFG)\nand reveal significant redundancy between conditional and unconditional\nfeatures within the same timestep. Capitalizing on these observations, we\nintroduce FasterCache to substantially accelerate diffusion-based video\ngeneration. Our key contributions include a dynamic feature reuse strategy that\npreserves both feature distinction and temporal continuity, and CFG-Cache which\noptimizes the reuse of conditional and unconditional outputs to further enhance\ninference speed without compromising video quality. We empirically evaluate\nFasterCache on recent video diffusion models. Experimental results show that\nFasterCache can significantly accelerate video generation (\\eg 1.67$\\times$\nspeedup on Vchitect-2.0) while keeping video quality comparable to the\nbaseline, and consistently outperform existing methods in both inference speed\nand video quality."
                },
                "authors": [
                    {
                        "name": "Zhengyao Lv"
                    },
                    {
                        "name": "Chenyang Si"
                    },
                    {
                        "name": "Junhao Song"
                    },
                    {
                        "name": "Zhenyu Yang"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Ziwei Liu"
                    },
                    {
                        "name": "Kwan-Yee K. Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Yee K. Wong"
                },
                "author": "Kwan-Yee K. Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19355v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19355v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19123v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19123v1",
                "updated": "2024-10-24T19:48:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    19,
                    48,
                    51,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T19:48:51Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    19,
                    48,
                    51,
                    3,
                    298,
                    0
                ],
                "title": "Read-ME: Refactorizing LLMs as Router-Decoupled Mixture of Experts with\n  System Co-Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Read-ME: Refactorizing LLMs as Router-Decoupled Mixture of Experts with\n  System Co-Design"
                },
                "summary": "The proliferation of large language models (LLMs) has led to the adoption of\nMixture-of-Experts (MoE) architectures that dynamically leverage specialized\nsubnetworks for improved efficiency and performance. Despite their benefits,\nMoE models face significant challenges during inference, including inefficient\nmemory management and suboptimal batching, due to misaligned design choices\nbetween the model architecture and the system policies. Furthermore, the\nconventional approach of training MoEs from scratch is increasingly prohibitive\nin terms of cost. In this paper, we propose a novel framework Read-ME that\ntransforms pre-trained dense LLMs into smaller MoE models (in contrast to\n\"upcycling\" generalist MoEs), avoiding the high costs of ground-up training.\nOur approach employs activation sparsity to extract experts. To compose\nexperts, we examine the widely-adopted layer-wise router design and show its\nredundancy, and thus we introduce the pre-gating router decoupled from the MoE\nbackbone that facilitates system-friendly pre-computing and lookahead\nscheduling, enhancing expert-aware batching and caching. Our codesign therefore\naddresses critical gaps on both the algorithmic and system fronts, establishing\na scalable and efficient alternative for LLM inference in resource-constrained\nsettings. Read-ME outperforms other popular open-source dense models of similar\nscales, achieving improvements of up to 10.1% on MMLU, and improving mean\nend-to-end latency up to 6.1%. Codes are available at:\nhttps://github.com/VITA-Group/READ-ME.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of large language models (LLMs) has led to the adoption of\nMixture-of-Experts (MoE) architectures that dynamically leverage specialized\nsubnetworks for improved efficiency and performance. Despite their benefits,\nMoE models face significant challenges during inference, including inefficient\nmemory management and suboptimal batching, due to misaligned design choices\nbetween the model architecture and the system policies. Furthermore, the\nconventional approach of training MoEs from scratch is increasingly prohibitive\nin terms of cost. In this paper, we propose a novel framework Read-ME that\ntransforms pre-trained dense LLMs into smaller MoE models (in contrast to\n\"upcycling\" generalist MoEs), avoiding the high costs of ground-up training.\nOur approach employs activation sparsity to extract experts. To compose\nexperts, we examine the widely-adopted layer-wise router design and show its\nredundancy, and thus we introduce the pre-gating router decoupled from the MoE\nbackbone that facilitates system-friendly pre-computing and lookahead\nscheduling, enhancing expert-aware batching and caching. Our codesign therefore\naddresses critical gaps on both the algorithmic and system fronts, establishing\na scalable and efficient alternative for LLM inference in resource-constrained\nsettings. Read-ME outperforms other popular open-source dense models of similar\nscales, achieving improvements of up to 10.1% on MMLU, and improving mean\nend-to-end latency up to 6.1%. Codes are available at:\nhttps://github.com/VITA-Group/READ-ME."
                },
                "authors": [
                    {
                        "name": "Ruisi Cai"
                    },
                    {
                        "name": "Yeonju Ro"
                    },
                    {
                        "name": "Geon-Woo Kim"
                    },
                    {
                        "name": "Peihao Wang"
                    },
                    {
                        "name": "Babak Ehteshami Bejnordi"
                    },
                    {
                        "name": "Aditya Akella"
                    },
                    {
                        "name": "Zhangyang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhangyang Wang"
                },
                "author": "Zhangyang Wang",
                "arxiv_comment": "38th Conference on Neural Information Processing Systems (NeurIPS\n  2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19123v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19123v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.15420v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.15420v2",
                "updated": "2024-10-24T16:40:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    16,
                    40,
                    10,
                    3,
                    298,
                    0
                ],
                "published": "2024-04-23T18:10:42Z",
                "published_parsed": [
                    2024,
                    4,
                    23,
                    18,
                    10,
                    42,
                    1,
                    114,
                    0
                ],
                "title": "XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XC-Cache: Cross-Attending to Cached Context for Efficient LLM Inference"
                },
                "summary": "In-context learning (ICL) approaches typically leverage prompting to\ncondition decoder-only language model generation on reference information.\nJust-in-time processing of a context is inefficient due to the quadratic cost\nof self-attention operations, and caching is desirable. However, caching\ntransformer states can easily require almost as much space as the model\nparameters. When the right context isn't known in advance, caching ICL can be\nchallenging. This work addresses these limitations by introducing models that,\ninspired by the encoder-decoder architecture, use cross-attention to condition\ngeneration on reference text without the prompt. More precisely, we leverage\npre-trained decoder-only models and only train a small number of added layers.\nWe use Question-Answering (QA) as a testbed to evaluate the ability of our\nmodels to perform conditional generation and observe that they outperform ICL,\nare comparable to fine-tuned prompted LLMs, and drastically reduce the space\nfootprint relative to standard KV caching by two orders of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) approaches typically leverage prompting to\ncondition decoder-only language model generation on reference information.\nJust-in-time processing of a context is inefficient due to the quadratic cost\nof self-attention operations, and caching is desirable. However, caching\ntransformer states can easily require almost as much space as the model\nparameters. When the right context isn't known in advance, caching ICL can be\nchallenging. This work addresses these limitations by introducing models that,\ninspired by the encoder-decoder architecture, use cross-attention to condition\ngeneration on reference text without the prompt. More precisely, we leverage\npre-trained decoder-only models and only train a small number of added layers.\nWe use Question-Answering (QA) as a testbed to evaluate the ability of our\nmodels to perform conditional generation and observe that they outperform ICL,\nare comparable to fine-tuned prompted LLMs, and drastically reduce the space\nfootprint relative to standard KV caching by two orders of magnitude."
                },
                "authors": [
                    {
                        "name": "Joo Monteiro"
                    },
                    {
                        "name": "tienne Marcotte"
                    },
                    {
                        "name": "Pierre-Andr Nol"
                    },
                    {
                        "name": "Valentina Zantedeschi"
                    },
                    {
                        "name": "David Vzquez"
                    },
                    {
                        "name": "Nicolas Chapados"
                    },
                    {
                        "name": "Christopher Pal"
                    },
                    {
                        "name": "Perouz Taslakian"
                    }
                ],
                "author_detail": {
                    "name": "Perouz Taslakian"
                },
                "author": "Perouz Taslakian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.15420v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.15420v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18517v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18517v1",
                "updated": "2024-10-24T08:06:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    8,
                    6,
                    41,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T08:06:41Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    8,
                    6,
                    41,
                    3,
                    298,
                    0
                ],
                "title": "KVSharer: Efficient Inference via Layer-Wise Dissimilar KV Cache Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVSharer: Efficient Inference via Layer-Wise Dissimilar KV Cache Sharing"
                },
                "summary": "The development of large language models (LLMs) has significantly expanded\nmodel sizes, resulting in substantial GPU memory requirements during inference.\nThe key and value storage of the attention map in the KV (key-value) cache\naccounts for more than 80\\% of this memory consumption. Nowadays, most existing\nKV cache compression methods focus on intra-layer compression within a single\nTransformer layer but few works consider layer-wise compression. In this paper,\nwe propose a plug-and-play method called \\textit{KVSharer}, which shares the KV\ncache between layers to achieve layer-wise compression. Rather than intuitively\nsharing based on higher similarity, we discover a counterintuitive phenomenon:\nsharing dissimilar KV caches better preserves the model performance.\nExperiments show that \\textit{KVSharer} can reduce KV cache computation by\n30\\%, thereby lowering memory consumption without significantly impacting model\nperformance and it can also achieve at least 1.3 times generation acceleration.\nAdditionally, we verify that \\textit{KVSharer} is compatible with existing\nintra-layer KV cache compression methods, and combining both can further save\nmemory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of large language models (LLMs) has significantly expanded\nmodel sizes, resulting in substantial GPU memory requirements during inference.\nThe key and value storage of the attention map in the KV (key-value) cache\naccounts for more than 80\\% of this memory consumption. Nowadays, most existing\nKV cache compression methods focus on intra-layer compression within a single\nTransformer layer but few works consider layer-wise compression. In this paper,\nwe propose a plug-and-play method called \\textit{KVSharer}, which shares the KV\ncache between layers to achieve layer-wise compression. Rather than intuitively\nsharing based on higher similarity, we discover a counterintuitive phenomenon:\nsharing dissimilar KV caches better preserves the model performance.\nExperiments show that \\textit{KVSharer} can reduce KV cache computation by\n30\\%, thereby lowering memory consumption without significantly impacting model\nperformance and it can also achieve at least 1.3 times generation acceleration.\nAdditionally, we verify that \\textit{KVSharer} is compatible with existing\nintra-layer KV cache compression methods, and combining both can further save\nmemory."
                },
                "authors": [
                    {
                        "name": "Yifei Yang"
                    },
                    {
                        "name": "Zouying Cao"
                    },
                    {
                        "name": "Qiguang Chen"
                    },
                    {
                        "name": "Libo Qin"
                    },
                    {
                        "name": "Dongjie Yang"
                    },
                    {
                        "name": "Hai Zhao"
                    },
                    {
                        "name": "Zhi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Chen"
                },
                "author": "Zhi Chen",
                "arxiv_comment": "Under Review by ICLR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18517v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18517v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18441v1",
                "updated": "2024-10-24T05:29:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    24,
                    5,
                    29,
                    20,
                    3,
                    298,
                    0
                ],
                "published": "2024-10-24T05:29:20Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    5,
                    29,
                    20,
                    3,
                    298,
                    0
                ],
                "title": "The Nature of Mathematical Modeling and Probabilistic Optimization\n  Engineering in Generative AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Nature of Mathematical Modeling and Probabilistic Optimization\n  Engineering in Generative AI"
                },
                "summary": "In this paper, we give an in-depth analysis on the mathematical problem\nformulations and the probabilistic optimization explorations for some of the\nkey components in Transformer model [33] in the field of generative AI. We\nexplore and discuss some potential further enhancement for current state of the\nart methods for some key underlying technologies of generative AI models from\nalgorithmic and probabilistic optimization perspective. In particular, we\npresent an optimal solution for sub-word encoding (SWE) based on similar\ninitial settings as that of byte-pair encoding (BPE) algorithm in [9] with\nsimilar objectives as that of WordPiece approach in [28, 31] to maximize the\nlikelihood of the training data. We also present cross entropy optimization\nmethod to optimize hyperparameters for word2vec model [17]. In addition, we\npropose a factored combination of rotary positional encoding (RoPE) [32] and\nattention with linear biases (ALiBi) [23] with a harmonic series. We also\npresent a probabilistic FlashAttention [6, 7] (PrFlashAttention) method with a\nprobability distribution over block distances in the matrix to decide which\nblock is likely to participate in a given round of attention computation while\nmaintaining the lower triangle shape of the tensor for autoregressive language\nmodels by re-shaping the tensors. Finally, we present staircase adaptive\nquantization (SAQ) of key-value (KV) cache for multi-query attention (MQA)\nbased on the framework presented in [16] to have gradual quantization\ndegradation while achieving reasonable model quality and cost savings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we give an in-depth analysis on the mathematical problem\nformulations and the probabilistic optimization explorations for some of the\nkey components in Transformer model [33] in the field of generative AI. We\nexplore and discuss some potential further enhancement for current state of the\nart methods for some key underlying technologies of generative AI models from\nalgorithmic and probabilistic optimization perspective. In particular, we\npresent an optimal solution for sub-word encoding (SWE) based on similar\ninitial settings as that of byte-pair encoding (BPE) algorithm in [9] with\nsimilar objectives as that of WordPiece approach in [28, 31] to maximize the\nlikelihood of the training data. We also present cross entropy optimization\nmethod to optimize hyperparameters for word2vec model [17]. In addition, we\npropose a factored combination of rotary positional encoding (RoPE) [32] and\nattention with linear biases (ALiBi) [23] with a harmonic series. We also\npresent a probabilistic FlashAttention [6, 7] (PrFlashAttention) method with a\nprobability distribution over block distances in the matrix to decide which\nblock is likely to participate in a given round of attention computation while\nmaintaining the lower triangle shape of the tensor for autoregressive language\nmodels by re-shaping the tensors. Finally, we present staircase adaptive\nquantization (SAQ) of key-value (KV) cache for multi-query attention (MQA)\nbased on the framework presented in [16] to have gradual quantization\ndegradation while achieving reasonable model quality and cost savings."
                },
                "authors": [
                    {
                        "name": "Fulu Li"
                    }
                ],
                "author_detail": {
                    "name": "Fulu Li"
                },
                "author": "Fulu Li",
                "arxiv_comment": "19 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18002v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18002v1",
                "updated": "2024-10-23T16:25:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    25,
                    22,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T16:25:22Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    16,
                    25,
                    22,
                    2,
                    297,
                    0
                ],
                "title": "Digital Network Twins for Next-generation Wireless: Creation,\n  Optimization, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Network Twins for Next-generation Wireless: Creation,\n  Optimization, and Challenges"
                },
                "summary": "Digital network twins (DNTs), by representing a physical network using a\nvirtual model, offer significant benefits such as streamlined network\ndevelopment, enhanced productivity, and cost reduction for next-generation\n(nextG) communication infrastructure. Existing works mainly describe the\ndeployment of DNT technologies in various service sections.The full life cycle\nof DNTs for telecommunication has not yet been comprehensively studied,\nparticularly in the aspects of fine-grained creation, real-time adaptation,\nresource-efficient deployment, and security protection. This article presents\nan in-depth overview of DNTs, exploring their concrete integration into\nnetworks and communication, covering the fundamental designs, the emergent\napplications, and critical challenges in multiple dimensions. We also include\ntwo detailed case studies to illustrate how DNTs can be applied in real-world\nscenarios such as wireless traffic forecasting and edge caching. Additionally,\na forward-looking vision of the research opportunities in tackling the\nchallenges of DNTs is provided, aiming to fully maximize the benefits of DNTs\nin nextG networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital network twins (DNTs), by representing a physical network using a\nvirtual model, offer significant benefits such as streamlined network\ndevelopment, enhanced productivity, and cost reduction for next-generation\n(nextG) communication infrastructure. Existing works mainly describe the\ndeployment of DNT technologies in various service sections.The full life cycle\nof DNTs for telecommunication has not yet been comprehensively studied,\nparticularly in the aspects of fine-grained creation, real-time adaptation,\nresource-efficient deployment, and security protection. This article presents\nan in-depth overview of DNTs, exploring their concrete integration into\nnetworks and communication, covering the fundamental designs, the emergent\napplications, and critical challenges in multiple dimensions. We also include\ntwo detailed case studies to illustrate how DNTs can be applied in real-world\nscenarios such as wireless traffic forecasting and edge caching. Additionally,\na forward-looking vision of the research opportunities in tackling the\nchallenges of DNTs is provided, aiming to fully maximize the benefits of DNTs\nin nextG networks."
                },
                "authors": [
                    {
                        "name": "Yuchen Liu"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Zifan Zhang"
                    },
                    {
                        "name": "Hanzhi Yu"
                    },
                    {
                        "name": "Mingzhe Chen"
                    }
                ],
                "author_detail": {
                    "name": "Mingzhe Chen"
                },
                "author": "Mingzhe Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18002v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.08437v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.08437v2",
                "updated": "2024-10-23T15:44:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    44,
                    9,
                    2,
                    297,
                    0
                ],
                "published": "2023-10-12T16:01:46Z",
                "published_parsed": [
                    2023,
                    10,
                    12,
                    16,
                    1,
                    46,
                    3,
                    285,
                    0
                ],
                "title": "Cold Start Latency in Serverless Computing: A Systematic Review,\n  Taxonomy, and Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cold Start Latency in Serverless Computing: A Systematic Review,\n  Taxonomy, and Future Directions"
                },
                "summary": "Recently, academics and the corporate sector have paid attention to\nserverless computing, which enables dynamic scalability and an economic model.\nIn serverless computing, users only pay for the time they actually use\nresources, enabling zero scaling to optimise cost and resource utilisation.\nHowever, this approach also introduces the serverless cold start problem.\nResearchers have developed various solutions to address the cold start problem,\nyet it remains an unresolved research area. In this article, we propose a\nsystematic literature review on clod start latency in serverless computing.\nFurthermore, we create a detailed taxonomy of approaches to cold start latency,\nwhich we use to investigate existing techniques for reducing the cold start\ntime and frequency. We have classified the current studies on cold start\nlatency into several categories such as caching and application-level\noptimisation-based solutions, as well as Artificial Intelligence (AI)/Machine\nLearning (ML)-based solutions. Moreover, we have analyzed the impact of cold\nstart latency on quality of service, explored current cold start latency\nmitigation methods, datasets, and implementation platforms, and classified them\ninto categories based on their common characteristics and features. Finally, we\noutline the open challenges and highlight the possible future directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, academics and the corporate sector have paid attention to\nserverless computing, which enables dynamic scalability and an economic model.\nIn serverless computing, users only pay for the time they actually use\nresources, enabling zero scaling to optimise cost and resource utilisation.\nHowever, this approach also introduces the serverless cold start problem.\nResearchers have developed various solutions to address the cold start problem,\nyet it remains an unresolved research area. In this article, we propose a\nsystematic literature review on clod start latency in serverless computing.\nFurthermore, we create a detailed taxonomy of approaches to cold start latency,\nwhich we use to investigate existing techniques for reducing the cold start\ntime and frequency. We have classified the current studies on cold start\nlatency into several categories such as caching and application-level\noptimisation-based solutions, as well as Artificial Intelligence (AI)/Machine\nLearning (ML)-based solutions. Moreover, we have analyzed the impact of cold\nstart latency on quality of service, explored current cold start latency\nmitigation methods, datasets, and implementation platforms, and classified them\ninto categories based on their common characteristics and features. Finally, we\noutline the open challenges and highlight the possible future directions."
                },
                "authors": [
                    {
                        "name": "Muhammed Golec"
                    },
                    {
                        "name": "Guneet Kaur Walia"
                    },
                    {
                        "name": "Mohit Kumar"
                    },
                    {
                        "name": "Felix Cuadrado"
                    },
                    {
                        "name": "Sukhpal Singh Gill"
                    },
                    {
                        "name": "Steve Uhlig"
                    }
                ],
                "author_detail": {
                    "name": "Steve Uhlig"
                },
                "author": "Steve Uhlig",
                "arxiv_doi": "10.1145/3700875",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3700875",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.08437v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.08437v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Preprint Version Accepted for Publication in ACM Computing Survey,\n  2024",
                "arxiv_journal_ref": "ACM Computing Surveys 2024",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17954v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17954v1",
                "updated": "2024-10-23T15:24:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    24,
                    54,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T15:24:54Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    15,
                    24,
                    54,
                    2,
                    297,
                    0
                ],
                "title": "ExpertFlow: Optimized Expert Activation and Token Allocation for\n  Efficient Mixture-of-Experts Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExpertFlow: Optimized Expert Activation and Token Allocation for\n  Efficient Mixture-of-Experts Inference"
                },
                "summary": "Sparse Mixture of Experts (MoE) models, while outperforming dense Large\nLanguage Models (LLMs) in terms of performance, face significant deployment\nchallenges during inference due to their high memory demands. Existing\noffloading techniques, which involve swapping activated and idle experts\nbetween the GPU and CPU, often suffer from rigid expert caching mechanisms.\nThese mechanisms fail to adapt to dynamic routing, leading to inefficient cache\nutilization, or incur prohibitive costs for prediction training. To tackle\nthese inference-specific challenges, we introduce ExpertFlow, a comprehensive\nsystem specifically designed to enhance inference efficiency by accommodating\nflexible routing and enabling efficient expert scheduling between CPU and GPU.\nThis reduces overhead and boosts system performance. Central to our approach is\na predictive routing path-based offloading mechanism that utilizes a\nlightweight predictor to accurately forecast routing paths before computation\nbegins. This proactive strategy allows for real-time error correction in expert\ncaching, significantly increasing cache hit ratios and reducing the frequency\nof expert transfers, thereby minimizing I/O overhead. Additionally, we\nimplement a dynamic token scheduling strategy that optimizes MoE inference by\nrearranging input tokens across different batches. This method not only reduces\nthe number of activated experts per batch but also improves computational\nefficiency. Our extensive experiments demonstrate that ExpertFlow achieves up\nto 93.72\\% GPU memory savings and enhances inference speed by 2 to 10 times\ncompared to baseline methods, highlighting its effectiveness and utility as a\nrobust solution for resource-constrained inference scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Mixture of Experts (MoE) models, while outperforming dense Large\nLanguage Models (LLMs) in terms of performance, face significant deployment\nchallenges during inference due to their high memory demands. Existing\noffloading techniques, which involve swapping activated and idle experts\nbetween the GPU and CPU, often suffer from rigid expert caching mechanisms.\nThese mechanisms fail to adapt to dynamic routing, leading to inefficient cache\nutilization, or incur prohibitive costs for prediction training. To tackle\nthese inference-specific challenges, we introduce ExpertFlow, a comprehensive\nsystem specifically designed to enhance inference efficiency by accommodating\nflexible routing and enabling efficient expert scheduling between CPU and GPU.\nThis reduces overhead and boosts system performance. Central to our approach is\na predictive routing path-based offloading mechanism that utilizes a\nlightweight predictor to accurately forecast routing paths before computation\nbegins. This proactive strategy allows for real-time error correction in expert\ncaching, significantly increasing cache hit ratios and reducing the frequency\nof expert transfers, thereby minimizing I/O overhead. Additionally, we\nimplement a dynamic token scheduling strategy that optimizes MoE inference by\nrearranging input tokens across different batches. This method not only reduces\nthe number of activated experts per batch but also improves computational\nefficiency. Our extensive experiments demonstrate that ExpertFlow achieves up\nto 93.72\\% GPU memory savings and enhances inference speed by 2 to 10 times\ncompared to baseline methods, highlighting its effectiveness and utility as a\nrobust solution for resource-constrained inference scenarios."
                },
                "authors": [
                    {
                        "name": "Xin He"
                    },
                    {
                        "name": "Shunkang Zhang"
                    },
                    {
                        "name": "Yuxin Wang"
                    },
                    {
                        "name": "Haiyan Yin"
                    },
                    {
                        "name": "Zihao Zeng"
                    },
                    {
                        "name": "Shaohuai Shi"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Ivor Tsang"
                    },
                    {
                        "name": "Ong Yew Soon"
                    }
                ],
                "author_detail": {
                    "name": "Ong Yew Soon"
                },
                "author": "Ong Yew Soon",
                "arxiv_comment": "Mixture-of-Experts, Inference, Offloading",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17954v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17954v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17897v1",
                "updated": "2024-10-23T14:15:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T14:15:07Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "title": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers"
                },
                "summary": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer, reducing the KV cache by nearly 50%. Comprehensive empirical\nevidence demonstrates that ResFormer mitigates attention concentration problem\nin deeper layers and enhances representation across most layers, outperforming\nthe vanilla Transformer, DenseFormer, and NeuTRENO in training error as well as\ndownstream tasks. SVFormer trains significantly faster than the vanilla\nTransformer and performs better than other methods like GQA and CLA, with\nperformance influenced by sequence length and cumulative learning rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer, reducing the KV cache by nearly 50%. Comprehensive empirical\nevidence demonstrates that ResFormer mitigates attention concentration problem\nin deeper layers and enhances representation across most layers, outperforming\nthe vanilla Transformer, DenseFormer, and NeuTRENO in training error as well as\ndownstream tasks. SVFormer trains significantly faster than the vanilla\nTransformer and performs better than other methods like GQA and CLA, with\nperformance influenced by sequence length and cumulative learning rate."
                },
                "authors": [
                    {
                        "name": "Zhanchao Zhou"
                    },
                    {
                        "name": "Tianyi Wu"
                    },
                    {
                        "name": "Zhiyun Jiang"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Lan"
                },
                "author": "Zhenzhong Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.05118v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.05118v3",
                "updated": "2024-10-23T10:39:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    10,
                    39,
                    15,
                    2,
                    297,
                    0
                ],
                "published": "2024-05-08T15:16:02Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    15,
                    16,
                    2,
                    2,
                    129,
                    0
                ],
                "title": "Full Version: (De/Re)-Composition of Data-Parallel Computations via\n  Multi-Dimensional Homomorphisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full Version: (De/Re)-Composition of Data-Parallel Computations via\n  Multi-Dimensional Homomorphisms"
                },
                "summary": "We formally introduce a systematic (de/re)-composition approach, based on the\nalgebraic formalism of \"Multi-Dimensional Homomorphisms (MDHs)\". Our approach\nis designed as general enough to be applicable to a wide range of data-parallel\ncomputations and for various kinds of target parallel architectures. To\nefficiently target the deep and complex memory and core hierarchies of\ncontemporary architectures, we exploit our introduced (de/re)-composition\napproach for a correct-by-construction, parametrized cache blocking and\nparallelization strategy. We show that our approach is powerful enough to\nexpress, in the same formalism, the (de/re)-composition strategies of different\nclasses of state-of-the-art approaches (scheduling-based, polyhedral, etc), and\nwe demonstrate that the parameters of our strategies enable systematically\ngenerating code that can be fully automatically optimized (auto-tuned) for the\nparticular target architecture and characteristics of the input and output data\n(e.g., their sizes and memory layouts). Particularly, our experiments confirm\nthat via auto-tuning, we achieve higher performance than state-of-the-art\napproaches, including hand-optimized solutions provided by vendors (such as\nNVIDIA cuBLAS/cuDNN and Intel oneMKL/oneDNN), on real-world data sets and for a\nvariety of data-parallel computations, including: linear algebra routines,\nstencil and quantum chemistry computations, data mining algorithms, and\ncomputations that recently gained high attention due to their relevance for\ndeep learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We formally introduce a systematic (de/re)-composition approach, based on the\nalgebraic formalism of \"Multi-Dimensional Homomorphisms (MDHs)\". Our approach\nis designed as general enough to be applicable to a wide range of data-parallel\ncomputations and for various kinds of target parallel architectures. To\nefficiently target the deep and complex memory and core hierarchies of\ncontemporary architectures, we exploit our introduced (de/re)-composition\napproach for a correct-by-construction, parametrized cache blocking and\nparallelization strategy. We show that our approach is powerful enough to\nexpress, in the same formalism, the (de/re)-composition strategies of different\nclasses of state-of-the-art approaches (scheduling-based, polyhedral, etc), and\nwe demonstrate that the parameters of our strategies enable systematically\ngenerating code that can be fully automatically optimized (auto-tuned) for the\nparticular target architecture and characteristics of the input and output data\n(e.g., their sizes and memory layouts). Particularly, our experiments confirm\nthat via auto-tuning, we achieve higher performance than state-of-the-art\napproaches, including hand-optimized solutions provided by vendors (such as\nNVIDIA cuBLAS/cuDNN and Intel oneMKL/oneDNN), on real-world data sets and for a\nvariety of data-parallel computations, including: linear algebra routines,\nstencil and quantum chemistry computations, data mining algorithms, and\ncomputations that recently gained high attention due to their relevance for\ndeep learning."
                },
                "authors": [
                    {
                        "name": "Ari Rasch"
                    }
                ],
                "author_detail": {
                    "name": "Ari Rasch"
                },
                "author": "Ari Rasch",
                "arxiv_doi": "10.1145/3665643",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3665643",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.05118v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.05118v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "A short version of this paper is published at ACM TOPLAS and\n  presented at PLDI'24",
                "arxiv_journal_ref": "ACM Trans. Program. Lang. Syst. (May 2024)",
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17635v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17635v1",
                "updated": "2024-10-23T07:53:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    7,
                    53,
                    29,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-23T07:53:29Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    7,
                    53,
                    29,
                    2,
                    297,
                    0
                ],
                "title": "Markov Chain of Thought for Efficient Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Markov Chain of Thought for Efficient Mathematical Reasoning"
                },
                "summary": "Chain of Thought (CoT) of multi-step benefits from the logical structure of\nthe reasoning steps and task-specific actions, significantly enhancing the\nmathematical reasoning capabilities of large language models. As the prevalence\nof long CoT, the number of reasoning steps exceeds manageable token limits and\nleads to higher computational demands. Inspired by the fundamental logic of\nhuman cognition, ``derive, then reduce'', we conceptualize the standard\nmulti-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we\nconsider the mathematical reasoning task, defining each reasoning step as text\naccompanied by a Python code snippet. To facilitate a longer reasoning path,\nself-correction is enabled through interactions with the code interpreter. Our\nMCoT aims to compress previous reasoning steps into a simplified question,\nenabling efficient next-step inference without relying on a lengthy KV cache.\nIn our experiments, we curate the \\texttt{MCoTInstruct} dataset, and the\nempirical results indicate that MCoT not only significantly enhances efficiency\nbut also maintains comparable accuracy. While much remains to be explored, this\nwork paves the way for exploring the long CoT reasoning abilities of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain of Thought (CoT) of multi-step benefits from the logical structure of\nthe reasoning steps and task-specific actions, significantly enhancing the\nmathematical reasoning capabilities of large language models. As the prevalence\nof long CoT, the number of reasoning steps exceeds manageable token limits and\nleads to higher computational demands. Inspired by the fundamental logic of\nhuman cognition, ``derive, then reduce'', we conceptualize the standard\nmulti-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we\nconsider the mathematical reasoning task, defining each reasoning step as text\naccompanied by a Python code snippet. To facilitate a longer reasoning path,\nself-correction is enabled through interactions with the code interpreter. Our\nMCoT aims to compress previous reasoning steps into a simplified question,\nenabling efficient next-step inference without relying on a lengthy KV cache.\nIn our experiments, we curate the \\texttt{MCoTInstruct} dataset, and the\nempirical results indicate that MCoT not only significantly enhances efficiency\nbut also maintains comparable accuracy. While much remains to be explored, this\nwork paves the way for exploring the long CoT reasoning abilities of LLMs."
                },
                "authors": [
                    {
                        "name": "Wen Yang"
                    },
                    {
                        "name": "Kai Fan"
                    },
                    {
                        "name": "Minpeng Liao"
                    }
                ],
                "author_detail": {
                    "name": "Minpeng Liao"
                },
                "author": "Minpeng Liao",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17635v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04870v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04870v5",
                "updated": "2024-10-23T05:55:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    5,
                    55,
                    31,
                    2,
                    297,
                    0
                ],
                "published": "2024-08-09T05:20:05Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    20,
                    5,
                    4,
                    222,
                    0
                ],
                "title": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfusedPilot: Confused Deputy Risks in RAG-based LLMs"
                },
                "summary": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Ayush RoyChowdhury"
                    },
                    {
                        "name": "Mulong Luo"
                    },
                    {
                        "name": "Prateek Sahu"
                    },
                    {
                        "name": "Sarbartha Banerjee"
                    },
                    {
                        "name": "Mohit Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Tiwari"
                },
                "author": "Mohit Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04870v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04870v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14740v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14740v2",
                "updated": "2024-10-23T01:08:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    23,
                    1,
                    8,
                    59,
                    2,
                    297,
                    0
                ],
                "published": "2024-10-17T08:33:39Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    8,
                    33,
                    39,
                    3,
                    291,
                    0
                ],
                "title": "Harnessing Your DRAM and SSD for Sustainable and Accessible LLM\n  Inference with Mixed-Precision and Multi-level Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Your DRAM and SSD for Sustainable and Accessible LLM\n  Inference with Mixed-Precision and Multi-level Caching"
                },
                "summary": "Although Large Language Models (LLMs) have demonstrated remarkable\ncapabilities, their massive parameter counts and associated extensive computing\nmake LLMs' deployment the main part of carbon emission from nowadays AI\napplications. Compared to modern GPUs like H$100$, it would be significantly\ncarbon-sustainable if we could leverage old-fashioned GPUs such as M$40$ (as\nshown in Figure 1, M$40$ only has one third carbon emission of H$100$'s) for\nLLM servings. However, the limited High Bandwidth Memory (HBM) available on\nsuch GPU often cannot support the loading of LLMs due to the gigantic model\nsize and intermediate activation data, making their serving challenging. For\ninstance, a LLaMA2 model with $70$B parameters typically requires $128$GB for\ninference, which substantially surpasses $24$GB HBM in a $3090$ GPU and remains\ninfeasible even considering the additional $64$GB DRAM. To address this\nchallenge, this paper proposes a mixed-precision with a model modularization\nalgorithm to enable LLM inference on outdated hardware with resource\nconstraints. (The precision denotes the numerical precision like FP16, INT8,\nINT4) and multi-level caching (M2Cache).)\n  Specifically, our M2Cache first modulizes neurons in LLM and creates their\nimportance ranking. Then, it adopts a dynamic sparse mixed-precision\nquantization mechanism in weight space to reduce computational demands and\ncommunication overhead at each decoding step. It collectively lowers the\noperational carbon emissions associated with LLM inference. Moreover, M2Cache\nintroduces a three-level cache management system with HBM, DRAM, and SSDs that\ncomplements the dynamic sparse mixed-precision inference. To enhance\ncommunication efficiency, M2Cache maintains a neuron-level mixed-precision LRU\ncache in HBM, a larger layer-aware cache in DRAM, and a full model in SSD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although Large Language Models (LLMs) have demonstrated remarkable\ncapabilities, their massive parameter counts and associated extensive computing\nmake LLMs' deployment the main part of carbon emission from nowadays AI\napplications. Compared to modern GPUs like H$100$, it would be significantly\ncarbon-sustainable if we could leverage old-fashioned GPUs such as M$40$ (as\nshown in Figure 1, M$40$ only has one third carbon emission of H$100$'s) for\nLLM servings. However, the limited High Bandwidth Memory (HBM) available on\nsuch GPU often cannot support the loading of LLMs due to the gigantic model\nsize and intermediate activation data, making their serving challenging. For\ninstance, a LLaMA2 model with $70$B parameters typically requires $128$GB for\ninference, which substantially surpasses $24$GB HBM in a $3090$ GPU and remains\ninfeasible even considering the additional $64$GB DRAM. To address this\nchallenge, this paper proposes a mixed-precision with a model modularization\nalgorithm to enable LLM inference on outdated hardware with resource\nconstraints. (The precision denotes the numerical precision like FP16, INT8,\nINT4) and multi-level caching (M2Cache).)\n  Specifically, our M2Cache first modulizes neurons in LLM and creates their\nimportance ranking. Then, it adopts a dynamic sparse mixed-precision\nquantization mechanism in weight space to reduce computational demands and\ncommunication overhead at each decoding step. It collectively lowers the\noperational carbon emissions associated with LLM inference. Moreover, M2Cache\nintroduces a three-level cache management system with HBM, DRAM, and SSDs that\ncomplements the dynamic sparse mixed-precision inference. To enhance\ncommunication efficiency, M2Cache maintains a neuron-level mixed-precision LRU\ncache in HBM, a larger layer-aware cache in DRAM, and a full model in SSD."
                },
                "authors": [
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Zhang Cao"
                    },
                    {
                        "name": "Huaizhi Qu"
                    },
                    {
                        "name": "Zhengyu Zhang"
                    },
                    {
                        "name": "Chang Guo"
                    },
                    {
                        "name": "Yanyong Zhang"
                    },
                    {
                        "name": "Zhichao Cao"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "24 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14740v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14740v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.11724v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.11724v2",
                "updated": "2024-10-22T19:07:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    22,
                    19,
                    7,
                    8,
                    1,
                    296,
                    0
                ],
                "published": "2024-05-20T01:57:34Z",
                "published_parsed": [
                    2024,
                    5,
                    20,
                    1,
                    57,
                    34,
                    0,
                    141,
                    0
                ],
                "title": "Token-wise Influential Training Data Retrieval for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token-wise Influential Training Data Retrieval for Large Language Models"
                },
                "summary": "Given a Large Language Model (LLM) generation, how can we identify which\ntraining data led to this generation? In this paper, we proposed RapidIn, a\nscalable framework adapting to LLMs for estimating the influence of each\ntraining data. The proposed framework consists of two stages: caching and\nretrieval. First, we compress the gradient vectors by over 200,000x, allowing\nthem to be cached on disk or in GPU/CPU memory. Then, given a generation,\nRapidIn efficiently traverses the cached gradients to estimate the influence\nwithin minutes, achieving over a 6,326x speedup. Moreover, RapidIn supports\nmulti-GPU parallelization to substantially accelerate caching and retrieval.\nOur empirical result confirms the efficiency and effectiveness of RapidIn.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given a Large Language Model (LLM) generation, how can we identify which\ntraining data led to this generation? In this paper, we proposed RapidIn, a\nscalable framework adapting to LLMs for estimating the influence of each\ntraining data. The proposed framework consists of two stages: caching and\nretrieval. First, we compress the gradient vectors by over 200,000x, allowing\nthem to be cached on disk or in GPU/CPU memory. Then, given a generation,\nRapidIn efficiently traverses the cached gradients to estimate the influence\nwithin minutes, achieving over a 6,326x speedup. Moreover, RapidIn supports\nmulti-GPU parallelization to substantially accelerate caching and retrieval.\nOur empirical result confirms the efficiency and effectiveness of RapidIn."
                },
                "authors": [
                    {
                        "name": "Huawei Lin"
                    },
                    {
                        "name": "Jikai Long"
                    },
                    {
                        "name": "Zhaozhuo Xu"
                    },
                    {
                        "name": "Weijie Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Weijie Zhao"
                },
                "author": "Weijie Zhao",
                "arxiv_comment": "Accepted to ACL 2024. Keywords: Influence Function, Influence\n  Estimation, Training Data Attribution",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.11724v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.11724v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18400v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18400v5",
                "updated": "2024-10-21T22:56:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    22,
                    56,
                    6,
                    0,
                    295,
                    0
                ],
                "published": "2024-05-28T17:40:48Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    17,
                    40,
                    48,
                    1,
                    149,
                    0
                ],
                "title": "Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Superposed Decoding: Multiple Generations from a Single Autoregressive\n  Inference Pass"
                },
                "summary": "Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many applications today provide users with multiple auto-complete drafts as\nthey type, including GitHub's code completion, Gmail's smart compose, and\nApple's messaging auto-suggestions. Under the hood, language models support\nthis by running an autoregressive inference pass to provide a draft.\nConsequently, providing $k$ drafts to the user requires running an expensive\nlanguage model $k$ times. To alleviate the computation cost of running $k$\ninference passes, we propose Superposed Decoding, a new decoding algorithm that\ngenerates $k$ drafts at the computation cost of one autoregressive inference\npass. We achieve this by feeding a superposition of the most recent token\nembeddings from the $k$ drafts as input to the next decoding step of the\nlanguage model. At every inference step we combine the $k$ drafts with the\ntop-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options,\nusing an n-gram interpolation with minimal compute overhead to filter out\nincoherent generations. Our experiments show that $k$ drafts from Superposed\nDecoding are at least as coherent and factual as Nucleus Sampling and Greedy\nDecoding respectively, while being at least $2.44\\times$ faster for $k\\ge3$. In\na compute-normalized setting, user evaluations demonstrably favor text\ngenerated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can\nalso be combined with other decoding strategies, resulting in universal\ncoverage gains when scaling inference time compute. Code and more examples\nopen-sourced at https://github.com/RAIVNLab/SuperposedDecoding."
                },
                "authors": [
                    {
                        "name": "Ethan Shen"
                    },
                    {
                        "name": "Alan Fan"
                    },
                    {
                        "name": "Sarah M. Pratt"
                    },
                    {
                        "name": "Jae Sung Park"
                    },
                    {
                        "name": "Matthew Wallingford"
                    },
                    {
                        "name": "Sham M. Kakade"
                    },
                    {
                        "name": "Ari Holtzman"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Ali Farhadi"
                    },
                    {
                        "name": "Aditya Kusupati"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Kusupati"
                },
                "author": "Aditya Kusupati",
                "arxiv_comment": "23 pages, 16 figures, accepted at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18400v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18400v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16218v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16218v1",
                "updated": "2024-10-21T17:23:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    17,
                    23,
                    3,
                    0,
                    295,
                    0
                ],
                "published": "2024-10-21T17:23:03Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    17,
                    23,
                    3,
                    0,
                    295,
                    0
                ],
                "title": "3 kV Monolithic Bidirectional GaN HEMT on Sapphire",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3 kV Monolithic Bidirectional GaN HEMT on Sapphire"
                },
                "summary": "More than 3 kV breakdown voltage was demonstrated in monolithic bidirectional\nGaN HEMTs for the first time having potential applications in 1200V or\n1700V-class novel power converters. The on resistance of the fabricated\ntransistors was ~20 ohm.mm or ~11 mili ohm.cm^2. Breakdown voltage was\noptimized by utilizing two field plates in either side of the transistor and\noptimizing their geometry. Shorter first field plate lengths (less than 2\nmicron) resulted in higher breakdown voltage and the possible reason for this\nwas discussed. The transistors had a steep subthreshold swing of 92 mV / dec.\nThe on/off ratio was greater than 10^5 and it was limited by the tool capacity.\nThe fabricated 3 kV transistor was benchmarked against the state-of-the-art\nmonolithic bidirectional GaN HEMTs in the performance matrices of breakdown\nvoltage and on resistance, that showed crucial progress.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More than 3 kV breakdown voltage was demonstrated in monolithic bidirectional\nGaN HEMTs for the first time having potential applications in 1200V or\n1700V-class novel power converters. The on resistance of the fabricated\ntransistors was ~20 ohm.mm or ~11 mili ohm.cm^2. Breakdown voltage was\noptimized by utilizing two field plates in either side of the transistor and\noptimizing their geometry. Shorter first field plate lengths (less than 2\nmicron) resulted in higher breakdown voltage and the possible reason for this\nwas discussed. The transistors had a steep subthreshold swing of 92 mV / dec.\nThe on/off ratio was greater than 10^5 and it was limited by the tool capacity.\nThe fabricated 3 kV transistor was benchmarked against the state-of-the-art\nmonolithic bidirectional GaN HEMTs in the performance matrices of breakdown\nvoltage and on resistance, that showed crucial progress."
                },
                "authors": [
                    {
                        "name": "Md Tahmidul Alam"
                    },
                    {
                        "name": "Swarnav Mukhopadhyay"
                    },
                    {
                        "name": "Md Mobinul Haque"
                    },
                    {
                        "name": "Shubhra S. Pasayat"
                    },
                    {
                        "name": "Chirag Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Chirag Gupta"
                },
                "author": "Chirag Gupta",
                "arxiv_comment": "4 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16218v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16218v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13761v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13761v2",
                "updated": "2024-10-21T15:59:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    15,
                    59,
                    18,
                    0,
                    295,
                    0
                ],
                "published": "2024-09-16T18:46:24Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    18,
                    46,
                    24,
                    0,
                    260,
                    0
                ],
                "title": "Do Large Language Models Need a Content Delivery Network?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Large Language Models Need a Content Delivery Network?"
                },
                "summary": "As the use of large language models (LLMs) expands rapidly, so does the range\nof knowledge needed to supplement various LLM queries. Thus, enabling flexible\nand efficient injection of new knowledge in LLM inference is critical. Three\nhigh-level options exist: (i) embedding the knowledge in LLM's weights (i.e.,\nfine-tuning), (ii) including the knowledge as a part of LLM's text input (i.e.,\nin-context learning), or (iii) injecting the KV caches of the new knowledge to\nLLM during prefill. This paper argues that, although fine-tuning and in-context\nlearning are popular, using KV caches as the medium of knowledge could\nsimultaneously enable more modular management of knowledge injection and more\nefficient LLM serving with low cost and fast response. To realize these\nbenefits, we envision a Knowledge Delivery Network (KDN), a new system\ncomponent in LLM services that dynamically optimizes the storage, transfer, and\ncomposition of KV cache across LLM engines and other compute and storage\nresources. We believe that, just like content delivery networks (CDNs), such as\nAkamai, enabled the success of the Internet ecosystem through their efficient\ndata delivery, KDNs will be critical to the success of LLM applications through\ntheir efficient knowledge delivery. We have open-sourced a KDN prototype at\nhttps://github.com/LMCache/LMCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the use of large language models (LLMs) expands rapidly, so does the range\nof knowledge needed to supplement various LLM queries. Thus, enabling flexible\nand efficient injection of new knowledge in LLM inference is critical. Three\nhigh-level options exist: (i) embedding the knowledge in LLM's weights (i.e.,\nfine-tuning), (ii) including the knowledge as a part of LLM's text input (i.e.,\nin-context learning), or (iii) injecting the KV caches of the new knowledge to\nLLM during prefill. This paper argues that, although fine-tuning and in-context\nlearning are popular, using KV caches as the medium of knowledge could\nsimultaneously enable more modular management of knowledge injection and more\nefficient LLM serving with low cost and fast response. To realize these\nbenefits, we envision a Knowledge Delivery Network (KDN), a new system\ncomponent in LLM services that dynamically optimizes the storage, transfer, and\ncomposition of KV cache across LLM engines and other compute and storage\nresources. We believe that, just like content delivery networks (CDNs), such as\nAkamai, enabled the success of the Internet ecosystem through their efficient\ndata delivery, KDNs will be critical to the success of LLM applications through\ntheir efficient knowledge delivery. We have open-sourced a KDN prototype at\nhttps://github.com/LMCache/LMCache."
                },
                "authors": [
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13761v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13761v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15908v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15908v1",
                "updated": "2024-10-21T11:29:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    11,
                    29,
                    49,
                    0,
                    295,
                    0
                ],
                "published": "2024-10-21T11:29:49Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    11,
                    29,
                    49,
                    0,
                    295,
                    0
                ],
                "title": "Formalising CXL Cache Coherence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formalising CXL Cache Coherence"
                },
                "summary": "We report our experience formally modelling and verifying CXL.cache, the\ninter-device cache coherence protocol of the Compute Express Link standard. We\nhave used the Isabelle proof assistant to create a formal model for CXL.cache\nbased on the prose English specification. This led to us identifying and\nproposing fixes to several problems we identified as unclear, ambiguous or\ninaccurate, some of which could lead to incoherence if left unfixed. Nearly all\nour issues and proposed fixes have been confirmed and tentatively accepted by\nthe CXL consortium for adoption, save for one which is still under discussion.\nTo validate the faithfulness of our model we performed scenario verification of\nessential restrictions such as \"Snoop-pushes-GO\", and produced a fully\nmechanised proof of a coherence property of the model. The considerable size of\nthis proof, comprising tens of thousands of lemmas, prompted us to develop new\nproof automation tools, which we have made available for other Isabelle users\nworking with similarly cumbersome proofs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report our experience formally modelling and verifying CXL.cache, the\ninter-device cache coherence protocol of the Compute Express Link standard. We\nhave used the Isabelle proof assistant to create a formal model for CXL.cache\nbased on the prose English specification. This led to us identifying and\nproposing fixes to several problems we identified as unclear, ambiguous or\ninaccurate, some of which could lead to incoherence if left unfixed. Nearly all\nour issues and proposed fixes have been confirmed and tentatively accepted by\nthe CXL consortium for adoption, save for one which is still under discussion.\nTo validate the faithfulness of our model we performed scenario verification of\nessential restrictions such as \"Snoop-pushes-GO\", and produced a fully\nmechanised proof of a coherence property of the model. The considerable size of\nthis proof, comprising tens of thousands of lemmas, prompted us to develop new\nproof automation tools, which we have made available for other Isabelle users\nworking with similarly cumbersome proofs."
                },
                "authors": [
                    {
                        "name": "Chengsong Tan"
                    },
                    {
                        "name": "Alastair F. Donaldson"
                    },
                    {
                        "name": "John Wickerson"
                    }
                ],
                "author_detail": {
                    "name": "John Wickerson"
                },
                "author": "John Wickerson",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15908v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15908v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14142v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14142v2",
                "updated": "2024-10-21T07:24:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    7,
                    24,
                    53,
                    0,
                    295,
                    0
                ],
                "published": "2024-10-18T03:30:25Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    3,
                    30,
                    25,
                    4,
                    292,
                    0
                ],
                "title": "Secure Collaborative Computation Offloading and Resource Allocation in\n  Cache-Assisted Ultra-Dense IoT Networks With Multi-Slope Channels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secure Collaborative Computation Offloading and Resource Allocation in\n  Cache-Assisted Ultra-Dense IoT Networks With Multi-Slope Channels"
                },
                "summary": "Cache-assisted ultra-dense mobile edge computing (MEC) networks are a\npromising solution for meeting the increasing demands of numerous\nInternet-of-Things mobile devices (IMDs). To address the complex interferences\ncaused by small base stations (SBSs) deployed densely in such networks, this\npaper explores the combination of orthogonal frequency division multiple access\n(OFDMA), non-orthogonal multiple access (NOMA), and base station (BS)\nclustering. Additionally, security measures are introduced to protect IMDs'\ntasks offloaded to BSs from potential eavesdropping and malicious attacks. As\nfor such a network framework, a computation offloading scheme is proposed to\nminimize IMDs' energy consumption while considering constraints such as delay,\npower, computing resources, and security costs, optimizing channel selections,\ntask execution decisions, device associations, power controls, security service\nassignments, and computing resource allocations. To solve the formulated\nproblem efficiently, we develop a further improved hierarchical adaptive search\n(FIHAS) algorithm, giving some insights into its parallel implementation,\ncomputation complexity, and convergence. Simulation results demonstrate that\nthe proposed algorithms can achieve lower total energy consumption and delay\ncompared to other algorithms when strict latency and cost constraints are\nimposed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-assisted ultra-dense mobile edge computing (MEC) networks are a\npromising solution for meeting the increasing demands of numerous\nInternet-of-Things mobile devices (IMDs). To address the complex interferences\ncaused by small base stations (SBSs) deployed densely in such networks, this\npaper explores the combination of orthogonal frequency division multiple access\n(OFDMA), non-orthogonal multiple access (NOMA), and base station (BS)\nclustering. Additionally, security measures are introduced to protect IMDs'\ntasks offloaded to BSs from potential eavesdropping and malicious attacks. As\nfor such a network framework, a computation offloading scheme is proposed to\nminimize IMDs' energy consumption while considering constraints such as delay,\npower, computing resources, and security costs, optimizing channel selections,\ntask execution decisions, device associations, power controls, security service\nassignments, and computing resource allocations. To solve the formulated\nproblem efficiently, we develop a further improved hierarchical adaptive search\n(FIHAS) algorithm, giving some insights into its parallel implementation,\ncomputation complexity, and convergence. Simulation results demonstrate that\nthe proposed algorithms can achieve lower total energy consumption and delay\ncompared to other algorithms when strict latency and cost constraints are\nimposed."
                },
                "authors": [
                    {
                        "name": "Tianqing Zhou"
                    },
                    {
                        "name": "Bobo Wang"
                    },
                    {
                        "name": "Dong Qin"
                    },
                    {
                        "name": "Xuefang Nie"
                    },
                    {
                        "name": "Nan Jiang"
                    },
                    {
                        "name": "Chunguo Li"
                    }
                ],
                "author_detail": {
                    "name": "Chunguo Li"
                },
                "author": "Chunguo Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14142v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14142v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15704v1",
                "updated": "2024-10-21T07:20:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    7,
                    20,
                    41,
                    0,
                    295,
                    0
                ],
                "published": "2024-10-21T07:20:41Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    7,
                    20,
                    41,
                    0,
                    295,
                    0
                ],
                "title": "Residual vector quantization for KV cache compression in large language\n  model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Residual vector quantization for KV cache compression in large language\n  model"
                },
                "summary": "KV cache compression methods have mainly relied on scalar quantization\ntechniques to reduce the memory requirements during decoding. In this work, we\napply residual vector quantization, which has been widely used for high\nfidelity audio compression, to compress KV cache in large language models\n(LLM). We adapt the standard recipe with minimal changes to compress the output\nof any key or value projection matrix in a pretrained LLM: we scale the vector\nby its standard deviation, divide channels into groups and then quantize each\ngroup with the same residual vector quantizer. We learn the codebook using\nexponential moving average and there are no other learnable parameters\nincluding the input and output projections normally used in a vector\nquantization set up. We find that a residual depth of 8 recovers most of the\nperformance of the unquantized model. We also find that grouping non-contiguous\nchannels together works better than grouping contiguous channels for\ncompressing key matrix and the method further benefits from a light weight\nfinetuning of LLM together with the quantization. Overall, the proposed\ntechnique is competitive with existing quantization methods while being much\nsimpler and results in 5.5x compression compared to half precision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache compression methods have mainly relied on scalar quantization\ntechniques to reduce the memory requirements during decoding. In this work, we\napply residual vector quantization, which has been widely used for high\nfidelity audio compression, to compress KV cache in large language models\n(LLM). We adapt the standard recipe with minimal changes to compress the output\nof any key or value projection matrix in a pretrained LLM: we scale the vector\nby its standard deviation, divide channels into groups and then quantize each\ngroup with the same residual vector quantizer. We learn the codebook using\nexponential moving average and there are no other learnable parameters\nincluding the input and output projections normally used in a vector\nquantization set up. We find that a residual depth of 8 recovers most of the\nperformance of the unquantized model. We also find that grouping non-contiguous\nchannels together works better than grouping contiguous channels for\ncompressing key matrix and the method further benefits from a light weight\nfinetuning of LLM together with the quantization. Overall, the proposed\ntechnique is competitive with existing quantization methods while being much\nsimpler and results in 5.5x compression compared to half precision."
                },
                "authors": [
                    {
                        "name": "Ankur Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Ankur Kumar"
                },
                "author": "Ankur Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16546v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16546v2",
                "updated": "2024-10-21T05:06:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    5,
                    6,
                    1,
                    0,
                    295,
                    0
                ],
                "published": "2024-09-25T01:39:02Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    1,
                    39,
                    2,
                    2,
                    269,
                    0
                ],
                "title": "AlignedKV: Reducing Memory Access of KV-Cache with Precision-Aligned\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AlignedKV: Reducing Memory Access of KV-Cache with Precision-Aligned\n  Quantization"
                },
                "summary": "Model quantization has become a crucial technique to address the issues of\nlarge memory consumption and long inference times associated with LLMs.\nMixed-precision quantization, which distinguishes between important and\nunimportant parameters, stands out among numerous quantization schemes as it\nachieves a balance between precision and compression rate. However, existing\napproaches can only identify important parameters through qualitative analysis\nand manual experiments without quantitatively analyzing how their importance is\ndetermined. We propose a new criterion, so-called 'precision alignment', to\nbuild a quantitative framework to holistically evaluate the importance of\nparameters in mixed-precision quantization. Our observations on floating point\naddition under various real-world scenarios suggest that two addends should\nhave identical precision, otherwise the information in the higher-precision\nnumber will be wasted. Such an observation offers an essential principle to\ndetermine the precision of each parameter in matrix multiplication operation.\nAs the first step towards applying the above discovery to large model\ninference, we develop a dynamic KV-Cache quantization technique to effectively\nreduce memory access latency. Different from existing quantization approaches\nthat focus on memory saving, this work directly aims to accelerate LLM\ninference through quantifying floating numbers. The proposed technique attains\na 25% saving of memory access and delivers up to 1.3x speedup in the\ncomputation of attention in the decoding phase of LLM, with almost no loss of\nprecision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model quantization has become a crucial technique to address the issues of\nlarge memory consumption and long inference times associated with LLMs.\nMixed-precision quantization, which distinguishes between important and\nunimportant parameters, stands out among numerous quantization schemes as it\nachieves a balance between precision and compression rate. However, existing\napproaches can only identify important parameters through qualitative analysis\nand manual experiments without quantitatively analyzing how their importance is\ndetermined. We propose a new criterion, so-called 'precision alignment', to\nbuild a quantitative framework to holistically evaluate the importance of\nparameters in mixed-precision quantization. Our observations on floating point\naddition under various real-world scenarios suggest that two addends should\nhave identical precision, otherwise the information in the higher-precision\nnumber will be wasted. Such an observation offers an essential principle to\ndetermine the precision of each parameter in matrix multiplication operation.\nAs the first step towards applying the above discovery to large model\ninference, we develop a dynamic KV-Cache quantization technique to effectively\nreduce memory access latency. Different from existing quantization approaches\nthat focus on memory saving, this work directly aims to accelerate LLM\ninference through quantifying floating numbers. The proposed technique attains\na 25% saving of memory access and delivers up to 1.3x speedup in the\ncomputation of attention in the decoding phase of LLM, with almost no loss of\nprecision."
                },
                "authors": [
                    {
                        "name": "Yifan Tan"
                    },
                    {
                        "name": "Haoze Wang"
                    },
                    {
                        "name": "Chao Yan"
                    },
                    {
                        "name": "Yangdong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yangdong Deng"
                },
                "author": "Yangdong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16546v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16546v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09202v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09202v2",
                "updated": "2024-10-21T02:35:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    21,
                    2,
                    35,
                    8,
                    0,
                    295,
                    0
                ],
                "published": "2024-09-13T21:31:45Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    21,
                    31,
                    45,
                    4,
                    257,
                    0
                ],
                "title": "WarmSwap: Sharing Dependencies for Accelerating Cold Starts in\n  Serverless Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WarmSwap: Sharing Dependencies for Accelerating Cold Starts in\n  Serverless Functions"
                },
                "summary": "This work presents WarmSwap, a novel provider-side cold-start optimization\nfor serverless computing. This optimization reduces cold-start time when\nbooting and loading dependencies at runtime inside a function container.\nPrevious approaches to the optimization of cold starts tend to fall into two\ncategories: optimizing the infrastructure of serverless computing to benefit\nall serverless functions; or function-specific tuning for individual serverless\nfunctions. In contrast, WarmSwap offers a broad middle ground, which optimizes\nentire categories of serverless functions. WarmSwap eliminates the need to\ninitialize middleware or software dependencies when launching a new serverless\ncontainer, by migrating a pre-initialized live dependency image to the new\nfunction instance. WarmSwap respects the provider's cache constraints, as a\nsingle pre-warmed dependency image in the cache is shared among all serverless\nfunctions requiring that software dependency image. WarmSwap has been tested on\nseven representative functions from FunctionBench. In those tests, WarmSwap\naccelerates dependency loading for serverless functions with large dependency\nrequirements by a factor ranging from 2.2 to 3.2. Simulation experiments using\nAzure traces indicate that WarmSwap can save 88\\% of optimization space when\nsharing a dependency image among ten different functions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents WarmSwap, a novel provider-side cold-start optimization\nfor serverless computing. This optimization reduces cold-start time when\nbooting and loading dependencies at runtime inside a function container.\nPrevious approaches to the optimization of cold starts tend to fall into two\ncategories: optimizing the infrastructure of serverless computing to benefit\nall serverless functions; or function-specific tuning for individual serverless\nfunctions. In contrast, WarmSwap offers a broad middle ground, which optimizes\nentire categories of serverless functions. WarmSwap eliminates the need to\ninitialize middleware or software dependencies when launching a new serverless\ncontainer, by migrating a pre-initialized live dependency image to the new\nfunction instance. WarmSwap respects the provider's cache constraints, as a\nsingle pre-warmed dependency image in the cache is shared among all serverless\nfunctions requiring that software dependency image. WarmSwap has been tested on\nseven representative functions from FunctionBench. In those tests, WarmSwap\naccelerates dependency loading for serverless functions with large dependency\nrequirements by a factor ranging from 2.2 to 3.2. Simulation experiments using\nAzure traces indicate that WarmSwap can save 88\\% of optimization space when\nsharing a dependency image among ten different functions."
                },
                "authors": [
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Devesh Tiwari"
                    },
                    {
                        "name": "Gene Cooperman"
                    }
                ],
                "author_detail": {
                    "name": "Gene Cooperman"
                },
                "author": "Gene Cooperman",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09202v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09202v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04053v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04053v2",
                "updated": "2024-10-20T13:37:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    20,
                    13,
                    37,
                    46,
                    6,
                    294,
                    0
                ],
                "published": "2024-07-04T16:51:17Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    16,
                    51,
                    17,
                    3,
                    186,
                    0
                ],
                "title": "Edge AI: A Taxonomy, Systematic Review and Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge AI: A Taxonomy, Systematic Review and Future Directions"
                },
                "summary": "Edge Artificial Intelligence (AI) incorporates a network of interconnected\nsystems and devices that receive, cache, process, and analyze data in close\ncommunication with the location where the data is captured with AI technology.\nRecent advancements in AI efficiency, the widespread use of Internet of Things\n(IoT) devices, and the emergence of edge computing have unlocked the enormous\nscope of Edge AI. Edge AI aims to optimize data processing efficiency and\nvelocity while ensuring data confidentiality and integrity. Despite being a\nrelatively new field of research from 2014 to the present, it has shown\nsignificant and rapid development over the last five years. This article\npresents a systematic literature review for Edge AI to discuss the existing\nresearch, recent advancements, and future research directions. We created a\ncollaborative edge AI learning system for cloud and edge computing analysis,\nincluding an in-depth study of the architectures that facilitate this\nmechanism. The taxonomy for Edge AI facilitates the classification and\nconfiguration of Edge AI systems while examining its potential influence across\nmany fields through compassing infrastructure, cloud computing, fog computing,\nservices, use cases, ML and deep learning, and resource management. This study\nhighlights the significance of Edge AI in processing real-time data at the edge\nof the network. Additionally, it emphasizes the research challenges encountered\nby Edge AI systems, including constraints on resources, vulnerabilities to\nsecurity threats, and problems with scalability. Finally, this study highlights\nthe potential future research directions that aim to address the current\nlimitations of Edge AI by providing innovative solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge Artificial Intelligence (AI) incorporates a network of interconnected\nsystems and devices that receive, cache, process, and analyze data in close\ncommunication with the location where the data is captured with AI technology.\nRecent advancements in AI efficiency, the widespread use of Internet of Things\n(IoT) devices, and the emergence of edge computing have unlocked the enormous\nscope of Edge AI. Edge AI aims to optimize data processing efficiency and\nvelocity while ensuring data confidentiality and integrity. Despite being a\nrelatively new field of research from 2014 to the present, it has shown\nsignificant and rapid development over the last five years. This article\npresents a systematic literature review for Edge AI to discuss the existing\nresearch, recent advancements, and future research directions. We created a\ncollaborative edge AI learning system for cloud and edge computing analysis,\nincluding an in-depth study of the architectures that facilitate this\nmechanism. The taxonomy for Edge AI facilitates the classification and\nconfiguration of Edge AI systems while examining its potential influence across\nmany fields through compassing infrastructure, cloud computing, fog computing,\nservices, use cases, ML and deep learning, and resource management. This study\nhighlights the significance of Edge AI in processing real-time data at the edge\nof the network. Additionally, it emphasizes the research challenges encountered\nby Edge AI systems, including constraints on resources, vulnerabilities to\nsecurity threats, and problems with scalability. Finally, this study highlights\nthe potential future research directions that aim to address the current\nlimitations of Edge AI by providing innovative solutions."
                },
                "authors": [
                    {
                        "name": "Sukhpal Singh Gill"
                    },
                    {
                        "name": "Muhammed Golec"
                    },
                    {
                        "name": "Jianmin Hu"
                    },
                    {
                        "name": "Minxian Xu"
                    },
                    {
                        "name": "Junhui Du"
                    },
                    {
                        "name": "Huaming Wu"
                    },
                    {
                        "name": "Guneet Kaur Walia"
                    },
                    {
                        "name": "Subramaniam Subramanian Murugesan"
                    },
                    {
                        "name": "Babar Ali"
                    },
                    {
                        "name": "Mohit Kumar"
                    },
                    {
                        "name": "Kejiang Ye"
                    },
                    {
                        "name": "Prabal Verma"
                    },
                    {
                        "name": "Surendra Kumar"
                    },
                    {
                        "name": "Felix Cuadrado"
                    },
                    {
                        "name": "Steve Uhlig"
                    }
                ],
                "author_detail": {
                    "name": "Steve Uhlig"
                },
                "author": "Steve Uhlig",
                "arxiv_doi": "10.1007/s10586-024-04686-y",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s10586-024-04686-y",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.04053v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04053v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Preprint Version Accepted for Publication in Springer Cluster\n  Computing, 2024",
                "arxiv_journal_ref": "Springer Cluster Computing, Volume 28, article number 18, pages\n  11953 - 11981, (2025)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15344v1",
                "updated": "2024-10-20T09:37:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    20,
                    9,
                    37,
                    7,
                    6,
                    294,
                    0
                ],
                "published": "2024-10-20T09:37:07Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    9,
                    37,
                    7,
                    6,
                    294,
                    0
                ],
                "title": "LLC Intra-set Write Balancing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLC Intra-set Write Balancing"
                },
                "summary": "The increasing use of Non-Volatile Memory (NVM) in computer architecture has\nbrought about new challenges, one of which is the write endurance problem.\nFrequent writes to a particular cache cell in NVM can lead to degradation of\nthe memory cell and reduce its lifespan. To solve this problem, we propose a\nsample-based blocking technique for the Last Level Cache (LLC). Our approach\ninvolves defining a threshold value and sampling a subset of cache sets. If the\nnumber of writes to a way in a sampled set exceeds the threshold, the way is\nblocked, and writes are redirected to other ways. We also maintain a history\nstructure to record the number of writes in a set and a PC-Table to use for\nblocking in unsampled sets. Based on blocking on sampled sets, variance of\nvalues stored in history is used to determine whether blocking had a positive\nimpact or not, and on this basis, value corresponding to instruction pointer is\nincremented or decremented. This value is later used for blocking in unsampled\nsets. Our results show that our approach significantly balances write traffic\nto the cache and improves the overall lifespan of the memory cells while having\nbetter performance to the base-line system. Our approach can also be applied to\nother cache hierarchies and NVM technologies to mitigate the problem of write\nendurance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing use of Non-Volatile Memory (NVM) in computer architecture has\nbrought about new challenges, one of which is the write endurance problem.\nFrequent writes to a particular cache cell in NVM can lead to degradation of\nthe memory cell and reduce its lifespan. To solve this problem, we propose a\nsample-based blocking technique for the Last Level Cache (LLC). Our approach\ninvolves defining a threshold value and sampling a subset of cache sets. If the\nnumber of writes to a way in a sampled set exceeds the threshold, the way is\nblocked, and writes are redirected to other ways. We also maintain a history\nstructure to record the number of writes in a set and a PC-Table to use for\nblocking in unsampled sets. Based on blocking on sampled sets, variance of\nvalues stored in history is used to determine whether blocking had a positive\nimpact or not, and on this basis, value corresponding to instruction pointer is\nincremented or decremented. This value is later used for blocking in unsampled\nsets. Our results show that our approach significantly balances write traffic\nto the cache and improves the overall lifespan of the memory cells while having\nbetter performance to the base-line system. Our approach can also be applied to\nother cache hierarchies and NVM technologies to mitigate the problem of write\nendurance."
                },
                "authors": [
                    {
                        "name": "Keshav Krishna"
                    },
                    {
                        "name": "Ayush Verma"
                    }
                ],
                "author_detail": {
                    "name": "Ayush Verma"
                },
                "author": "Ayush Verma",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15332v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15332v1",
                "updated": "2024-10-20T08:42:29Z",
                "updated_parsed": [
                    2024,
                    10,
                    20,
                    8,
                    42,
                    29,
                    6,
                    294,
                    0
                ],
                "published": "2024-10-20T08:42:29Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    8,
                    42,
                    29,
                    6,
                    294,
                    0
                ],
                "title": "EPIC: Efficient Position-Independent Context Caching for Serving Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EPIC: Efficient Position-Independent Context Caching for Serving Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) are critical for a wide range of applications,\nbut serving them efficiently becomes increasingly challenging as inputs become\nmore complex. Context caching improves serving performance by exploiting\ninter-request dependency and reusing key-value (KV) cache across requests, thus\nimproving time-to-first-token (TTFT). However, existing prefix-based context\ncaching requires exact token prefix matches, limiting cache reuse in few-shot\nlearning, multi-document QA, or retrieval-augmented generation, where prefixes\nmay vary. In this paper, we present EPIC, an LLM serving system that introduces\nposition-independent context caching (PIC), enabling modular KV cache reuse\nregardless of token chunk position (or prefix). EPIC features two key designs:\nAttnLink, which leverages static attention sparsity to minimize recomputation\nfor accuracy recovery, and KVSplit, a customizable chunking method that\npreserves semantic coherence. Our experiments demonstrate that Epic delivers up\nto 8x improvements in TTFT and 7x throughput over existing systems, with\nnegligible or no accuracy loss. By addressing the limitations of traditional\ncaching approaches, Epic enables more scalable and efficient LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are critical for a wide range of applications,\nbut serving them efficiently becomes increasingly challenging as inputs become\nmore complex. Context caching improves serving performance by exploiting\ninter-request dependency and reusing key-value (KV) cache across requests, thus\nimproving time-to-first-token (TTFT). However, existing prefix-based context\ncaching requires exact token prefix matches, limiting cache reuse in few-shot\nlearning, multi-document QA, or retrieval-augmented generation, where prefixes\nmay vary. In this paper, we present EPIC, an LLM serving system that introduces\nposition-independent context caching (PIC), enabling modular KV cache reuse\nregardless of token chunk position (or prefix). EPIC features two key designs:\nAttnLink, which leverages static attention sparsity to minimize recomputation\nfor accuracy recovery, and KVSplit, a customizable chunking method that\npreserves semantic coherence. Our experiments demonstrate that Epic delivers up\nto 8x improvements in TTFT and 7x throughput over existing systems, with\nnegligible or no accuracy loss. By addressing the limitations of traditional\ncaching approaches, Epic enables more scalable and efficient LLM inference."
                },
                "authors": [
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Wenrui Huang"
                    },
                    {
                        "name": "Haoyi Wang"
                    },
                    {
                        "name": "Weidong Wang"
                    },
                    {
                        "name": "Tiancheng Hu"
                    },
                    {
                        "name": "Qin Zhang"
                    },
                    {
                        "name": "Hao Feng"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Tao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Tao Xie"
                },
                "author": "Tao Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15332v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15332v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15252v1",
                "updated": "2024-10-20T02:17:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    20,
                    2,
                    17,
                    35,
                    6,
                    294,
                    0
                ],
                "published": "2024-10-20T02:17:35Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    2,
                    17,
                    35,
                    6,
                    294,
                    0
                ],
                "title": "Lossless KV Cache Compression to 2%",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lossless KV Cache Compression to 2%"
                },
                "summary": "Large language models have revolutionized data processing in numerous\ndomains, with their ability to handle extended context reasoning receiving\nnotable recognition. To speed up inference, maintaining a key-value (KV) cache\nmemory is essential. Nonetheless, the growing demands for KV cache memory\ncreate significant hurdles for efficient implementation. This work introduces a\nnovel architecture, Cross-Layer Latent Attention (CLLA), aimed at compressing\nthe KV cache to less than 2% of its original size while maintaining comparable\nperformance levels. CLLA integrates multiple aspects of KV cache compression,\nincluding attention head/dimension reduction, layer sharing, and quantization\ntechniques, into a cohesive framework. Our extensive experiments demonstrate\nthat CLLA achieves lossless performance on most tasks while utilizing minimal\nKV cache, marking a significant advancement in practical KV cache compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have revolutionized data processing in numerous\ndomains, with their ability to handle extended context reasoning receiving\nnotable recognition. To speed up inference, maintaining a key-value (KV) cache\nmemory is essential. Nonetheless, the growing demands for KV cache memory\ncreate significant hurdles for efficient implementation. This work introduces a\nnovel architecture, Cross-Layer Latent Attention (CLLA), aimed at compressing\nthe KV cache to less than 2% of its original size while maintaining comparable\nperformance levels. CLLA integrates multiple aspects of KV cache compression,\nincluding attention head/dimension reduction, layer sharing, and quantization\ntechniques, into a cohesive framework. Our extensive experiments demonstrate\nthat CLLA achieves lossless performance on most tasks while utilizing minimal\nKV cache, marking a significant advancement in practical KV cache compression."
                },
                "authors": [
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "J. N. Han"
                    },
                    {
                        "name": "Kan Wu"
                    },
                    {
                        "name": "Ruobing Xie"
                    },
                    {
                        "name": "An Wang"
                    },
                    {
                        "name": "Xingwu Sun"
                    },
                    {
                        "name": "Zhanhui Kang"
                    }
                ],
                "author_detail": {
                    "name": "Zhanhui Kang"
                },
                "author": "Zhanhui Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2206.05579v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2206.05579v4",
                "updated": "2024-10-19T12:15:50Z",
                "updated_parsed": [
                    2024,
                    10,
                    19,
                    12,
                    15,
                    50,
                    5,
                    293,
                    0
                ],
                "published": "2022-06-11T17:52:10Z",
                "published_parsed": [
                    2022,
                    6,
                    11,
                    17,
                    52,
                    10,
                    5,
                    162,
                    0
                ],
                "title": "Online Paging with Heterogeneous Cache Slots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Paging with Heterogeneous Cache Slots"
                },
                "summary": "It is natural to generalize the online $k$-Server problem by allowing each\nrequest to specify not only a point $p$, but also a subset $S$ of servers that\nmay serve it. For uniform metrics, the problem is equivalent to a\ngeneralization of Paging in which each request specifies not only a page $p$,\nbut also a subset $S$ of cache slots, and is satisfied by having a copy of $p$\nin some slot in $S$. We call this problem Slot-Heterogenous Paging.\n  We parameterize the problem by specifying a family $\\mathcal S \\subseteq\n2^{[k]}$ of requestable slot sets, and we establish bounds on the competitive\nratio as a function of the cache size $k$ and family $\\mathcal S$:\n  - If all request sets are allowed ($\\mathcal\nS=2^{[k]}\\setminus\\{\\emptyset\\}$), the optimal deterministic and randomized\ncompetitive ratios are exponentially worse than for standard \\Paging ($\\mathcal\nS=\\{[k]\\}$).\n  - As a function of $|\\mathcal S|$ and $k$, the optimal deterministic ratio is\npolynomial: at most $O(k^2|\\mathcal S|)$ and at least $\\Omega(\\sqrt{|\\mathcal\nS|})$.\n  - For any laminar family $\\mathcal S$ of height $h$, the optimal ratios are\n$O(hk)$ (deterministic) and $O(h^2\\log k)$ (randomized).\n  - The special case of laminar $\\mathcal S$ that we call All-or-One Paging\nextends standard Paging by allowing each request to specify a specific slot to\nput the requested page in. The optimal deterministic ratio for weighted\nAll-or-One Paging is $\\Theta(k)$. Offline All-or-One Paging is NP-hard.\n  Some results for the laminar case are shown via a reduction to the\ngeneralization of Paging in which each request specifies a set $\\mathcal P of\npages, and is satisfied by fetching any page from $\\mathcal P into the cache.\nThe optimal ratios for the latter problem (with laminar family of height $h$)\nare at most $hk$ (deterministic) and $h\\,H_k$ (randomized).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is natural to generalize the online $k$-Server problem by allowing each\nrequest to specify not only a point $p$, but also a subset $S$ of servers that\nmay serve it. For uniform metrics, the problem is equivalent to a\ngeneralization of Paging in which each request specifies not only a page $p$,\nbut also a subset $S$ of cache slots, and is satisfied by having a copy of $p$\nin some slot in $S$. We call this problem Slot-Heterogenous Paging.\n  We parameterize the problem by specifying a family $\\mathcal S \\subseteq\n2^{[k]}$ of requestable slot sets, and we establish bounds on the competitive\nratio as a function of the cache size $k$ and family $\\mathcal S$:\n  - If all request sets are allowed ($\\mathcal\nS=2^{[k]}\\setminus\\{\\emptyset\\}$), the optimal deterministic and randomized\ncompetitive ratios are exponentially worse than for standard \\Paging ($\\mathcal\nS=\\{[k]\\}$).\n  - As a function of $|\\mathcal S|$ and $k$, the optimal deterministic ratio is\npolynomial: at most $O(k^2|\\mathcal S|)$ and at least $\\Omega(\\sqrt{|\\mathcal\nS|})$.\n  - For any laminar family $\\mathcal S$ of height $h$, the optimal ratios are\n$O(hk)$ (deterministic) and $O(h^2\\log k)$ (randomized).\n  - The special case of laminar $\\mathcal S$ that we call All-or-One Paging\nextends standard Paging by allowing each request to specify a specific slot to\nput the requested page in. The optimal deterministic ratio for weighted\nAll-or-One Paging is $\\Theta(k)$. Offline All-or-One Paging is NP-hard.\n  Some results for the laminar case are shown via a reduction to the\ngeneralization of Paging in which each request specifies a set $\\mathcal P of\npages, and is satisfied by fetching any page from $\\mathcal P into the cache.\nThe optimal ratios for the latter problem (with laminar family of height $h$)\nare at most $hk$ (deterministic) and $h\\,H_k$ (randomized)."
                },
                "authors": [
                    {
                        "name": "Marek Chrobak"
                    },
                    {
                        "name": "Samuel Haney"
                    },
                    {
                        "name": "Mehraneh Liaee"
                    },
                    {
                        "name": "Debmalya Panigrahi"
                    },
                    {
                        "name": "Rajmohan Rajaraman"
                    },
                    {
                        "name": "Ravi Sundaram"
                    },
                    {
                        "name": "Neal E. Young"
                    }
                ],
                "author_detail": {
                    "name": "Neal E. Young"
                },
                "author": "Neal E. Young",
                "arxiv_doi": "10.1007/s00453-024-01270-z",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s00453-024-01270-z",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2206.05579v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2206.05579v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "conference and journal versions appear in STACS 2023 and Algorithmica\n  (2004)",
                "arxiv_journal_ref": "Algorithmica (2004)",
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.2.0; F.1.2; C.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12876v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12876v2",
                "updated": "2024-10-19T08:45:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    19,
                    8,
                    45,
                    11,
                    5,
                    293,
                    0
                ],
                "published": "2024-10-15T05:01:19Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    1,
                    19,
                    1,
                    289,
                    0
                ],
                "title": "In-context KV-Cache Eviction for LLMs via Attention-Gate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context KV-Cache Eviction for LLMs via Attention-Gate"
                },
                "summary": "The KV-Cache technique has become the standard for the inference of large\nlanguage models (LLMs). It caches states of self-attention to avoid\nrecomputation. Yet, it is widely criticized that KV-Cache can become a\nbottleneck of the LLM inference system, especially when confronted with\nultra-large models and long-context queries. A natural remedy is to discard the\nKV-Cache for less important tokens, with StreamingLLM as an example, but the\nused static eviction strategies cannot flexibly adapt to varying contexts.\nRemedies like H2O leverage accumulative attention scores to perform dynamic\neviction but suffer from the attention bias issue in capturing contextual\ninformation. This paper bridges this gap by devising a parameterized KV-Cache\neviction mechanism, dubbed as Attention-Gate, which accepts the whole context\nas input and yields eviction flags for each token to realize in-context\neviction. The subsequent self-attention module proceeds according to the flags\nand only the KV states for the remaining tokens need to be cached. The\nAttention-Gates can vary among different heads and layers and be trivially\nplugged into pre-trained LLMs, tuned by cost-effective continual pre-training\nor supervised fine-tuning objectives to acquire what to discard. The\ncomputational and memory overhead introduced by Attention-Gates is minimal. Our\nmethod is validated across multiple tasks, demonstrating both efficiency and\nadaptability. After a highly efficient continual pre-training, it achieves\nhigher average accuracy and evicts more tokens compared to traditional\ntraining-free methods. In supervised fine-tuning, it not only evicts many\ntokens but also outperforms LoRA-finetuned LLMs on some datasets, such as RTE,\nwhere it improves accuracy by 13.9% while evicting 62.8% of tokens, showing\nthat effective eviction of redundant tokens can even enhance performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The KV-Cache technique has become the standard for the inference of large\nlanguage models (LLMs). It caches states of self-attention to avoid\nrecomputation. Yet, it is widely criticized that KV-Cache can become a\nbottleneck of the LLM inference system, especially when confronted with\nultra-large models and long-context queries. A natural remedy is to discard the\nKV-Cache for less important tokens, with StreamingLLM as an example, but the\nused static eviction strategies cannot flexibly adapt to varying contexts.\nRemedies like H2O leverage accumulative attention scores to perform dynamic\neviction but suffer from the attention bias issue in capturing contextual\ninformation. This paper bridges this gap by devising a parameterized KV-Cache\neviction mechanism, dubbed as Attention-Gate, which accepts the whole context\nas input and yields eviction flags for each token to realize in-context\neviction. The subsequent self-attention module proceeds according to the flags\nand only the KV states for the remaining tokens need to be cached. The\nAttention-Gates can vary among different heads and layers and be trivially\nplugged into pre-trained LLMs, tuned by cost-effective continual pre-training\nor supervised fine-tuning objectives to acquire what to discard. The\ncomputational and memory overhead introduced by Attention-Gates is minimal. Our\nmethod is validated across multiple tasks, demonstrating both efficiency and\nadaptability. After a highly efficient continual pre-training, it achieves\nhigher average accuracy and evicts more tokens compared to traditional\ntraining-free methods. In supervised fine-tuning, it not only evicts many\ntokens but also outperforms LoRA-finetuned LLMs on some datasets, such as RTE,\nwhere it improves accuracy by 13.9% while evicting 62.8% of tokens, showing\nthat effective eviction of redundant tokens can even enhance performance."
                },
                "authors": [
                    {
                        "name": "Zihao Zeng"
                    },
                    {
                        "name": "Bokai Lin"
                    },
                    {
                        "name": "Tianqi Hou"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Zhijie Deng"
                    }
                ],
                "author_detail": {
                    "name": "Zhijie Deng"
                },
                "author": "Zhijie Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12876v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10593v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10593v3",
                "updated": "2024-10-18T19:30:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    19,
                    30,
                    35,
                    4,
                    292,
                    0
                ],
                "published": "2024-09-16T17:36:50Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    36,
                    50,
                    0,
                    260,
                    0
                ],
                "title": "CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context\n  Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context\n  Scenarios"
                },
                "summary": "Large Language Models (LLMs) have been widely adopted to process long-context\ntasks. However, the large memory overhead of the key-value (KV) cache poses\nsignificant challenges in long-context scenarios. Existing training-free KV\ncache compression methods typically focus on quantization and token pruning,\nwhich have compression limits, and excessive sparsity can lead to severe\nperformance degradation. Other methods design new architectures with less KV\noverhead but require significant training overhead. To address the above two\ndrawbacks, we further explore the redundancy in the channel dimension and apply\nan architecture-level design with minor training costs. Therefore, we introduce\nCSKV, a training-efficient Channel Shrinking technique for KV cache\ncompression: (1) We first analyze the singular value distribution of the KV\ncache, revealing significant redundancy and compression potential along the\nchannel dimension. Based on this observation, we propose using low-rank\ndecomposition for key and value layers and storing the low-dimension features.\n(2) To preserve model performance, we introduce a bi-branch KV cache, including\na window-based full-precision KV cache and a low-precision compressed KV cache.\n(3) To reduce the training costs, we minimize the layer-wise reconstruction\nloss for the compressed KV cache instead of retraining the entire LLMs.\nExtensive experiments show that CSKV can reduce the memory overhead of the KV\ncache by 80% while maintaining the model's long-context capability. Moreover,\nwe show that our method can be seamlessly combined with quantization to further\nreduce the memory overhead, achieving a compression ratio of up to 95%. Code is\navailable at https://github.com/wln20/CSKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely adopted to process long-context\ntasks. However, the large memory overhead of the key-value (KV) cache poses\nsignificant challenges in long-context scenarios. Existing training-free KV\ncache compression methods typically focus on quantization and token pruning,\nwhich have compression limits, and excessive sparsity can lead to severe\nperformance degradation. Other methods design new architectures with less KV\noverhead but require significant training overhead. To address the above two\ndrawbacks, we further explore the redundancy in the channel dimension and apply\nan architecture-level design with minor training costs. Therefore, we introduce\nCSKV, a training-efficient Channel Shrinking technique for KV cache\ncompression: (1) We first analyze the singular value distribution of the KV\ncache, revealing significant redundancy and compression potential along the\nchannel dimension. Based on this observation, we propose using low-rank\ndecomposition for key and value layers and storing the low-dimension features.\n(2) To preserve model performance, we introduce a bi-branch KV cache, including\na window-based full-precision KV cache and a low-precision compressed KV cache.\n(3) To reduce the training costs, we minimize the layer-wise reconstruction\nloss for the compressed KV cache instead of retraining the entire LLMs.\nExtensive experiments show that CSKV can reduce the memory overhead of the KV\ncache by 80% while maintaining the model's long-context capability. Moreover,\nwe show that our method can be seamlessly combined with quantization to further\nreduce the memory overhead, achieving a compression ratio of up to 95%. Code is\navailable at https://github.com/wln20/CSKV."
                },
                "authors": [
                    {
                        "name": "Luning Wang"
                    },
                    {
                        "name": "Shiyao Li"
                    },
                    {
                        "name": "Xuefei Ning"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Shengen Yan"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "arxiv_comment": "4th NeurIPS Efficient Natural Language and Speech Processing Workshop\n  (ENLSP-IV 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10593v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10593v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14346v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14346v2",
                "updated": "2024-10-18T13:59:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    59,
                    54,
                    4,
                    292,
                    0
                ],
                "published": "2024-07-19T14:28:53Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    14,
                    28,
                    53,
                    4,
                    201,
                    0
                ],
                "title": "Improving Retrieval in Sponsored Search by Leveraging Query Context\n  Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Retrieval in Sponsored Search by Leveraging Query Context\n  Signals"
                },
                "summary": "Accurately retrieving relevant bid keywords for user queries is critical in\nSponsored Search but remains challenging, particularly for short, ambiguous\nqueries. Existing dense and generative retrieval models often fail to capture\nnuanced user intent in these cases. To address this, we propose an approach to\nenhance query understanding by augmenting queries with rich contextual signals\nderived from web search results and large language models, stored in an online\ncache. Specifically, we use web search titles and snippets to ground queries in\nreal-world information and utilize GPT-4 to generate query rewrites and\nexplanations that clarify user intent. These signals are efficiently integrated\nthrough a Fusion-in-Decoder based Unity architecture, enabling both dense and\ngenerative retrieval with serving costs on par with traditional context-free\nmodels. To address scenarios where context is unavailable in the cache, we\nintroduce context glancing, a curriculum learning strategy that improves model\nrobustness and performance even without contextual signals during inference.\nExtensive offline experiments demonstrate that our context-aware approach\nsubstantially outperforms context-free models. Furthermore, online A/B testing\non a prominent search engine across 160+ countries shows significant\nimprovements in user engagement and revenue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately retrieving relevant bid keywords for user queries is critical in\nSponsored Search but remains challenging, particularly for short, ambiguous\nqueries. Existing dense and generative retrieval models often fail to capture\nnuanced user intent in these cases. To address this, we propose an approach to\nenhance query understanding by augmenting queries with rich contextual signals\nderived from web search results and large language models, stored in an online\ncache. Specifically, we use web search titles and snippets to ground queries in\nreal-world information and utilize GPT-4 to generate query rewrites and\nexplanations that clarify user intent. These signals are efficiently integrated\nthrough a Fusion-in-Decoder based Unity architecture, enabling both dense and\ngenerative retrieval with serving costs on par with traditional context-free\nmodels. To address scenarios where context is unavailable in the cache, we\nintroduce context glancing, a curriculum learning strategy that improves model\nrobustness and performance even without contextual signals during inference.\nExtensive offline experiments demonstrate that our context-aware approach\nsubstantially outperforms context-free models. Furthermore, online A/B testing\non a prominent search engine across 160+ countries shows significant\nimprovements in user engagement and revenue."
                },
                "authors": [
                    {
                        "name": "Akash Kumar Mohankumar"
                    },
                    {
                        "name": "Gururaj K"
                    },
                    {
                        "name": "Gagan Madan"
                    },
                    {
                        "name": "Amit Singh"
                    }
                ],
                "author_detail": {
                    "name": "Amit Singh"
                },
                "author": "Amit Singh",
                "arxiv_comment": "Accepted to EMNLP 2024 Industry Track. 10 pages, 10 tables, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14346v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14346v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14442v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14442v1",
                "updated": "2024-10-18T13:01:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    1,
                    14,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-18T13:01:14Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    1,
                    14,
                    4,
                    292,
                    0
                ],
                "title": "A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference"
                },
                "summary": "Recently, sharing key-value (KV) cache across layers has been found effective\nin efficient inference of large language models (LLMs). To systematically\ninvestigate different techniques of cross-layer KV sharing, we propose a\nunified framework that covers several recent methods and their novel variants.\nWe conduct comprehensive experiments on all the configurations of the\nframework, evaluating their generation throughput and performance in language\nmodeling and downstream tasks. We find that when reducing the size of the KV\ncache by 2x, most configurations can achieve competitive performance to and\nhigher throughput than standard transformers, but when further reducing the\nsize of the KV cache, pairing queries of all layers with KVs of upper layers\ncan better maintain performance, although it also introduces additional\ntraining cost and prefilling latency. We hope that this work will help users\nchoose the appropriate approach according to their requirements and facilitate\nresearch on the acceleration of LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, sharing key-value (KV) cache across layers has been found effective\nin efficient inference of large language models (LLMs). To systematically\ninvestigate different techniques of cross-layer KV sharing, we propose a\nunified framework that covers several recent methods and their novel variants.\nWe conduct comprehensive experiments on all the configurations of the\nframework, evaluating their generation throughput and performance in language\nmodeling and downstream tasks. We find that when reducing the size of the KV\ncache by 2x, most configurations can achieve competitive performance to and\nhigher throughput than standard transformers, but when further reducing the\nsize of the KV cache, pairing queries of all layers with KVs of upper layers\ncan better maintain performance, although it also introduces additional\ntraining cost and prefilling latency. We hope that this work will help users\nchoose the appropriate approach according to their requirements and facilitate\nresearch on the acceleration of LLM inference."
                },
                "authors": [
                    {
                        "name": "You Wu"
                    },
                    {
                        "name": "Haoyi Wu"
                    },
                    {
                        "name": "Kewei Tu"
                    }
                ],
                "author_detail": {
                    "name": "Kewei Tu"
                },
                "author": "Kewei Tu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14442v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14442v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10859v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10859v2",
                "updated": "2024-10-18T10:02:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    18,
                    10,
                    2,
                    3,
                    4,
                    292,
                    0
                ],
                "published": "2024-10-07T13:46:06Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    13,
                    46,
                    6,
                    0,
                    281,
                    0
                ],
                "title": "FAME: Towards Factual Multi-Task Model Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAME: Towards Factual Multi-Task Model Editing"
                },
                "summary": "Large language models (LLMs) embed extensive knowledge and utilize it to\nperform exceptionally well across various tasks. Nevertheless, outdated\nknowledge or factual errors within LLMs can lead to misleading or incorrect\nresponses, causing significant issues in practical applications. To rectify the\nfatal flaw without the necessity for costly model retraining, various model\nediting approaches have been proposed to correct inaccurate knowledge within\nLLMs in a cost-efficient way. To evaluate these model editing methods, previous\nwork introduced a series of datasets. However, most of the previous datasets\nonly contain fabricated data in a single format, which diverges from real-world\nmodel editing scenarios, raising doubts about their usability in practice. To\nfacilitate the application of model editing in real-world scenarios, we propose\nthe challenge of practicality. To resolve such challenges and effectively\nenhance the capabilities of LLMs, we present FAME, an factual, comprehensive,\nand multi-task dataset, which is designed to enhance the practicality of model\nediting. We then propose SKEME, a model editing method that uses a novel\ncaching mechanism to ensure synchronization with the real world. The\nexperiments demonstrate that SKEME performs excellently across various tasks\nand scenarios, confirming its practicality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) embed extensive knowledge and utilize it to\nperform exceptionally well across various tasks. Nevertheless, outdated\nknowledge or factual errors within LLMs can lead to misleading or incorrect\nresponses, causing significant issues in practical applications. To rectify the\nfatal flaw without the necessity for costly model retraining, various model\nediting approaches have been proposed to correct inaccurate knowledge within\nLLMs in a cost-efficient way. To evaluate these model editing methods, previous\nwork introduced a series of datasets. However, most of the previous datasets\nonly contain fabricated data in a single format, which diverges from real-world\nmodel editing scenarios, raising doubts about their usability in practice. To\nfacilitate the application of model editing in real-world scenarios, we propose\nthe challenge of practicality. To resolve such challenges and effectively\nenhance the capabilities of LLMs, we present FAME, an factual, comprehensive,\nand multi-task dataset, which is designed to enhance the practicality of model\nediting. We then propose SKEME, a model editing method that uses a novel\ncaching mechanism to ensure synchronization with the real world. The\nexperiments demonstrate that SKEME performs excellently across various tasks\nand scenarios, confirming its practicality."
                },
                "authors": [
                    {
                        "name": "Li Zeng"
                    },
                    {
                        "name": "Yingyu Shan"
                    },
                    {
                        "name": "Zeming Liu"
                    },
                    {
                        "name": "Jiashu Yao"
                    },
                    {
                        "name": "Yuhang Guo"
                    }
                ],
                "author_detail": {
                    "name": "Yuhang Guo"
                },
                "author": "Yuhang Guo",
                "arxiv_comment": "9 pages, 3 figures. This paper has been accepted by EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10859v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10859v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14003v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14003v1",
                "updated": "2024-10-17T20:11:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    20,
                    11,
                    34,
                    3,
                    291,
                    0
                ],
                "published": "2024-10-17T20:11:34Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    20,
                    11,
                    34,
                    3,
                    291,
                    0
                ],
                "title": "Per-Bank Bandwidth Regulation of Shared Last-Level Cache for Real-Time\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Per-Bank Bandwidth Regulation of Shared Last-Level Cache for Real-Time\n  Systems"
                },
                "summary": "Modern commercial-off-the-shelf (COTS) multicore processors have advanced\nmemory hierarchies that enhance memory-level parallelism (MLP), which is\ncrucial for high performance. To support high MLP, shared last-level caches\n(LLCs) are divided into multiple banks, allowing parallel access. However,\nuneven distribution of cache requests from the cores, especially when requests\nfrom multiple cores are concentrated on a single bank, can result in\nsignificant contention affecting all cores that access the cache. Such cache\nbank contention can even be maliciously induced -- known as cache bank-aware\ndenial-of-service (DoS) attacks -- in order to jeopardize the system's timing\npredictability.\n  In this paper, we propose a per-bank bandwidth regulation approach for\nmulti-banked shared LLC based multicore real-time systems. By regulating\nbandwidth on a per-bank basis, the approach aims to prevent unnecessary\nthrottling of cache accesses to non-contended banks, thus improving overall\nperformance (throughput) without compromising isolation benefits of throttling.\nWe implement our approach on a RISC-V system-on-chip (SoC) platform using\nFireSim and evaluate extensively using both synthetic and real-world workloads.\nOur evaluation results show that the proposed per-bank regulation approach\neffectively protects real-time tasks from co-running cache bank-aware DoS\nattacks, and offers up to a 3.66$\\times$ performance improvement for the\nthrottled benign best-effort tasks compared to prior bank-oblivious bandwidth\nthrottling approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern commercial-off-the-shelf (COTS) multicore processors have advanced\nmemory hierarchies that enhance memory-level parallelism (MLP), which is\ncrucial for high performance. To support high MLP, shared last-level caches\n(LLCs) are divided into multiple banks, allowing parallel access. However,\nuneven distribution of cache requests from the cores, especially when requests\nfrom multiple cores are concentrated on a single bank, can result in\nsignificant contention affecting all cores that access the cache. Such cache\nbank contention can even be maliciously induced -- known as cache bank-aware\ndenial-of-service (DoS) attacks -- in order to jeopardize the system's timing\npredictability.\n  In this paper, we propose a per-bank bandwidth regulation approach for\nmulti-banked shared LLC based multicore real-time systems. By regulating\nbandwidth on a per-bank basis, the approach aims to prevent unnecessary\nthrottling of cache accesses to non-contended banks, thus improving overall\nperformance (throughput) without compromising isolation benefits of throttling.\nWe implement our approach on a RISC-V system-on-chip (SoC) platform using\nFireSim and evaluate extensively using both synthetic and real-world workloads.\nOur evaluation results show that the proposed per-bank regulation approach\neffectively protects real-time tasks from co-running cache bank-aware DoS\nattacks, and offers up to a 3.66$\\times$ performance improvement for the\nthrottled benign best-effort tasks compared to prior bank-oblivious bandwidth\nthrottling approaches."
                },
                "authors": [
                    {
                        "name": "Connor Sullivan"
                    },
                    {
                        "name": "Alex Manley"
                    },
                    {
                        "name": "Mohammad Alian"
                    },
                    {
                        "name": "Heechul Yun"
                    }
                ],
                "author_detail": {
                    "name": "Heechul Yun"
                },
                "author": "Heechul Yun",
                "arxiv_comment": "RTSS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14003v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14003v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13846v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13846v1",
                "updated": "2024-10-17T17:58:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    17,
                    58,
                    14,
                    3,
                    291,
                    0
                ],
                "published": "2024-10-17T17:58:14Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    17,
                    58,
                    14,
                    3,
                    291,
                    0
                ],
                "title": "SimLayerKV: A Simple Framework for Layer-Level KV Cache Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimLayerKV: A Simple Framework for Layer-Level KV Cache Reduction"
                },
                "summary": "Recent advancements in large language models (LLMs) have extended their\ncapabilities to handle long contexts. However, increasing the number of model\nlayers and the length of input sequences significantly escalates the memory\nrequired to store key-value (KV) cache, posing challenges for efficient\ninference. To mitigate this issue, we present SimLayerKV, a simple yet\neffective method that reduces inter-layer KV cache redundancies by selectively\ndropping cache in identified lazy layers. Our approach is based on the\nobservation that certain layers in long-context LLMs exhibit \"lazy\" behavior,\ncontributing less to modeling long-range dependencies compared to non-lazy\nlayers. By analyzing attention weight patterns, we find that the behavior of\nthese lazy layers is consistent across tokens during generation for a given\ninput. This insight motivates our SimLayerKV, which identifies lazy layers and\nreduces their KV cache accordingly. SimLayerKV is training-free, generalizable,\nand can be implemented with only seven lines of code. We conduct extensive\nexperiments on three representative LLMs, e.g., LLaMA2-7B, LLaMA3-8B, and\nMistral-7B across 16 tasks from the LongBench benchmark. The results\ndemonstrate that SimLayerKV achieves a KV cache compression ratio of 5$\\times$\nwith only a 1.2% performance drop when combined with 4-bit quantization. Our\ncode is available at https://github.com/sail-sg/SimLayerKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have extended their\ncapabilities to handle long contexts. However, increasing the number of model\nlayers and the length of input sequences significantly escalates the memory\nrequired to store key-value (KV) cache, posing challenges for efficient\ninference. To mitigate this issue, we present SimLayerKV, a simple yet\neffective method that reduces inter-layer KV cache redundancies by selectively\ndropping cache in identified lazy layers. Our approach is based on the\nobservation that certain layers in long-context LLMs exhibit \"lazy\" behavior,\ncontributing less to modeling long-range dependencies compared to non-lazy\nlayers. By analyzing attention weight patterns, we find that the behavior of\nthese lazy layers is consistent across tokens during generation for a given\ninput. This insight motivates our SimLayerKV, which identifies lazy layers and\nreduces their KV cache accordingly. SimLayerKV is training-free, generalizable,\nand can be implemented with only seven lines of code. We conduct extensive\nexperiments on three representative LLMs, e.g., LLaMA2-7B, LLaMA3-8B, and\nMistral-7B across 16 tasks from the LongBench benchmark. The results\ndemonstrate that SimLayerKV achieves a KV cache compression ratio of 5$\\times$\nwith only a 1.2% performance drop when combined with 4-bit quantization. Our\ncode is available at https://github.com/sail-sg/SimLayerKV."
                },
                "authors": [
                    {
                        "name": "Xuan Zhang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Wei Gao"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13846v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13846v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15355v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15355v4",
                "updated": "2024-10-17T15:27:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    15,
                    27,
                    30,
                    3,
                    291,
                    0
                ],
                "published": "2024-09-14T02:34:26Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    2,
                    34,
                    26,
                    5,
                    258,
                    0
                ],
                "title": "Block-Attention for Efficient RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Block-Attention for Efficient RAG"
                },
                "summary": "We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context. Instead,\nBlock-Attention divides retrieved documents into discrete blocks, with each\nblock independently calculating key-value (KV) states except for the final\nblock. In RAG scenarios, by defining each passage as a block, Block-Attention\nenables us to reuse the KV states of passages that have been seen before,\nthereby significantly reducing the latency and the computation overhead during\ninference. The implementation of Block-Attention involves block segmentation,\nposition re-encoding, and fine-tuning the LLM to adapt to the Block-Attention\nmechanism. Experiments on four RAG benchmarks demonstrate that after block\nfine-tuning, the Block-Attention model achieves performance comparable to\nself-attention models (68.4\\% vs 67.9\\% on Llama3) or even superior performance\n(62.8\\% vs 59.6\\% on Mistral). Notably, Block-Attention significantly reduces\nthe time to first token (TTFT) and floating point operations (FLOPs) to a very\nlow level. It only takes 45 ms to output the first token for an input sequence\nwith a total length of 32K. Compared to the self-attention models, the time\nconsumption and corresponding FLOPs are reduced by 98.7\\% and 99.8\\%,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Block-Attention, an attention mechanism designed to address the\nincreased inference latency and cost in Retrieval-Augmented Generation (RAG)\nscenarios. Traditional approaches often encode the entire context. Instead,\nBlock-Attention divides retrieved documents into discrete blocks, with each\nblock independently calculating key-value (KV) states except for the final\nblock. In RAG scenarios, by defining each passage as a block, Block-Attention\nenables us to reuse the KV states of passages that have been seen before,\nthereby significantly reducing the latency and the computation overhead during\ninference. The implementation of Block-Attention involves block segmentation,\nposition re-encoding, and fine-tuning the LLM to adapt to the Block-Attention\nmechanism. Experiments on four RAG benchmarks demonstrate that after block\nfine-tuning, the Block-Attention model achieves performance comparable to\nself-attention models (68.4\\% vs 67.9\\% on Llama3) or even superior performance\n(62.8\\% vs 59.6\\% on Mistral). Notably, Block-Attention significantly reduces\nthe time to first token (TTFT) and floating point operations (FLOPs) to a very\nlow level. It only takes 45 ms to output the first token for an input sequence\nwith a total length of 32K. Compared to the self-attention models, the time\nconsumption and corresponding FLOPs are reduced by 98.7\\% and 99.8\\%,\nrespectively."
                },
                "authors": [
                    {
                        "name": "East Sun"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Lan Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lan Tian"
                },
                "author": "Lan Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15355v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15355v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.07979v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.07979v2",
                "updated": "2024-10-17T08:54:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    8,
                    54,
                    37,
                    3,
                    291,
                    0
                ],
                "published": "2024-04-11T17:57:22Z",
                "published_parsed": [
                    2024,
                    4,
                    11,
                    17,
                    57,
                    22,
                    3,
                    102,
                    0
                ],
                "title": "LLoCO: Learning Long Contexts Offline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLoCO: Learning Long Contexts Offline"
                },
                "summary": "Processing long contexts remains a challenge for large language models (LLMs)\ndue to the quadratic computational and memory overhead of the self-attention\nmechanism and the substantial KV cache sizes during generation. We propose\nLLoCO, a novel approach to address this problem by learning contexts offline\nthrough context compression and in-domain parameter-efficient finetuning with\nLoRA. Our method enables an LLM to create a concise representation of the\noriginal context and efficiently retrieve relevant information to answer\nquestions accurately. Our approach extends the effective context window of a 4k\ntoken LLaMA2-7B model to handle up to 128k tokens. We evaluate our approach on\nseveral long-context question-answering datasets, demonstrating that LLoCO\nsignificantly outperforms in-context learning while using $30\\times$ fewer\ntokens during inference. LLoCO achieves up to $7.62\\times$ speed-up during\ninference and $11.52\\times$ higher throughput during finetuning, substantially\nreduces the cost of long document question answering. This makes it a promising\nsolution for efficient long context processing. Our code is publicly available\non https://github.com/jeffreysijuntan/lloco.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long contexts remains a challenge for large language models (LLMs)\ndue to the quadratic computational and memory overhead of the self-attention\nmechanism and the substantial KV cache sizes during generation. We propose\nLLoCO, a novel approach to address this problem by learning contexts offline\nthrough context compression and in-domain parameter-efficient finetuning with\nLoRA. Our method enables an LLM to create a concise representation of the\noriginal context and efficiently retrieve relevant information to answer\nquestions accurately. Our approach extends the effective context window of a 4k\ntoken LLaMA2-7B model to handle up to 128k tokens. We evaluate our approach on\nseveral long-context question-answering datasets, demonstrating that LLoCO\nsignificantly outperforms in-context learning while using $30\\times$ fewer\ntokens during inference. LLoCO achieves up to $7.62\\times$ speed-up during\ninference and $11.52\\times$ higher throughput during finetuning, substantially\nreduces the cost of long document question answering. This makes it a promising\nsolution for efficient long context processing. Our code is publicly available\non https://github.com/jeffreysijuntan/lloco."
                },
                "authors": [
                    {
                        "name": "Sijun Tan"
                    },
                    {
                        "name": "Xiuyu Li"
                    },
                    {
                        "name": "Shishir Patil"
                    },
                    {
                        "name": "Ziyang Wu"
                    },
                    {
                        "name": "Tianjun Zhang"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Raluca Ada Popa"
                    }
                ],
                "author_detail": {
                    "name": "Raluca Ada Popa"
                },
                "author": "Raluca Ada Popa",
                "arxiv_comment": "EMNLP 2024. The first two authors contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.07979v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.07979v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18126v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18126v1",
                "updated": "2024-10-17T04:37:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    4,
                    37,
                    43,
                    3,
                    291,
                    0
                ],
                "published": "2024-10-17T04:37:43Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    4,
                    37,
                    43,
                    3,
                    291,
                    0
                ],
                "title": "Leveraging Hardware Performance Counters for Predicting Workload\n  Interference in Vector Supercomputers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Hardware Performance Counters for Predicting Workload\n  Interference in Vector Supercomputers"
                },
                "summary": "In the rapidly evolving domain of high-performance computing (HPC),\nheterogeneous architectures such as the SX-Aurora TSUBASA (SX-AT) system\narchitecture, which integrate diverse processor types, present both\nopportunities and challenges for optimizing resource utilization. This paper\ninvestigates workload interference within an SX-AT system, with a specific\nfocus on resource contention between Vector Hosts (VHs) and Vector Engines\n(VEs). Through comprehensive empirical analysis, the study identifies key\nfactors contributing to performance degradation, such as cache and memory\nbandwidth contention, when jobs with varying computational demands share\nresources. To address these issues, we develop a predictive model that\nleverages hardware performance counters (HCs) and machine learning (ML)\nalgorithms to classify and predict workload interference. Our results\ndemonstrate that the model accurately forecasts performance degradation,\noffering valuable insights for future research on optimizing job scheduling and\nresource allocation. This approach highlights the importance of adaptive\nresource management strategies in maintaining system efficiency and provides a\nfoundation for future enhancements in heterogeneous supercomputing\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the rapidly evolving domain of high-performance computing (HPC),\nheterogeneous architectures such as the SX-Aurora TSUBASA (SX-AT) system\narchitecture, which integrate diverse processor types, present both\nopportunities and challenges for optimizing resource utilization. This paper\ninvestigates workload interference within an SX-AT system, with a specific\nfocus on resource contention between Vector Hosts (VHs) and Vector Engines\n(VEs). Through comprehensive empirical analysis, the study identifies key\nfactors contributing to performance degradation, such as cache and memory\nbandwidth contention, when jobs with varying computational demands share\nresources. To address these issues, we develop a predictive model that\nleverages hardware performance counters (HCs) and machine learning (ML)\nalgorithms to classify and predict workload interference. Our results\ndemonstrate that the model accurately forecasts performance degradation,\noffering valuable insights for future research on optimizing job scheduling and\nresource allocation. This approach highlights the importance of adaptive\nresource management strategies in maintaining system efficiency and provides a\nfoundation for future enhancements in heterogeneous supercomputing\nenvironments."
                },
                "authors": [
                    {
                        "name": "Shubham"
                    },
                    {
                        "name": "Keichi Takahashi"
                    },
                    {
                        "name": "Hiroyuki Takizawa"
                    }
                ],
                "author_detail": {
                    "name": "Hiroyuki Takizawa"
                },
                "author": "Hiroyuki Takizawa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18126v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18126v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13212v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13212v1",
                "updated": "2024-10-17T04:35:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    17,
                    4,
                    35,
                    57,
                    3,
                    291,
                    0
                ],
                "published": "2024-10-17T04:35:57Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    4,
                    35,
                    57,
                    3,
                    291,
                    0
                ],
                "title": "AsymKV: Enabling 1-Bit Quantization of KV Cache with Layer-Wise\n  Asymmetric Quantization Configurations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsymKV: Enabling 1-Bit Quantization of KV Cache with Layer-Wise\n  Asymmetric Quantization Configurations"
                },
                "summary": "Large language models have shown exceptional capabilities in a wide range of\ntasks, such as text generation and video generation, among others. However, due\nto their massive parameter count, these models often require substantial\nstorage space, imposing significant constraints on the machines deploying LLMs.\nTo overcome this limitation, one research direction proposes to compress the\nmodels using integer replacements for floating-point numbers, in a process\nknown as Quantization. Some recent studies suggest quantizing the key and value\ncache (KV Cache) of LLMs, and designing quantization techniques that treat the\nkey and value matrices equivalently.\n  This work delves deeper into the asymmetric structural roles of KV Cache, a\nphenomenon where the transformer's output loss is more sensitive to the\nquantization of key matrices. We conduct a systematic examination of the\nattention output error resulting from key and value quantization. The\nphenomenon inspires us to propose an asymmetric quantization strategy. Our\napproach allows for 1-bit quantization of the KV cache by implementing distinct\nconfigurations for key and value matrices. We carry out experiments across a\nvariety of datasets, demonstrating that our proposed model allows for the\nquantization of up to 75% decoder layers with 1 bit, while simultaneously\nmaintaining performance levels comparable to those of the models with floating\nparameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have shown exceptional capabilities in a wide range of\ntasks, such as text generation and video generation, among others. However, due\nto their massive parameter count, these models often require substantial\nstorage space, imposing significant constraints on the machines deploying LLMs.\nTo overcome this limitation, one research direction proposes to compress the\nmodels using integer replacements for floating-point numbers, in a process\nknown as Quantization. Some recent studies suggest quantizing the key and value\ncache (KV Cache) of LLMs, and designing quantization techniques that treat the\nkey and value matrices equivalently.\n  This work delves deeper into the asymmetric structural roles of KV Cache, a\nphenomenon where the transformer's output loss is more sensitive to the\nquantization of key matrices. We conduct a systematic examination of the\nattention output error resulting from key and value quantization. The\nphenomenon inspires us to propose an asymmetric quantization strategy. Our\napproach allows for 1-bit quantization of the KV cache by implementing distinct\nconfigurations for key and value matrices. We carry out experiments across a\nvariety of datasets, demonstrating that our proposed model allows for the\nquantization of up to 75% decoder layers with 1 bit, while simultaneously\nmaintaining performance levels comparable to those of the models with floating\nparameters."
                },
                "authors": [
                    {
                        "name": "Qian Tao"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.13212v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13212v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.08895v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.08895v3",
                "updated": "2024-10-16T17:54:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    54,
                    15,
                    2,
                    290,
                    0
                ],
                "published": "2024-01-17T00:36:58Z",
                "published_parsed": [
                    2024,
                    1,
                    17,
                    0,
                    36,
                    58,
                    2,
                    17,
                    0
                ],
                "title": "cedar: Optimized and Unified Machine Learning Input Data Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "cedar: Optimized and Unified Machine Learning Input Data Pipelines"
                },
                "summary": "The input data pipeline is an essential component of each machine learning\n(ML) training job. It is responsible for reading massive amounts of training\ndata, processing batches of samples using complex transformations, and loading\nthem onto training nodes at low latency and high throughput. Performant input\ndata systems are becoming increasingly critical, driven by skyrocketing data\nvolumes and training throughput demands. Unfortunately, current input data\nsystems cannot fully leverage key performance optimizations, resulting in\nhugely inefficient infrastructures that require significant resources - or\nworse - underutilize expensive accelerators.\n  To address these demands, we present cedar, an optimized and unified\nprogramming framework for ML input data pipelines. cedar allows users to define\ninput data pipelines using composable operators that support arbitrary ML\nframeworks and libraries. cedar introduces an extensible optimizer that\nsystematically applies a complex combination of optimizations (e.g.,\noffloading, caching, prefetching, fusion, and reordering). It orchestrates\nprocessing across a customizable set of local and distributed compute resources\nin order to improve processing performance and efficiency, all without user\ninput. Across eight pipelines, cedar improves performance by up to 1.87x to\n10.65x compared to state-of-the-art input data systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The input data pipeline is an essential component of each machine learning\n(ML) training job. It is responsible for reading massive amounts of training\ndata, processing batches of samples using complex transformations, and loading\nthem onto training nodes at low latency and high throughput. Performant input\ndata systems are becoming increasingly critical, driven by skyrocketing data\nvolumes and training throughput demands. Unfortunately, current input data\nsystems cannot fully leverage key performance optimizations, resulting in\nhugely inefficient infrastructures that require significant resources - or\nworse - underutilize expensive accelerators.\n  To address these demands, we present cedar, an optimized and unified\nprogramming framework for ML input data pipelines. cedar allows users to define\ninput data pipelines using composable operators that support arbitrary ML\nframeworks and libraries. cedar introduces an extensible optimizer that\nsystematically applies a complex combination of optimizations (e.g.,\noffloading, caching, prefetching, fusion, and reordering). It orchestrates\nprocessing across a customizable set of local and distributed compute resources\nin order to improve processing performance and efficiency, all without user\ninput. Across eight pipelines, cedar improves performance by up to 1.87x to\n10.65x compared to state-of-the-art input data systems."
                },
                "authors": [
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Emanuel Adamiak"
                    },
                    {
                        "name": "Christos Kozyrakis"
                    }
                ],
                "author_detail": {
                    "name": "Christos Kozyrakis"
                },
                "author": "Christos Kozyrakis",
                "arxiv_comment": "Accepted to PVLDB Volume 18",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.08895v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.08895v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12749v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12749v1",
                "updated": "2024-10-16T17:10:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    10,
                    48,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T17:10:48Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    17,
                    10,
                    48,
                    2,
                    290,
                    0
                ],
                "title": "Toleo: Scaling Freshness to Tera-scale Memory using CXL and PIM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toleo: Scaling Freshness to Tera-scale Memory using CXL and PIM"
                },
                "summary": "Trusted hardware's freshness guarantee ensures that an adversary cannot\nreplay an old value in response to a memory read request. They rely on\nmaintaining a version number for each cache block and ensuring their integrity\nusing a Merkle tree. However, these existing solutions protect only a small\namount of main memory (few MBs), as the extraneous memory accesses to the\nMerkle tree increase prohibitively with the protected memory size. We present\nToleo, which uses trusted smart memory connected through a secure CXL IDE\nnetwork to safely store version numbers. Toleo eliminates the need for an\nunscalable Merkle tree to protect the integrity of version numbers by instead\nusing smart memory as the root of trust. Additionally, Toleo ensures version\nconfidentiality which enables stealth versions that reduce the version storage\noverhead in half.\n  Furthermore, in the absence of Merkle tree imposed constraints, we\neffectively exploit version locality at page granularity to compress version\nnumber by a factor of 240. These space optimizations make it feasible for one\n168 GB Toleo smart memory device to provide freshness to a 28 TB CXL-expanded\nmain memory pool in a rack server for a negligible performance overhead. We\nanalyze the benefits of Toleo using several privacy-sensitive genomics, graph,\ngenerative AI, and database workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trusted hardware's freshness guarantee ensures that an adversary cannot\nreplay an old value in response to a memory read request. They rely on\nmaintaining a version number for each cache block and ensuring their integrity\nusing a Merkle tree. However, these existing solutions protect only a small\namount of main memory (few MBs), as the extraneous memory accesses to the\nMerkle tree increase prohibitively with the protected memory size. We present\nToleo, which uses trusted smart memory connected through a secure CXL IDE\nnetwork to safely store version numbers. Toleo eliminates the need for an\nunscalable Merkle tree to protect the integrity of version numbers by instead\nusing smart memory as the root of trust. Additionally, Toleo ensures version\nconfidentiality which enables stealth versions that reduce the version storage\noverhead in half.\n  Furthermore, in the absence of Merkle tree imposed constraints, we\neffectively exploit version locality at page granularity to compress version\nnumber by a factor of 240. These space optimizations make it feasible for one\n168 GB Toleo smart memory device to provide freshness to a 28 TB CXL-expanded\nmain memory pool in a rack server for a negligible performance overhead. We\nanalyze the benefits of Toleo using several privacy-sensitive genomics, graph,\ngenerative AI, and database workloads."
                },
                "authors": [
                    {
                        "name": "Juechu Dong"
                    },
                    {
                        "name": "Jonah Rosenblum"
                    },
                    {
                        "name": "Satish Narayanasamy"
                    }
                ],
                "author_detail": {
                    "name": "Satish Narayanasamy"
                },
                "author": "Satish Narayanasamy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12749v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12749v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12605v1",
                "updated": "2024-10-16T14:24:16Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    24,
                    16,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T14:24:16Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    24,
                    16,
                    2,
                    290,
                    0
                ],
                "title": "Advancing Web Browser Forensics: Critical Evaluation of Emerging Tools\n  and Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Web Browser Forensics: Critical Evaluation of Emerging Tools\n  and Techniques"
                },
                "summary": "As the use of web browsers continues to grow, the potential for cybercrime\nand web-related criminal activities also increases. Digital forensic\ninvestigators must understand how different browsers function and the critical\nareas to consider during web forensic analysis. Web forensics, a subfield of\ndigital forensics, involves collecting and analyzing browser artifacts, such as\nbrowser history, search keywords, and downloads, which serve as potential\nevidence. While existing research has provided valuable insights, many studies\nfocus on individual browsing modes or limited forensic scenarios, leaving gaps\nin understanding the full scope of data retention and recovery across different\nmodes and browsers. This paper addresses these gaps by defining four browsing\nscenarios and critically analyzing browser artifacts across normal, private,\nand portable modes using various forensic tools. We define four browsing\nscenarios to perform a comprehensive evaluation of popular browsers -- Google\nChrome, Mozilla Firefox, Brave, Tor, and Microsoft Edge -- by monitoring\nchanges in key data storage areas such as cache files, cookies, browsing\nhistory, and local storage across different browsing modes. Overall, this paper\ncontributes to a deeper understanding of browser forensic analysis and\nidentifies key areas for enhancing privacy protection and forensic\nmethodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the use of web browsers continues to grow, the potential for cybercrime\nand web-related criminal activities also increases. Digital forensic\ninvestigators must understand how different browsers function and the critical\nareas to consider during web forensic analysis. Web forensics, a subfield of\ndigital forensics, involves collecting and analyzing browser artifacts, such as\nbrowser history, search keywords, and downloads, which serve as potential\nevidence. While existing research has provided valuable insights, many studies\nfocus on individual browsing modes or limited forensic scenarios, leaving gaps\nin understanding the full scope of data retention and recovery across different\nmodes and browsers. This paper addresses these gaps by defining four browsing\nscenarios and critically analyzing browser artifacts across normal, private,\nand portable modes using various forensic tools. We define four browsing\nscenarios to perform a comprehensive evaluation of popular browsers -- Google\nChrome, Mozilla Firefox, Brave, Tor, and Microsoft Edge -- by monitoring\nchanges in key data storage areas such as cache files, cookies, browsing\nhistory, and local storage across different browsing modes. Overall, this paper\ncontributes to a deeper understanding of browser forensic analysis and\nidentifies key areas for enhancing privacy protection and forensic\nmethodologies."
                },
                "authors": [
                    {
                        "name": "Rishal Ravikesh Chand"
                    },
                    {
                        "name": "Neeraj Anand Sharma"
                    },
                    {
                        "name": "Muhammad Ashad Kabir"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Ashad Kabir"
                },
                "author": "Muhammad Ashad Kabir",
                "arxiv_comment": "34 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12513v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12513v1",
                "updated": "2024-10-16T12:45:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T12:45:35Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "title": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction"
                },
                "summary": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across domanins such as vision and language processing. However,\ndue to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit 2) Input-agnostic heuristics\nwhere tokens exit at pre-determined layers irrespective of input sequence. Both\nthe above strategies have limitations - the former cannot be applied to handle\nKV Caching necessary for speed-ups in modern framework and the latter does not\ncapture the variation in layer importance across tasks or more generally,\nacross input sequences. To address both limitations, we propose FIRST, an\nalgorithm that reduces inference latency by using layer-specific routers to\nselect a subset of transformer layers adaptively for each input sequence - the\nprompt (during prefill stage) decides which layers will be skipped during\ndecoding. FIRST preserves compatibility with KV caching enabling faster\ninference while being quality-aware. FIRST is model-agnostic and can be easily\nenabled on any pre-trained LLM. We further improve performance by incorporating\nLoRA adapters for fine-tuning on external datasets, enhancing task-specific\naccuracy while maintaining latency benefits. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on task. Extensive\nexperiments show that FIRST significantly reduces latency while retaining\ncompetitive performance (as compared to baselines), making our approach an\nefficient solution for LLM deployment in low-resource environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across domanins such as vision and language processing. However,\ndue to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit 2) Input-agnostic heuristics\nwhere tokens exit at pre-determined layers irrespective of input sequence. Both\nthe above strategies have limitations - the former cannot be applied to handle\nKV Caching necessary for speed-ups in modern framework and the latter does not\ncapture the variation in layer importance across tasks or more generally,\nacross input sequences. To address both limitations, we propose FIRST, an\nalgorithm that reduces inference latency by using layer-specific routers to\nselect a subset of transformer layers adaptively for each input sequence - the\nprompt (during prefill stage) decides which layers will be skipped during\ndecoding. FIRST preserves compatibility with KV caching enabling faster\ninference while being quality-aware. FIRST is model-agnostic and can be easily\nenabled on any pre-trained LLM. We further improve performance by incorporating\nLoRA adapters for fine-tuning on external datasets, enhancing task-specific\naccuracy while maintaining latency benefits. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on task. Extensive\nexperiments show that FIRST significantly reduces latency while retaining\ncompetitive performance (as compared to baselines), making our approach an\nefficient solution for LLM deployment in low-resource environments."
                },
                "authors": [
                    {
                        "name": "Akriti Jain"
                    },
                    {
                        "name": "Saransh Sharma"
                    },
                    {
                        "name": "Koyel Mukherjee"
                    },
                    {
                        "name": "Soumyabrata Pal"
                    }
                ],
                "author_detail": {
                    "name": "Soumyabrata Pal"
                },
                "author": "Soumyabrata Pal",
                "arxiv_comment": "17 pages, 6 figures, Submitted to ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12513v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12513v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12423v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12423v1",
                "updated": "2024-10-16T10:06:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    10,
                    6,
                    22,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T10:06:22Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    10,
                    6,
                    22,
                    2,
                    290,
                    0
                ],
                "title": "An O(m+n)-Space Spatiotemporal Denoising Filter with Cache-Like Memories\n  for Dynamic Vision Sensors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An O(m+n)-Space Spatiotemporal Denoising Filter with Cache-Like Memories\n  for Dynamic Vision Sensors"
                },
                "summary": "Dynamic vision sensor (DVS) is novel neuromorphic imaging device that\ngenerates asynchronous events. Despite the high temporal resolution and high\ndynamic range features, DVS is faced with background noise problem.\nSpatiotemporal filter is an effective and hardware-friendly solution for DVS\ndenoising but previous designs have large memory overhead or degraded\nperformance issues. In this paper, we present a lightweight and real-time\nspatiotemporal denoising filter with set-associative cache-like memories, which\nhas low space complexity of \\text{O(m+n)} for DVS of $m\\times n$ resolution. A\ntwo-stage pipeline for memory access with read cancellation feature is proposed\nto reduce power consumption. Further the bitwidth redundancy for event storage\nis exploited to minimize the memory footprint. We implemented our design on\nFPGA and experimental results show that it achieves state-of-the-art\nperformance compared with previous spatiotemporal filters while maintaining low\nresource utilization and low power consumption of about 125mW to 210mW at\n100MHz clock frequency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic vision sensor (DVS) is novel neuromorphic imaging device that\ngenerates asynchronous events. Despite the high temporal resolution and high\ndynamic range features, DVS is faced with background noise problem.\nSpatiotemporal filter is an effective and hardware-friendly solution for DVS\ndenoising but previous designs have large memory overhead or degraded\nperformance issues. In this paper, we present a lightweight and real-time\nspatiotemporal denoising filter with set-associative cache-like memories, which\nhas low space complexity of \\text{O(m+n)} for DVS of $m\\times n$ resolution. A\ntwo-stage pipeline for memory access with read cancellation feature is proposed\nto reduce power consumption. Further the bitwidth redundancy for event storage\nis exploited to minimize the memory footprint. We implemented our design on\nFPGA and experimental results show that it achieves state-of-the-art\nperformance compared with previous spatiotemporal filters while maintaining low\nresource utilization and low power consumption of about 125mW to 210mW at\n100MHz clock frequency."
                },
                "authors": [
                    {
                        "name": "Qinghang Zhao"
                    },
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Yixi Ji"
                    },
                    {
                        "name": "Jinjian Wu"
                    },
                    {
                        "name": "Guangming Shi"
                    }
                ],
                "author_detail": {
                    "name": "Guangming Shi"
                },
                "author": "Guangming Shi",
                "arxiv_doi": "10.1145/3676536.3676710",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676536.3676710",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.12423v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12423v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14731v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14731v1",
                "updated": "2024-10-16T08:34:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    8,
                    34,
                    51,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T08:34:51Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    8,
                    34,
                    51,
                    2,
                    290,
                    0
                ],
                "title": "MatryoshkaKV: Adaptive KV Compression via Trainable Orthogonal\n  Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MatryoshkaKV: Adaptive KV Compression via Trainable Orthogonal\n  Projection"
                },
                "summary": "KV cache has become a de facto technique for the inference of large language\nmodels (LLMs), where tensors of shape (layer number, head number, sequence\nlength, feature dimension) are introduced to cache historical information for\nself-attention. As the size of the model and data grows, the KV cache can\nquickly become a bottleneck within the system in both storage and memory\ntransfer. To address this, prior studies usually focus on the first three axes\nof the cache tensors for compression. This paper supplements them, focusing on\nthe feature dimension axis, by utilizing low-rank projection matrices to\ntransform the cache features into spaces with reduced dimensions. We begin by\ninvestigating the canonical orthogonal projection method for data compression\nthrough principal component analysis (PCA). We observe the issue with PCA\nprojection where significant performance degradation is observed at low\ncompression rates. To bridge the gap, we propose to directly tune the\northogonal projection matrices with a distillation objective using an elaborate\nMatryoshka training strategy. After training, we adaptively search for the\noptimal compression rates for various layers and heads given varying\ncompression budgets. Compared to previous works, our method can easily embrace\npre-trained LLMs and hold a smooth tradeoff between performance and compression\nrate. We empirically witness the high data efficiency of our training procedure\nand find that our method can sustain over 90% performance with an average KV\ncache compression rate of 60% (and up to 75% in certain extreme scenarios) for\npopular LLMs like LLaMA2-7B-base and Mistral-7B-v0.3-base.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache has become a de facto technique for the inference of large language\nmodels (LLMs), where tensors of shape (layer number, head number, sequence\nlength, feature dimension) are introduced to cache historical information for\nself-attention. As the size of the model and data grows, the KV cache can\nquickly become a bottleneck within the system in both storage and memory\ntransfer. To address this, prior studies usually focus on the first three axes\nof the cache tensors for compression. This paper supplements them, focusing on\nthe feature dimension axis, by utilizing low-rank projection matrices to\ntransform the cache features into spaces with reduced dimensions. We begin by\ninvestigating the canonical orthogonal projection method for data compression\nthrough principal component analysis (PCA). We observe the issue with PCA\nprojection where significant performance degradation is observed at low\ncompression rates. To bridge the gap, we propose to directly tune the\northogonal projection matrices with a distillation objective using an elaborate\nMatryoshka training strategy. After training, we adaptively search for the\noptimal compression rates for various layers and heads given varying\ncompression budgets. Compared to previous works, our method can easily embrace\npre-trained LLMs and hold a smooth tradeoff between performance and compression\nrate. We empirically witness the high data efficiency of our training procedure\nand find that our method can sustain over 90% performance with an average KV\ncache compression rate of 60% (and up to 75% in certain extreme scenarios) for\npopular LLMs like LLaMA2-7B-base and Mistral-7B-v0.3-base."
                },
                "authors": [
                    {
                        "name": "Bokai Lin"
                    },
                    {
                        "name": "Zihao Zeng"
                    },
                    {
                        "name": "Zipeng Xiao"
                    },
                    {
                        "name": "Siqi Kou"
                    },
                    {
                        "name": "Tianqi Hou"
                    },
                    {
                        "name": "Xiaofeng Gao"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Zhijie Deng"
                    }
                ],
                "author_detail": {
                    "name": "Zhijie Deng"
                },
                "author": "Zhijie Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14731v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14731v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12168v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12168v1",
                "updated": "2024-10-16T02:16:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    16,
                    2,
                    16,
                    53,
                    2,
                    290,
                    0
                ],
                "published": "2024-10-16T02:16:53Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    2,
                    16,
                    53,
                    2,
                    290,
                    0
                ],
                "title": "COMET: Towards Partical W4A4KV4 LLMs Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COMET: Towards Partical W4A4KV4 LLMs Serving"
                },
                "summary": "Quantization is a widely-used compression technology to reduce the overhead\nof serving large language models (LLMs) on terminal devices and in cloud data\ncenters. However, prevalent quantization methods, such as 8-bit\nweight-activation or 4-bit weight-only quantization, achieve limited\nperformance improvements due to poor support for low-precision (e.g., 4-bit)\nactivation. This work, for the first time, realizes practical W4A4KV4 serving\nfor LLMs, fully utilizing the INT4 tensor cores on modern GPUs and reducing the\nmemory bottleneck caused by the KV cache. Specifically, we propose a novel\nfine-grained mixed-precision quantization algorithm (FMPQ) that compresses most\nactivations into 4-bit with negligible accuracy loss. To support\nmixed-precision matrix multiplication for W4A4 and W4A8, we develop a highly\noptimized W4Ax kernel. Our approach introduces a novel mixed-precision data\nlayout to facilitate access and fast dequantization for activation and weight\ntensors, utilizing the GPU's software pipeline to hide the overhead of data\nloading and conversion. Additionally, we propose fine-grained streaming\nmultiprocessor (SM) scheduling to achieve load balance across different SMs. We\nintegrate the optimized W4Ax kernel into our inference framework, COMET, and\nprovide efficient management to support popular LLMs such as LLaMA-3-70B.\nExtensive evaluations demonstrate that, when running LLaMA family models on a\nsingle A100-80G-SMX4, COMET achieves a kernel-level speedup of\n\\textbf{$2.88\\times$} over cuBLAS and a \\textbf{$2.02 \\times$} throughput\nimprovement compared to TensorRT-LLM from an end-to-end framework perspective.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization is a widely-used compression technology to reduce the overhead\nof serving large language models (LLMs) on terminal devices and in cloud data\ncenters. However, prevalent quantization methods, such as 8-bit\nweight-activation or 4-bit weight-only quantization, achieve limited\nperformance improvements due to poor support for low-precision (e.g., 4-bit)\nactivation. This work, for the first time, realizes practical W4A4KV4 serving\nfor LLMs, fully utilizing the INT4 tensor cores on modern GPUs and reducing the\nmemory bottleneck caused by the KV cache. Specifically, we propose a novel\nfine-grained mixed-precision quantization algorithm (FMPQ) that compresses most\nactivations into 4-bit with negligible accuracy loss. To support\nmixed-precision matrix multiplication for W4A4 and W4A8, we develop a highly\noptimized W4Ax kernel. Our approach introduces a novel mixed-precision data\nlayout to facilitate access and fast dequantization for activation and weight\ntensors, utilizing the GPU's software pipeline to hide the overhead of data\nloading and conversion. Additionally, we propose fine-grained streaming\nmultiprocessor (SM) scheduling to achieve load balance across different SMs. We\nintegrate the optimized W4Ax kernel into our inference framework, COMET, and\nprovide efficient management to support popular LLMs such as LLaMA-3-70B.\nExtensive evaluations demonstrate that, when running LLaMA family models on a\nsingle A100-80G-SMX4, COMET achieves a kernel-level speedup of\n\\textbf{$2.88\\times$} over cuBLAS and a \\textbf{$2.02 \\times$} throughput\nimprovement compared to TensorRT-LLM from an end-to-end framework perspective."
                },
                "authors": [
                    {
                        "name": "Lian Liu"
                    },
                    {
                        "name": "Haimeng Ren"
                    },
                    {
                        "name": "Long Cheng"
                    },
                    {
                        "name": "Zhaohui Xu"
                    },
                    {
                        "name": "Yudong Pan"
                    },
                    {
                        "name": "Mengdi Wang"
                    },
                    {
                        "name": "Xiaowei Li"
                    },
                    {
                        "name": "Yinhe Han"
                    },
                    {
                        "name": "Ying Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ying Wang"
                },
                "author": "Ying Wang",
                "arxiv_comment": "14 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12168v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12168v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02536v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02536v2",
                "updated": "2024-10-15T15:58:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    15,
                    58,
                    7,
                    1,
                    289,
                    0
                ],
                "published": "2024-06-04T17:55:38Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    55,
                    38,
                    1,
                    156,
                    0
                ],
                "title": "Mitigate Position Bias in Large Language Models via Scaling a Single\n  Dimension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigate Position Bias in Large Language Models via Scaling a Single\n  Dimension"
                },
                "summary": "Large Language Models (LLMs) are increasingly applied in various real-world\nscenarios due to their excellent generalization capabilities and robust\ngenerative abilities. However, they exhibit position bias, also known as \"lost\nin the middle\", a phenomenon that is especially pronounced in long-context\nscenarios, which indicates the placement of the key information in different\npositions of a prompt can significantly affect accuracy. This paper first\nexplores the micro-level manifestations of position bias, concluding that\nattention weights are a micro-level expression of position bias. It further\nidentifies that, in addition to position embeddings, causal attention mask also\ncontributes to position bias by creating position-specific hidden states. Based\non these insights, we propose a method to mitigate position bias by scaling\nthis positional hidden states. Experiments on the NaturalQuestions\nMulti-document QA, KV retrieval, LongBench and timeline reorder tasks, using\nvarious models including RoPE models, context windowextended models, and Alibi\nmodels, demonstrate the effectiveness and generalizability of our approach. Our\nmethod can improve performance by up to 15.2% by modifying just one dimension\nof hidden states. Our code is available at https://aka.ms/PositionalHidden.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly applied in various real-world\nscenarios due to their excellent generalization capabilities and robust\ngenerative abilities. However, they exhibit position bias, also known as \"lost\nin the middle\", a phenomenon that is especially pronounced in long-context\nscenarios, which indicates the placement of the key information in different\npositions of a prompt can significantly affect accuracy. This paper first\nexplores the micro-level manifestations of position bias, concluding that\nattention weights are a micro-level expression of position bias. It further\nidentifies that, in addition to position embeddings, causal attention mask also\ncontributes to position bias by creating position-specific hidden states. Based\non these insights, we propose a method to mitigate position bias by scaling\nthis positional hidden states. Experiments on the NaturalQuestions\nMulti-document QA, KV retrieval, LongBench and timeline reorder tasks, using\nvarious models including RoPE models, context windowextended models, and Alibi\nmodels, demonstrate the effectiveness and generalizability of our approach. Our\nmethod can improve performance by up to 15.2% by modifying just one dimension\nof hidden states. Our code is available at https://aka.ms/PositionalHidden."
                },
                "authors": [
                    {
                        "name": "Yijiong Yu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Qianhui Wu"
                    },
                    {
                        "name": "Chin-Yew Lin"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Yongfeng Huang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02536v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02536v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11417v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11417v1",
                "updated": "2024-10-15T09:07:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    9,
                    7,
                    25,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T09:07:25Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    9,
                    7,
                    25,
                    1,
                    289,
                    0
                ],
                "title": "VidCompress: Memory-Enhanced Temporal Compression for Video\n  Understanding in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VidCompress: Memory-Enhanced Temporal Compression for Video\n  Understanding in Large Language Models"
                },
                "summary": "Video-based multimodal large language models (Video-LLMs) possess significant\npotential for video understanding tasks. However, most Video-LLMs treat videos\nas a sequential set of individual frames, which results in insufficient\ntemporal-spatial interaction that hinders fine-grained comprehension and\ndifficulty in processing longer videos due to limited visual token capacity. To\naddress these challenges, we propose VidCompress, a novel Video-LLM featuring\nmemory-enhanced temporal compression. VidCompress employs a dual-compressor\napproach: a memory-enhanced compressor captures both short-term and long-term\ntemporal relationships in videos and compresses the visual tokens using a\nmultiscale transformer with a memory-cache mechanism, while a text-perceived\ncompressor generates condensed visual tokens by utilizing Q-Former and\nintegrating temporal contexts into query embeddings with cross attention.\nExperiments on several VideoQA datasets and comprehensive benchmarks\ndemonstrate that VidCompress efficiently models complex temporal-spatial\nrelations and significantly outperforms existing Video-LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-based multimodal large language models (Video-LLMs) possess significant\npotential for video understanding tasks. However, most Video-LLMs treat videos\nas a sequential set of individual frames, which results in insufficient\ntemporal-spatial interaction that hinders fine-grained comprehension and\ndifficulty in processing longer videos due to limited visual token capacity. To\naddress these challenges, we propose VidCompress, a novel Video-LLM featuring\nmemory-enhanced temporal compression. VidCompress employs a dual-compressor\napproach: a memory-enhanced compressor captures both short-term and long-term\ntemporal relationships in videos and compresses the visual tokens using a\nmultiscale transformer with a memory-cache mechanism, while a text-perceived\ncompressor generates condensed visual tokens by utilizing Q-Former and\nintegrating temporal contexts into query embeddings with cross attention.\nExperiments on several VideoQA datasets and comprehensive benchmarks\ndemonstrate that VidCompress efficiently models complex temporal-spatial\nrelations and significantly outperforms existing Video-LLMs."
                },
                "authors": [
                    {
                        "name": "Xiaohan Lan"
                    },
                    {
                        "name": "Yitian Yuan"
                    },
                    {
                        "name": "Zequn Jie"
                    },
                    {
                        "name": "Lin Ma"
                    }
                ],
                "author_detail": {
                    "name": "Lin Ma"
                },
                "author": "Lin Ma",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11417v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11417v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09297v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09297v3",
                "updated": "2024-10-15T08:45:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    8,
                    45,
                    18,
                    1,
                    289,
                    0
                ],
                "published": "2024-06-13T16:33:44Z",
                "published_parsed": [
                    2024,
                    6,
                    13,
                    16,
                    33,
                    44,
                    3,
                    165,
                    0
                ],
                "title": "MLKV: Multi-Layer Key-Value Heads for Memory Efficient Transformer\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MLKV: Multi-Layer Key-Value Heads for Memory Efficient Transformer\n  Decoding"
                },
                "summary": "Auto-regressive inference of transformers benefit greatly from Key-Value (KV)\ncaching, but can lead to major memory bottlenecks as model size, batch size,\nand sequence length grow at scale. We introduce Multi-Layer Key-Value (MLKV)\nsharing, a novel approach extending KV sharing across transformer layers to\nreduce memory usage beyond what was possible with Multi-Query Attention (MQA)\nand Grouped-Query Attention (GQA). Evaluations on various NLP benchmarks and\ninference metrics using uptrained Pythia-160M variants demonstrate that MLKV\nsignificantly reduces memory usage with minimal performance loss, reducing KV\ncache size down to a factor of 6x compared to MQA. These results highlight\nMLKV's potential for efficient deployment of transformer models at scale. We\nprovide code at https://github.com/zaydzuhri/pythia-mlkv",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive inference of transformers benefit greatly from Key-Value (KV)\ncaching, but can lead to major memory bottlenecks as model size, batch size,\nand sequence length grow at scale. We introduce Multi-Layer Key-Value (MLKV)\nsharing, a novel approach extending KV sharing across transformer layers to\nreduce memory usage beyond what was possible with Multi-Query Attention (MQA)\nand Grouped-Query Attention (GQA). Evaluations on various NLP benchmarks and\ninference metrics using uptrained Pythia-160M variants demonstrate that MLKV\nsignificantly reduces memory usage with minimal performance loss, reducing KV\ncache size down to a factor of 6x compared to MQA. These results highlight\nMLKV's potential for efficient deployment of transformer models at scale. We\nprovide code at https://github.com/zaydzuhri/pythia-mlkv"
                },
                "authors": [
                    {
                        "name": "Zayd Muhammad Kawakibi Zuhri"
                    },
                    {
                        "name": "Muhammad Farid Adilazuarda"
                    },
                    {
                        "name": "Ayu Purwarianti"
                    },
                    {
                        "name": "Alham Fikri Aji"
                    }
                ],
                "author_detail": {
                    "name": "Alham Fikri Aji"
                },
                "author": "Alham Fikri Aji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09297v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09297v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09827v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09827v2",
                "updated": "2024-10-15T06:09:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    6,
                    9,
                    35,
                    1,
                    289,
                    0
                ],
                "published": "2024-06-14T08:32:45Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    8,
                    32,
                    45,
                    4,
                    166,
                    0
                ],
                "title": "A Training-free Sub-quadratic Cost Transformer Model Serving Framework\n  With Hierarchically Pruned Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Training-free Sub-quadratic Cost Transformer Model Serving Framework\n  With Hierarchically Pruned Attention"
                },
                "summary": "In modern large language models (LLMs), increasing the context length is\ncrucial for improving comprehension and coherence in long-context, multi-modal,\nand retrieval-augmented language generation. While many recent transformer\nmodels attempt to extend their context length over a million tokens, they\nremain impractical due to the quadratic time and space complexities. Although\nrecent works on linear and sparse attention mechanisms can achieve this goal,\ntheir real-world applicability is often limited by the need to re-train from\nscratch and significantly worse performance. In response, we propose a novel\napproach, Hierarchically Pruned Attention (HiP), which reduces the time\ncomplexity of the attention mechanism to $O(T \\log T)$ and the space complexity\nto $O(T)$, where $T$ is the sequence length. We notice a pattern in the\nattention scores of pretrained LLMs where tokens close together tend to have\nsimilar scores, which we call ``attention locality''. Based on this\nobservation, we utilize a novel tree-search-like algorithm that estimates the\ntop-$k$ key tokens for a given query on the fly, which is mathematically\nguaranteed to have better performance than random attention pruning. In\naddition to improving the time complexity of the attention mechanism, we\nfurther optimize GPU memory usage by implementing KV cache offloading, which\nstores only $O(\\log T)$ tokens on the GPU while maintaining similar decoding\nthroughput. Experiments on benchmarks show that HiP, with its training-free\nnature, significantly reduces both prefill and decoding latencies, as well as\nmemory usage, while maintaining high-quality generation with minimal\ndegradation. HiP enables pretrained LLMs to scale up to millions of tokens on\ncommodity GPUs, potentially unlocking long-context LLM applications previously\ndeemed infeasible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern large language models (LLMs), increasing the context length is\ncrucial for improving comprehension and coherence in long-context, multi-modal,\nand retrieval-augmented language generation. While many recent transformer\nmodels attempt to extend their context length over a million tokens, they\nremain impractical due to the quadratic time and space complexities. Although\nrecent works on linear and sparse attention mechanisms can achieve this goal,\ntheir real-world applicability is often limited by the need to re-train from\nscratch and significantly worse performance. In response, we propose a novel\napproach, Hierarchically Pruned Attention (HiP), which reduces the time\ncomplexity of the attention mechanism to $O(T \\log T)$ and the space complexity\nto $O(T)$, where $T$ is the sequence length. We notice a pattern in the\nattention scores of pretrained LLMs where tokens close together tend to have\nsimilar scores, which we call ``attention locality''. Based on this\nobservation, we utilize a novel tree-search-like algorithm that estimates the\ntop-$k$ key tokens for a given query on the fly, which is mathematically\nguaranteed to have better performance than random attention pruning. In\naddition to improving the time complexity of the attention mechanism, we\nfurther optimize GPU memory usage by implementing KV cache offloading, which\nstores only $O(\\log T)$ tokens on the GPU while maintaining similar decoding\nthroughput. Experiments on benchmarks show that HiP, with its training-free\nnature, significantly reduces both prefill and decoding latencies, as well as\nmemory usage, while maintaining high-quality generation with minimal\ndegradation. HiP enables pretrained LLMs to scale up to millions of tokens on\ncommodity GPUs, potentially unlocking long-context LLM applications previously\ndeemed infeasible."
                },
                "authors": [
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Geon Park"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Jaduk Suh"
                    },
                    {
                        "name": "Jina Kim"
                    },
                    {
                        "name": "Wonyoung Jeong"
                    },
                    {
                        "name": "Bumsik Kim"
                    },
                    {
                        "name": "Hyemin Lee"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "arxiv_comment": "44 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09827v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09827v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11305v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11305v1",
                "updated": "2024-10-15T05:57:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    57,
                    51,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T05:57:51Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    57,
                    51,
                    1,
                    289,
                    0
                ],
                "title": "QSpec: Speculative Decoding with Complementary Quantization Schemes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QSpec: Speculative Decoding with Complementary Quantization Schemes"
                },
                "summary": "Quantization has been substantially adopted to accelerate inference and\nreduce memory consumption of large language models (LLMs). While\nactivation-weight joint quantization speeds up the inference process through\nlow-precision kernels, we demonstrate that it suffers severe performance\ndegradation on multi-step reasoning tasks, rendering it ineffective. We propose\na novel quantization paradigm called QSPEC, which seamlessly integrates two\ncomplementary quantization schemes for speculative decoding. Leveraging nearly\ncost-free execution switching, QSPEC drafts tokens with low-precision, fast\nactivation-weight quantization, and verifies them with high-precision\nweight-only quantization, effectively combining the strengths of both\nquantization schemes. Compared to high-precision quantization methods, QSPEC\nempirically boosts token generation throughput by up to 1.80x without any\nquality compromise, distinguishing it from other low-precision quantization\napproaches. This enhancement is also consistent across various serving tasks,\nmodel sizes, quantization methods, and batch sizes. Unlike existing speculative\ndecoding techniques, our approach reuses weights and the KV cache, avoiding\nadditional memory overhead. Furthermore, QSPEC offers a plug-and-play advantage\nwithout requiring any training. We believe that QSPEC demonstrates unique\nstrengths for future deployment of high-fidelity quantization schemes,\nparticularly in memory-constrained scenarios (e.g., edge devices).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization has been substantially adopted to accelerate inference and\nreduce memory consumption of large language models (LLMs). While\nactivation-weight joint quantization speeds up the inference process through\nlow-precision kernels, we demonstrate that it suffers severe performance\ndegradation on multi-step reasoning tasks, rendering it ineffective. We propose\na novel quantization paradigm called QSPEC, which seamlessly integrates two\ncomplementary quantization schemes for speculative decoding. Leveraging nearly\ncost-free execution switching, QSPEC drafts tokens with low-precision, fast\nactivation-weight quantization, and verifies them with high-precision\nweight-only quantization, effectively combining the strengths of both\nquantization schemes. Compared to high-precision quantization methods, QSPEC\nempirically boosts token generation throughput by up to 1.80x without any\nquality compromise, distinguishing it from other low-precision quantization\napproaches. This enhancement is also consistent across various serving tasks,\nmodel sizes, quantization methods, and batch sizes. Unlike existing speculative\ndecoding techniques, our approach reuses weights and the KV cache, avoiding\nadditional memory overhead. Furthermore, QSPEC offers a plug-and-play advantage\nwithout requiring any training. We believe that QSPEC demonstrates unique\nstrengths for future deployment of high-fidelity quantization schemes,\nparticularly in memory-constrained scenarios (e.g., edge devices)."
                },
                "authors": [
                    {
                        "name": "Juntao Zhao"
                    },
                    {
                        "name": "Wenhao Lu"
                    },
                    {
                        "name": "Sheng Wang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Chuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Wu"
                },
                "author": "Chuan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11305v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11305v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.00080v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.00080v3",
                "updated": "2024-10-15T05:34:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    34,
                    7,
                    1,
                    289,
                    0
                ],
                "published": "2024-04-30T16:35:08Z",
                "published_parsed": [
                    2024,
                    4,
                    30,
                    16,
                    35,
                    8,
                    1,
                    121,
                    0
                ],
                "title": "Recommenadation aided Caching using Combinatorial Multi-armed Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommenadation aided Caching using Combinatorial Multi-armed Bandits"
                },
                "summary": "We study content caching with recommendations in a wireless network where the\nusers are connected through a base station equipped with a finite-capacity\ncache. We assume a fixed set of contents with unknown user preferences and\ncontent popularities. The base station can cache a subset of the contents and\ncan also recommend subsets of the contents to different users in order to\nencourage them to request the recommended contents. Recommendations, depending\non their acceptability, can thus be used to increase cache hits. We first\nassume that the users' recommendation acceptabilities are known and formulate\nthe cache hit optimization problem as a combinatorial multi-armed bandit\n(CMAB). We propose a UCB-based algorithm to decide which contents to cache and\nrecommend and provide an upper bound on the regret of this algorithm.\nSubsequently, we consider a more general scenario where the users'\nrecommendation acceptabilities are also unknown and propose another UCB-based\nalgorithm that learns these as well. We numerically demonstrate the performance\nof our algorithms and compare these to state-of-the-art algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study content caching with recommendations in a wireless network where the\nusers are connected through a base station equipped with a finite-capacity\ncache. We assume a fixed set of contents with unknown user preferences and\ncontent popularities. The base station can cache a subset of the contents and\ncan also recommend subsets of the contents to different users in order to\nencourage them to request the recommended contents. Recommendations, depending\non their acceptability, can thus be used to increase cache hits. We first\nassume that the users' recommendation acceptabilities are known and formulate\nthe cache hit optimization problem as a combinatorial multi-armed bandit\n(CMAB). We propose a UCB-based algorithm to decide which contents to cache and\nrecommend and provide an upper bound on the regret of this algorithm.\nSubsequently, we consider a more general scenario where the users'\nrecommendation acceptabilities are also unknown and propose another UCB-based\nalgorithm that learns these as well. We numerically demonstrate the performance\nof our algorithms and compare these to state-of-the-art algorithms."
                },
                "authors": [
                    {
                        "name": "Pavamana K J"
                    },
                    {
                        "name": "Chandramani Kishore Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Kishore Singh"
                },
                "author": "Chandramani Kishore Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.00080v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.00080v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11260v1",
                "updated": "2024-10-15T04:35:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    15,
                    4,
                    35,
                    49,
                    1,
                    289,
                    0
                ],
                "published": "2024-10-15T04:35:49Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    4,
                    35,
                    49,
                    1,
                    289,
                    0
                ],
                "title": "A Zoned Storage Optimized Flash Cache on ZNS SSDs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Zoned Storage Optimized Flash Cache on ZNS SSDs"
                },
                "summary": "Zoned Namespace SSDs (ZNS) are introduced recently to mitigate the block\ninterface penalties of flash-based SSDs. It is a good opportunity for flash\ncache to address cache throughput and write amplification (WA) issues by fully\ncontrolling data allocation and garbage collection via zone-based interfaces.\nHowever, there are several critical challenges that need to be addressed\nincluding zone-interface compatibility, data management of large zone size, and\na better tradeoff between throughput, cache hit ratio, and WA.\n  In this paper, we present Z-CacheLib, a zoned storage optimized flash cache\non ZNS SSDs. In Z-CacheLib, we propose: 1) a new zStorage Engine for ZNS SSDs\nwith low mapping and operational overhead, and 2) a novel zCache Engine with\ncross-layer optimizations to resolve the throughput regression and WA issues of\ngarbage collection, which consists of delayed data eviction with virtual\nover-provisioning (vOP), a top-down eviction policy (zLRU) optimized from LRU,\nand a bottom-up drop mechanism (zDrop) for low WA. Our evaluation shows that\nZ-CacheLib can achieve up to 2X throughput, 5% improvement hit ratio, and\nalmost no WA compared to CacheLib with compatible regular SSDs, demonstrating\nbenefits of using ZNS SSDs for cache. Moreover, Z-CacheLib can achieve up to 6X\nthroughput and 92% WA reduction compared with F2FS-based scheme.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zoned Namespace SSDs (ZNS) are introduced recently to mitigate the block\ninterface penalties of flash-based SSDs. It is a good opportunity for flash\ncache to address cache throughput and write amplification (WA) issues by fully\ncontrolling data allocation and garbage collection via zone-based interfaces.\nHowever, there are several critical challenges that need to be addressed\nincluding zone-interface compatibility, data management of large zone size, and\na better tradeoff between throughput, cache hit ratio, and WA.\n  In this paper, we present Z-CacheLib, a zoned storage optimized flash cache\non ZNS SSDs. In Z-CacheLib, we propose: 1) a new zStorage Engine for ZNS SSDs\nwith low mapping and operational overhead, and 2) a novel zCache Engine with\ncross-layer optimizations to resolve the throughput regression and WA issues of\ngarbage collection, which consists of delayed data eviction with virtual\nover-provisioning (vOP), a top-down eviction policy (zLRU) optimized from LRU,\nand a bottom-up drop mechanism (zDrop) for low WA. Our evaluation shows that\nZ-CacheLib can achieve up to 2X throughput, 5% improvement hit ratio, and\nalmost no WA compared to CacheLib with compatible regular SSDs, demonstrating\nbenefits of using ZNS SSDs for cache. Moreover, Z-CacheLib can achieve up to 6X\nthroughput and 92% WA reduction compared with F2FS-based scheme."
                },
                "authors": [
                    {
                        "name": "Chongzhuo Yang"
                    },
                    {
                        "name": "Chang Guo"
                    },
                    {
                        "name": "Ming Zhao"
                    },
                    {
                        "name": "Zhichao Cao"
                    }
                ],
                "author_detail": {
                    "name": "Zhichao Cao"
                },
                "author": "Zhichao Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03058v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03058v4",
                "updated": "2024-10-14T19:12:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    19,
                    12,
                    48,
                    0,
                    288,
                    0
                ],
                "published": "2024-05-05T21:41:43Z",
                "published_parsed": [
                    2024,
                    5,
                    5,
                    21,
                    41,
                    43,
                    6,
                    126,
                    0
                ],
                "title": "Enhancing High-Level Synthesis with Automated Pragma Insertion and Code\n  Transformation Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing High-Level Synthesis with Automated Pragma Insertion and Code\n  Transformation Framework"
                },
                "summary": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results."
                },
                "authors": [
                    {
                        "name": "Stphane Pouget"
                    },
                    {
                        "name": "Louis-Nol Pouchet"
                    },
                    {
                        "name": "Jason Cong"
                    }
                ],
                "author_detail": {
                    "name": "Jason Cong"
                },
                "author": "Jason Cong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.03058v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03058v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10819v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10819v1",
                "updated": "2024-10-14T17:59:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    59,
                    58,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:59:58Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    59,
                    58,
                    0,
                    288,
                    0
                ],
                "title": "DuoAttention: Efficient Long-Context LLM Inference with Retrieval and\n  Streaming Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DuoAttention: Efficient Long-Context LLM Inference with Retrieval and\n  Streaming Heads"
                },
                "summary": "Deploying long-context large language models (LLMs) is essential but poses\nsignificant computational and memory challenges. Caching all Key and Value (KV)\nstates across all attention heads consumes substantial memory. Existing KV\ncache pruning methods either damage the long-context capabilities of LLMs or\noffer only limited efficiency improvements. In this paper, we identify that\nonly a fraction of attention heads, a.k.a, Retrieval Heads, are critical for\nprocessing long contexts and require full attention across all tokens. In\ncontrast, all other heads, which primarily focus on recent tokens and attention\nsinks--referred to as Streaming Heads--do not require full attention. Based on\nthis insight, we introduce DuoAttention, a framework that only applies a full\nKV cache to retrieval heads while using a light-weight, constant-length KV\ncache for streaming heads, which reduces both LLM's decoding and pre-filling\nmemory and latency without compromising its long-context abilities.\nDuoAttention uses a lightweight, optimization-based algorithm with synthetic\ndata to identify retrieval heads accurately. Our method significantly reduces\nlong-context inference memory by up to 2.55x for MHA and 1.67x for GQA models\nwhile speeding up decoding by up to 2.18x and 1.50x and accelerating\npre-filling by up to 1.73x and 1.63x for MHA and GQA models, respectively, with\nminimal accuracy loss compared to full attention. Notably, combined with\nquantization, DuoAttention enables Llama-3-8B decoding with 3.3 million context\nlength on a single A100 GPU. Code is provided in\nhttps://github.com/mit-han-lab/duo-attention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying long-context large language models (LLMs) is essential but poses\nsignificant computational and memory challenges. Caching all Key and Value (KV)\nstates across all attention heads consumes substantial memory. Existing KV\ncache pruning methods either damage the long-context capabilities of LLMs or\noffer only limited efficiency improvements. In this paper, we identify that\nonly a fraction of attention heads, a.k.a, Retrieval Heads, are critical for\nprocessing long contexts and require full attention across all tokens. In\ncontrast, all other heads, which primarily focus on recent tokens and attention\nsinks--referred to as Streaming Heads--do not require full attention. Based on\nthis insight, we introduce DuoAttention, a framework that only applies a full\nKV cache to retrieval heads while using a light-weight, constant-length KV\ncache for streaming heads, which reduces both LLM's decoding and pre-filling\nmemory and latency without compromising its long-context abilities.\nDuoAttention uses a lightweight, optimization-based algorithm with synthetic\ndata to identify retrieval heads accurately. Our method significantly reduces\nlong-context inference memory by up to 2.55x for MHA and 1.67x for GQA models\nwhile speeding up decoding by up to 2.18x and 1.50x and accelerating\npre-filling by up to 1.73x and 1.63x for MHA and GQA models, respectively, with\nminimal accuracy loss compared to full attention. Notably, combined with\nquantization, DuoAttention enables Llama-3-8B decoding with 3.3 million context\nlength on a single A100 GPU. Code is provided in\nhttps://github.com/mit-han-lab/duo-attention."
                },
                "authors": [
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Jingwei Zuo"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10819v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10819v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10781v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10781v1",
                "updated": "2024-10-14T17:50:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    50,
                    28,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T17:50:28Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    17,
                    50,
                    28,
                    0,
                    288,
                    0
                ],
                "title": "When Attention Sink Emerges in Language Models: An Empirical View",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Attention Sink Emerges in Language Models: An Empirical View"
                },
                "summary": "Language Models (LMs) assign significant attention to the first token, even\nif it is not semantically important, which is known as attention sink. This\nphenomenon has been widely adopted in applications such as streaming/long\ncontext generation, KV cache optimization, inference acceleration, model\nquantization, and others. Despite its widespread use, a deep understanding of\nattention sink in LMs is still lacking. In this work, we first demonstrate that\nattention sinks exist universally in LMs with various inputs, even in small\nmodels. Furthermore, attention sink is observed to emerge during the LM\npre-training, motivating us to investigate how optimization, data distribution,\nloss function, and model architecture in LM pre-training influence its\nemergence. We highlight that attention sink emerges after effective\noptimization on sufficient training data. The sink position is highly\ncorrelated with the loss function and data distribution. Most importantly, we\nfind that attention sink acts more like key biases, storing extra attention\nscores, which could be non-informative and not contribute to the value\ncomputation. We also observe that this phenomenon (at least partially) stems\nfrom tokens' inner dependence on attention scores as a result of softmax\nnormalization. After relaxing such dependence by replacing softmax attention\nwith other attention operations, such as sigmoid attention without\nnormalization, attention sinks do not emerge in LMs up to 1B parameters. The\ncode is available at https://github.com/sail-sg/Attention-Sink.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models (LMs) assign significant attention to the first token, even\nif it is not semantically important, which is known as attention sink. This\nphenomenon has been widely adopted in applications such as streaming/long\ncontext generation, KV cache optimization, inference acceleration, model\nquantization, and others. Despite its widespread use, a deep understanding of\nattention sink in LMs is still lacking. In this work, we first demonstrate that\nattention sinks exist universally in LMs with various inputs, even in small\nmodels. Furthermore, attention sink is observed to emerge during the LM\npre-training, motivating us to investigate how optimization, data distribution,\nloss function, and model architecture in LM pre-training influence its\nemergence. We highlight that attention sink emerges after effective\noptimization on sufficient training data. The sink position is highly\ncorrelated with the loss function and data distribution. Most importantly, we\nfind that attention sink acts more like key biases, storing extra attention\nscores, which could be non-informative and not contribute to the value\ncomputation. We also observe that this phenomenon (at least partially) stems\nfrom tokens' inner dependence on attention scores as a result of softmax\nnormalization. After relaxing such dependence by replacing softmax attention\nwith other attention operations, such as sigmoid attention without\nnormalization, attention sinks do not emerge in LMs up to 1B parameters. The\ncode is available at https://github.com/sail-sg/Attention-Sink."
                },
                "authors": [
                    {
                        "name": "Xiangming Gu"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Fengzhuo Zhang"
                    },
                    {
                        "name": "Cunxiao Du"
                    },
                    {
                        "name": "Ye Wang"
                    },
                    {
                        "name": "Min Lin"
                    }
                ],
                "author_detail": {
                    "name": "Min Lin"
                },
                "author": "Min Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10781v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10781v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10511v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10511v1",
                "updated": "2024-10-14T13:49:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    49,
                    6,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T13:49:06Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    13,
                    49,
                    6,
                    0,
                    288,
                    0
                ],
                "title": "Customize Your Visual Autoregressive Recipe with Set Autoregressive\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Customize Your Visual Autoregressive Recipe with Set Autoregressive\n  Modeling"
                },
                "summary": "We introduce a new paradigm for AutoRegressive (AR) image generation, termed\nSet AutoRegressive Modeling (SAR). SAR generalizes the conventional AR to the\nnext-set setting, i.e., splitting the sequence into arbitrary sets containing\nmultiple tokens, rather than outputting each token in a fixed raster order. To\naccommodate SAR, we develop a straightforward architecture termed Fully Masked\nTransformer. We reveal that existing AR variants correspond to specific design\nchoices of sequence order and output intervals within the SAR framework, with\nAR and Masked AR (MAR) as two extreme instances. Notably, SAR facilitates a\nseamless transition from AR to MAR, where intermediate states allow for\ntraining a causal model that benefits from both few-step inference and KV cache\nacceleration, thus leveraging the advantages of both AR and MAR. On the\nImageNet benchmark, we carefully explore the properties of SAR by analyzing the\nimpact of sequence order and output intervals on performance, as well as the\ngeneralization ability regarding inference order and steps. We further validate\nthe potential of SAR by training a 900M text-to-image model capable of\nsynthesizing photo-realistic images with any resolution. We hope our work may\ninspire more exploration and application of AR-based modeling across diverse\nmodalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a new paradigm for AutoRegressive (AR) image generation, termed\nSet AutoRegressive Modeling (SAR). SAR generalizes the conventional AR to the\nnext-set setting, i.e., splitting the sequence into arbitrary sets containing\nmultiple tokens, rather than outputting each token in a fixed raster order. To\naccommodate SAR, we develop a straightforward architecture termed Fully Masked\nTransformer. We reveal that existing AR variants correspond to specific design\nchoices of sequence order and output intervals within the SAR framework, with\nAR and Masked AR (MAR) as two extreme instances. Notably, SAR facilitates a\nseamless transition from AR to MAR, where intermediate states allow for\ntraining a causal model that benefits from both few-step inference and KV cache\nacceleration, thus leveraging the advantages of both AR and MAR. On the\nImageNet benchmark, we carefully explore the properties of SAR by analyzing the\nimpact of sequence order and output intervals on performance, as well as the\ngeneralization ability regarding inference order and steps. We further validate\nthe potential of SAR by training a 900M text-to-image model capable of\nsynthesizing photo-realistic images with any resolution. We hope our work may\ninspire more exploration and application of AR-based modeling across diverse\nmodalities."
                },
                "authors": [
                    {
                        "name": "Wenze Liu"
                    },
                    {
                        "name": "Le Zhuo"
                    },
                    {
                        "name": "Yi Xin"
                    },
                    {
                        "name": "Sheng Xia"
                    },
                    {
                        "name": "Peng Gao"
                    },
                    {
                        "name": "Xiangyu Yue"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Yue"
                },
                "author": "Xiangyu Yue",
                "arxiv_comment": "19 pages, 17 figures, 8 tables, github repo:\n  https://github.com/poppuppy/SAR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10511v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10511v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05317v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05317v2",
                "updated": "2024-10-14T09:35:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    9,
                    35,
                    35,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-05T03:47:06Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    3,
                    47,
                    6,
                    5,
                    279,
                    0
                ],
                "title": "Accelerating Diffusion Transformers with Token-wise Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformers with Token-wise Feature Caching"
                },
                "summary": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality."
                },
                "authors": [
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Siteng Huang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05317v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05317v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.13378v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.13378v2",
                "updated": "2024-10-14T07:58:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    7,
                    58,
                    39,
                    0,
                    288,
                    0
                ],
                "published": "2024-05-22T06:19:43Z",
                "published_parsed": [
                    2024,
                    5,
                    22,
                    6,
                    19,
                    43,
                    2,
                    143,
                    0
                ],
                "title": "FedCache 2.0: Federated Edge Learning with Knowledge Caching and Dataset\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedCache 2.0: Federated Edge Learning with Knowledge Caching and Dataset\n  Distillation"
                },
                "summary": "Federated Edge Learning (FEL) has emerged as a promising approach for\nenabling edge devices to collaboratively train machine learning models while\npreserving data privacy. Despite its advantages, practical FEL deployment faces\nsignificant challenges related to device constraints and device-server\ninteractions, necessitating heterogeneous, user-adaptive model training with\nlimited and uncertain communication. In this paper, we introduce FedCache 2.0,\na novel personalized FEL architecture that simultaneously addresses these\nchallenges. FedCache 2.0 incorporates the benefits of both dataset distillation\nand knowledge cache-driven federated learning by storing and organizing\ndistilled data as knowledge in the server-side knowledge cache. Moreover, a\ndevice-centric cache sampling strategy is introduced to tailor transferred\nknowledge for individual devices within controlled communication bandwidth.\nExtensive experiments on five datasets covering image recognition, audio\nunderstanding, and mobile sensor data mining tasks demonstrate that (1)\nFedCache 2.0 significantly outperforms state-of-the-art methods regardless of\nmodel structures, data distributions, and modalities. (2) FedCache 2.0 can\ntrain splendid personalized on-device models with at least $\\times$28.6\nimprovement in communication efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Edge Learning (FEL) has emerged as a promising approach for\nenabling edge devices to collaboratively train machine learning models while\npreserving data privacy. Despite its advantages, practical FEL deployment faces\nsignificant challenges related to device constraints and device-server\ninteractions, necessitating heterogeneous, user-adaptive model training with\nlimited and uncertain communication. In this paper, we introduce FedCache 2.0,\na novel personalized FEL architecture that simultaneously addresses these\nchallenges. FedCache 2.0 incorporates the benefits of both dataset distillation\nand knowledge cache-driven federated learning by storing and organizing\ndistilled data as knowledge in the server-side knowledge cache. Moreover, a\ndevice-centric cache sampling strategy is introduced to tailor transferred\nknowledge for individual devices within controlled communication bandwidth.\nExtensive experiments on five datasets covering image recognition, audio\nunderstanding, and mobile sensor data mining tasks demonstrate that (1)\nFedCache 2.0 significantly outperforms state-of-the-art methods regardless of\nmodel structures, data distributions, and modalities. (2) FedCache 2.0 can\ntrain splendid personalized on-device models with at least $\\times$28.6\nimprovement in communication efficiency."
                },
                "authors": [
                    {
                        "name": "Quyang Pan"
                    },
                    {
                        "name": "Sheng Sun"
                    },
                    {
                        "name": "Zhiyuan Wu"
                    },
                    {
                        "name": "Yuwei Wang"
                    },
                    {
                        "name": "Min Liu"
                    },
                    {
                        "name": "Bo Gao"
                    },
                    {
                        "name": "Jingyuan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jingyuan Wang"
                },
                "author": "Jingyuan Wang",
                "arxiv_comment": "17 pages, 7 figures, 14 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.13378v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.13378v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10157v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10157v1",
                "updated": "2024-10-14T04:49:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    4,
                    49,
                    22,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T04:49:22Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    4,
                    49,
                    22,
                    0,
                    288,
                    0
                ],
                "title": "Caching Content Placement and Beamforming Co-design for IRS-Aided MIMO\n  Systems with Imperfect CSI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching Content Placement and Beamforming Co-design for IRS-Aided MIMO\n  Systems with Imperfect CSI"
                },
                "summary": "When offloading links encounter deep fading and obstruction, edge caching\ncannot fully enhance wireless network performance and improve the QoS of edge\nnodes, as it fails to effectively reduce backhaul burden. The emerging\ntechnology of intelligent reflecting surfaces (IRS) compensates for this\ndisadvantage by creating a smart and reconfigurable wireless environment.\nSubsequently, we jointly design content placement and active/passive\nbeamforming to minimize network costs under imperfect channel state information\n(CSI) in the IRS-oriented edge caching system. This minimization problem is\ndecomposed into two subproblems. The content placement subproblem is addressed\nby applying KKT optimality conditions. We then develop the alternating\noptimization method to resolve precoder and reflection beamforming.\nSpecifically, we reduce transmission power by first fixing the phase shift,\nreducing the problem to a convex one relative to the precoder, which is solved\nthrough convex optimization. Next, we fix the precoder and resolve the\nresulting reflection beamforming problem using the penalty convex-concave\nprocedure (CCP) method. Results demonstrate that our proposed method\noutperforms uniform caching and random phase approaches in reducing\ntransmission power and saving network costs. Eventually, the proposed approach\noffers potential improvements in the caching optimization and transmission\nrobustness of wireless communication with imperfect CSI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When offloading links encounter deep fading and obstruction, edge caching\ncannot fully enhance wireless network performance and improve the QoS of edge\nnodes, as it fails to effectively reduce backhaul burden. The emerging\ntechnology of intelligent reflecting surfaces (IRS) compensates for this\ndisadvantage by creating a smart and reconfigurable wireless environment.\nSubsequently, we jointly design content placement and active/passive\nbeamforming to minimize network costs under imperfect channel state information\n(CSI) in the IRS-oriented edge caching system. This minimization problem is\ndecomposed into two subproblems. The content placement subproblem is addressed\nby applying KKT optimality conditions. We then develop the alternating\noptimization method to resolve precoder and reflection beamforming.\nSpecifically, we reduce transmission power by first fixing the phase shift,\nreducing the problem to a convex one relative to the precoder, which is solved\nthrough convex optimization. Next, we fix the precoder and resolve the\nresulting reflection beamforming problem using the penalty convex-concave\nprocedure (CCP) method. Results demonstrate that our proposed method\noutperforms uniform caching and random phase approaches in reducing\ntransmission power and saving network costs. Eventually, the proposed approach\noffers potential improvements in the caching optimization and transmission\nrobustness of wireless communication with imperfect CSI."
                },
                "authors": [
                    {
                        "name": "Meng Gao"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Huafu Li"
                    },
                    {
                        "name": "Junqi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Junqi Guo"
                },
                "author": "Junqi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10157v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10157v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10149v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10149v1",
                "updated": "2024-10-14T04:30:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    4,
                    30,
                    38,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T04:30:38Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    4,
                    30,
                    38,
                    0,
                    288,
                    0
                ],
                "title": "Fast and Accurate Neural Rendering Using Semi-Gradients",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and Accurate Neural Rendering Using Semi-Gradients"
                },
                "summary": "We propose a simple yet effective neural network-based framework for global\nillumination rendering. Recently, rendering techniques that learn neural\nradiance caches by minimizing the difference (i.e., residual) between the left\nand right sides of the rendering equation have been suggested. Due to their\nease of implementation and the advantage of excluding path integral\ncalculations, these techniques have been applied to various fields, such as\nfree-viewpoint rendering, differentiable rendering, and real-time rendering.\nHowever, issues of slow training and occasionally darkened renders have been\nnoted. We identify the cause of these issues as the bias and high variance\npresent in the gradient estimates of the existing residual-based objective\nfunction. To address this, we introduce a new objective function that maintains\nthe same global optimum as before but allows for unbiased and low-variance\ngradient estimates, enabling faster and more accurate training of neural\nnetworks. In conclusion, this method is simply implemented by ignoring the\npartial derivatives of the right-hand side, and theoretical and experimental\nanalyses demonstrate the effectiveness of the proposed loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a simple yet effective neural network-based framework for global\nillumination rendering. Recently, rendering techniques that learn neural\nradiance caches by minimizing the difference (i.e., residual) between the left\nand right sides of the rendering equation have been suggested. Due to their\nease of implementation and the advantage of excluding path integral\ncalculations, these techniques have been applied to various fields, such as\nfree-viewpoint rendering, differentiable rendering, and real-time rendering.\nHowever, issues of slow training and occasionally darkened renders have been\nnoted. We identify the cause of these issues as the bias and high variance\npresent in the gradient estimates of the existing residual-based objective\nfunction. To address this, we introduce a new objective function that maintains\nthe same global optimum as before but allows for unbiased and low-variance\ngradient estimates, enabling faster and more accurate training of neural\nnetworks. In conclusion, this method is simply implemented by ignoring the\npartial derivatives of the right-hand side, and theoretical and experimental\nanalyses demonstrate the effectiveness of the proposed loss."
                },
                "authors": [
                    {
                        "name": "In-Young Cho"
                    },
                    {
                        "name": "Jaewoong Cho"
                    }
                ],
                "author_detail": {
                    "name": "Jaewoong Cho"
                },
                "author": "Jaewoong Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10149v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10149v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10071v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10071v1",
                "updated": "2024-10-14T01:25:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    14,
                    1,
                    25,
                    56,
                    0,
                    288,
                    0
                ],
                "published": "2024-10-14T01:25:56Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    1,
                    25,
                    56,
                    0,
                    288,
                    0
                ],
                "title": "Content Caching-Assisted Vehicular Edge Computing Using Multi-Agent\n  Graph Attention Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content Caching-Assisted Vehicular Edge Computing Using Multi-Agent\n  Graph Attention Reinforcement Learning"
                },
                "summary": "In order to avoid repeated task offloading and realize the reuse of popular\ntask computing results, we construct a novel content caching-assisted vehicular\nedge computing (VEC) framework. In the face of irregular network topology and\nunknown environmental dynamics, we further propose a multi-agent graph\nattention reinforcement learning (MGARL) based edge caching scheme, which\nutilizes the graph attention convolution kernel to integrate the neighboring\nnodes' features of each agent and further enhance the cooperation among agents.\nOur simulation results show that our proposed scheme is capable of improving\nthe utilization of caching resources while reducing the long-term task\ncomputing latency compared to the baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In order to avoid repeated task offloading and realize the reuse of popular\ntask computing results, we construct a novel content caching-assisted vehicular\nedge computing (VEC) framework. In the face of irregular network topology and\nunknown environmental dynamics, we further propose a multi-agent graph\nattention reinforcement learning (MGARL) based edge caching scheme, which\nutilizes the graph attention convolution kernel to integrate the neighboring\nnodes' features of each agent and further enhance the cooperation among agents.\nOur simulation results show that our proposed scheme is capable of improving\nthe utilization of caching resources while reducing the long-term task\ncomputing latency compared to the baselines."
                },
                "authors": [
                    {
                        "name": "Jinjin Shen"
                    },
                    {
                        "name": "Yan Lin"
                    },
                    {
                        "name": "Yijin Zhang"
                    },
                    {
                        "name": "Weibin Zhang"
                    },
                    {
                        "name": "Feng Shu"
                    },
                    {
                        "name": "Jun Li"
                    }
                ],
                "author_detail": {
                    "name": "Jun Li"
                },
                "author": "Jun Li",
                "arxiv_comment": "6 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10071v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10071v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09533v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09533v1",
                "updated": "2024-10-12T13:45:26Z",
                "updated_parsed": [
                    2024,
                    10,
                    12,
                    13,
                    45,
                    26,
                    5,
                    286,
                    0
                ],
                "published": "2024-10-12T13:45:26Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    13,
                    45,
                    26,
                    5,
                    286,
                    0
                ],
                "title": "Leveraging Semantic Cues from Foundation Vision Models for Enhanced\n  Local Feature Correspondence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Semantic Cues from Foundation Vision Models for Enhanced\n  Local Feature Correspondence"
                },
                "summary": "Visual correspondence is a crucial step in key computer vision tasks,\nincluding camera localization, image registration, and structure from motion.\nThe most effective techniques for matching keypoints currently involve using\nlearned sparse or dense matchers, which need pairs of images. These neural\nnetworks have a good general understanding of features from both images, but\nthey often struggle to match points from different semantic areas. This paper\npresents a new method that uses semantic cues from foundation vision model\nfeatures (like DINOv2) to enhance local feature matching by incorporating\nsemantic reasoning into existing descriptors. Therefore, the learned\ndescriptors do not require image pairs at inference time, allowing feature\ncaching and fast matching using similarity search, unlike learned matchers. We\npresent adapted versions of six existing descriptors, with an average increase\nin performance of 29% in camera localization, with comparable accuracy to\nexisting matchers as LightGlue and LoFTR in two existing benchmarks. Both code\nand trained models are available at\nhttps://www.verlab.dcc.ufmg.br/descriptors/reasoning_accv24",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual correspondence is a crucial step in key computer vision tasks,\nincluding camera localization, image registration, and structure from motion.\nThe most effective techniques for matching keypoints currently involve using\nlearned sparse or dense matchers, which need pairs of images. These neural\nnetworks have a good general understanding of features from both images, but\nthey often struggle to match points from different semantic areas. This paper\npresents a new method that uses semantic cues from foundation vision model\nfeatures (like DINOv2) to enhance local feature matching by incorporating\nsemantic reasoning into existing descriptors. Therefore, the learned\ndescriptors do not require image pairs at inference time, allowing feature\ncaching and fast matching using similarity search, unlike learned matchers. We\npresent adapted versions of six existing descriptors, with an average increase\nin performance of 29% in camera localization, with comparable accuracy to\nexisting matchers as LightGlue and LoFTR in two existing benchmarks. Both code\nand trained models are available at\nhttps://www.verlab.dcc.ufmg.br/descriptors/reasoning_accv24"
                },
                "authors": [
                    {
                        "name": "Felipe Cadar"
                    },
                    {
                        "name": "Guilherme Potje"
                    },
                    {
                        "name": "Renato Martins"
                    },
                    {
                        "name": "Cdric Demonceaux"
                    },
                    {
                        "name": "Erickson R. Nascimento"
                    }
                ],
                "author_detail": {
                    "name": "Erickson R. Nascimento"
                },
                "author": "Erickson R. Nascimento",
                "arxiv_comment": "Accepted in ACCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09533v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09533v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09479v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09479v1",
                "updated": "2024-10-12T10:38:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    12,
                    10,
                    38,
                    39,
                    5,
                    286,
                    0
                ],
                "published": "2024-10-12T10:38:39Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    10,
                    38,
                    39,
                    5,
                    286,
                    0
                ],
                "title": "Viscoelastic Effects on the Hydrodynamics of an Active Compound Particle",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Viscoelastic Effects on the Hydrodynamics of an Active Compound Particle"
                },
                "summary": "Understanding the hydrodynamics of microswimmers in viscoelastic fluids and\nconfined environments is crucial for interpreting their behaviour in natural\nsettings and designing synthetic microswimmers for practical applications like\ncargo transport. In this study, we explore the hydrodynamics of a concentric\nactive compound particle - a model microswimmer (a squirmer) positioned at the\ncentre of a viscoelastic fluid droplet (a model cargo) suspended in another\nviscoelastic medium. We consider the Oldroyd-B constitutive model to\ncharacterize the fluids and employ a perturbative approach in the Deborah\nnumber to analyze viscoelastic effects analytically, assuming a small Capillary\nnumber so that the droplet remains spherical and does not deform. We examine\nthree cases: (i) a squirmer confined within a viscoelastic fluid droplet\nsuspended in a Newtonian fluid, (ii) a squirmer confined within a Newtonian\nfluid droplet suspended in a viscoelastic fluid, and (iii) a squirmer confined\nwithin a viscoelastic fluid droplet suspended in another viscoelastic fluid.\nOur findings reveal that the swimming speeds of the squirmer and the droplet\nare determined by the complex interplay of viscoelasticity, the size ratio of\nthe droplet to the squirmer (confinement strength), and the viscosity ratio of\nthe surrounding fluid to the droplet fluid. A critical aspect of this\ninteraction is the positioning of stagnation points within the fluid flow,\nwhich governs the distribution of polymeric stress. This distribution, in turn,\nplays a crucial role in determining the influence of viscoelasticity on the\nsquirmer's dynamics. Our analysis suggests that viscoelastic effects can either\nenhance or hinder the swimming speed of the squirmer when confined in a\ndroplet, depending on the specific configuration of the system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the hydrodynamics of microswimmers in viscoelastic fluids and\nconfined environments is crucial for interpreting their behaviour in natural\nsettings and designing synthetic microswimmers for practical applications like\ncargo transport. In this study, we explore the hydrodynamics of a concentric\nactive compound particle - a model microswimmer (a squirmer) positioned at the\ncentre of a viscoelastic fluid droplet (a model cargo) suspended in another\nviscoelastic medium. We consider the Oldroyd-B constitutive model to\ncharacterize the fluids and employ a perturbative approach in the Deborah\nnumber to analyze viscoelastic effects analytically, assuming a small Capillary\nnumber so that the droplet remains spherical and does not deform. We examine\nthree cases: (i) a squirmer confined within a viscoelastic fluid droplet\nsuspended in a Newtonian fluid, (ii) a squirmer confined within a Newtonian\nfluid droplet suspended in a viscoelastic fluid, and (iii) a squirmer confined\nwithin a viscoelastic fluid droplet suspended in another viscoelastic fluid.\nOur findings reveal that the swimming speeds of the squirmer and the droplet\nare determined by the complex interplay of viscoelasticity, the size ratio of\nthe droplet to the squirmer (confinement strength), and the viscosity ratio of\nthe surrounding fluid to the droplet fluid. A critical aspect of this\ninteraction is the positioning of stagnation points within the fluid flow,\nwhich governs the distribution of polymeric stress. This distribution, in turn,\nplays a crucial role in determining the influence of viscoelasticity on the\nsquirmer's dynamics. Our analysis suggests that viscoelastic effects can either\nenhance or hinder the swimming speed of the squirmer when confined in a\ndroplet, depending on the specific configuration of the system."
                },
                "authors": [
                    {
                        "name": "KVS Chaithanya"
                    },
                    {
                        "name": "Sumesh P. Thampi"
                    }
                ],
                "author_detail": {
                    "name": "Sumesh P. Thampi"
                },
                "author": "Sumesh P. Thampi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09479v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09479v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09397v1",
                "updated": "2024-10-12T07:01:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    12,
                    7,
                    1,
                    30,
                    5,
                    286,
                    0
                ],
                "published": "2024-10-12T07:01:30Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    7,
                    1,
                    30,
                    5,
                    286,
                    0
                ],
                "title": "Fine-grained Attention I/O Complexity: Comprehensive Analysis for\n  Backward Passes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-grained Attention I/O Complexity: Comprehensive Analysis for\n  Backward Passes"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nprocessing long-context information. However, the quadratic complexity of\nattention computation with respect to sequence length poses significant\ncomputational challenges, and I/O aware algorithms have been proposed. This\npaper presents a comprehensive analysis of the I/O complexity for attention\nmechanisms, focusing on backward passes by categorizing into small and large\ncache scenarios. Using the red-blue pebble game framework, we establish tight\nbounds on I/O complexity across all cache sizes. We confirm that the de facto\nstandard I/O aware algorithm FlashAttention is optimal for both forward and\nbackward passes for the large cache size scenario. For small cache sizes, we\nprovide an algorithm that improves over existing methods and achieves the tight\nbounds. Additionally, we extend our analysis to sparse attention, a mainstream\nspeeding-up approach, deriving fine-grained lower bounds for both forward and\nbackward passes and both small and large caches. Our findings complete the\ntheoretical foundation for I/O complexity in attention mechanisms, offering\ninsights for designing efficient algorithms of LLM training and inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nprocessing long-context information. However, the quadratic complexity of\nattention computation with respect to sequence length poses significant\ncomputational challenges, and I/O aware algorithms have been proposed. This\npaper presents a comprehensive analysis of the I/O complexity for attention\nmechanisms, focusing on backward passes by categorizing into small and large\ncache scenarios. Using the red-blue pebble game framework, we establish tight\nbounds on I/O complexity across all cache sizes. We confirm that the de facto\nstandard I/O aware algorithm FlashAttention is optimal for both forward and\nbackward passes for the large cache size scenario. For small cache sizes, we\nprovide an algorithm that improves over existing methods and achieves the tight\nbounds. Additionally, we extend our analysis to sparse attention, a mainstream\nspeeding-up approach, deriving fine-grained lower bounds for both forward and\nbackward passes and both small and large caches. Our findings complete the\ntheoretical foundation for I/O complexity in attention mechanisms, offering\ninsights for designing efficient algorithms of LLM training and inference."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Li"
                    },
                    {
                        "name": "Yingyu Liang"
                    },
                    {
                        "name": "Zhenmei Shi"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Yufa Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yufa Zhou"
                },
                "author": "Yufa Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14360v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14360v3",
                "updated": "2024-10-12T02:11:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    12,
                    2,
                    11,
                    14,
                    5,
                    286,
                    0
                ],
                "published": "2024-09-22T08:30:43Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    8,
                    30,
                    43,
                    6,
                    266,
                    0
                ],
                "title": "In-place Switch: Reprogramming based SLC Cache Design for Hybrid 3D SSDs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-place Switch: Reprogramming based SLC Cache Design for Hybrid 3D SSDs"
                },
                "summary": "Recently, 3D SSDs are widely adopted in PCs, data centers, and cloud storage\nsystems. To increase capacity, high bit-density cells, such as Triple-Level\nCell (TLC), are utilized within 3D SSDs. However, due to the inferior\nperformance of TLC, a portion of TLCs is configured to operate as Single-Level\nCell (SLC) to provide high performance, with host data initially directed to\nthe SLCs. In SLC/TLC hybrid 3D SSDs, a portion of the TLC space is designated\nas an SLC cache to achieve high SSD performance by writing host data at the SLC\nspeed. Given the limited size of the SLC cache, block reclamation is necessary\nto free up the SLC cache during idle periods. However, our preliminary studies\nindicate that the SLC cache can lead to a performance cliff if filled rapidly\nand cause significant write amplification when data migration occurs during\nidle times.\n  In this work, we propose leveraging a reprogram operation to address these\nchallenges. Specifically, when the SLC cache is full or during idle periods, a\nreprogram operation is performed to switch used SLC pages to TLC pages in place\n(termed In-place Switch, IPS). Subsequently, other free TLC space is allocated\nas the new SLC cache. IPS can continuously provide sufficient SLC cache within\nSSDs, significantly improving write performance and reducing write\namplification. Experimental results demonstrate that IPS can reduce write\nlatency and write amplification by up to 0.75 times and 0.53 times,\nrespectively, compared to state-of-the-art SLC cache technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, 3D SSDs are widely adopted in PCs, data centers, and cloud storage\nsystems. To increase capacity, high bit-density cells, such as Triple-Level\nCell (TLC), are utilized within 3D SSDs. However, due to the inferior\nperformance of TLC, a portion of TLCs is configured to operate as Single-Level\nCell (SLC) to provide high performance, with host data initially directed to\nthe SLCs. In SLC/TLC hybrid 3D SSDs, a portion of the TLC space is designated\nas an SLC cache to achieve high SSD performance by writing host data at the SLC\nspeed. Given the limited size of the SLC cache, block reclamation is necessary\nto free up the SLC cache during idle periods. However, our preliminary studies\nindicate that the SLC cache can lead to a performance cliff if filled rapidly\nand cause significant write amplification when data migration occurs during\nidle times.\n  In this work, we propose leveraging a reprogram operation to address these\nchallenges. Specifically, when the SLC cache is full or during idle periods, a\nreprogram operation is performed to switch used SLC pages to TLC pages in place\n(termed In-place Switch, IPS). Subsequently, other free TLC space is allocated\nas the new SLC cache. IPS can continuously provide sufficient SLC cache within\nSSDs, significantly improving write performance and reducing write\namplification. Experimental results demonstrate that IPS can reduce write\nlatency and write amplification by up to 0.75 times and 0.53 times,\nrespectively, compared to state-of-the-art SLC cache technologies."
                },
                "authors": [
                    {
                        "name": "Xufeng Yang"
                    },
                    {
                        "name": "Zhengjian Cong"
                    },
                    {
                        "name": "Congming Gao"
                    }
                ],
                "author_detail": {
                    "name": "Congming Gao"
                },
                "author": "Congming Gao",
                "arxiv_comment": "This paper has been submitted to NAS'24 (The 17th International\n  Conference on Networking, Architecture and Storage)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14360v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14360v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09237v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09237v1",
                "updated": "2024-10-11T20:23:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    20,
                    23,
                    0,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T20:23:00Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    20,
                    23,
                    0,
                    4,
                    285,
                    0
                ],
                "title": "Foundation Model-Powered 3D Few-Shot Class Incremental Learning via\n  Training-free Adaptor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation Model-Powered 3D Few-Shot Class Incremental Learning via\n  Training-free Adaptor"
                },
                "summary": "Recent advances in deep learning for processing point clouds hold increased\ninterest in Few-Shot Class Incremental Learning (FSCIL) for 3D computer vision.\nThis paper introduces a new method to tackle the Few-Shot Continual Incremental\nLearning (FSCIL) problem in 3D point cloud environments. We leverage a\nfoundational 3D model trained extensively on point cloud data. Drawing from\nrecent improvements in foundation models, known for their ability to work well\nacross different tasks, we propose a novel strategy that does not require\nadditional training to adapt to new tasks. Our approach uses a dual cache\nsystem: first, it uses previous test samples based on how confident the model\nwas in its predictions to prevent forgetting, and second, it includes a small\nnumber of new task samples to prevent overfitting. This dynamic adaptation\nensures strong performance across different learning tasks without needing lots\nof fine-tuning. We tested our approach on datasets like ModelNet, ShapeNet,\nScanObjectNN, and CO3D, showing that it outperforms other FSCIL methods and\ndemonstrating its effectiveness and versatility. The code is available at\n\\url{https://github.com/ahmadisahar/ACCV_FCIL3D}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in deep learning for processing point clouds hold increased\ninterest in Few-Shot Class Incremental Learning (FSCIL) for 3D computer vision.\nThis paper introduces a new method to tackle the Few-Shot Continual Incremental\nLearning (FSCIL) problem in 3D point cloud environments. We leverage a\nfoundational 3D model trained extensively on point cloud data. Drawing from\nrecent improvements in foundation models, known for their ability to work well\nacross different tasks, we propose a novel strategy that does not require\nadditional training to adapt to new tasks. Our approach uses a dual cache\nsystem: first, it uses previous test samples based on how confident the model\nwas in its predictions to prevent forgetting, and second, it includes a small\nnumber of new task samples to prevent overfitting. This dynamic adaptation\nensures strong performance across different learning tasks without needing lots\nof fine-tuning. We tested our approach on datasets like ModelNet, ShapeNet,\nScanObjectNN, and CO3D, showing that it outperforms other FSCIL methods and\ndemonstrating its effectiveness and versatility. The code is available at\n\\url{https://github.com/ahmadisahar/ACCV_FCIL3D}."
                },
                "authors": [
                    {
                        "name": "Sahar Ahmadi"
                    },
                    {
                        "name": "Ali Cheraghian"
                    },
                    {
                        "name": "Morteza Saberi"
                    },
                    {
                        "name": "Md. Towsif Abir"
                    },
                    {
                        "name": "Hamidreza Dastmalchi"
                    },
                    {
                        "name": "Farookh Hussain"
                    },
                    {
                        "name": "Shafin Rahman"
                    }
                ],
                "author_detail": {
                    "name": "Shafin Rahman"
                },
                "author": "Shafin Rahman",
                "arxiv_comment": "ACCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09237v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09237v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08895v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08895v1",
                "updated": "2024-10-11T15:12:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    15,
                    12,
                    30,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T15:12:30Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    15,
                    12,
                    30,
                    4,
                    285,
                    0
                ],
                "title": "Calibrated Cache Model for Few-Shot Vision-Language Model Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Calibrated Cache Model for Few-Shot Vision-Language Model Adaptation"
                },
                "summary": "Cache-based approaches stand out as both effective and efficient for adapting\nvision-language models (VLMs). Nonetheless, the existing cache model overlooks\nthree crucial aspects. 1) Pre-trained VLMs are mainly optimized for image-text\nsimilarity, neglecting the importance of image-image similarity, leading to a\ngap between pre-training and adaptation. 2) The current cache model is based on\nthe Nadaraya-Watson (N-W) estimator, which disregards the intricate\nrelationships among training samples while constructing weight function. 3)\nUnder the condition of limited samples, the logits generated by cache model are\nof high uncertainty, directly using these logits without accounting for the\nconfidence could be problematic. This work presents three calibration modules\naimed at addressing the above challenges. Similarity Calibration refines the\nimage-image similarity by using unlabeled images. We add a learnable projection\nlayer with residual connection on top of the pre-trained image encoder of CLIP\nand optimize the parameters by minimizing self-supervised contrastive loss.\nWeight Calibration introduces a precision matrix into the weight function to\nadequately model the relation between training samples, transforming the\nexisting cache model to a Gaussian Process (GP) regressor, which could be more\naccurate than N-W estimator. Confidence Calibration leverages the predictive\nvariances computed by GP Regression to dynamically re-scale the logits of cache\nmodel, ensuring that the cache model's outputs are appropriately adjusted based\non their confidence levels. Besides, to reduce the high complexity of GPs, we\nfurther propose a group-based learning strategy. Integrating the above designs,\nwe propose both training-free and training-required variants. Extensive\nexperiments on 11 few-shot classification datasets validate that the proposed\nmethods can achieve state-of-the-art performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-based approaches stand out as both effective and efficient for adapting\nvision-language models (VLMs). Nonetheless, the existing cache model overlooks\nthree crucial aspects. 1) Pre-trained VLMs are mainly optimized for image-text\nsimilarity, neglecting the importance of image-image similarity, leading to a\ngap between pre-training and adaptation. 2) The current cache model is based on\nthe Nadaraya-Watson (N-W) estimator, which disregards the intricate\nrelationships among training samples while constructing weight function. 3)\nUnder the condition of limited samples, the logits generated by cache model are\nof high uncertainty, directly using these logits without accounting for the\nconfidence could be problematic. This work presents three calibration modules\naimed at addressing the above challenges. Similarity Calibration refines the\nimage-image similarity by using unlabeled images. We add a learnable projection\nlayer with residual connection on top of the pre-trained image encoder of CLIP\nand optimize the parameters by minimizing self-supervised contrastive loss.\nWeight Calibration introduces a precision matrix into the weight function to\nadequately model the relation between training samples, transforming the\nexisting cache model to a Gaussian Process (GP) regressor, which could be more\naccurate than N-W estimator. Confidence Calibration leverages the predictive\nvariances computed by GP Regression to dynamically re-scale the logits of cache\nmodel, ensuring that the cache model's outputs are appropriately adjusted based\non their confidence levels. Besides, to reduce the high complexity of GPs, we\nfurther propose a group-based learning strategy. Integrating the above designs,\nwe propose both training-free and training-required variants. Extensive\nexperiments on 11 few-shot classification datasets validate that the proposed\nmethods can achieve state-of-the-art performance."
                },
                "authors": [
                    {
                        "name": "Kun Ding"
                    },
                    {
                        "name": "Qiang Yu"
                    },
                    {
                        "name": "Haojian Zhang"
                    },
                    {
                        "name": "Gaofeng Meng"
                    },
                    {
                        "name": "Shiming Xiang"
                    }
                ],
                "author_detail": {
                    "name": "Shiming Xiang"
                },
                "author": "Shiming Xiang",
                "arxiv_comment": "submitted to IJCV",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08895v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08895v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08760v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08760v1",
                "updated": "2024-10-11T12:19:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    12,
                    19,
                    18,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T12:19:18Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    12,
                    19,
                    18,
                    4,
                    285,
                    0
                ],
                "title": "Unlocking FedNL: Self-Contained Compute-Optimized Implementation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking FedNL: Self-Contained Compute-Optimized Implementation"
                },
                "summary": "Federated Learning (FL) is an emerging paradigm that enables intelligent\nagents to collaboratively train Machine Learning (ML) models in a distributed\nmanner, eliminating the need for sharing their local data. The recent work\n(arXiv:2106.02969) introduces a family of Federated Newton Learn (FedNL)\nalgorithms, marking a significant step towards applying second-order methods to\nFL and large-scale optimization. However, the reference FedNL prototype\nexhibits three serious practical drawbacks: (i) It requires 4.8 hours to launch\na single experiment in a sever-grade workstation; (ii) The prototype only\nsimulates multi-node setting; (iii) Prototype integration into\nresource-constrained applications is challenging. To bridge the gap between\ntheory and practice, we present a self-contained implementation of FedNL,\nFedNL-LS, FedNL-PP for single-node and multi-node settings. Our work resolves\nthe aforementioned issues and reduces the wall clock time by x1000. With this\nFedNL outperforms alternatives for training logistic regression in a\nsingle-node -- CVXPY (arXiv:1603.00943), and in a multi-node -- Apache Spark\n(arXiv:1505.06807), Ray/Scikit-Learn (arXiv:1712.05889). Finally, we propose\ntwo practical-orientated compressors for FedNL - adaptive TopLEK and\ncache-aware RandSeqK, which fulfill the theory of FedNL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) is an emerging paradigm that enables intelligent\nagents to collaboratively train Machine Learning (ML) models in a distributed\nmanner, eliminating the need for sharing their local data. The recent work\n(arXiv:2106.02969) introduces a family of Federated Newton Learn (FedNL)\nalgorithms, marking a significant step towards applying second-order methods to\nFL and large-scale optimization. However, the reference FedNL prototype\nexhibits three serious practical drawbacks: (i) It requires 4.8 hours to launch\na single experiment in a sever-grade workstation; (ii) The prototype only\nsimulates multi-node setting; (iii) Prototype integration into\nresource-constrained applications is challenging. To bridge the gap between\ntheory and practice, we present a self-contained implementation of FedNL,\nFedNL-LS, FedNL-PP for single-node and multi-node settings. Our work resolves\nthe aforementioned issues and reduces the wall clock time by x1000. With this\nFedNL outperforms alternatives for training logistic regression in a\nsingle-node -- CVXPY (arXiv:1603.00943), and in a multi-node -- Apache Spark\n(arXiv:1505.06807), Ray/Scikit-Learn (arXiv:1712.05889). Finally, we propose\ntwo practical-orientated compressors for FedNL - adaptive TopLEK and\ncache-aware RandSeqK, which fulfill the theory of FedNL."
                },
                "authors": [
                    {
                        "name": "Konstantin Burlachenko"
                    },
                    {
                        "name": "Peter Richtrik"
                    }
                ],
                "author_detail": {
                    "name": "Peter Richtrik"
                },
                "author": "Peter Richtrik",
                "arxiv_comment": "55 pages, 12 figures, 12 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08760v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08760v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.4; C.3; I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08618v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08618v1",
                "updated": "2024-10-11T08:33:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    8,
                    33,
                    58,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T08:33:58Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    8,
                    33,
                    58,
                    4,
                    285,
                    0
                ],
                "title": "AsyncFS: Metadata Updates Made Asynchronous for Distributed Filesystems\n  with In-Network Coordination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsyncFS: Metadata Updates Made Asynchronous for Distributed Filesystems\n  with In-Network Coordination"
                },
                "summary": "Distributed filesystems typically employ synchronous metadata updates, facing\ninherent challenges for access efficiency, load balancing, and directory\ncontention, especially under dynamic and skewed workloads. This paper argues\nthat synchronous updates are overly conservative for distributed filesystems.\nWe propose AsyncFS with asynchronous metadata updates, allowing operations to\nreturn early and defer directory updates until respective read to enable\nlatency hiding and conflict resolution. The key challenge is efficiently\nmaintaining the synchronous semantics of metadata updates. To address this,\nAsyncFS is co-designed with a programmable switch, leveraging the constrained\non-switch resources to holistically track directory states in the network with\nnegligible cost. This allows AsyncFS to timely aggregate and efficiently apply\ndelayed updates using batching and consolidation before directory reads.\nEvaluation shows that AsyncFS achieves up to 13.34$\\times$ and 3.85$\\times$\nhigher throughput, and 61.6% and 57.3% lower latency than two state-of-the-art\ndistributed filesystems, InfiniFS and CFS-KV, respectively, on skewed\nworkloads. For real-world workloads, AsyncFS improves end-to-end throughput by\n21.1$\\times$, 1.1$\\times$ and 30.1% over Ceph, IndexFS and CFS-KV,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed filesystems typically employ synchronous metadata updates, facing\ninherent challenges for access efficiency, load balancing, and directory\ncontention, especially under dynamic and skewed workloads. This paper argues\nthat synchronous updates are overly conservative for distributed filesystems.\nWe propose AsyncFS with asynchronous metadata updates, allowing operations to\nreturn early and defer directory updates until respective read to enable\nlatency hiding and conflict resolution. The key challenge is efficiently\nmaintaining the synchronous semantics of metadata updates. To address this,\nAsyncFS is co-designed with a programmable switch, leveraging the constrained\non-switch resources to holistically track directory states in the network with\nnegligible cost. This allows AsyncFS to timely aggregate and efficiently apply\ndelayed updates using batching and consolidation before directory reads.\nEvaluation shows that AsyncFS achieves up to 13.34$\\times$ and 3.85$\\times$\nhigher throughput, and 61.6% and 57.3% lower latency than two state-of-the-art\ndistributed filesystems, InfiniFS and CFS-KV, respectively, on skewed\nworkloads. For real-world workloads, AsyncFS improves end-to-end throughput by\n21.1$\\times$, 1.1$\\times$ and 30.1% over Ceph, IndexFS and CFS-KV,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Jingwei Xu"
                    },
                    {
                        "name": "Mingkai Dong"
                    },
                    {
                        "name": "Qiulin Tian"
                    },
                    {
                        "name": "Ziyi Tian"
                    },
                    {
                        "name": "Tong Xin"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08618v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08618v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08584v1",
                "updated": "2024-10-11T07:24:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    7,
                    24,
                    21,
                    4,
                    285,
                    0
                ],
                "published": "2024-10-11T07:24:21Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    7,
                    24,
                    21,
                    4,
                    285,
                    0
                ],
                "title": "ZipVL: Efficient Large Vision-Language Models with Dynamic Token\n  Sparsification and KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipVL: Efficient Large Vision-Language Models with Dynamic Token\n  Sparsification and KV Cache Compression"
                },
                "summary": "The efficiency of large vision-language models (LVLMs) is constrained by the\ncomputational bottleneck of the attention mechanism during the prefill phase\nand the memory bottleneck of fetching the key-value (KV) cache in the decoding\nphase, particularly in scenarios involving high-resolution images or videos.\nVisual content often exhibits substantial redundancy, resulting in highly\nsparse attention maps within LVLMs. This sparsity can be leveraged to\naccelerate attention computation or compress the KV cache through various\napproaches. However, most studies focus on addressing only one of these\nbottlenecks and do not adequately support dynamic adjustment of sparsity\nconcerning distinct layers or tasks. In this paper, we present ZipVL, an\nefficient inference framework designed for LVLMs that resolves both computation\nand memory bottlenecks through a dynamic ratio allocation strategy of important\ntokens. This ratio is adaptively determined based on the layer-specific\ndistribution of attention scores, rather than fixed hyper-parameters, thereby\nimproving efficiency for less complex tasks while maintaining high performance\nfor more challenging ones. Then we select important tokens based on their\nnormalized attention scores and perform attention mechanism solely on those\nimportant tokens to accelerate the prefill phase. To mitigate the memory\nbottleneck in the decoding phase, we employ mixed-precision quantization to the\nKV cache, where high-bit quantization is used for caches of important tokens,\nwhile low-bit quantization is applied to those of less importance. Our\nexperiments demonstrate that ZipVL can accelerate the prefill phase by\n2.6$\\times$ and reduce GPU memory usage by 50.0%, with a minimal accuracy\nreduction of only 0.2% on Video-MME benchmark over LongVA-7B model, effectively\nenhancing the generation efficiency of LVLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of large vision-language models (LVLMs) is constrained by the\ncomputational bottleneck of the attention mechanism during the prefill phase\nand the memory bottleneck of fetching the key-value (KV) cache in the decoding\nphase, particularly in scenarios involving high-resolution images or videos.\nVisual content often exhibits substantial redundancy, resulting in highly\nsparse attention maps within LVLMs. This sparsity can be leveraged to\naccelerate attention computation or compress the KV cache through various\napproaches. However, most studies focus on addressing only one of these\nbottlenecks and do not adequately support dynamic adjustment of sparsity\nconcerning distinct layers or tasks. In this paper, we present ZipVL, an\nefficient inference framework designed for LVLMs that resolves both computation\nand memory bottlenecks through a dynamic ratio allocation strategy of important\ntokens. This ratio is adaptively determined based on the layer-specific\ndistribution of attention scores, rather than fixed hyper-parameters, thereby\nimproving efficiency for less complex tasks while maintaining high performance\nfor more challenging ones. Then we select important tokens based on their\nnormalized attention scores and perform attention mechanism solely on those\nimportant tokens to accelerate the prefill phase. To mitigate the memory\nbottleneck in the decoding phase, we employ mixed-precision quantization to the\nKV cache, where high-bit quantization is used for caches of important tokens,\nwhile low-bit quantization is applied to those of less importance. Our\nexperiments demonstrate that ZipVL can accelerate the prefill phase by\n2.6$\\times$ and reduce GPU memory usage by 50.0%, with a minimal accuracy\nreduction of only 0.2% on Video-MME benchmark over LongVA-7B model, effectively\nenhancing the generation efficiency of LVLMs."
                },
                "authors": [
                    {
                        "name": "Yefei He"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Hong Zhou"
                    },
                    {
                        "name": "Kaipeng Zhang"
                    },
                    {
                        "name": "Bohan Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Bohan Zhuang"
                },
                "author": "Bohan Zhuang",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.03462v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.03462v3",
                "updated": "2024-10-11T02:18:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    11,
                    2,
                    18,
                    24,
                    4,
                    285,
                    0
                ],
                "published": "2024-01-07T11:57:40Z",
                "published_parsed": [
                    2024,
                    1,
                    7,
                    11,
                    57,
                    40,
                    6,
                    7,
                    0
                ],
                "title": "Long Context Compression with Activation Beacon",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long Context Compression with Activation Beacon"
                },
                "summary": "Long context compression is a critical research problem due to its\nsignificance in reducing the high computational and memory costs associated\nwith LLMs. In this paper, we propose Activation Beacon, a plug-in module for\ntransformer-based LLMs that targets effective, efficient, and flexible\ncompression of long contexts. To achieve this, our method introduces the\nfollowing technical designs. 1) We directly compress the activations (i.e. keys\nand values at every layer), rather than leveraging soft prompts to relay\ninformation (which constitute a major bottleneck to encapsulate the complex\ninformation within long contexts). 2) We tailor the compression workflow, where\neach fine-grained input unit is progressively compressed, enabling high-quality\ncompression and efficient computation during both training and inference. 3) We\ntrain the model through compression-based auto-regression, making full use of\nplain texts and instructional data to optimize the model's compression\nperformance. 4) During training, we randomly sample a compression ratio at each\nstep, teaching the model to support a wide range of compression configurations.\nExtensive evaluations are conducted on various long-context tasks whose lengths\n(e.g., 128K) may far exceed the maximum training length (20K), such as document\nunderstanding, few-shot learning, and Needle-in-a-Haystack. Whilst existing\nmethods struggle to handle these challenging tasks, Activation Beacon maintains\na comparable performance to the uncompressed baseline across various scenarios,\nachieving a 2x acceleration in inference time and an 8x reduction of memory\ncosts for KV cache. Our data, model, and code have been released at\n\\url{https://github.com/FlagOpen/FlagEmbedding/}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long context compression is a critical research problem due to its\nsignificance in reducing the high computational and memory costs associated\nwith LLMs. In this paper, we propose Activation Beacon, a plug-in module for\ntransformer-based LLMs that targets effective, efficient, and flexible\ncompression of long contexts. To achieve this, our method introduces the\nfollowing technical designs. 1) We directly compress the activations (i.e. keys\nand values at every layer), rather than leveraging soft prompts to relay\ninformation (which constitute a major bottleneck to encapsulate the complex\ninformation within long contexts). 2) We tailor the compression workflow, where\neach fine-grained input unit is progressively compressed, enabling high-quality\ncompression and efficient computation during both training and inference. 3) We\ntrain the model through compression-based auto-regression, making full use of\nplain texts and instructional data to optimize the model's compression\nperformance. 4) During training, we randomly sample a compression ratio at each\nstep, teaching the model to support a wide range of compression configurations.\nExtensive evaluations are conducted on various long-context tasks whose lengths\n(e.g., 128K) may far exceed the maximum training length (20K), such as document\nunderstanding, few-shot learning, and Needle-in-a-Haystack. Whilst existing\nmethods struggle to handle these challenging tasks, Activation Beacon maintains\na comparable performance to the uncompressed baseline across various scenarios,\nachieving a 2x acceleration in inference time and an 8x reduction of memory\ncosts for KV cache. Our data, model, and code have been released at\n\\url{https://github.com/FlagOpen/FlagEmbedding/}."
                },
                "authors": [
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Shitao Xiao"
                    },
                    {
                        "name": "Ninglu Shao"
                    },
                    {
                        "name": "Qiwei Ye"
                    },
                    {
                        "name": "Zhicheng Dou"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Dou"
                },
                "author": "Zhicheng Dou",
                "arxiv_comment": "Newer version of Activation Beacon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.03462v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.03462v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2410.22334v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22334v1",
                "updated": "2024-10-29T17:59:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    59,
                    59,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T17:59:59Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    59,
                    59,
                    1,
                    303,
                    0
                ],
                "title": "Absolute Dimensions of the Interferometric Binary HD 174881: A Test of\n  Stellar Evolution Models for Evolved Stars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Absolute Dimensions of the Interferometric Binary HD 174881: A Test of\n  Stellar Evolution Models for Evolved Stars"
                },
                "summary": "We report high-resolution spectroscopic monitoring and long-baseline\ninterferometric observations with the PTI of the 215-day binary system HD\n174881 (K1 II-III), composed of two giant stars. The system is spatially\nresolved with the PTI, as well as in archival measurements with the CHARA\nArray. Our analysis of these observations, along with an analysis of the\nspectral energy distribution, have allowed us to infer accurate values for the\nabsolute masses ($3.367^{+0.045}_{-0.041} M_{\\odot}$ and\n$3.476^{+0.043}_{-0.043} M_{\\odot}$), radii ($34.0 \\pm 1.3 R_{\\odot}$ and $22.7\n\\pm 1.8 R_{\\odot}$), effective temperatures ($4620 \\pm 100$ K and $4880 \\pm\n150$ K), and bolometric luminosities of both components, as well as other\nproperties including the orbital parallax (distance). These provide valuable\ntests of stellar evolution models for evolved stars, which are still relatively\nuncommon compared to the situation for main-sequence stars. We find generally\ngood agreement of all of these properties of HD 174881 with two sets of recent\nmodels (MIST, and PARSEC) at compositions near solar, for ages of 255-273 Myr.\nWe also find evidence of an infrared excess, based largely on the flux\nmeasurements from IRAS at 60 and 100 microns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report high-resolution spectroscopic monitoring and long-baseline\ninterferometric observations with the PTI of the 215-day binary system HD\n174881 (K1 II-III), composed of two giant stars. The system is spatially\nresolved with the PTI, as well as in archival measurements with the CHARA\nArray. Our analysis of these observations, along with an analysis of the\nspectral energy distribution, have allowed us to infer accurate values for the\nabsolute masses ($3.367^{+0.045}_{-0.041} M_{\\odot}$ and\n$3.476^{+0.043}_{-0.043} M_{\\odot}$), radii ($34.0 \\pm 1.3 R_{\\odot}$ and $22.7\n\\pm 1.8 R_{\\odot}$), effective temperatures ($4620 \\pm 100$ K and $4880 \\pm\n150$ K), and bolometric luminosities of both components, as well as other\nproperties including the orbital parallax (distance). These provide valuable\ntests of stellar evolution models for evolved stars, which are still relatively\nuncommon compared to the situation for main-sequence stars. We find generally\ngood agreement of all of these properties of HD 174881 with two sets of recent\nmodels (MIST, and PARSEC) at compositions near solar, for ages of 255-273 Myr.\nWe also find evidence of an infrared excess, based largely on the flux\nmeasurements from IRAS at 60 and 100 microns."
                },
                "authors": [
                    {
                        "name": "Guillermo Torres"
                    },
                    {
                        "name": "Andrew F. Boden"
                    },
                    {
                        "name": "John D. Monnier"
                    },
                    {
                        "name": "Gerard T. van Belle"
                    }
                ],
                "author_detail": {
                    "name": "Gerard T. van Belle"
                },
                "arxiv_affiliation": "Lowell Obs.",
                "author": "Gerard T. van Belle",
                "arxiv_comment": "14 pages in emulateapj format, including 12 figures and 8 tables.\n  Accepted for publication in The Astrophysical Journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22334v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22334v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22319v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22319v1",
                "updated": "2024-10-29T17:55:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    55,
                    17,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T17:55:17Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    55,
                    17,
                    1,
                    303,
                    0
                ],
                "title": "A wiggling filamentary jet at the origin of the blazar multi-wavelength\n  behaviour",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A wiggling filamentary jet at the origin of the blazar multi-wavelength\n  behaviour"
                },
                "summary": "Blazars are beamed active galactic nuclei known for their strong\nmulti-wavelength variability on timescales from years down to minutes. We aim\nto investigate the suitability of the twisting jet model presented in previous\nworks to explain the multi-wavelength behaviour of BL Lacertae, the prototype\nof one of the blazar classes. According to this model, the jet is\ninhomogeneous, curved, and twisting, and the long-term variability is due to\nchanges in the Doppler factor due to variations in the orientation of the\njet-emitting regions. We analysed optical data of the source obtained during\nmonitoring campaigns organised by the Whole Earth Blazar Telescope (WEBT) in\n2019-2022, together with radio data from the WEBT and other teams, and\ngamma-ray data from the Fermi satellite. In this period, BL Lacertae underwent\nan extraordinary activity phase, reaching its historical optical and gamma-ray\nbrightness maxima. The application of the twisting jet model to the source\nlight curves allows us to infer the wiggling motion of the optical, radio, and\ngamma-ray jet-emitting regions. The optical-radio correlation shows that the\nchanges in the radio viewing angle follow those in the optical viewing angle by\nabout 120 days, and it suggests that the jet is composed of plasma filaments,\nwhich is in agreement with some radio high-resolution observations of other\nsources. The gamma-ray emitting region is found to be co-spatial with the\noptical one, and the analysis of the gamma-optical correlation is consistent\nwith both the geometric interpretation and a synchrotron self-Compton (SSC)\norigin of the high-energy photons. We propose a geometric scenario where the\njet is made up of a pair of emitting plasma filaments in a sort of double-helix\ncurved rotating structure, whose wiggling motion produces changes in the\nDoppler beaming and can thus explain the observed multi-wavelength long-term\nvariability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Blazars are beamed active galactic nuclei known for their strong\nmulti-wavelength variability on timescales from years down to minutes. We aim\nto investigate the suitability of the twisting jet model presented in previous\nworks to explain the multi-wavelength behaviour of BL Lacertae, the prototype\nof one of the blazar classes. According to this model, the jet is\ninhomogeneous, curved, and twisting, and the long-term variability is due to\nchanges in the Doppler factor due to variations in the orientation of the\njet-emitting regions. We analysed optical data of the source obtained during\nmonitoring campaigns organised by the Whole Earth Blazar Telescope (WEBT) in\n2019-2022, together with radio data from the WEBT and other teams, and\ngamma-ray data from the Fermi satellite. In this period, BL Lacertae underwent\nan extraordinary activity phase, reaching its historical optical and gamma-ray\nbrightness maxima. The application of the twisting jet model to the source\nlight curves allows us to infer the wiggling motion of the optical, radio, and\ngamma-ray jet-emitting regions. The optical-radio correlation shows that the\nchanges in the radio viewing angle follow those in the optical viewing angle by\nabout 120 days, and it suggests that the jet is composed of plasma filaments,\nwhich is in agreement with some radio high-resolution observations of other\nsources. The gamma-ray emitting region is found to be co-spatial with the\noptical one, and the analysis of the gamma-optical correlation is consistent\nwith both the geometric interpretation and a synchrotron self-Compton (SSC)\norigin of the high-energy photons. We propose a geometric scenario where the\njet is made up of a pair of emitting plasma filaments in a sort of double-helix\ncurved rotating structure, whose wiggling motion produces changes in the\nDoppler beaming and can thus explain the observed multi-wavelength long-term\nvariability."
                },
                "authors": [
                    {
                        "name": "C. M. Raiteri"
                    },
                    {
                        "name": "M. Villata"
                    },
                    {
                        "name": "M. I. Carnerero"
                    },
                    {
                        "name": "S. O. Kurtanidze"
                    },
                    {
                        "name": "D. O. Mirzaqulov"
                    },
                    {
                        "name": "E. Bentez"
                    },
                    {
                        "name": "G. Bonnoli"
                    },
                    {
                        "name": "D. Carosati"
                    },
                    {
                        "name": "J. A. Acosta-Pulido"
                    },
                    {
                        "name": "I. Agudo"
                    },
                    {
                        "name": "T. S. Andreeva"
                    },
                    {
                        "name": "G. Apolonio"
                    },
                    {
                        "name": "R. Bachev"
                    },
                    {
                        "name": "G. A. Borman"
                    },
                    {
                        "name": "V. Bozhilov"
                    },
                    {
                        "name": "L. F. Brown"
                    },
                    {
                        "name": "W. Carbonell"
                    },
                    {
                        "name": "C. Casadio"
                    },
                    {
                        "name": "W. P. Chen"
                    },
                    {
                        "name": "G. Damljanovic"
                    },
                    {
                        "name": "S. A. Ehgamberdiev"
                    },
                    {
                        "name": "D. Elsaesser"
                    },
                    {
                        "name": "J. Escudero"
                    },
                    {
                        "name": "M. Feige"
                    },
                    {
                        "name": "A. Fuentes"
                    },
                    {
                        "name": "D. Gabellini"
                    },
                    {
                        "name": "K. Gazeas"
                    },
                    {
                        "name": "M. Giroletti"
                    },
                    {
                        "name": "T. S. Grishina"
                    },
                    {
                        "name": "A. C. Gupta"
                    },
                    {
                        "name": "M. A. Gurwell"
                    },
                    {
                        "name": "V. A. Hagen-Thorn"
                    },
                    {
                        "name": "G. M. Hamed"
                    },
                    {
                        "name": "D. Hiriart"
                    },
                    {
                        "name": "M. Hodges"
                    },
                    {
                        "name": "R. Z. ivanidze"
                    },
                    {
                        "name": "D. V. Ivanov"
                    },
                    {
                        "name": "M. D. Joner"
                    },
                    {
                        "name": "S. G. Jorstad"
                    },
                    {
                        "name": "M. D. Jovanovic"
                    },
                    {
                        "name": "S. Kiehlmann"
                    },
                    {
                        "name": "G. N. Kimeridze"
                    },
                    {
                        "name": "E. N. Kopatskaya"
                    },
                    {
                        "name": "Yu. A. Kovalev"
                    },
                    {
                        "name": "Y. Y. Kovalev"
                    },
                    {
                        "name": "O. M. Kurtanidze"
                    },
                    {
                        "name": "A. Kurtenkov"
                    },
                    {
                        "name": "E. G. Larionova"
                    },
                    {
                        "name": "A. Lessing"
                    },
                    {
                        "name": "H. C. Lin"
                    },
                    {
                        "name": "J. M. Lpez"
                    },
                    {
                        "name": "C. Lorey"
                    },
                    {
                        "name": "J. Ludwig"
                    },
                    {
                        "name": "N. Marchili"
                    },
                    {
                        "name": "A. Marchini"
                    },
                    {
                        "name": "A. P. Marscher"
                    },
                    {
                        "name": "K. Matsumoto"
                    },
                    {
                        "name": "W. Max-Moerbeck"
                    },
                    {
                        "name": "B. Mihov"
                    },
                    {
                        "name": "M. Minev"
                    },
                    {
                        "name": "M. G. Mingaliev"
                    },
                    {
                        "name": "A. Modaressi"
                    },
                    {
                        "name": "D. A. Morozova"
                    },
                    {
                        "name": "F. Mortari"
                    },
                    {
                        "name": "T. V. Mufakharov"
                    },
                    {
                        "name": "I. Myserlis"
                    },
                    {
                        "name": "M. G. Nikolashvili"
                    },
                    {
                        "name": "T. J. Pearson"
                    },
                    {
                        "name": "A. V. Popkov"
                    },
                    {
                        "name": "I. A. Rahimov"
                    },
                    {
                        "name": "A. C. S. Readhead"
                    },
                    {
                        "name": "D. Reinhart"
                    },
                    {
                        "name": "R. Reeves"
                    },
                    {
                        "name": "S. Righini"
                    },
                    {
                        "name": "F. D. Romanov"
                    },
                    {
                        "name": "S. S. Savchenko"
                    },
                    {
                        "name": "E. Semkov"
                    },
                    {
                        "name": "E. V. Shishkina"
                    },
                    {
                        "name": "L. A. Sigua"
                    },
                    {
                        "name": "L. Slavcheva-Mihova"
                    },
                    {
                        "name": "Yu. V. Sotnikova"
                    },
                    {
                        "name": "R. Steineke"
                    },
                    {
                        "name": "M. Stojanovic"
                    },
                    {
                        "name": "A. Strigachev"
                    },
                    {
                        "name": "A. Takey"
                    },
                    {
                        "name": "E. Traianou"
                    },
                    {
                        "name": "Yu. V. Troitskaya"
                    },
                    {
                        "name": "I. S. Troitskiy"
                    },
                    {
                        "name": "A. L. Tsai"
                    },
                    {
                        "name": "A. Valcheva"
                    },
                    {
                        "name": "A. A. Vasilyev"
                    },
                    {
                        "name": "G. Verna"
                    },
                    {
                        "name": "O. Vince"
                    },
                    {
                        "name": "K. Vrontaki"
                    },
                    {
                        "name": "Z. R. Weaver"
                    },
                    {
                        "name": "J. Webb"
                    },
                    {
                        "name": "Q. X. Yuldoshev"
                    },
                    {
                        "name": "E. Zaharieva"
                    },
                    {
                        "name": "A. V. Zhovtan"
                    }
                ],
                "author_detail": {
                    "name": "A. V. Zhovtan"
                },
                "author": "A. V. Zhovtan",
                "arxiv_comment": "In press for A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22319v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22319v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22318v1",
                "updated": "2024-10-29T17:55:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    55,
                    14,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T17:55:14Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    55,
                    14,
                    1,
                    303,
                    0
                ],
                "title": "Online Detecting LLM-Generated Texts via Sequential Hypothesis Testing\n  by Betting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Detecting LLM-Generated Texts via Sequential Hypothesis Testing\n  by Betting"
                },
                "summary": "Developing algorithms to differentiate between machine-generated texts and\nhuman-written texts has garnered substantial attention in recent years.\nExisting methods in this direction typically concern an offline setting where a\ndataset containing a mix of real and machine-generated texts is given upfront,\nand the task is to determine whether each sample in the dataset is from a large\nlanguage model (LLM) or a human. However, in many practical scenarios, sources\nsuch as news websites, social media accounts, or on other forums publish\ncontent in a streaming fashion. Therefore, in this online scenario, how to\nquickly and accurately determine whether the source is an LLM with strong\nstatistical guarantees is crucial for these media or platforms to function\neffectively and prevent the spread of misinformation and other potential misuse\nof LLMs. To tackle the problem of online detection, we develop an algorithm\nbased on the techniques of sequential hypothesis testing by betting that not\nonly builds upon and complements existing offline detection techniques but also\nenjoys statistical guarantees, which include a controlled false positive rate\nand the expected time to correctly identify a source as an LLM. Experiments\nwere conducted to demonstrate the effectiveness of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing algorithms to differentiate between machine-generated texts and\nhuman-written texts has garnered substantial attention in recent years.\nExisting methods in this direction typically concern an offline setting where a\ndataset containing a mix of real and machine-generated texts is given upfront,\nand the task is to determine whether each sample in the dataset is from a large\nlanguage model (LLM) or a human. However, in many practical scenarios, sources\nsuch as news websites, social media accounts, or on other forums publish\ncontent in a streaming fashion. Therefore, in this online scenario, how to\nquickly and accurately determine whether the source is an LLM with strong\nstatistical guarantees is crucial for these media or platforms to function\neffectively and prevent the spread of misinformation and other potential misuse\nof LLMs. To tackle the problem of online detection, we develop an algorithm\nbased on the techniques of sequential hypothesis testing by betting that not\nonly builds upon and complements existing offline detection techniques but also\nenjoys statistical guarantees, which include a controlled false positive rate\nand the expected time to correctly identify a source as an LLM. Experiments\nwere conducted to demonstrate the effectiveness of our method."
                },
                "authors": [
                    {
                        "name": "Can Chen"
                    },
                    {
                        "name": "Jun-Kun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun-Kun Wang"
                },
                "author": "Jun-Kun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22316v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22316v1",
                "updated": "2024-10-29T17:55:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    55,
                    0,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T17:55:00Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    55,
                    0,
                    1,
                    303,
                    0
                ],
                "title": "Understanding Synthetic Context Extension via Retrieval Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Synthetic Context Extension via Retrieval Heads"
                },
                "summary": "Long-context LLMs are increasingly in demand for applications such as\nretrieval-augmented generation. To defray the cost of pretraining LLMs over\nlong contexts, recent work takes an approach of synthetic context extension:\nfine-tuning LLMs with synthetically generated long-context data in a\npost-training stage. However, it remains unclear how and why this synthetic\ncontext extension imparts abilities for downstream long-context tasks. In this\npaper, we investigate fine-tuning on synthetic data for three long-context\ntasks that require retrieval and reasoning. We vary the realism of \"needle\"\nconcepts to be retrieved and diversity of the surrounding \"haystack\" context,\nfrom using LLMs to construct synthetic documents to using templated relations\nand creating symbolic datasets. We find that models trained on synthetic data\nfall short of the real data, but surprisingly, the mismatch can be interpreted\nand even predicted in terms of a special set of attention heads that are\nresponsible for retrieval over long context: retrieval heads (Wu et al., 2024).\nThe retrieval heads learned on synthetic data are mostly subsets of the\nretrieval heads learned on real data, and there is a strong correlation between\nthe recall of heads learned and the downstream performance of a model.\nFurthermore, with attention knockout and activation patching, we\nmechanistically show that retrieval heads are necessary and explain model\nperformance, although they are not totally sufficient. Our results shed light\non how to interpret synthetic data fine-tuning performance and how to approach\ncreating better data for learning real-world capabilities over long contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context LLMs are increasingly in demand for applications such as\nretrieval-augmented generation. To defray the cost of pretraining LLMs over\nlong contexts, recent work takes an approach of synthetic context extension:\nfine-tuning LLMs with synthetically generated long-context data in a\npost-training stage. However, it remains unclear how and why this synthetic\ncontext extension imparts abilities for downstream long-context tasks. In this\npaper, we investigate fine-tuning on synthetic data for three long-context\ntasks that require retrieval and reasoning. We vary the realism of \"needle\"\nconcepts to be retrieved and diversity of the surrounding \"haystack\" context,\nfrom using LLMs to construct synthetic documents to using templated relations\nand creating symbolic datasets. We find that models trained on synthetic data\nfall short of the real data, but surprisingly, the mismatch can be interpreted\nand even predicted in terms of a special set of attention heads that are\nresponsible for retrieval over long context: retrieval heads (Wu et al., 2024).\nThe retrieval heads learned on synthetic data are mostly subsets of the\nretrieval heads learned on real data, and there is a strong correlation between\nthe recall of heads learned and the downstream performance of a model.\nFurthermore, with attention knockout and activation patching, we\nmechanistically show that retrieval heads are necessary and explain model\nperformance, although they are not totally sufficient. Our results shed light\non how to interpret synthetic data fine-tuning performance and how to approach\ncreating better data for learning real-world capabilities over long contexts."
                },
                "authors": [
                    {
                        "name": "Xinyu Zhao"
                    },
                    {
                        "name": "Fangcong Yin"
                    },
                    {
                        "name": "Greg Durrett"
                    }
                ],
                "author_detail": {
                    "name": "Greg Durrett"
                },
                "author": "Greg Durrett",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22316v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22316v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22315v1",
                "updated": "2024-10-29T17:54:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    54,
                    17,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T17:54:17Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    54,
                    17,
                    1,
                    303,
                    0
                ],
                "title": "Natural Language Inference Improves Compositionality in Vision-Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language Inference Improves Compositionality in Vision-Language\n  Models"
                },
                "summary": "Compositional reasoning in Vision-Language Models (VLMs) remains challenging\nas these models often struggle to relate objects, attributes, and spatial\nrelationships. Recent methods aim to address these limitations by relying on\nthe semantics of the textual description, using Large Language Models (LLMs) to\nbreak them down into subsets of questions and answers. However, these methods\nprimarily operate on the surface level, failing to incorporate deeper lexical\nunderstanding while introducing incorrect assumptions generated by the LLM. In\nresponse to these issues, we present Caption Expansion with Contradictions and\nEntailments (CECE), a principled approach that leverages Natural Language\nInference (NLI) to generate entailments and contradictions from a given\npremise. CECE produces lexically diverse sentences while maintaining their core\nmeaning. Through extensive experiments, we show that CECE enhances\ninterpretability and reduces overreliance on biased or superficial features. By\nbalancing CECE along the original premise, we achieve significant improvements\nover previous methods without requiring additional fine-tuning, producing\nstate-of-the-art results on benchmarks that score agreement with human\njudgments for image-text alignment, and achieving an increase in performance on\nWinoground of +19.2% (group score) and +12.9% on EqBen (group score) over the\nbest prior work (finetuned with targeted data).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compositional reasoning in Vision-Language Models (VLMs) remains challenging\nas these models often struggle to relate objects, attributes, and spatial\nrelationships. Recent methods aim to address these limitations by relying on\nthe semantics of the textual description, using Large Language Models (LLMs) to\nbreak them down into subsets of questions and answers. However, these methods\nprimarily operate on the surface level, failing to incorporate deeper lexical\nunderstanding while introducing incorrect assumptions generated by the LLM. In\nresponse to these issues, we present Caption Expansion with Contradictions and\nEntailments (CECE), a principled approach that leverages Natural Language\nInference (NLI) to generate entailments and contradictions from a given\npremise. CECE produces lexically diverse sentences while maintaining their core\nmeaning. Through extensive experiments, we show that CECE enhances\ninterpretability and reduces overreliance on biased or superficial features. By\nbalancing CECE along the original premise, we achieve significant improvements\nover previous methods without requiring additional fine-tuning, producing\nstate-of-the-art results on benchmarks that score agreement with human\njudgments for image-text alignment, and achieving an increase in performance on\nWinoground of +19.2% (group score) and +12.9% on EqBen (group score) over the\nbest prior work (finetuned with targeted data)."
                },
                "authors": [
                    {
                        "name": "Paola Cascante-Bonilla"
                    },
                    {
                        "name": "Yu Hou"
                    },
                    {
                        "name": "Yang Trista Cao"
                    },
                    {
                        "name": "Hal Daum III"
                    },
                    {
                        "name": "Rachel Rudinger"
                    }
                ],
                "author_detail": {
                    "name": "Rachel Rudinger"
                },
                "author": "Rachel Rudinger",
                "arxiv_comment": "Project page: https://cece-vlm.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22309v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22309v1",
                "updated": "2024-10-29T17:53:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    53,
                    10,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T17:53:10Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    53,
                    10,
                    1,
                    303,
                    0
                ],
                "title": "GPT-4o reads the mind in the eyes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT-4o reads the mind in the eyes"
                },
                "summary": "Large Language Models (LLMs) are capable of reproducing human-like\ninferences, including inferences about emotions and mental states, from text.\nWhether this capability extends beyond text to other modalities remains\nunclear. Humans possess a sophisticated ability to read the mind in the eyes of\nother people. Here we tested whether this ability is also present in GPT-4o, a\nmultimodal LLM. Using two versions of a widely used theory of mind test, the\nReading the Mind in Eyes Test and the Multiracial Reading the Mind in the Eyes\nTest, we found that GPT-4o outperformed humans in interpreting mental states\nfrom upright faces but underperformed humans when faces were inverted. While\nhumans in our sample showed no difference between White and Non-white faces,\nGPT-4o's accuracy was higher for White than for Non-white faces. GPT-4o's\nerrors were not random but revealed a highly consistent, yet incorrect,\nprocessing of mental-state information across trials, with an\norientation-dependent error structure that qualitatively differed from that of\nhumans for inverted faces but not for upright faces. These findings highlight\nhow advanced mental state inference abilities and human-like face processing\nsignatures, such as inversion effects, coexist in GPT-4o alongside substantial\ndifferences in information processing compared to humans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are capable of reproducing human-like\ninferences, including inferences about emotions and mental states, from text.\nWhether this capability extends beyond text to other modalities remains\nunclear. Humans possess a sophisticated ability to read the mind in the eyes of\nother people. Here we tested whether this ability is also present in GPT-4o, a\nmultimodal LLM. Using two versions of a widely used theory of mind test, the\nReading the Mind in Eyes Test and the Multiracial Reading the Mind in the Eyes\nTest, we found that GPT-4o outperformed humans in interpreting mental states\nfrom upright faces but underperformed humans when faces were inverted. While\nhumans in our sample showed no difference between White and Non-white faces,\nGPT-4o's accuracy was higher for White than for Non-white faces. GPT-4o's\nerrors were not random but revealed a highly consistent, yet incorrect,\nprocessing of mental-state information across trials, with an\norientation-dependent error structure that qualitatively differed from that of\nhumans for inverted faces but not for upright faces. These findings highlight\nhow advanced mental state inference abilities and human-like face processing\nsignatures, such as inversion effects, coexist in GPT-4o alongside substantial\ndifferences in information processing compared to humans."
                },
                "authors": [
                    {
                        "name": "James W. A. Strachan"
                    },
                    {
                        "name": "Oriana Pansardi"
                    },
                    {
                        "name": "Eugenio Scaliti"
                    },
                    {
                        "name": "Marco Celotto"
                    },
                    {
                        "name": "Krati Saxena"
                    },
                    {
                        "name": "Chunzhi Yi"
                    },
                    {
                        "name": "Fabio Manzi"
                    },
                    {
                        "name": "Alessandro Rufo"
                    },
                    {
                        "name": "Guido Manzi"
                    },
                    {
                        "name": "Michael S. A. Graziano"
                    },
                    {
                        "name": "Stefano Panzeri"
                    },
                    {
                        "name": "Cristina Becchio"
                    }
                ],
                "author_detail": {
                    "name": "Cristina Becchio"
                },
                "author": "Cristina Becchio",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22309v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22309v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22307v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22307v1",
                "updated": "2024-10-29T17:52:45Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    52,
                    45,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T17:52:45Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    52,
                    45,
                    1,
                    303,
                    0
                ],
                "title": "SVIP: Towards Verifiable Inference of Open-source Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SVIP: Towards Verifiable Inference of Open-source Large Language Models"
                },
                "summary": "Open-source Large Language Models (LLMs) have recently demonstrated\nremarkable capabilities in natural language understanding and generation,\nleading to widespread adoption across various domains. However, their\nincreasing model sizes render local deployment impractical for individual\nusers, pushing many to rely on computing service providers for inference\nthrough a blackbox API. This reliance introduces a new risk: a computing\nprovider may stealthily substitute the requested LLM with a smaller, less\ncapable model without consent from users, thereby delivering inferior outputs\nwhile benefiting from cost savings. In this paper, we formalize the problem of\nverifiable inference for LLMs. Existing verifiable computing solutions based on\ncryptographic or game-theoretic techniques are either computationally\nuneconomical or rest on strong assumptions. We introduce SVIP, a secret-based\nverifiable LLM inference protocol that leverages intermediate outputs from LLM\nas unique model identifiers. By training a proxy task on these outputs and\nrequiring the computing provider to return both the generated text and the\nprocessed intermediate outputs, users can reliably verify whether the computing\nprovider is acting honestly. In addition, the integration of a secret mechanism\nfurther enhances the security of our protocol. We thoroughly analyze our\nprotocol under multiple strong and adaptive adversarial scenarios. Our\nextensive experiments demonstrate that SVIP is accurate, generalizable,\ncomputationally efficient, and resistant to various attacks. Notably, SVIP\nachieves false negative rates below 5% and false positive rates below 3%, while\nrequiring less than 0.01 seconds per query for verification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-source Large Language Models (LLMs) have recently demonstrated\nremarkable capabilities in natural language understanding and generation,\nleading to widespread adoption across various domains. However, their\nincreasing model sizes render local deployment impractical for individual\nusers, pushing many to rely on computing service providers for inference\nthrough a blackbox API. This reliance introduces a new risk: a computing\nprovider may stealthily substitute the requested LLM with a smaller, less\ncapable model without consent from users, thereby delivering inferior outputs\nwhile benefiting from cost savings. In this paper, we formalize the problem of\nverifiable inference for LLMs. Existing verifiable computing solutions based on\ncryptographic or game-theoretic techniques are either computationally\nuneconomical or rest on strong assumptions. We introduce SVIP, a secret-based\nverifiable LLM inference protocol that leverages intermediate outputs from LLM\nas unique model identifiers. By training a proxy task on these outputs and\nrequiring the computing provider to return both the generated text and the\nprocessed intermediate outputs, users can reliably verify whether the computing\nprovider is acting honestly. In addition, the integration of a secret mechanism\nfurther enhances the security of our protocol. We thoroughly analyze our\nprotocol under multiple strong and adaptive adversarial scenarios. Our\nextensive experiments demonstrate that SVIP is accurate, generalizable,\ncomputationally efficient, and resistant to various attacks. Notably, SVIP\nachieves false negative rates below 5% and false positive rates below 3%, while\nrequiring less than 0.01 seconds per query for verification."
                },
                "authors": [
                    {
                        "name": "Yifan Sun"
                    },
                    {
                        "name": "Yuhang Li"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Yuchen Jin"
                    },
                    {
                        "name": "Huan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Zhang"
                },
                "author": "Huan Zhang",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22307v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22307v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01801v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01801v2",
                "updated": "2024-10-29T17:50:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    50,
                    34,
                    1,
                    303,
                    0
                ],
                "published": "2024-06-03T21:42:06Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    21,
                    42,
                    6,
                    0,
                    155,
                    0
                ],
                "title": "Fearless Stochasticity in Expectation Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fearless Stochasticity in Expectation Propagation"
                },
                "summary": "Expectation propagation (EP) is a family of algorithms for performing\napproximate inference in probabilistic models. The updates of EP involve the\nevaluation of moments -- expectations of certain functions -- which can be\nestimated from Monte Carlo (MC) samples. However, the updates are not robust to\nMC noise when performed naively, and various prior works have attempted to\naddress this issue in different ways. In this work, we provide a novel\nperspective on the moment-matching updates of EP; namely, that they perform\nnatural-gradient-based optimisation of a variational objective. We use this\ninsight to motivate two new EP variants, with updates that are particularly\nwell-suited to MC estimation. They remain stable and are most sample-efficient\nwhen estimated with just a single sample. These new variants combine the\nbenefits of their predecessors and address key weaknesses. In particular, they\nare easier to tune, offer an improved speed-accuracy trade-off, and do not rely\non the use of debiasing estimators. We demonstrate their efficacy on a variety\nof probabilistic inference tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expectation propagation (EP) is a family of algorithms for performing\napproximate inference in probabilistic models. The updates of EP involve the\nevaluation of moments -- expectations of certain functions -- which can be\nestimated from Monte Carlo (MC) samples. However, the updates are not robust to\nMC noise when performed naively, and various prior works have attempted to\naddress this issue in different ways. In this work, we provide a novel\nperspective on the moment-matching updates of EP; namely, that they perform\nnatural-gradient-based optimisation of a variational objective. We use this\ninsight to motivate two new EP variants, with updates that are particularly\nwell-suited to MC estimation. They remain stable and are most sample-efficient\nwhen estimated with just a single sample. These new variants combine the\nbenefits of their predecessors and address key weaknesses. In particular, they\nare easier to tune, offer an improved speed-accuracy trade-off, and do not rely\non the use of debiasing estimators. We demonstrate their efficacy on a variety\nof probabilistic inference tasks."
                },
                "authors": [
                    {
                        "name": "Jonathan So"
                    },
                    {
                        "name": "Richard E. Turner"
                    }
                ],
                "author_detail": {
                    "name": "Richard E. Turner"
                },
                "author": "Richard E. Turner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01801v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01801v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22304v1",
                "updated": "2024-10-29T17:50:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    50,
                    31,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T17:50:31Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    50,
                    31,
                    1,
                    303,
                    0
                ],
                "title": "Flow-DPO: Improving LLM Mathematical Reasoning through Online\n  Multi-Agent Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flow-DPO: Improving LLM Mathematical Reasoning through Online\n  Multi-Agent Learning"
                },
                "summary": "Mathematical reasoning is a crucial capability for Large Language Models\n(LLMs), yet generating detailed and accurate reasoning traces remains a\nsignificant challenge. This paper introduces a novel approach to produce\nhigh-quality reasoning traces for LLM fine-tuning using online learning\n\\textbf{Flows}. Our method employs an incremental output production Flow, where\ncomponent LLMs collaboratively construct solutions through iterative\ncommunication. We train the Flow using online Direct Preference Optimization\n(DPO) learning with rollouts, generating DPO pairs for each training example\nand updating models in real-time. We directly compare the quality of reasoning\ntraces generated by our method with those produced through direct model\ninference, demonstrating the effectiveness of our approach in improving LLM\nperformance in mathematical reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mathematical reasoning is a crucial capability for Large Language Models\n(LLMs), yet generating detailed and accurate reasoning traces remains a\nsignificant challenge. This paper introduces a novel approach to produce\nhigh-quality reasoning traces for LLM fine-tuning using online learning\n\\textbf{Flows}. Our method employs an incremental output production Flow, where\ncomponent LLMs collaboratively construct solutions through iterative\ncommunication. We train the Flow using online Direct Preference Optimization\n(DPO) learning with rollouts, generating DPO pairs for each training example\nand updating models in real-time. We directly compare the quality of reasoning\ntraces generated by our method with those produced through direct model\ninference, demonstrating the effectiveness of our approach in improving LLM\nperformance in mathematical reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Yihe Deng"
                    },
                    {
                        "name": "Paul Mineiro"
                    }
                ],
                "author_detail": {
                    "name": "Paul Mineiro"
                },
                "author": "Paul Mineiro",
                "arxiv_comment": "5 pages, 4 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22302v1",
                "updated": "2024-10-29T17:48:47Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    48,
                    47,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T17:48:47Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    48,
                    47,
                    1,
                    303,
                    0
                ],
                "title": "Foreground signals minimally affect inference of high-mass binary black\n  holes in next generation gravitational-wave detectors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foreground signals minimally affect inference of high-mass binary black\n  holes in next generation gravitational-wave detectors"
                },
                "summary": "Next-generation gravitational-wave observatories are expected to detect over\na thousand compact binary coalescence signals daily, with some lasting from\nminutes to hours. Consequently, multiple signals will overlap in the\ntime-frequency plane, generating a \"foreground noise\" that predominantly\naffects the low-frequency range, where binary neutron star inspiral evolution\nis gradual. This study investigates the impact of such foreground noise on\nparameter estimation for short-duration binary black hole signals, particularly\nthose with high detector-frame masses and/or located at large redshifts. Our\nresults show a reduction in detection sensitivity by approximately 25\\% when\nthe noise power spectrum deviates by up to 50\\% from Gaussian noise due to\nforeground contamination. Despite this, using standard parameter estimation\ntechniques without subtracting overlapping signals, we find that foreground\nnoise has minimal impact, primarily affecting precision. These findings suggest\nthat even in the presence of substantial foreground noise, global-fit\ntechniques, and/or signal subtraction will not be necessary, as accurate\nrecovery of system parameters is achievable with minimal loss in precision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next-generation gravitational-wave observatories are expected to detect over\na thousand compact binary coalescence signals daily, with some lasting from\nminutes to hours. Consequently, multiple signals will overlap in the\ntime-frequency plane, generating a \"foreground noise\" that predominantly\naffects the low-frequency range, where binary neutron star inspiral evolution\nis gradual. This study investigates the impact of such foreground noise on\nparameter estimation for short-duration binary black hole signals, particularly\nthose with high detector-frame masses and/or located at large redshifts. Our\nresults show a reduction in detection sensitivity by approximately 25\\% when\nthe noise power spectrum deviates by up to 50\\% from Gaussian noise due to\nforeground contamination. Despite this, using standard parameter estimation\ntechniques without subtracting overlapping signals, we find that foreground\nnoise has minimal impact, primarily affecting precision. These findings suggest\nthat even in the presence of substantial foreground noise, global-fit\ntechniques, and/or signal subtraction will not be necessary, as accurate\nrecovery of system parameters is achievable with minimal loss in precision."
                },
                "authors": [
                    {
                        "name": "Ish Gupta"
                    },
                    {
                        "name": "Koustav Chandra"
                    },
                    {
                        "name": "B. S. Sathyaprakash"
                    }
                ],
                "author_detail": {
                    "name": "B. S. Sathyaprakash"
                },
                "author": "B. S. Sathyaprakash",
                "arxiv_comment": "12 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22296v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22296v1",
                "updated": "2024-10-29T17:45:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    45,
                    57,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T17:45:57Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    45,
                    57,
                    1,
                    303,
                    0
                ],
                "title": "LLMs are Highly-Constrained Biophysical Sequence Optimizers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are Highly-Constrained Biophysical Sequence Optimizers"
                },
                "summary": "Large language models (LLMs) have recently shown significant potential in\nvarious biological tasks such as protein engineering and molecule design. These\ntasks typically involve black-box discrete sequence optimization, where the\nchallenge lies in generating sequences that are not only biologically feasible\nbut also adhere to hard fine-grained constraints. However, LLMs often struggle\nwith such constraints, especially in biological contexts where verifying\ncandidate solutions is costly and time-consuming. In this study, we explore the\npossibility of employing LLMs as highly-constrained bilevel optimizers through\na methodology we refer to as Language Model Optimization with Margin\nExpectation (LLOME). This approach combines both offline and online\noptimization, utilizing limited oracle evaluations to iteratively enhance the\nsequences generated by the LLM. We additionally propose a novel training\nobjective -- Margin-Aligned Expectation (MargE) -- that trains the LLM to\nsmoothly interpolate between the reward and reference distributions. Lastly, we\nintroduce a synthetic test suite that bears strong geometric similarity to real\nbiophysical problems and enables rapid evaluation of LLM optimizers without\ntime-consuming lab validation. Our findings reveal that, in comparison to\ngenetic algorithm baselines, LLMs achieve significantly lower regret solutions\nwhile requiring fewer test function evaluations. However, we also observe that\nLLMs exhibit moderate miscalibration, are susceptible to generator collapse,\nand have difficulty finding the optimal solution when no explicit ground truth\nrewards are available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have recently shown significant potential in\nvarious biological tasks such as protein engineering and molecule design. These\ntasks typically involve black-box discrete sequence optimization, where the\nchallenge lies in generating sequences that are not only biologically feasible\nbut also adhere to hard fine-grained constraints. However, LLMs often struggle\nwith such constraints, especially in biological contexts where verifying\ncandidate solutions is costly and time-consuming. In this study, we explore the\npossibility of employing LLMs as highly-constrained bilevel optimizers through\na methodology we refer to as Language Model Optimization with Margin\nExpectation (LLOME). This approach combines both offline and online\noptimization, utilizing limited oracle evaluations to iteratively enhance the\nsequences generated by the LLM. We additionally propose a novel training\nobjective -- Margin-Aligned Expectation (MargE) -- that trains the LLM to\nsmoothly interpolate between the reward and reference distributions. Lastly, we\nintroduce a synthetic test suite that bears strong geometric similarity to real\nbiophysical problems and enables rapid evaluation of LLM optimizers without\ntime-consuming lab validation. Our findings reveal that, in comparison to\ngenetic algorithm baselines, LLMs achieve significantly lower regret solutions\nwhile requiring fewer test function evaluations. However, we also observe that\nLLMs exhibit moderate miscalibration, are susceptible to generator collapse,\nand have difficulty finding the optimal solution when no explicit ground truth\nrewards are available."
                },
                "authors": [
                    {
                        "name": "Angelica Chen"
                    },
                    {
                        "name": "Samuel D. Stanton"
                    },
                    {
                        "name": "Robert G. Alberstein"
                    },
                    {
                        "name": "Andrew M. Watkins"
                    },
                    {
                        "name": "Richard Bonneau"
                    },
                    {
                        "name": "Vladimir Gligorijevi"
                    },
                    {
                        "name": "Kyunghyun Cho"
                    },
                    {
                        "name": "Nathan C. Frey"
                    }
                ],
                "author_detail": {
                    "name": "Nathan C. Frey"
                },
                "author": "Nathan C. Frey",
                "arxiv_comment": "Supercedes arXiv:2407.00236v1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22296v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22296v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19487v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19487v2",
                "updated": "2024-10-29T17:44:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    44,
                    3,
                    1,
                    303,
                    0
                ],
                "published": "2024-05-29T20:05:46Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    20,
                    5,
                    46,
                    2,
                    150,
                    0
                ],
                "title": "A Full-duplex Speech Dialogue Scheme Based On Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Full-duplex Speech Dialogue Scheme Based On Large Language Models"
                },
                "summary": "We present a generative dialogue system capable of operating in a full-duplex\nmanner, allowing for seamless interaction. It is based on a large language\nmodel (LLM) carefully aligned to be aware of a perception module, a motor\nfunction module, and the concept of a simple finite state machine (called\nneural FSM) with two states. The perception and motor function modules operate\nin tandem, allowing the system to speak and listen to the user simultaneously.\nThe LLM generates textual tokens for inquiry responses and makes autonomous\ndecisions to start responding to, wait for, or interrupt the user by emitting\ncontrol tokens to the neural FSM. All these tasks of the LLM are carried out as\nnext token prediction on a serialized view of the dialogue in real-time. In\nautomatic quality evaluations simulating real-life interaction, the proposed\nsystem reduces the average conversation response latency by more than threefold\ncompared with LLM-based half-duplex dialogue systems while responding within\nless than 500 milliseconds in more than 50% of evaluated interactions. Running\nan LLM with only 8 billion parameters, our system exhibits an 8% higher\ninterruption precision rate than the best available commercial LLM for\nvoice-based dialogue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a generative dialogue system capable of operating in a full-duplex\nmanner, allowing for seamless interaction. It is based on a large language\nmodel (LLM) carefully aligned to be aware of a perception module, a motor\nfunction module, and the concept of a simple finite state machine (called\nneural FSM) with two states. The perception and motor function modules operate\nin tandem, allowing the system to speak and listen to the user simultaneously.\nThe LLM generates textual tokens for inquiry responses and makes autonomous\ndecisions to start responding to, wait for, or interrupt the user by emitting\ncontrol tokens to the neural FSM. All these tasks of the LLM are carried out as\nnext token prediction on a serialized view of the dialogue in real-time. In\nautomatic quality evaluations simulating real-life interaction, the proposed\nsystem reduces the average conversation response latency by more than threefold\ncompared with LLM-based half-duplex dialogue systems while responding within\nless than 500 milliseconds in more than 50% of evaluated interactions. Running\nan LLM with only 8 billion parameters, our system exhibits an 8% higher\ninterruption precision rate than the best available commercial LLM for\nvoice-based dialogue."
                },
                "authors": [
                    {
                        "name": "Peng Wang"
                    },
                    {
                        "name": "Songshuo Lu"
                    },
                    {
                        "name": "Yaohua Tang"
                    },
                    {
                        "name": "Sijie Yan"
                    },
                    {
                        "name": "Wei Xia"
                    },
                    {
                        "name": "Yuanjun Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Yuanjun Xiong"
                },
                "author": "Yuanjun Xiong",
                "arxiv_comment": "Accepted to NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.19487v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19487v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22293v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22293v1",
                "updated": "2024-10-29T17:43:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    43,
                    6,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T17:43:06Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    43,
                    6,
                    1,
                    303,
                    0
                ],
                "title": "Fine-Tuning LLMs for Code Mutation: A New Era of Cyber Threats",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Tuning LLMs for Code Mutation: A New Era of Cyber Threats"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have significantly\nimproved their capabilities in natural language processing and code synthesis,\nenabling more complex applications across different fields. This paper explores\nthe application of LLMs in the context of code mutation, a process where the\nstructure of program code is altered without changing its functionality.\nTraditionally, code mutation has been employed to increase software robustness\nin mission-critical applications. Additionally, mutation engines have been\nexploited by malware developers to evade the signature-based detection methods\nemployed by malware detection systems. Existing code mutation engines, often\nused by such threat actors, typically result in only limited variations in the\nmalware, which can still be identified through static code analysis. However,\nthe agility demonstrated by an LLM-based code synthesizer could significantly\nchange this threat landscape by allowing for more complex code mutations that\nare not easily detected using static analysis. One can increase variations of\ncodes synthesized by a pre-trained LLM through fine-tuning and retraining. This\nprocess is what we refer to as code mutation training. In this paper, we\npropose a novel definition of code mutation training tailored for pre-trained\nLLM-based code synthesizers and demonstrate this training on a lightweight\npre-trained model. Our approach involves restructuring (i.e., mutating) code at\nthe subroutine level, which allows for more manageable mutations while\nmaintaining the semantic integrity verified through unit testing. Our\nexperimental results illustrate the effectiveness of our approach in improving\ncode mutation capabilities of LLM-based program synthesizers in producing\nvaried and functionally correct code solutions, showcasing their potential to\ntransform the landscape of code mutation and the threats associated with it.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have significantly\nimproved their capabilities in natural language processing and code synthesis,\nenabling more complex applications across different fields. This paper explores\nthe application of LLMs in the context of code mutation, a process where the\nstructure of program code is altered without changing its functionality.\nTraditionally, code mutation has been employed to increase software robustness\nin mission-critical applications. Additionally, mutation engines have been\nexploited by malware developers to evade the signature-based detection methods\nemployed by malware detection systems. Existing code mutation engines, often\nused by such threat actors, typically result in only limited variations in the\nmalware, which can still be identified through static code analysis. However,\nthe agility demonstrated by an LLM-based code synthesizer could significantly\nchange this threat landscape by allowing for more complex code mutations that\nare not easily detected using static analysis. One can increase variations of\ncodes synthesized by a pre-trained LLM through fine-tuning and retraining. This\nprocess is what we refer to as code mutation training. In this paper, we\npropose a novel definition of code mutation training tailored for pre-trained\nLLM-based code synthesizers and demonstrate this training on a lightweight\npre-trained model. Our approach involves restructuring (i.e., mutating) code at\nthe subroutine level, which allows for more manageable mutations while\nmaintaining the semantic integrity verified through unit testing. Our\nexperimental results illustrate the effectiveness of our approach in improving\ncode mutation capabilities of LLM-based program synthesizers in producing\nvaried and functionally correct code solutions, showcasing their potential to\ntransform the landscape of code mutation and the threats associated with it."
                },
                "authors": [
                    {
                        "name": "Mohammad Setak"
                    },
                    {
                        "name": "Pooria Madani"
                    }
                ],
                "author_detail": {
                    "name": "Pooria Madani"
                },
                "author": "Pooria Madani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22293v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22293v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22292v1",
                "updated": "2024-10-29T17:42:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    42,
                    56,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T17:42:56Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    42,
                    56,
                    1,
                    303,
                    0
                ],
                "title": "Batch, match, and patch: low-rank approximations for score-based\n  variational inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch, match, and patch: low-rank approximations for score-based\n  variational inference"
                },
                "summary": "Black-box variational inference (BBVI) scales poorly to high dimensional\nproblems when it is used to estimate a multivariate Gaussian approximation with\na full covariance matrix. In this paper, we extend the batch-and-match (BaM)\nframework for score-based BBVI to problems where it is prohibitively expensive\nto store such covariance matrices, let alone to estimate them. Unlike classical\nalgorithms for BBVI, which use gradient descent to minimize the reverse\nKullback-Leibler divergence, BaM uses more specialized updates to match the\nscores of the target density and its Gaussian approximation. We extend the\nupdates for BaM by integrating them with a more compact parameterization of\nfull covariance matrices. In particular, borrowing ideas from factor analysis,\nwe add an extra step to each iteration of BaM -- a patch -- that projects each\nnewly updated covariance matrix into a more efficiently parameterized family of\ndiagonal plus low rank matrices. We evaluate this approach on a variety of\nsynthetic target distributions and real-world problems in high-dimensional\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Black-box variational inference (BBVI) scales poorly to high dimensional\nproblems when it is used to estimate a multivariate Gaussian approximation with\na full covariance matrix. In this paper, we extend the batch-and-match (BaM)\nframework for score-based BBVI to problems where it is prohibitively expensive\nto store such covariance matrices, let alone to estimate them. Unlike classical\nalgorithms for BBVI, which use gradient descent to minimize the reverse\nKullback-Leibler divergence, BaM uses more specialized updates to match the\nscores of the target density and its Gaussian approximation. We extend the\nupdates for BaM by integrating them with a more compact parameterization of\nfull covariance matrices. In particular, borrowing ideas from factor analysis,\nwe add an extra step to each iteration of BaM -- a patch -- that projects each\nnewly updated covariance matrix into a more efficiently parameterized family of\ndiagonal plus low rank matrices. We evaluate this approach on a variety of\nsynthetic target distributions and real-world problems in high-dimensional\ninference."
                },
                "authors": [
                    {
                        "name": "Chirag Modi"
                    },
                    {
                        "name": "Diana Cai"
                    },
                    {
                        "name": "Lawrence K. Saul"
                    }
                ],
                "author_detail": {
                    "name": "Lawrence K. Saul"
                },
                "author": "Lawrence K. Saul",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22284v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22284v1",
                "updated": "2024-10-29T17:36:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    36,
                    59,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T17:36:59Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    36,
                    59,
                    1,
                    303,
                    0
                ],
                "title": "Embedding-based classifiers can detect prompt injection attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embedding-based classifiers can detect prompt injection attacks"
                },
                "summary": "Large Language Models (LLMs) are seeing significant adoption in every type of\norganization due to their exceptional generative capabilities. However, LLMs\nare found to be vulnerable to various adversarial attacks, particularly prompt\ninjection attacks, which trick them into producing harmful or inappropriate\ncontent. Adversaries execute such attacks by crafting malicious prompts to\ndeceive the LLMs. In this paper, we propose a novel approach based on\nembedding-based Machine Learning (ML) classifiers to protect LLM-based\napplications against this severe threat. We leverage three commonly used\nembedding models to generate embeddings of malicious and benign prompts and\nutilize ML classifiers to predict whether an input prompt is malicious. Out of\nseveral traditional ML methods, we achieve the best performance with\nclassifiers built using Random Forest and XGBoost. Our classifiers outperform\nstate-of-the-art prompt injection classifiers available in open-source\nimplementations, which use encoder-only neural networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are seeing significant adoption in every type of\norganization due to their exceptional generative capabilities. However, LLMs\nare found to be vulnerable to various adversarial attacks, particularly prompt\ninjection attacks, which trick them into producing harmful or inappropriate\ncontent. Adversaries execute such attacks by crafting malicious prompts to\ndeceive the LLMs. In this paper, we propose a novel approach based on\nembedding-based Machine Learning (ML) classifiers to protect LLM-based\napplications against this severe threat. We leverage three commonly used\nembedding models to generate embeddings of malicious and benign prompts and\nutilize ML classifiers to predict whether an input prompt is malicious. Out of\nseveral traditional ML methods, we achieve the best performance with\nclassifiers built using Random Forest and XGBoost. Our classifiers outperform\nstate-of-the-art prompt injection classifiers available in open-source\nimplementations, which use encoder-only neural networks."
                },
                "authors": [
                    {
                        "name": "Md. Ahsan Ayub"
                    },
                    {
                        "name": "Subhabrata Majumdar"
                    }
                ],
                "author_detail": {
                    "name": "Subhabrata Majumdar"
                },
                "author": "Subhabrata Majumdar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22284v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22284v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22282v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22282v1",
                "updated": "2024-10-29T17:35:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    35,
                    46,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T17:35:46Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    35,
                    46,
                    1,
                    303,
                    0
                ],
                "title": "Whose ChatGPT? Unveiling Real-World Educational Inequalities Introduced\n  by Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Whose ChatGPT? Unveiling Real-World Educational Inequalities Introduced\n  by Large Language Models"
                },
                "summary": "The universal availability of ChatGPT and other similar tools since late 2022\nhas prompted tremendous public excitement and experimental effort about the\npotential of large language models (LLMs) to improve learning experience and\noutcomes, especially for learners from disadvantaged backgrounds. However,\nlittle research has systematically examined the real-world impacts of LLM\navailability on educational equity beyond theoretical projections and\ncontrolled studies of innovative LLM applications. To depict trends of post-LLM\ninequalities, we analyze 1,140,328 academic writing submissions from 16,791\ncollege students across 2,391 courses between 2021 and 2024 at a public,\nminority-serving institution in the US. We find that students' overall writing\nquality gradually increased following the availability of LLMs and that the\nwriting quality gaps between linguistically advantaged and disadvantaged\nstudents became increasingly narrower. However, this equitizing effect was more\nconcentrated on students with higher socioeconomic status. These findings shed\nlight on the digital divides in the era of LLMs and raise questions about the\nequity benefits of LLMs in early stages and highlight the need for researchers\nand practitioners on developing responsible practices to improve educational\nequity through LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The universal availability of ChatGPT and other similar tools since late 2022\nhas prompted tremendous public excitement and experimental effort about the\npotential of large language models (LLMs) to improve learning experience and\noutcomes, especially for learners from disadvantaged backgrounds. However,\nlittle research has systematically examined the real-world impacts of LLM\navailability on educational equity beyond theoretical projections and\ncontrolled studies of innovative LLM applications. To depict trends of post-LLM\ninequalities, we analyze 1,140,328 academic writing submissions from 16,791\ncollege students across 2,391 courses between 2021 and 2024 at a public,\nminority-serving institution in the US. We find that students' overall writing\nquality gradually increased following the availability of LLMs and that the\nwriting quality gaps between linguistically advantaged and disadvantaged\nstudents became increasingly narrower. However, this equitizing effect was more\nconcentrated on students with higher socioeconomic status. These findings shed\nlight on the digital divides in the era of LLMs and raise questions about the\nequity benefits of LLMs in early stages and highlight the need for researchers\nand practitioners on developing responsible practices to improve educational\nequity through LLMs."
                },
                "authors": [
                    {
                        "name": "Renzhe Yu"
                    },
                    {
                        "name": "Zhen Xu"
                    },
                    {
                        "name": "Sky CH-Wang"
                    },
                    {
                        "name": "Richard Arum"
                    }
                ],
                "author_detail": {
                    "name": "Richard Arum"
                },
                "author": "Richard Arum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22282v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22282v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.09252v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.09252v3",
                "updated": "2024-10-29T17:34:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    34,
                    54,
                    1,
                    303,
                    0
                ],
                "published": "2024-07-12T13:30:44Z",
                "published_parsed": [
                    2024,
                    7,
                    12,
                    13,
                    30,
                    44,
                    4,
                    194,
                    0
                ],
                "title": "Context Embeddings for Efficient Answer Generation in RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context Embeddings for Efficient Answer Generation in RAG"
                },
                "summary": "Retrieval-Augmented Generation (RAG) allows overcoming the limited knowledge\nof LLMs by extending the input with external information. As a consequence, the\ncontextual inputs to the model become much longer which slows down decoding\ntime directly translating to the time a user has to wait for an answer. We\naddress this challenge by presenting COCOM, an effective context compression\nmethod, reducing long contexts to only a handful of Context Embeddings speeding\nup the generation time by a large margin. Our method allows for different\ncompression rates trading off decoding time for answer quality. Compared to\nearlier methods, COCOM allows for handling multiple contexts more effectively,\nsignificantly reducing decoding time for long inputs. Our method demonstrates a\nspeed-up of up to 5.69 $\\times$ while achieving higher performance compared to\nexisting efficient context compression methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) allows overcoming the limited knowledge\nof LLMs by extending the input with external information. As a consequence, the\ncontextual inputs to the model become much longer which slows down decoding\ntime directly translating to the time a user has to wait for an answer. We\naddress this challenge by presenting COCOM, an effective context compression\nmethod, reducing long contexts to only a handful of Context Embeddings speeding\nup the generation time by a large margin. Our method allows for different\ncompression rates trading off decoding time for answer quality. Compared to\nearlier methods, COCOM allows for handling multiple contexts more effectively,\nsignificantly reducing decoding time for long inputs. Our method demonstrates a\nspeed-up of up to 5.69 $\\times$ while achieving higher performance compared to\nexisting efficient context compression methods."
                },
                "authors": [
                    {
                        "name": "David Rau"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Herv Djean"
                    },
                    {
                        "name": "Stphane Clinchant"
                    }
                ],
                "author_detail": {
                    "name": "Stphane Clinchant"
                },
                "author": "Stphane Clinchant",
                "arxiv_comment": "10 pages",
                "arxiv_journal_ref": "WSDM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.09252v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.09252v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19274v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19274v2",
                "updated": "2024-10-29T17:33:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    33,
                    19,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-25T03:01:19Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    3,
                    1,
                    19,
                    4,
                    299,
                    0
                ],
                "title": "Ripple: Accelerating LLM Inference on Smartphones with Correlation-Aware\n  Neuron Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ripple: Accelerating LLM Inference on Smartphones with Correlation-Aware\n  Neuron Management"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains, yet deploying them on mobile devices remains an arduous challenge due\nto their extensive computational and memory demands. While lightweight LLMs\nhave been developed to fit mobile environments, they suffer from degraded model\naccuracy. In contrast, sparsity-based techniques minimize DRAM usage by\nselectively transferring only relevant neurons to DRAM while retaining the full\nmodel in external storage, such as flash. However, such approaches are\ncritically limited by numerous I/O operations, particularly on smartphones with\nsevere IOPS constraints.\n  In this paper, we propose Ripple, a novel approach that accelerates LLM\ninference on smartphones by optimizing neuron placement in flash memory. Ripple\nleverages the concept of Neuron Co-Activation, where neurons frequently\nactivated together are linked to facilitate continuous read access and optimize\ndata transfer efficiency. Our approach incorporates a two-stage solution: an\noffline stage that reorganizes neuron placement based on co-activation\npatterns, and an online stage that employs tailored data access and caching\nstrategies to align well with hardware characteristics. Evaluations conducted\non a variety of smartphones and LLMs demonstrate that Ripple achieves up to\n5.93x improvements in I/O latency compared to the state-of-the-art. As the\nfirst solution to optimize storage placement under sparsity, Ripple explores a\nnew optimization space at the intersection of sparsity-driven algorithm and\nstorage-level system co-design in LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains, yet deploying them on mobile devices remains an arduous challenge due\nto their extensive computational and memory demands. While lightweight LLMs\nhave been developed to fit mobile environments, they suffer from degraded model\naccuracy. In contrast, sparsity-based techniques minimize DRAM usage by\nselectively transferring only relevant neurons to DRAM while retaining the full\nmodel in external storage, such as flash. However, such approaches are\ncritically limited by numerous I/O operations, particularly on smartphones with\nsevere IOPS constraints.\n  In this paper, we propose Ripple, a novel approach that accelerates LLM\ninference on smartphones by optimizing neuron placement in flash memory. Ripple\nleverages the concept of Neuron Co-Activation, where neurons frequently\nactivated together are linked to facilitate continuous read access and optimize\ndata transfer efficiency. Our approach incorporates a two-stage solution: an\noffline stage that reorganizes neuron placement based on co-activation\npatterns, and an online stage that employs tailored data access and caching\nstrategies to align well with hardware characteristics. Evaluations conducted\non a variety of smartphones and LLMs demonstrate that Ripple achieves up to\n5.93x improvements in I/O latency compared to the state-of-the-art. As the\nfirst solution to optimize storage placement under sparsity, Ripple explores a\nnew optimization space at the intersection of sparsity-driven algorithm and\nstorage-level system co-design in LLM inference."
                },
                "authors": [
                    {
                        "name": "Tuowei Wang"
                    },
                    {
                        "name": "Ruwen Fan"
                    },
                    {
                        "name": "Minxing Huang"
                    },
                    {
                        "name": "Zixu Hao"
                    },
                    {
                        "name": "Kun Li"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Youyou Lu"
                    },
                    {
                        "name": "Yaoxue Zhang"
                    },
                    {
                        "name": "Ju Ren"
                    }
                ],
                "author_detail": {
                    "name": "Ju Ren"
                },
                "author": "Ju Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19274v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19274v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22274v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22274v1",
                "updated": "2024-10-29T17:29:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    29,
                    42,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T17:29:42Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    29,
                    42,
                    1,
                    303,
                    0
                ],
                "title": "Bayesian analyses of the A2HDM with low-mass scalars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian analyses of the A2HDM with low-mass scalars"
                },
                "summary": "Two-Higgs-doublet models come with an augmented parameter space which allows\nthem to possibly solve some of the shortcomings of the Standard Model, and\nopens the window to a plethora of new phenomena to be discovered. The\nintroduction of scalar-mediated tree-level flavour-changing neutral currents\nmay be tackled with the imposition of extra symmetries on the model or,\nalternatively, by demanding a strict proportionality between the\nflavour-changing couplings and fermion mass matrices. The latter is the very\nidea behind the Aligned-Two-Higgs-Doublet Model (A2HDM). The coefficients that\ngovern such proportionality are, in general, complex and, therefore, possible\nnew sources of CP violation, a calling card of this class of models. We present\nhere the results of new state-of-the-art analyses of the A2HDM where, in\nparticular, we ascertain whether current data allows the A2HDM to accommodate\nextra scalars lighter than the 125 GeV Higgs boson. To this effect, we make use\nof theoretical constraints, bounds from Higgs searches at the LHC and LEP,\nelectroweak precision observables, and a set of flavour observables, all\nglobally combined within HEPfit, a software with a Bayesian Markov Chain Monte\nCarlo approach to statistical inference. Focusing on the light-pseudoscalar\nscenario, we find a region of parameter space compatible with all the\nconstraints we impose.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-Higgs-doublet models come with an augmented parameter space which allows\nthem to possibly solve some of the shortcomings of the Standard Model, and\nopens the window to a plethora of new phenomena to be discovered. The\nintroduction of scalar-mediated tree-level flavour-changing neutral currents\nmay be tackled with the imposition of extra symmetries on the model or,\nalternatively, by demanding a strict proportionality between the\nflavour-changing couplings and fermion mass matrices. The latter is the very\nidea behind the Aligned-Two-Higgs-Doublet Model (A2HDM). The coefficients that\ngovern such proportionality are, in general, complex and, therefore, possible\nnew sources of CP violation, a calling card of this class of models. We present\nhere the results of new state-of-the-art analyses of the A2HDM where, in\nparticular, we ascertain whether current data allows the A2HDM to accommodate\nextra scalars lighter than the 125 GeV Higgs boson. To this effect, we make use\nof theoretical constraints, bounds from Higgs searches at the LHC and LEP,\nelectroweak precision observables, and a set of flavour observables, all\nglobally combined within HEPfit, a software with a Bayesian Markov Chain Monte\nCarlo approach to statistical inference. Focusing on the light-pseudoscalar\nscenario, we find a region of parameter space compatible with all the\nconstraints we impose."
                },
                "authors": [
                    {
                        "name": "Antonio M. Coutinho"
                    },
                    {
                        "name": "Anirban Karan"
                    },
                    {
                        "name": "Vctor Miralles"
                    },
                    {
                        "name": "Antonio Pich"
                    }
                ],
                "author_detail": {
                    "name": "Antonio Pich"
                },
                "author": "Antonio Pich",
                "arxiv_comment": "5 pages, 1 figure, 1 table, Proceedings of LHCP2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22274v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22274v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01878v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01878v3",
                "updated": "2024-10-29T17:29:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    29,
                    37,
                    1,
                    303,
                    0
                ],
                "published": "2024-07-02T01:37:56Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    1,
                    37,
                    56,
                    1,
                    184,
                    0
                ],
                "title": "Compare without Despair: Reliable Preference Evaluation with Generation\n  Separability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compare without Despair: Reliable Preference Evaluation with Generation\n  Separability"
                },
                "summary": "Human evaluation of generated language through pairwise preference judgments\nis pervasive. However, under common scenarios, such as when generations from a\nmodel pair are very similar, or when stochastic decoding results in large\nvariations in generations, it results in inconsistent preference ratings. We\naddress these challenges by introducing a meta-evaluation measure,\nseparability, which estimates how suitable a test instance is for pairwise\npreference evaluation. For a candidate test instance, separability samples\nmultiple generations from a pair of models, and measures how distinguishable\nthe two sets of generations are. Our experiments show that instances with high\nseparability values yield more consistent preference ratings from both human-\nand auto-raters. Further, the distribution of separability allows insights into\nwhich test benchmarks are more valuable for comparing models. Finally, we\nincorporate separability into ELO ratings, accounting for how suitable each\ntest instance might be for reliably ranking LLMs. Overall, separability has\nimplications for consistent, efficient and robust preference evaluation of LLMs\nwith both human- and auto-raters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human evaluation of generated language through pairwise preference judgments\nis pervasive. However, under common scenarios, such as when generations from a\nmodel pair are very similar, or when stochastic decoding results in large\nvariations in generations, it results in inconsistent preference ratings. We\naddress these challenges by introducing a meta-evaluation measure,\nseparability, which estimates how suitable a test instance is for pairwise\npreference evaluation. For a candidate test instance, separability samples\nmultiple generations from a pair of models, and measures how distinguishable\nthe two sets of generations are. Our experiments show that instances with high\nseparability values yield more consistent preference ratings from both human-\nand auto-raters. Further, the distribution of separability allows insights into\nwhich test benchmarks are more valuable for comparing models. Finally, we\nincorporate separability into ELO ratings, accounting for how suitable each\ntest instance might be for reliably ranking LLMs. Overall, separability has\nimplications for consistent, efficient and robust preference evaluation of LLMs\nwith both human- and auto-raters."
                },
                "authors": [
                    {
                        "name": "Sayan Ghosh"
                    },
                    {
                        "name": "Tejas Srinivasan"
                    },
                    {
                        "name": "Swabha Swayamdipta"
                    }
                ],
                "author_detail": {
                    "name": "Swabha Swayamdipta"
                },
                "author": "Swabha Swayamdipta",
                "arxiv_comment": "Corrected description of reference in Related Work; Findings of EMNLP\n  2024 Camera Ready Version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.01878v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01878v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01489v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01489v2",
                "updated": "2024-10-29T17:29:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    29,
                    27,
                    1,
                    303,
                    0
                ],
                "published": "2024-07-01T17:24:45Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    17,
                    24,
                    45,
                    0,
                    183,
                    0
                ],
                "title": "Agentless: Demystifying LLM-based Software Engineering Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentless: Demystifying LLM-based Software Engineering Agents"
                },
                "summary": "Recent advancements in large language models (LLMs) have significantly\nadvanced the automation of software development tasks, including code\nsynthesis, program repair, and test generation. More recently, researchers and\nindustry practitioners have developed various autonomous LLM agents to perform\nend-to-end software development tasks. These agents are equipped with the\nability to use tools, run commands, observe feedback from the environment, and\nplan for future actions. However, the complexity of these agent-based\napproaches, together with the limited abilities of current LLMs, raises the\nfollowing question: Do we really have to employ complex autonomous software\nagents? To attempt to answer this question, we build Agentless -- an agentless\napproach to automatically solve software development problems. Compared to the\nverbose and complex setup of agent-based approaches, Agentless employs a\nsimplistic three-phase process of localization, repair, and patch validation,\nwithout letting the LLM decide future actions or operate with complex tools.\nOur results on the popular SWE-bench Lite benchmark show that surprisingly the\nsimplistic Agentless is able to achieve both the highest performance (32.00%,\n96 correct fixes) and low cost ($0.70) compared with all existing open-source\nsoftware agents! Furthermore, we manually classified the problems in SWE-bench\nLite and found problems with exact ground truth patch or\ninsufficient/misleading issue descriptions. As such, we construct SWE-bench\nLite-S by excluding such problematic issues to perform more rigorous evaluation\nand comparison. Our work highlights the current overlooked potential of a\nsimple, interpretable technique in autonomous software development. We hope\nAgentless will help reset the baseline, starting point, and horizon for\nautonomous software agents, and inspire future work along this crucial\ndirection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have significantly\nadvanced the automation of software development tasks, including code\nsynthesis, program repair, and test generation. More recently, researchers and\nindustry practitioners have developed various autonomous LLM agents to perform\nend-to-end software development tasks. These agents are equipped with the\nability to use tools, run commands, observe feedback from the environment, and\nplan for future actions. However, the complexity of these agent-based\napproaches, together with the limited abilities of current LLMs, raises the\nfollowing question: Do we really have to employ complex autonomous software\nagents? To attempt to answer this question, we build Agentless -- an agentless\napproach to automatically solve software development problems. Compared to the\nverbose and complex setup of agent-based approaches, Agentless employs a\nsimplistic three-phase process of localization, repair, and patch validation,\nwithout letting the LLM decide future actions or operate with complex tools.\nOur results on the popular SWE-bench Lite benchmark show that surprisingly the\nsimplistic Agentless is able to achieve both the highest performance (32.00%,\n96 correct fixes) and low cost ($0.70) compared with all existing open-source\nsoftware agents! Furthermore, we manually classified the problems in SWE-bench\nLite and found problems with exact ground truth patch or\ninsufficient/misleading issue descriptions. As such, we construct SWE-bench\nLite-S by excluding such problematic issues to perform more rigorous evaluation\nand comparison. Our work highlights the current overlooked potential of a\nsimple, interpretable technique in autonomous software development. We hope\nAgentless will help reset the baseline, starting point, and horizon for\nautonomous software agents, and inspire future work along this crucial\ndirection."
                },
                "authors": [
                    {
                        "name": "Chunqiu Steven Xia"
                    },
                    {
                        "name": "Yinlin Deng"
                    },
                    {
                        "name": "Soren Dunn"
                    },
                    {
                        "name": "Lingming Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lingming Zhang"
                },
                "author": "Lingming Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.01489v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01489v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22269v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22269v1",
                "updated": "2024-10-29T17:27:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    27,
                    58,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T17:27:58Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    27,
                    58,
                    1,
                    303,
                    0
                ],
                "title": "Fourier Head: Helping Large Language Models Learn Complex Probability\n  Distributions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fourier Head: Helping Large Language Models Learn Complex Probability\n  Distributions"
                },
                "summary": "As the quality of large language models has improved, there has been\nincreased interest in using them to model non-linguistic tokens. For example,\nthe Decision Transformer recasts agentic decision making as a sequence modeling\nproblem, using a decoder-only LLM to model the distribution over the discrete\naction space for an Atari agent. However, when adapting LLMs to non-linguistic\ndomains, it remains unclear if softmax over discrete bins captures the\ncontinuous structure of the tokens and the potentially complex distributions\nneeded for high quality token generation. We introduce a neural network layer,\nconstructed using Fourier series, which we can easily substitute for any linear\nlayer if we want the outputs to have a more continuous structure. We perform\nextensive analysis on synthetic datasets, as well as on large-scale decision\nmaking and time series forecasting tasks. We also provide theoretical evidence\nthat this layer can better learn signal from data while ignoring high-frequency\nnoise. All of our results support the effectiveness of our proposed Fourier\nhead in scenarios where the underlying data distribution has a natural\ncontinuous structure. For example, the Fourier head improves a Decision\nTransformer agent's returns by 46% on the Atari Seaquest game, and increases a\nstate-of-the-art times series foundation model's forecasting performance by\n3.5% across 20 benchmarks unseen during training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the quality of large language models has improved, there has been\nincreased interest in using them to model non-linguistic tokens. For example,\nthe Decision Transformer recasts agentic decision making as a sequence modeling\nproblem, using a decoder-only LLM to model the distribution over the discrete\naction space for an Atari agent. However, when adapting LLMs to non-linguistic\ndomains, it remains unclear if softmax over discrete bins captures the\ncontinuous structure of the tokens and the potentially complex distributions\nneeded for high quality token generation. We introduce a neural network layer,\nconstructed using Fourier series, which we can easily substitute for any linear\nlayer if we want the outputs to have a more continuous structure. We perform\nextensive analysis on synthetic datasets, as well as on large-scale decision\nmaking and time series forecasting tasks. We also provide theoretical evidence\nthat this layer can better learn signal from data while ignoring high-frequency\nnoise. All of our results support the effectiveness of our proposed Fourier\nhead in scenarios where the underlying data distribution has a natural\ncontinuous structure. For example, the Fourier head improves a Decision\nTransformer agent's returns by 46% on the Atari Seaquest game, and increases a\nstate-of-the-art times series foundation model's forecasting performance by\n3.5% across 20 benchmarks unseen during training."
                },
                "authors": [
                    {
                        "name": "Nate Gillman"
                    },
                    {
                        "name": "Daksh Aggarwal"
                    },
                    {
                        "name": "Michael Freeman"
                    },
                    {
                        "name": "Saurabh Singh"
                    },
                    {
                        "name": "Chen Sun"
                    }
                ],
                "author_detail": {
                    "name": "Chen Sun"
                },
                "author": "Chen Sun",
                "arxiv_comment": "Project page and code are at https://nategillman.com/fourier-head",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22269v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22249v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22249v1",
                "updated": "2024-10-29T17:13:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    13,
                    54,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T17:13:54Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    13,
                    54,
                    1,
                    303,
                    0
                ],
                "title": "Pushing the Performance Envelope of DNN-based Recommendation Systems\n  Inference on GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pushing the Performance Envelope of DNN-based Recommendation Systems\n  Inference on GPUs"
                },
                "summary": "Personalized recommendation is a ubiquitous application on the internet, with\nmany industries and hyperscalers extensively leveraging Deep Learning\nRecommendation Models (DLRMs) for their personalization needs (like ad serving\nor movie suggestions). With growing model and dataset sizes pushing computation\nand memory requirements, GPUs are being increasingly preferred for executing\nDLRM inference. However, serving newer DLRMs, while meeting acceptable\nlatencies, continues to remain challenging, making traditional deployments\nincreasingly more GPU-hungry, resulting in higher inference serving costs. In\nthis paper, we show that the embedding stage continues to be the primary\nbottleneck in the GPU inference pipeline, leading up to a 3.2x embedding-only\nperformance slowdown.\n  To thoroughly grasp the problem, we conduct a detailed microarchitecture\ncharacterization and highlight the presence of low occupancy in the standard\nembedding kernels. By leveraging direct compiler optimizations, we achieve\noptimal occupancy, pushing the performance by up to 53%. Yet, long memory\nlatency stalls continue to exist. To tackle this challenge, we propose\nspecialized plug-and-play-based software prefetching and L2 pinning techniques,\nwhich help in hiding and decreasing the latencies. Further, we propose\ncombining them, as they complement each other. Experimental evaluations using\nA100 GPUs with large models and datasets show that our proposed techniques\nimprove performance by up to 103% for the embedding stage, and up to 77% for\nthe overall DLRM inference pipeline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized recommendation is a ubiquitous application on the internet, with\nmany industries and hyperscalers extensively leveraging Deep Learning\nRecommendation Models (DLRMs) for their personalization needs (like ad serving\nor movie suggestions). With growing model and dataset sizes pushing computation\nand memory requirements, GPUs are being increasingly preferred for executing\nDLRM inference. However, serving newer DLRMs, while meeting acceptable\nlatencies, continues to remain challenging, making traditional deployments\nincreasingly more GPU-hungry, resulting in higher inference serving costs. In\nthis paper, we show that the embedding stage continues to be the primary\nbottleneck in the GPU inference pipeline, leading up to a 3.2x embedding-only\nperformance slowdown.\n  To thoroughly grasp the problem, we conduct a detailed microarchitecture\ncharacterization and highlight the presence of low occupancy in the standard\nembedding kernels. By leveraging direct compiler optimizations, we achieve\noptimal occupancy, pushing the performance by up to 53%. Yet, long memory\nlatency stalls continue to exist. To tackle this challenge, we propose\nspecialized plug-and-play-based software prefetching and L2 pinning techniques,\nwhich help in hiding and decreasing the latencies. Further, we propose\ncombining them, as they complement each other. Experimental evaluations using\nA100 GPUs with large models and datasets show that our proposed techniques\nimprove performance by up to 103% for the embedding stage, and up to 77% for\nthe overall DLRM inference pipeline."
                },
                "authors": [
                    {
                        "name": "Rishabh Jain"
                    },
                    {
                        "name": "Vivek M. Bhasi"
                    },
                    {
                        "name": "Adwait Jog"
                    },
                    {
                        "name": "Anand Sivasubramaniam"
                    },
                    {
                        "name": "Mahmut T. Kandemir"
                    },
                    {
                        "name": "Chita R. Das"
                    }
                ],
                "author_detail": {
                    "name": "Chita R. Das"
                },
                "author": "Chita R. Das",
                "arxiv_comment": "This work has been accepted in the 57th MICRO\n  (https://microarch.org/micro57/program/). Please check appendix for details\n  on reproducing our work including codebase and steps",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22249v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22249v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22240v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22240v1",
                "updated": "2024-10-29T17:05:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    5,
                    25,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T17:05:25Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    5,
                    25,
                    1,
                    303,
                    0
                ],
                "title": "Are Decoder-Only Large Language Models the Silver Bullet for Code\n  Search?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Decoder-Only Large Language Models the Silver Bullet for Code\n  Search?"
                },
                "summary": "Code search is crucial for code reuse, enabling developers to efficiently\nlocate relevant snippets. Current methods rely on encoder-based models, which\nsuffer from limitations such as poor generalization and restricted input\nlengths. Decoder-only large language models (LLMs), with their extensive\npre-training, larger size, and longer input capabilities, offer potential\nsolutions to these issues, yet their effectiveness in code search remains\nunderexplored. To fill this gap, our study presents the first systematic\nexploration of decoder-only LLMs for code search. We evaluate nine\nstate-of-the-art decoder-only models using two fine-tuning methods, two\ndatasets (CSN and CoSQA$^+$), and three model sizes. Our findings reveal that\nfine-tuned CodeGemma significantly outperforms encoder-only models like\nUniXcoder, achieving a 5.57% improvement in MRR on CSN and a 49.6% increase in\nMAP on CoSQA$^+$ compared to zero-shot UniXcoder. These results highlight the\nsuperior performance and adaptability of decoder-only models. Additionally, we\nprovide valuable insights into optimizing these models for code search,\ncovering aspects such as model selection, fine-tuning methods, training data,\nand model size, and discussing their strengths and limitations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code search is crucial for code reuse, enabling developers to efficiently\nlocate relevant snippets. Current methods rely on encoder-based models, which\nsuffer from limitations such as poor generalization and restricted input\nlengths. Decoder-only large language models (LLMs), with their extensive\npre-training, larger size, and longer input capabilities, offer potential\nsolutions to these issues, yet their effectiveness in code search remains\nunderexplored. To fill this gap, our study presents the first systematic\nexploration of decoder-only LLMs for code search. We evaluate nine\nstate-of-the-art decoder-only models using two fine-tuning methods, two\ndatasets (CSN and CoSQA$^+$), and three model sizes. Our findings reveal that\nfine-tuned CodeGemma significantly outperforms encoder-only models like\nUniXcoder, achieving a 5.57% improvement in MRR on CSN and a 49.6% increase in\nMAP on CoSQA$^+$ compared to zero-shot UniXcoder. These results highlight the\nsuperior performance and adaptability of decoder-only models. Additionally, we\nprovide valuable insights into optimizing these models for code search,\ncovering aspects such as model selection, fine-tuning methods, training data,\nand model size, and discussing their strengths and limitations."
                },
                "authors": [
                    {
                        "name": "Yuxuan Chen"
                    },
                    {
                        "name": "Guangsheng Ou"
                    },
                    {
                        "name": "Mingwei Liu"
                    },
                    {
                        "name": "Yanlin Wang"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22240v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22240v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22225v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22225v1",
                "updated": "2024-10-29T16:54:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    16,
                    54,
                    15,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T16:54:15Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    16,
                    54,
                    15,
                    1,
                    303,
                    0
                ],
                "title": "CaStL: Constraints as Specifications through LLM Translation for\n  Long-Horizon Task and Motion Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CaStL: Constraints as Specifications through LLM Translation for\n  Long-Horizon Task and Motion Planning"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable ability in\nlong-horizon Task and Motion Planning (TAMP) by translating clear and\nstraightforward natural language problems into formal specifications such as\nthe Planning Domain Definition Language (PDDL). However, real-world problems\nare often ambiguous and involve many complex constraints. In this paper, we\nintroduce Constraints as Specifications through LLMs (CaStL), a framework that\nidentifies constraints such as goal conditions, action ordering, and action\nblocking from natural language in multiple stages. CaStL translates these\nconstraints into PDDL and Python scripts, which are solved using an custom PDDL\nsolver. Tested across three PDDL domains, CaStL significantly improves\nconstraint handling and planning success rates from natural language\nspecification in complex scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable ability in\nlong-horizon Task and Motion Planning (TAMP) by translating clear and\nstraightforward natural language problems into formal specifications such as\nthe Planning Domain Definition Language (PDDL). However, real-world problems\nare often ambiguous and involve many complex constraints. In this paper, we\nintroduce Constraints as Specifications through LLMs (CaStL), a framework that\nidentifies constraints such as goal conditions, action ordering, and action\nblocking from natural language in multiple stages. CaStL translates these\nconstraints into PDDL and Python scripts, which are solved using an custom PDDL\nsolver. Tested across three PDDL domains, CaStL significantly improves\nconstraint handling and planning success rates from natural language\nspecification in complex scenarios."
                },
                "authors": [
                    {
                        "name": "Weihang Guo"
                    },
                    {
                        "name": "Zachary Kingston"
                    },
                    {
                        "name": "Lydia E. Kavraki"
                    }
                ],
                "author_detail": {
                    "name": "Lydia E. Kavraki"
                },
                "author": "Lydia E. Kavraki",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22225v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22225v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22217v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22217v1",
                "updated": "2024-10-29T16:48:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    16,
                    48,
                    22,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T16:48:22Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    16,
                    48,
                    22,
                    1,
                    303,
                    0
                ],
                "title": "Towards Unifying Understanding and Generation in the Era of Vision\n  Foundation Models: A Survey from the Autoregression Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Unifying Understanding and Generation in the Era of Vision\n  Foundation Models: A Survey from the Autoregression Perspective"
                },
                "summary": "Autoregression in large language models (LLMs) has shown impressive\nscalability by unifying all language tasks into the next token prediction\nparadigm. Recently, there is a growing interest in extending this success to\nvision foundation models. In this survey, we review the recent advances and\ndiscuss future directions for autoregressive vision foundation models. First,\nwe present the trend for next generation of vision foundation models, i.e.,\nunifying both understanding and generation in vision tasks. We then analyze the\nlimitations of existing vision foundation models, and present a formal\ndefinition of autoregression with its advantages. Later, we categorize\nautoregressive vision foundation models from their vision tokenizers and\nautoregression backbones. Finally, we discuss several promising research\nchallenges and directions. To the best of our knowledge, this is the first\nsurvey to comprehensively summarize autoregressive vision foundation models\nunder the trend of unifying understanding and generation. A collection of\nrelated resources is available at https://github.com/EmmaSRH/ARVFM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregression in large language models (LLMs) has shown impressive\nscalability by unifying all language tasks into the next token prediction\nparadigm. Recently, there is a growing interest in extending this success to\nvision foundation models. In this survey, we review the recent advances and\ndiscuss future directions for autoregressive vision foundation models. First,\nwe present the trend for next generation of vision foundation models, i.e.,\nunifying both understanding and generation in vision tasks. We then analyze the\nlimitations of existing vision foundation models, and present a formal\ndefinition of autoregression with its advantages. Later, we categorize\nautoregressive vision foundation models from their vision tokenizers and\nautoregression backbones. Finally, we discuss several promising research\nchallenges and directions. To the best of our knowledge, this is the first\nsurvey to comprehensively summarize autoregressive vision foundation models\nunder the trend of unifying understanding and generation. A collection of\nrelated resources is available at https://github.com/EmmaSRH/ARVFM."
                },
                "authors": [
                    {
                        "name": "Shenghao Xie"
                    },
                    {
                        "name": "Wenqiang Zu"
                    },
                    {
                        "name": "Mingyang Zhao"
                    },
                    {
                        "name": "Duo Su"
                    },
                    {
                        "name": "Shilong Liu"
                    },
                    {
                        "name": "Ruohua Shi"
                    },
                    {
                        "name": "Guoqi Li"
                    },
                    {
                        "name": "Shanghang Zhang"
                    },
                    {
                        "name": "Lei Ma"
                    }
                ],
                "author_detail": {
                    "name": "Lei Ma"
                },
                "author": "Lei Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22217v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22217v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19759v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19759v3",
                "updated": "2024-10-30T06:12:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    6,
                    12,
                    49,
                    2,
                    304,
                    0
                ],
                "published": "2024-09-29T20:14:50Z",
                "published_parsed": [
                    2024,
                    9,
                    29,
                    20,
                    14,
                    50,
                    6,
                    273,
                    0
                ],
                "title": "Balancing Cost and Effectiveness of Synthetic Data Generation Strategies\n  for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Balancing Cost and Effectiveness of Synthetic Data Generation Strategies\n  for LLMs"
                },
                "summary": "As large language models (LLMs) are applied to more use cases, creating high\nquality, task-specific datasets for fine-tuning becomes a bottleneck for model\nimprovement. Using high quality human data has been the most common approach to\nunlock model performance, but is prohibitively expensive in many scenarios.\nSeveral alternative methods have also emerged, such as generating synthetic or\nhybrid data, but the effectiveness of these approaches remain unclear,\nespecially in resource-constrained scenarios and tasks that are not easily\nverified. To investigate this, we group various synthetic data generation\nstrategies into three representative categories -- Answer Augmentation,\nQuestion Rephrase and New Question -- and study the performance of student LLMs\ntrained under various constraints, namely seed instruction set size and query\nbudget. We demonstrate that these strategies are not equally effective across\nsettings. Notably, the optimal data generation strategy depends strongly on the\nratio between the available teacher query budget and the size of the seed\ninstruction set. When this ratio is low, generating new answers to existing\nquestions proves most effective, but as this ratio increases, generating new\nquestions becomes optimal. Across all tasks, we find that choice of\naugmentation method and other design choices matter substantially more in low\nto mid data regimes than in high data regimes. We provide a practical framework\nfor selecting the appropriate augmentation method across settings, taking into\naccount additional factors such as the scalability of each method, the\nimportance of verifying synthetic data, and the use of different LLMs for\nsynthetic data generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) are applied to more use cases, creating high\nquality, task-specific datasets for fine-tuning becomes a bottleneck for model\nimprovement. Using high quality human data has been the most common approach to\nunlock model performance, but is prohibitively expensive in many scenarios.\nSeveral alternative methods have also emerged, such as generating synthetic or\nhybrid data, but the effectiveness of these approaches remain unclear,\nespecially in resource-constrained scenarios and tasks that are not easily\nverified. To investigate this, we group various synthetic data generation\nstrategies into three representative categories -- Answer Augmentation,\nQuestion Rephrase and New Question -- and study the performance of student LLMs\ntrained under various constraints, namely seed instruction set size and query\nbudget. We demonstrate that these strategies are not equally effective across\nsettings. Notably, the optimal data generation strategy depends strongly on the\nratio between the available teacher query budget and the size of the seed\ninstruction set. When this ratio is low, generating new answers to existing\nquestions proves most effective, but as this ratio increases, generating new\nquestions becomes optimal. Across all tasks, we find that choice of\naugmentation method and other design choices matter substantially more in low\nto mid data regimes than in high data regimes. We provide a practical framework\nfor selecting the appropriate augmentation method across settings, taking into\naccount additional factors such as the scalability of each method, the\nimportance of verifying synthetic data, and the use of different LLMs for\nsynthetic data generation."
                },
                "authors": [
                    {
                        "name": "Yung-Chieh Chan"
                    },
                    {
                        "name": "George Pu"
                    },
                    {
                        "name": "Apaar Shanker"
                    },
                    {
                        "name": "Parth Suresh"
                    },
                    {
                        "name": "Penn Jenks"
                    },
                    {
                        "name": "John Heyer"
                    },
                    {
                        "name": "Sam Denton"
                    }
                ],
                "author_detail": {
                    "name": "Sam Denton"
                },
                "author": "Sam Denton",
                "arxiv_comment": "NeurIPS '24 Workshop on Fine-Tuning in Modern Machine Learning:\n  Principles and Scalability",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19759v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19759v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22211v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22211v1",
                "updated": "2024-10-29T16:39:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    16,
                    39,
                    28,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T16:39:28Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    16,
                    39,
                    28,
                    1,
                    303,
                    0
                ],
                "title": "ProMQA: Question Answering Dataset for Multimodal Procedural Activity\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProMQA: Question Answering Dataset for Multimodal Procedural Activity\n  Understanding"
                },
                "summary": "Multimodal systems have great potential to assist humans in procedural\nactivities, where people follow instructions to achieve their goals. Despite\ndiverse application scenarios, systems are typically evaluated on traditional\nclassification tasks, e.g., action recognition or temporal action segmentation.\nIn this paper, we present a novel evaluation dataset, ProMQA, to measure system\nadvancements in application-oriented scenarios. ProMQA consists of 401\nmultimodal procedural QA pairs on user recording of procedural activities\ncoupled with their corresponding instruction. For QA annotation, we take a\ncost-effective human-LLM collaborative approach, where the existing annotation\nis augmented with LLM-generated QA pairs that are later verified by humans. We\nthen provide the benchmark results to set the baseline performance on ProMQA.\nOur experiment reveals a significant gap between human performance and that of\ncurrent systems, including competitive proprietary multimodal models. We hope\nour dataset sheds light on new aspects of models' multimodal understanding\ncapabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal systems have great potential to assist humans in procedural\nactivities, where people follow instructions to achieve their goals. Despite\ndiverse application scenarios, systems are typically evaluated on traditional\nclassification tasks, e.g., action recognition or temporal action segmentation.\nIn this paper, we present a novel evaluation dataset, ProMQA, to measure system\nadvancements in application-oriented scenarios. ProMQA consists of 401\nmultimodal procedural QA pairs on user recording of procedural activities\ncoupled with their corresponding instruction. For QA annotation, we take a\ncost-effective human-LLM collaborative approach, where the existing annotation\nis augmented with LLM-generated QA pairs that are later verified by humans. We\nthen provide the benchmark results to set the baseline performance on ProMQA.\nOur experiment reveals a significant gap between human performance and that of\ncurrent systems, including competitive proprietary multimodal models. We hope\nour dataset sheds light on new aspects of models' multimodal understanding\ncapabilities."
                },
                "authors": [
                    {
                        "name": "Kimihiro Hasegawa"
                    },
                    {
                        "name": "Wiradee Imrattanatrai"
                    },
                    {
                        "name": "Zhi-Qi Cheng"
                    },
                    {
                        "name": "Masaki Asada"
                    },
                    {
                        "name": "Susan Holm"
                    },
                    {
                        "name": "Yuran Wang"
                    },
                    {
                        "name": "Ken Fukuda"
                    },
                    {
                        "name": "Teruko Mitamura"
                    }
                ],
                "author_detail": {
                    "name": "Teruko Mitamura"
                },
                "author": "Teruko Mitamura",
                "arxiv_comment": "18 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22211v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22211v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.17999v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.17999v5",
                "updated": "2024-10-29T16:24:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    16,
                    24,
                    19,
                    1,
                    303,
                    0
                ],
                "published": "2023-10-27T09:17:00Z",
                "published_parsed": [
                    2023,
                    10,
                    27,
                    9,
                    17,
                    0,
                    4,
                    300,
                    0
                ],
                "title": "Automated threshold selection and associated inference uncertainty for\n  univariate extremes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated threshold selection and associated inference uncertainty for\n  univariate extremes"
                },
                "summary": "Threshold selection is a fundamental problem in any threshold-based extreme\nvalue analysis. While models are asymptotically motivated, selecting an\nappropriate threshold for finite samples is difficult and highly subjective\nthrough standard methods. Inference for high quantiles can also be highly\nsensitive to the choice of threshold. Too low a threshold choice leads to bias\nin the fit of the extreme value model, while too high a choice leads to\nunnecessary additional uncertainty in the estimation of model parameters. We\ndevelop a novel methodology for automated threshold selection that directly\ntackles this bias-variance trade-off. We also develop a method to account for\nthe uncertainty in the threshold estimation and propagate this uncertainty\nthrough to high quantile inference. Through a simulation study, we demonstrate\nthe effectiveness of our method for threshold selection and subsequent extreme\nquantile estimation, relative to the leading existing methods, and show how the\nmethod's effectiveness is not sensitive to the tuning parameters. We apply our\nmethod to the well-known, troublesome example of the River Nidd dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Threshold selection is a fundamental problem in any threshold-based extreme\nvalue analysis. While models are asymptotically motivated, selecting an\nappropriate threshold for finite samples is difficult and highly subjective\nthrough standard methods. Inference for high quantiles can also be highly\nsensitive to the choice of threshold. Too low a threshold choice leads to bias\nin the fit of the extreme value model, while too high a choice leads to\nunnecessary additional uncertainty in the estimation of model parameters. We\ndevelop a novel methodology for automated threshold selection that directly\ntackles this bias-variance trade-off. We also develop a method to account for\nthe uncertainty in the threshold estimation and propagate this uncertainty\nthrough to high quantile inference. Through a simulation study, we demonstrate\nthe effectiveness of our method for threshold selection and subsequent extreme\nquantile estimation, relative to the leading existing methods, and show how the\nmethod's effectiveness is not sensitive to the tuning parameters. We apply our\nmethod to the well-known, troublesome example of the River Nidd dataset."
                },
                "authors": [
                    {
                        "name": "Conor Murphy"
                    },
                    {
                        "name": "Jonathan A. Tawn"
                    },
                    {
                        "name": "Zak Varty"
                    }
                ],
                "author_detail": {
                    "name": "Zak Varty"
                },
                "author": "Zak Varty",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.17999v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.17999v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.14763v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.14763v3",
                "updated": "2024-10-29T16:20:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    16,
                    20,
                    19,
                    1,
                    303,
                    0
                ],
                "published": "2023-10-23T10:01:50Z",
                "published_parsed": [
                    2023,
                    10,
                    23,
                    10,
                    1,
                    50,
                    0,
                    296,
                    0
                ],
                "title": "Externally Valid Policy Evaluation Combining Trial and Observational\n  Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Externally Valid Policy Evaluation Combining Trial and Observational\n  Data"
                },
                "summary": "Randomized trials are widely considered as the gold standard for evaluating\nthe effects of decision policies. Trial data is, however, drawn from a\npopulation which may differ from the intended target population and this raises\na problem of external validity (aka. generalizability). In this paper we seek\nto use trial data to draw valid inferences about the outcome of a policy on the\ntarget population. Additional covariate data from the target population is used\nto model the sampling of individuals in the trial study. We develop a method\nthat yields certifiably valid trial-based policy evaluations under any\nspecified range of model miscalibrations. The method is nonparametric and the\nvalidity is assured even with finite samples. The certified policy evaluations\nare illustrated using both simulated and real data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Randomized trials are widely considered as the gold standard for evaluating\nthe effects of decision policies. Trial data is, however, drawn from a\npopulation which may differ from the intended target population and this raises\na problem of external validity (aka. generalizability). In this paper we seek\nto use trial data to draw valid inferences about the outcome of a policy on the\ntarget population. Additional covariate data from the target population is used\nto model the sampling of individuals in the trial study. We develop a method\nthat yields certifiably valid trial-based policy evaluations under any\nspecified range of model miscalibrations. The method is nonparametric and the\nvalidity is assured even with finite samples. The certified policy evaluations\nare illustrated using both simulated and real data."
                },
                "authors": [
                    {
                        "name": "Sofia Ek"
                    },
                    {
                        "name": "Dave Zachariah"
                    }
                ],
                "author_detail": {
                    "name": "Dave Zachariah"
                },
                "author": "Dave Zachariah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.14763v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.14763v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22182v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22182v1",
                "updated": "2024-10-29T16:19:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    16,
                    19,
                    8,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T16:19:08Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    16,
                    19,
                    8,
                    1,
                    303,
                    0
                ],
                "title": "Synthetic Data Generation with Large Language Models for Personalized\n  Community Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic Data Generation with Large Language Models for Personalized\n  Community Question Answering"
                },
                "summary": "Personalization in Information Retrieval (IR) is a topic studied by the\nresearch community since a long time. However, there is still a lack of\ndatasets to conduct large-scale evaluations of personalized IR; this is mainly\ndue to the fact that collecting and curating high-quality user-related\ninformation requires significant costs and time investment. Furthermore, the\ncreation of datasets for Personalized IR (PIR) tasks is affected by both\nprivacy concerns and the need for accurate user-related data, which are often\nnot publicly available. Recently, researchers have started to explore the use\nof Large Language Models (LLMs) to generate synthetic datasets, which is a\npossible solution to generate data for low-resource tasks. In this paper, we\ninvestigate the potential of Large Language Models (LLMs) for generating\nsynthetic documents to train an IR system for a Personalized Community Question\nAnswering task. To study the effectiveness of IR models fine-tuned on\nLLM-generated data, we introduce a new dataset, named Sy-SE-PQA. We build\nSy-SE-PQA based on an existing dataset, SE-PQA, which consists of questions and\nanswers posted on the popular StackExchange communities. Starting from\nquestions in SE-PQA, we generate synthetic answers using different prompt\ntechniques and LLMs. Our findings suggest that LLMs have high potential in\ngenerating data tailored to users' needs. The synthetic data can replace\nhuman-written training data, even if the generated data may contain incorrect\ninformation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalization in Information Retrieval (IR) is a topic studied by the\nresearch community since a long time. However, there is still a lack of\ndatasets to conduct large-scale evaluations of personalized IR; this is mainly\ndue to the fact that collecting and curating high-quality user-related\ninformation requires significant costs and time investment. Furthermore, the\ncreation of datasets for Personalized IR (PIR) tasks is affected by both\nprivacy concerns and the need for accurate user-related data, which are often\nnot publicly available. Recently, researchers have started to explore the use\nof Large Language Models (LLMs) to generate synthetic datasets, which is a\npossible solution to generate data for low-resource tasks. In this paper, we\ninvestigate the potential of Large Language Models (LLMs) for generating\nsynthetic documents to train an IR system for a Personalized Community Question\nAnswering task. To study the effectiveness of IR models fine-tuned on\nLLM-generated data, we introduce a new dataset, named Sy-SE-PQA. We build\nSy-SE-PQA based on an existing dataset, SE-PQA, which consists of questions and\nanswers posted on the popular StackExchange communities. Starting from\nquestions in SE-PQA, we generate synthetic answers using different prompt\ntechniques and LLMs. Our findings suggest that LLMs have high potential in\ngenerating data tailored to users' needs. The synthetic data can replace\nhuman-written training data, even if the generated data may contain incorrect\ninformation."
                },
                "authors": [
                    {
                        "name": "Marco Braga"
                    },
                    {
                        "name": "Pranav Kasela"
                    },
                    {
                        "name": "Alessandro Raganato"
                    },
                    {
                        "name": "Gabriella Pasi"
                    }
                ],
                "author_detail": {
                    "name": "Gabriella Pasi"
                },
                "author": "Gabriella Pasi",
                "arxiv_comment": "Accepted in WI-IAT '24",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22182v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22182v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22177v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22177v1",
                "updated": "2024-10-29T16:15:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    16,
                    15,
                    59,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T16:15:59Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    16,
                    15,
                    59,
                    1,
                    303,
                    0
                ],
                "title": "Analyzing Multimodal Interaction Strategies for LLM-Assisted\n  Manipulation of 3D Scenes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing Multimodal Interaction Strategies for LLM-Assisted\n  Manipulation of 3D Scenes"
                },
                "summary": "As more applications of large language models (LLMs) for 3D content for\nimmersive environments emerge, it is crucial to study user behaviour to\nidentify interaction patterns and potential barriers to guide the future design\nof immersive content creation and editing systems which involve LLMs. In an\nempirical user study with 12 participants, we combine quantitative usage data\nwith post-experience questionnaire feedback to reveal common interaction\npatterns and key barriers in LLM-assisted 3D scene editing systems. We identify\nopportunities for improving natural language interfaces in 3D design tools and\npropose design recommendations for future LLM-integrated 3D content creation\nsystems. Through an empirical study, we demonstrate that LLM-assisted\ninteractive systems can be used productively in immersive environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As more applications of large language models (LLMs) for 3D content for\nimmersive environments emerge, it is crucial to study user behaviour to\nidentify interaction patterns and potential barriers to guide the future design\nof immersive content creation and editing systems which involve LLMs. In an\nempirical user study with 12 participants, we combine quantitative usage data\nwith post-experience questionnaire feedback to reveal common interaction\npatterns and key barriers in LLM-assisted 3D scene editing systems. We identify\nopportunities for improving natural language interfaces in 3D design tools and\npropose design recommendations for future LLM-integrated 3D content creation\nsystems. Through an empirical study, we demonstrate that LLM-assisted\ninteractive systems can be used productively in immersive environments."
                },
                "authors": [
                    {
                        "name": "Junlong Chen"
                    },
                    {
                        "name": "Jens Grubert"
                    },
                    {
                        "name": "Per Ola Kristensson"
                    }
                ],
                "author_detail": {
                    "name": "Per Ola Kristensson"
                },
                "author": "Per Ola Kristensson",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22177v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22177v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22168v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22168v1",
                "updated": "2024-10-29T16:07:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    16,
                    7,
                    27,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T16:07:27Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    16,
                    7,
                    27,
                    1,
                    303,
                    0
                ],
                "title": "Ultraheavy multiscattering dark matter: DUNE, CYGNUS, kilotonne\n  detectors, and tidal streams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultraheavy multiscattering dark matter: DUNE, CYGNUS, kilotonne\n  detectors, and tidal streams"
                },
                "summary": "In direct searches of dark matter, multi-scatter signatures are now being\nsought to probe scattering cross sections that are large enough to make the\ndetector optically thick to incident particles. We provide some significant\nupdates to the multi-scatter program. Using considerations of energy\ndeposition, we derive the reaches in cross section and mass of various proposed\nlarge-volume detectors: a kilotonne fiducial mass \"module of opportunity\" at\nDUNE, a kilotonne xenon detector suggested for neutrinoless double beta decay,\nthe gaseous detector CYGNUS, and the dark matter detectors XLZD and Argo. Where\nthe velocity vector can be reconstructed event-by-event, the Galactic dark\nmatter velocity distribution may be inferred. We exploit this to show that halo\nsubstructure such as tidal streams can be picked up if they make up about 10%\nof the local dark matter density.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In direct searches of dark matter, multi-scatter signatures are now being\nsought to probe scattering cross sections that are large enough to make the\ndetector optically thick to incident particles. We provide some significant\nupdates to the multi-scatter program. Using considerations of energy\ndeposition, we derive the reaches in cross section and mass of various proposed\nlarge-volume detectors: a kilotonne fiducial mass \"module of opportunity\" at\nDUNE, a kilotonne xenon detector suggested for neutrinoless double beta decay,\nthe gaseous detector CYGNUS, and the dark matter detectors XLZD and Argo. Where\nthe velocity vector can be reconstructed event-by-event, the Galactic dark\nmatter velocity distribution may be inferred. We exploit this to show that halo\nsubstructure such as tidal streams can be picked up if they make up about 10%\nof the local dark matter density."
                },
                "authors": [
                    {
                        "name": "Harsh Aggarwal"
                    },
                    {
                        "name": "Nirmal Raj"
                    }
                ],
                "author_detail": {
                    "name": "Nirmal Raj"
                },
                "author": "Nirmal Raj",
                "arxiv_comment": "9 pages revtex4 + references, 3 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22168v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22168v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22162v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22162v1",
                "updated": "2024-10-29T15:58:47Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    58,
                    47,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T15:58:47Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    58,
                    47,
                    1,
                    303,
                    0
                ],
                "title": "Planet-star interactions with precise transit timing. IV. Probing the\n  regime of dynamical tides for GK host stars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Planet-star interactions with precise transit timing. IV. Probing the\n  regime of dynamical tides for GK host stars"
                },
                "summary": "Statistical studies show that stars of GK spectral types, with masses below\n1.1 Sun mass, are depleted in hot Jupiters. This finding is evidence of tidal\norbital decay during the main-sequence lifetime. Theoretical considerations\nshow that in some configurations, the tidal energy dissipation can be boosted\nby non-linear effects in dynamical tides, which are wave-like responses to\ntidal forcing. To probe the regime of these dynamical tides in GK stars, we\nsearched for orbital period shortening for 6 selected hot Jupiters in systems\nwith 0.8-1 Sun mass host stars: HATS-18, HIP 65A, TrES-3, WASP-19, WASP-43, and\nWASP-173A. For the hot Jupiters of our sample, we analysed transit timing data\nsets based on mid-transit points homogeneously determined from observations\nperformed with the Transiting Exoplanet Survey Satellite and high-quality data\navailable in the literature. For the TrES-3 system, we also used new transit\nlight curves we acquired with ground-based telescopes. The mid-transit times\nwere searched for shortening of orbital periods through statistical testing of\nquadratic transit ephemerides. Theoretical predictions on the dissipation rate\nfor dynamical tides were calculated under the regimes of internal gravity waves\n(IGWs) undergoing wave breaking (WB) in stellar centres and weak non-linear\n(WNL) wave-wave interactions in radiative layers. Stellar parameters of the\nhost stars, such as mass and age, which were used in those computations, were\nhomogeneously redetermined using evolutionary models with the Bayesian\ninference. We found that transit times follow the refined linear ephemerides\nfor all ultra-hot Jupiters of our sample. Non-detection of orbital decay\nallowed us to place lower constraints on the tidal dissipation rates in those\nplanet-star systems. In three systems, HATS-18, WASP-19, and WASP-43, we reject\na scenario with total dissipation of IGWs. We conclude that...",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical studies show that stars of GK spectral types, with masses below\n1.1 Sun mass, are depleted in hot Jupiters. This finding is evidence of tidal\norbital decay during the main-sequence lifetime. Theoretical considerations\nshow that in some configurations, the tidal energy dissipation can be boosted\nby non-linear effects in dynamical tides, which are wave-like responses to\ntidal forcing. To probe the regime of these dynamical tides in GK stars, we\nsearched for orbital period shortening for 6 selected hot Jupiters in systems\nwith 0.8-1 Sun mass host stars: HATS-18, HIP 65A, TrES-3, WASP-19, WASP-43, and\nWASP-173A. For the hot Jupiters of our sample, we analysed transit timing data\nsets based on mid-transit points homogeneously determined from observations\nperformed with the Transiting Exoplanet Survey Satellite and high-quality data\navailable in the literature. For the TrES-3 system, we also used new transit\nlight curves we acquired with ground-based telescopes. The mid-transit times\nwere searched for shortening of orbital periods through statistical testing of\nquadratic transit ephemerides. Theoretical predictions on the dissipation rate\nfor dynamical tides were calculated under the regimes of internal gravity waves\n(IGWs) undergoing wave breaking (WB) in stellar centres and weak non-linear\n(WNL) wave-wave interactions in radiative layers. Stellar parameters of the\nhost stars, such as mass and age, which were used in those computations, were\nhomogeneously redetermined using evolutionary models with the Bayesian\ninference. We found that transit times follow the refined linear ephemerides\nfor all ultra-hot Jupiters of our sample. Non-detection of orbital decay\nallowed us to place lower constraints on the tidal dissipation rates in those\nplanet-star systems. In three systems, HATS-18, WASP-19, and WASP-43, we reject\na scenario with total dissipation of IGWs. We conclude that..."
                },
                "authors": [
                    {
                        "name": "G. Maciejewski"
                    },
                    {
                        "name": "J. Golonka"
                    },
                    {
                        "name": "M. Fernandez"
                    },
                    {
                        "name": "J. Ohlert"
                    },
                    {
                        "name": "V. Casanova"
                    },
                    {
                        "name": "D. Perez Medialdea"
                    }
                ],
                "author_detail": {
                    "name": "D. Perez Medialdea"
                },
                "author": "D. Perez Medialdea",
                "arxiv_comment": "Accepted for publication in A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22162v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22162v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22160v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22160v1",
                "updated": "2024-10-29T15:54:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    54,
                    43,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T15:54:43Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    54,
                    43,
                    1,
                    303,
                    0
                ],
                "title": "A Gaussian Process Generative Model for QCD Equation of State",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Gaussian Process Generative Model for QCD Equation of State"
                },
                "summary": "We develop a generative model for the nuclear matter equation of state at\nzero net baryon density using the Gaussian Process Regression method. We impose\nfirst-principles theoretical constraints from lattice QCD and hadron resonance\ngas at high- and low-temperature regions, respectively. By allowing the trained\nGaussian Process Regression model to vary freely near the phase transition\nregion, we generate random smooth cross-over equations of state with different\nspeeds of sound that do not rely on specific parameterizations. We explore a\ncollection of experimental observable dependencies on the generated equations\nof state, which paves the groundwork for future Bayesian inference studies to\nuse experimental measurements from relativistic heavy-ion collisions to\nconstrain the nuclear matter equation of state.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop a generative model for the nuclear matter equation of state at\nzero net baryon density using the Gaussian Process Regression method. We impose\nfirst-principles theoretical constraints from lattice QCD and hadron resonance\ngas at high- and low-temperature regions, respectively. By allowing the trained\nGaussian Process Regression model to vary freely near the phase transition\nregion, we generate random smooth cross-over equations of state with different\nspeeds of sound that do not rely on specific parameterizations. We explore a\ncollection of experimental observable dependencies on the generated equations\nof state, which paves the groundwork for future Bayesian inference studies to\nuse experimental measurements from relativistic heavy-ion collisions to\nconstrain the nuclear matter equation of state."
                },
                "authors": [
                    {
                        "name": "Jiaxuan Gong"
                    },
                    {
                        "name": "Hendrik Roch"
                    },
                    {
                        "name": "Chun Shen"
                    }
                ],
                "author_detail": {
                    "name": "Chun Shen"
                },
                "author": "Chun Shen",
                "arxiv_comment": "12 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22160v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22160v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "nucl-th",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22159v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22159v2",
                "updated": "2024-10-30T06:11:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    6,
                    11,
                    54,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-29T15:54:09Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    54,
                    9,
                    1,
                    303,
                    0
                ],
                "title": "Training LLMs for Generating IEC 61131-3 Structured Text with Online\n  Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training LLMs for Generating IEC 61131-3 Structured Text with Online\n  Feedback"
                },
                "summary": "The advent of large language models (LLMs), such as GPT-4, has enabled\nsignificant advancements in generating code across various domains. However,\nthese models face unique challenges when generating IEC 61131-3 Structured Text\n(ST) code due to limited data in public training datasets and the complexity of\nST language syntax. This paper proposes a novel approach to training LLMs that\nemphasizes improving the quality of learning data through an online process\ninvolving compiler feedback and evaluation from a secondary LLM. In this\nframework, the primary LLM generates new training samples, which are\nsubsequently evaluated by a compiler for syntactical correctness and by a\nspecialized LLM that excels at assessing semantic accuracy, though it is not\noptimized for code generation itself. Through iterative refinement of the\ntraining data, this approach results in marked improvements for the trained\nLLM, leading to higher compilation success rates and better semantic precision.\nAs a result, the framework proves highly suitable for industrial automation\napplications and outperforms state-of-the-art models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of large language models (LLMs), such as GPT-4, has enabled\nsignificant advancements in generating code across various domains. However,\nthese models face unique challenges when generating IEC 61131-3 Structured Text\n(ST) code due to limited data in public training datasets and the complexity of\nST language syntax. This paper proposes a novel approach to training LLMs that\nemphasizes improving the quality of learning data through an online process\ninvolving compiler feedback and evaluation from a secondary LLM. In this\nframework, the primary LLM generates new training samples, which are\nsubsequently evaluated by a compiler for syntactical correctness and by a\nspecialized LLM that excels at assessing semantic accuracy, though it is not\noptimized for code generation itself. Through iterative refinement of the\ntraining data, this approach results in marked improvements for the trained\nLLM, leading to higher compilation success rates and better semantic precision.\nAs a result, the framework proves highly suitable for industrial automation\napplications and outperforms state-of-the-art models."
                },
                "authors": [
                    {
                        "name": "Aaron Haag"
                    },
                    {
                        "name": "Bertram Fuchs"
                    },
                    {
                        "name": "Altay Kacan"
                    },
                    {
                        "name": "Oliver Lohse"
                    }
                ],
                "author_detail": {
                    "name": "Oliver Lohse"
                },
                "author": "Oliver Lohse",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22159v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22159v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22153v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22153v1",
                "updated": "2024-10-29T15:51:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    51,
                    24,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T15:51:24Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    51,
                    24,
                    1,
                    303,
                    0
                ],
                "title": "Benchmarking LLM Guardrails in Handling Multilingual Toxicity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking LLM Guardrails in Handling Multilingual Toxicity"
                },
                "summary": "With the ubiquity of Large Language Models (LLMs), guardrails have become\ncrucial to detect and defend against toxic content. However, with the\nincreasing pervasiveness of LLMs in multilingual scenarios, their effectiveness\nin handling multilingual toxic inputs remains unclear. In this work, we\nintroduce a comprehensive multilingual test suite, spanning seven datasets and\nover ten languages, to benchmark the performance of state-of-the-art\nguardrails. We also investigates the resilience of guardrails against recent\njailbreaking techniques, and assess the impact of in-context safety policies\nand language resource availability on guardrails' performance. Our findings\nshow that existing guardrails are still ineffective at handling multilingual\ntoxicity and lack robustness against jailbreaking prompts. This work aims to\nidentify the limitations of guardrails and to build a more reliable and\ntrustworthy LLMs in multilingual scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the ubiquity of Large Language Models (LLMs), guardrails have become\ncrucial to detect and defend against toxic content. However, with the\nincreasing pervasiveness of LLMs in multilingual scenarios, their effectiveness\nin handling multilingual toxic inputs remains unclear. In this work, we\nintroduce a comprehensive multilingual test suite, spanning seven datasets and\nover ten languages, to benchmark the performance of state-of-the-art\nguardrails. We also investigates the resilience of guardrails against recent\njailbreaking techniques, and assess the impact of in-context safety policies\nand language resource availability on guardrails' performance. Our findings\nshow that existing guardrails are still ineffective at handling multilingual\ntoxicity and lack robustness against jailbreaking prompts. This work aims to\nidentify the limitations of guardrails and to build a more reliable and\ntrustworthy LLMs in multilingual scenarios."
                },
                "authors": [
                    {
                        "name": "Yahan Yang"
                    },
                    {
                        "name": "Soham Dan"
                    },
                    {
                        "name": "Dan Roth"
                    },
                    {
                        "name": "Insup Lee"
                    }
                ],
                "author_detail": {
                    "name": "Insup Lee"
                },
                "author": "Insup Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22153v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22143v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22143v1",
                "updated": "2024-10-29T15:40:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    40,
                    7,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T15:40:07Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    40,
                    7,
                    1,
                    303,
                    0
                ],
                "title": "AmpleGCG-Plus: A Strong Generative Model of Adversarial Suffixes to\n  Jailbreak LLMs with Higher Success Rates in Fewer Attempts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AmpleGCG-Plus: A Strong Generative Model of Adversarial Suffixes to\n  Jailbreak LLMs with Higher Success Rates in Fewer Attempts"
                },
                "summary": "Although large language models (LLMs) are typically aligned, they remain\nvulnerable to jailbreaking through either carefully crafted prompts in natural\nlanguage or, interestingly, gibberish adversarial suffixes. However, gibberish\ntokens have received relatively less attention despite their success in\nattacking aligned LLMs. Recent work, AmpleGCG~\\citep{liao2024amplegcg},\ndemonstrates that a generative model can quickly produce numerous customizable\ngibberish adversarial suffixes for any harmful query, exposing a range of\nalignment gaps in out-of-distribution (OOD) language spaces. To bring more\nattention to this area, we introduce AmpleGCG-Plus, an enhanced version that\nachieves better performance in fewer attempts. Through a series of exploratory\nexperiments, we identify several training strategies to improve the learning of\ngibberish suffixes. Our results, verified under a strict evaluation setting,\nshow that it outperforms AmpleGCG on both open-weight and closed-source models,\nachieving increases in attack success rate (ASR) of up to 17\\% in the white-box\nsetting against Llama-2-7B-chat, and more than tripling ASR in the black-box\nsetting against GPT-4. Notably, AmpleGCG-Plus jailbreaks the newer GPT-4o\nseries of models at similar rates to GPT-4, and, uncovers vulnerabilities\nagainst the recently proposed circuit breakers defense. We publicly release\nAmpleGCG-Plus along with our collected training datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although large language models (LLMs) are typically aligned, they remain\nvulnerable to jailbreaking through either carefully crafted prompts in natural\nlanguage or, interestingly, gibberish adversarial suffixes. However, gibberish\ntokens have received relatively less attention despite their success in\nattacking aligned LLMs. Recent work, AmpleGCG~\\citep{liao2024amplegcg},\ndemonstrates that a generative model can quickly produce numerous customizable\ngibberish adversarial suffixes for any harmful query, exposing a range of\nalignment gaps in out-of-distribution (OOD) language spaces. To bring more\nattention to this area, we introduce AmpleGCG-Plus, an enhanced version that\nachieves better performance in fewer attempts. Through a series of exploratory\nexperiments, we identify several training strategies to improve the learning of\ngibberish suffixes. Our results, verified under a strict evaluation setting,\nshow that it outperforms AmpleGCG on both open-weight and closed-source models,\nachieving increases in attack success rate (ASR) of up to 17\\% in the white-box\nsetting against Llama-2-7B-chat, and more than tripling ASR in the black-box\nsetting against GPT-4. Notably, AmpleGCG-Plus jailbreaks the newer GPT-4o\nseries of models at similar rates to GPT-4, and, uncovers vulnerabilities\nagainst the recently proposed circuit breakers defense. We publicly release\nAmpleGCG-Plus along with our collected training datasets."
                },
                "authors": [
                    {
                        "name": "Vishal Kumar"
                    },
                    {
                        "name": "Zeyi Liao"
                    },
                    {
                        "name": "Jaylen Jones"
                    },
                    {
                        "name": "Huan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Huan Sun"
                },
                "author": "Huan Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22143v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22143v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22136v1",
                "updated": "2024-10-29T15:32:36Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    32,
                    36,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T15:32:36Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    32,
                    36,
                    1,
                    303,
                    0
                ],
                "title": "SimRec: Mitigating the Cold-Start Problem in Sequential Recommendation\n  by Integrating Item Similarity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SimRec: Mitigating the Cold-Start Problem in Sequential Recommendation\n  by Integrating Item Similarity"
                },
                "summary": "Sequential recommendation systems often struggle to make predictions or take\naction when dealing with cold-start items that have limited amount of\ninteractions. In this work, we propose SimRec - a new approach to mitigate the\ncold-start problem in sequential recommendation systems. SimRec addresses this\nchallenge by leveraging the inherent similarity among items, incorporating item\nsimilarities into the training process through a customized loss function.\nImportantly, this enhancement is attained with identical model architecture and\nthe same amount of trainable parameters, resulting in the same inference time\nand requiring minimal additional effort. This novel approach results in a\nrobust contextual sequential recommendation model capable of effectively\nhandling rare items, including those that were not explicitly seen during\ntraining, thereby enhancing overall recommendation performance. Rigorous\nevaluations against multiple baselines on diverse datasets showcase SimRec's\nsuperiority, particularly in scenarios involving items occurring less than 10\ntimes in the training data. The experiments reveal an impressive improvement,\nwith SimRec achieving up to 78% higher HR@10 compared to SASRec. Notably,\nSimRec outperforms strong baselines on sparse datasets while delivering on-par\nperformance on dense datasets. Our code is available at\nhttps://github.com/amazon-science/sequential-recommendation-using-similarity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential recommendation systems often struggle to make predictions or take\naction when dealing with cold-start items that have limited amount of\ninteractions. In this work, we propose SimRec - a new approach to mitigate the\ncold-start problem in sequential recommendation systems. SimRec addresses this\nchallenge by leveraging the inherent similarity among items, incorporating item\nsimilarities into the training process through a customized loss function.\nImportantly, this enhancement is attained with identical model architecture and\nthe same amount of trainable parameters, resulting in the same inference time\nand requiring minimal additional effort. This novel approach results in a\nrobust contextual sequential recommendation model capable of effectively\nhandling rare items, including those that were not explicitly seen during\ntraining, thereby enhancing overall recommendation performance. Rigorous\nevaluations against multiple baselines on diverse datasets showcase SimRec's\nsuperiority, particularly in scenarios involving items occurring less than 10\ntimes in the training data. The experiments reveal an impressive improvement,\nwith SimRec achieving up to 78% higher HR@10 compared to SASRec. Notably,\nSimRec outperforms strong baselines on sparse datasets while delivering on-par\nperformance on dense datasets. Our code is available at\nhttps://github.com/amazon-science/sequential-recommendation-using-similarity."
                },
                "authors": [
                    {
                        "name": "Shaked Brody"
                    },
                    {
                        "name": "Shoval Lagziel"
                    }
                ],
                "author_detail": {
                    "name": "Shoval Lagziel"
                },
                "author": "Shoval Lagziel",
                "arxiv_comment": "ACM RecSys 2024 Workshop on Context-Aware Recommender Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.11208v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.11208v2",
                "updated": "2024-10-29T15:32:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    32,
                    4,
                    1,
                    303,
                    0
                ],
                "published": "2024-02-17T06:48:45Z",
                "published_parsed": [
                    2024,
                    2,
                    17,
                    6,
                    48,
                    45,
                    5,
                    48,
                    0
                ],
                "title": "Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based\n  Agents"
                },
                "summary": "Driven by the rapid development of Large Language Models (LLMs), LLM-based\nagents have been developed to handle various real-world applications, including\nfinance, healthcare, and shopping, etc. It is crucial to ensure the reliability\nand security of LLM-based agents during applications. However, the safety\nissues of LLM-based agents are currently under-explored. In this work, we take\nthe first step to investigate one of the typical safety threats, backdoor\nattack, to LLM-based agents. We first formulate a general framework of agent\nbackdoor attacks, then we present a thorough analysis of different forms of\nagent backdoor attacks. Specifically, compared with traditional backdoor\nattacks on LLMs that are only able to manipulate the user inputs and model\noutputs, agent backdoor attacks exhibit more diverse and covert forms: (1) From\nthe perspective of the final attacking outcomes, the agent backdoor attacker\ncan not only choose to manipulate the final output distribution, but also\nintroduce the malicious behavior in an intermediate reasoning step only, while\nkeeping the final output correct. (2) Furthermore, the former category can be\ndivided into two subcategories based on trigger locations, in which the\nbackdoor trigger can either be hidden in the user query or appear in an\nintermediate observation returned by the external environment. We implement the\nabove variations of agent backdoor attacks on two typical agent tasks including\nweb shopping and tool utilization. Extensive experiments show that LLM-based\nagents suffer severely from backdoor attacks and such backdoor vulnerability\ncannot be easily mitigated by current textual backdoor defense algorithms. This\nindicates an urgent need for further research on the development of targeted\ndefenses against backdoor attacks on LLM-based agents. Warning: This paper may\ncontain biased content.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Driven by the rapid development of Large Language Models (LLMs), LLM-based\nagents have been developed to handle various real-world applications, including\nfinance, healthcare, and shopping, etc. It is crucial to ensure the reliability\nand security of LLM-based agents during applications. However, the safety\nissues of LLM-based agents are currently under-explored. In this work, we take\nthe first step to investigate one of the typical safety threats, backdoor\nattack, to LLM-based agents. We first formulate a general framework of agent\nbackdoor attacks, then we present a thorough analysis of different forms of\nagent backdoor attacks. Specifically, compared with traditional backdoor\nattacks on LLMs that are only able to manipulate the user inputs and model\noutputs, agent backdoor attacks exhibit more diverse and covert forms: (1) From\nthe perspective of the final attacking outcomes, the agent backdoor attacker\ncan not only choose to manipulate the final output distribution, but also\nintroduce the malicious behavior in an intermediate reasoning step only, while\nkeeping the final output correct. (2) Furthermore, the former category can be\ndivided into two subcategories based on trigger locations, in which the\nbackdoor trigger can either be hidden in the user query or appear in an\nintermediate observation returned by the external environment. We implement the\nabove variations of agent backdoor attacks on two typical agent tasks including\nweb shopping and tool utilization. Extensive experiments show that LLM-based\nagents suffer severely from backdoor attacks and such backdoor vulnerability\ncannot be easily mitigated by current textual backdoor defense algorithms. This\nindicates an urgent need for further research on the development of targeted\ndefenses against backdoor attacks on LLM-based agents. Warning: This paper may\ncontain biased content."
                },
                "authors": [
                    {
                        "name": "Wenkai Yang"
                    },
                    {
                        "name": "Xiaohan Bi"
                    },
                    {
                        "name": "Yankai Lin"
                    },
                    {
                        "name": "Sishuo Chen"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Xu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xu Sun"
                },
                "author": "Xu Sun",
                "arxiv_comment": "Accepted at NeurIPS 2024, camera ready version. Code and data are\n  available at https://github.com/lancopku/agent-backdoor-attacks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.11208v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.11208v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22134v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22134v1",
                "updated": "2024-10-29T15:31:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    31,
                    27,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T15:31:27Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    31,
                    27,
                    1,
                    303,
                    0
                ],
                "title": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching"
                },
                "summary": "The promising applications of large language models are often constrained by\nthe limited GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help mitigate this issue by activating only a subset of the\nmodel's parameters during computation, allowing the unused parameters to be\noffloaded to host memory and reducing overall GPU memory demand. However,\nexisting cache-based offloading solutions handle cache misses reactively and\nsignificantly impact system performance. In this paper, we propose ProMoE, a\nnovel proactive caching system that leverages intermediate model results to\npredict subsequent parameter usage. By proactively fetching experts in advance,\nProMoE removes the loading time from the critical path and diminishes the\nperformance overhead of offloading. Our evaluations demonstrate that ProMoE\nachieves an average speedup of 2.13x and 2.84x in the prefill and decode stages\nrespectively, compared to existing offloading solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The promising applications of large language models are often constrained by\nthe limited GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help mitigate this issue by activating only a subset of the\nmodel's parameters during computation, allowing the unused parameters to be\noffloaded to host memory and reducing overall GPU memory demand. However,\nexisting cache-based offloading solutions handle cache misses reactively and\nsignificantly impact system performance. In this paper, we propose ProMoE, a\nnovel proactive caching system that leverages intermediate model results to\npredict subsequent parameter usage. By proactively fetching experts in advance,\nProMoE removes the loading time from the critical path and diminishes the\nperformance overhead of offloading. Our evaluations demonstrate that ProMoE\nachieves an average speedup of 2.13x and 2.84x in the prefill and decode stages\nrespectively, compared to existing offloading solutions."
                },
                "authors": [
                    {
                        "name": "Xiaoniu Song"
                    },
                    {
                        "name": "Zihang Zhong"
                    },
                    {
                        "name": "Rong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Rong Chen"
                },
                "author": "Rong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22134v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22134v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22129v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22129v1",
                "updated": "2024-10-29T15:28:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    28,
                    19,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T15:28:19Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    28,
                    19,
                    1,
                    303,
                    0
                ],
                "title": "Improving Performance of Commercially Available AI Products in a\n  Multi-Agent Configuration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Performance of Commercially Available AI Products in a\n  Multi-Agent Configuration"
                },
                "summary": "In recent years, with the rapid advancement of large language models (LLMs),\nmulti-agent systems have become increasingly more capable of practical\napplication. At the same time, the software development industry has had a\nnumber of new AI-powered tools developed that improve the software development\nlifecycle (SDLC). Academically, much attention has been paid to the role of\nmulti-agent systems to the SDLC. And, while single-agent systems have\nfrequently been examined in real-world applications, we have seen comparatively\nfew real-world examples of publicly available commercial tools working together\nin a multi-agent system with measurable improvements. In this experiment we\ntest context sharing between Crowdbotics PRD AI, a tool for generating software\nrequirements using AI, and GitHub Copilot, an AI pair-programming tool. By\nsharing business requirements from PRD AI, we improve the code suggestion\ncapabilities of GitHub Copilot by 13.8% and developer task success rate by\n24.5% -- demonstrating a real-world example of commercially-available AI\nsystems working together with improved outcomes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, with the rapid advancement of large language models (LLMs),\nmulti-agent systems have become increasingly more capable of practical\napplication. At the same time, the software development industry has had a\nnumber of new AI-powered tools developed that improve the software development\nlifecycle (SDLC). Academically, much attention has been paid to the role of\nmulti-agent systems to the SDLC. And, while single-agent systems have\nfrequently been examined in real-world applications, we have seen comparatively\nfew real-world examples of publicly available commercial tools working together\nin a multi-agent system with measurable improvements. In this experiment we\ntest context sharing between Crowdbotics PRD AI, a tool for generating software\nrequirements using AI, and GitHub Copilot, an AI pair-programming tool. By\nsharing business requirements from PRD AI, we improve the code suggestion\ncapabilities of GitHub Copilot by 13.8% and developer task success rate by\n24.5% -- demonstrating a real-world example of commercially-available AI\nsystems working together with improved outcomes."
                },
                "authors": [
                    {
                        "name": "Cory Hymel"
                    },
                    {
                        "name": "Sida Peng"
                    },
                    {
                        "name": "Kevin Xu"
                    },
                    {
                        "name": "Charath Ranganathan"
                    }
                ],
                "author_detail": {
                    "name": "Charath Ranganathan"
                },
                "author": "Charath Ranganathan",
                "arxiv_comment": "7 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22129v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22129v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.11652v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.11652v3",
                "updated": "2024-10-29T15:26:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    26,
                    15,
                    1,
                    303,
                    0
                ],
                "published": "2024-02-18T17:13:46Z",
                "published_parsed": [
                    2024,
                    2,
                    18,
                    17,
                    13,
                    46,
                    6,
                    49,
                    0
                ],
                "title": "Doubly Robust Inference in Causal Latent Factor Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Doubly Robust Inference in Causal Latent Factor Models"
                },
                "summary": "This article introduces a new estimator of average treatment effects under\nunobserved confounding in modern data-rich environments featuring large numbers\nof units and outcomes. The proposed estimator is doubly robust, combining\noutcome imputation, inverse probability weighting, and a novel cross-fitting\nprocedure for matrix completion. We derive finite-sample and asymptotic\nguarantees, and show that the error of the new estimator converges to a\nmean-zero Gaussian distribution at a parametric rate. Simulation results\ndemonstrate the relevance of the formal properties of the estimators analyzed\nin this article.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This article introduces a new estimator of average treatment effects under\nunobserved confounding in modern data-rich environments featuring large numbers\nof units and outcomes. The proposed estimator is doubly robust, combining\noutcome imputation, inverse probability weighting, and a novel cross-fitting\nprocedure for matrix completion. We derive finite-sample and asymptotic\nguarantees, and show that the error of the new estimator converges to a\nmean-zero Gaussian distribution at a parametric rate. Simulation results\ndemonstrate the relevance of the formal properties of the estimators analyzed\nin this article."
                },
                "authors": [
                    {
                        "name": "Alberto Abadie"
                    },
                    {
                        "name": "Anish Agarwal"
                    },
                    {
                        "name": "Raaz Dwivedi"
                    },
                    {
                        "name": "Abhin Shah"
                    }
                ],
                "author_detail": {
                    "name": "Abhin Shah"
                },
                "author": "Abhin Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.11652v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.11652v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14822v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14822v2",
                "updated": "2024-10-29T15:26:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    26,
                    0,
                    1,
                    303,
                    0
                ],
                "published": "2024-05-23T17:39:09Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    17,
                    39,
                    9,
                    3,
                    144,
                    0
                ],
                "title": "PaGoDA: Progressive Growing of a One-Step Generator from a\n  Low-Resolution Diffusion Teacher",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PaGoDA: Progressive Growing of a One-Step Generator from a\n  Low-Resolution Diffusion Teacher"
                },
                "summary": "The diffusion model performs remarkable in generating high-dimensional\ncontent but is computationally intensive, especially during training. We\npropose Progressive Growing of Diffusion Autoencoder (PaGoDA), a novel pipeline\nthat reduces the training costs through three stages: training diffusion on\ndownsampled data, distilling the pretrained diffusion, and progressive\nsuper-resolution. With the proposed pipeline, PaGoDA achieves a $64\\times$\nreduced cost in training its diffusion model on 8x downsampled data; while at\nthe inference, with the single-step, it performs state-of-the-art on ImageNet\nacross all resolutions from 64x64 to 512x512, and text-to-image. PaGoDA's\npipeline can be applied directly in the latent space, adding compression\nalongside the pre-trained autoencoder in Latent Diffusion Models (e.g., Stable\nDiffusion). The code is available at https://github.com/sony/pagoda.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The diffusion model performs remarkable in generating high-dimensional\ncontent but is computationally intensive, especially during training. We\npropose Progressive Growing of Diffusion Autoencoder (PaGoDA), a novel pipeline\nthat reduces the training costs through three stages: training diffusion on\ndownsampled data, distilling the pretrained diffusion, and progressive\nsuper-resolution. With the proposed pipeline, PaGoDA achieves a $64\\times$\nreduced cost in training its diffusion model on 8x downsampled data; while at\nthe inference, with the single-step, it performs state-of-the-art on ImageNet\nacross all resolutions from 64x64 to 512x512, and text-to-image. PaGoDA's\npipeline can be applied directly in the latent space, adding compression\nalongside the pre-trained autoencoder in Latent Diffusion Models (e.g., Stable\nDiffusion). The code is available at https://github.com/sony/pagoda."
                },
                "authors": [
                    {
                        "name": "Dongjun Kim"
                    },
                    {
                        "name": "Chieh-Hsin Lai"
                    },
                    {
                        "name": "Wei-Hsiang Liao"
                    },
                    {
                        "name": "Yuhta Takida"
                    },
                    {
                        "name": "Naoki Murata"
                    },
                    {
                        "name": "Toshimitsu Uesaka"
                    },
                    {
                        "name": "Yuki Mitsufuji"
                    },
                    {
                        "name": "Stefano Ermon"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Ermon"
                },
                "author": "Stefano Ermon",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14822v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14822v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05024v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05024v2",
                "updated": "2024-10-29T15:24:47Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    24,
                    47,
                    1,
                    303,
                    0
                ],
                "published": "2024-03-08T04:02:34Z",
                "published_parsed": [
                    2024,
                    3,
                    8,
                    4,
                    2,
                    34,
                    4,
                    68,
                    0
                ],
                "title": "A Probabilistic Hadamard U-Net for MRI Bias Field Correction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Probabilistic Hadamard U-Net for MRI Bias Field Correction"
                },
                "summary": "Magnetic field inhomogeneity correction remains a challenging task in MRI\nanalysis. Most established techniques are designed for brain MRI by supposing\nthat image intensities in the identical tissue follow a uniform distribution.\nSuch an assumption cannot be easily applied to other organs, especially those\nthat are small in size and heterogeneous in texture (large variations in\nintensity), such as the prostate. To address this problem, this paper proposes\na probabilistic Hadamard U-Net (PHU-Net) for prostate MRI bias field\ncorrection. First, a novel Hadamard U-Net (HU-Net) is introduced to extract the\nlow-frequency scalar field, multiplied by the original input to obtain the\nprototypical corrected image. HU-Net converts the input image from the time\ndomain into the frequency domain via Hadamard transform. In the frequency\ndomain, high-frequency components are eliminated using the trainable filter\n(scaling layer), hard-thresholding layer, and sparsity penalty. Next, a\nconditional variational autoencoder is used to encode possible bias\nfield-corrected variants into a low-dimensional latent space. Random samples\ndrawn from latent space are then incorporated with a prototypical corrected\nimage to generate multiple plausible images. Experimental results demonstrate\nthe effectiveness of PHU-Net in correcting bias-field in prostate MRI with a\nfast inference speed. It has also been shown that prostate MRI segmentation\naccuracy improves with the high-quality corrected images from PHU-Net. The code\nwill be available in the final version of this manuscript.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Magnetic field inhomogeneity correction remains a challenging task in MRI\nanalysis. Most established techniques are designed for brain MRI by supposing\nthat image intensities in the identical tissue follow a uniform distribution.\nSuch an assumption cannot be easily applied to other organs, especially those\nthat are small in size and heterogeneous in texture (large variations in\nintensity), such as the prostate. To address this problem, this paper proposes\na probabilistic Hadamard U-Net (PHU-Net) for prostate MRI bias field\ncorrection. First, a novel Hadamard U-Net (HU-Net) is introduced to extract the\nlow-frequency scalar field, multiplied by the original input to obtain the\nprototypical corrected image. HU-Net converts the input image from the time\ndomain into the frequency domain via Hadamard transform. In the frequency\ndomain, high-frequency components are eliminated using the trainable filter\n(scaling layer), hard-thresholding layer, and sparsity penalty. Next, a\nconditional variational autoencoder is used to encode possible bias\nfield-corrected variants into a low-dimensional latent space. Random samples\ndrawn from latent space are then incorporated with a prototypical corrected\nimage to generate multiple plausible images. Experimental results demonstrate\nthe effectiveness of PHU-Net in correcting bias-field in prostate MRI with a\nfast inference speed. It has also been shown that prostate MRI segmentation\naccuracy improves with the high-quality corrected images from PHU-Net. The code\nwill be available in the final version of this manuscript."
                },
                "authors": [
                    {
                        "name": "Xin Zhu"
                    },
                    {
                        "name": "Hongyi Pan"
                    },
                    {
                        "name": "Yury Velichko"
                    },
                    {
                        "name": "Adam B. Murphy"
                    },
                    {
                        "name": "Ashley Ross"
                    },
                    {
                        "name": "Baris Turkbey"
                    },
                    {
                        "name": "Ahmet Enis Cetin"
                    },
                    {
                        "name": "Ulas Bagci"
                    }
                ],
                "author_detail": {
                    "name": "Ulas Bagci"
                },
                "author": "Ulas Bagci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05024v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05024v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22119v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22119v1",
                "updated": "2024-10-29T15:21:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    21,
                    39,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T15:21:39Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    21,
                    39,
                    1,
                    303,
                    0
                ],
                "title": "Deep Q-Exponential Processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Q-Exponential Processes"
                },
                "summary": "Motivated by deep neural networks, the deep Gaussian process (DGP)\ngeneralizes the standard GP by stacking multiple layers of GPs. Despite the\nenhanced expressiveness, GP, as an $L_2$ regularization prior, tends to be\nover-smooth and sub-optimal for inhomogeneous subjects, such as images with\nedges. Recently, Q-exponential process (Q-EP) has been proposed as an $L_q$\nrelaxation to GP and demonstrated with more desirable regularization properties\nthrough a parameter $q>0$ with $q=2$ corresponding to GP. Sharing the similar\ntractability of posterior and predictive distributions with GP, Q-EP can also\nbe stacked to improve its modeling flexibility. In this paper, we generalize\nQ-EP to deep Q-EP to enjoy both proper regularization and improved\nexpressiveness. The generalization is realized by introducing shallow Q-EP as a\nlatent variable model and then building a hierarchy of the shallow Q-EP layers.\nSparse approximation by inducing points and scalable variational strategy are\napplied to facilitate the inference. We demonstrate the numerical advantages of\nthe proposed deep Q-EP model by comparing with multiple state-of-the-art deep\nprobabilistic models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated by deep neural networks, the deep Gaussian process (DGP)\ngeneralizes the standard GP by stacking multiple layers of GPs. Despite the\nenhanced expressiveness, GP, as an $L_2$ regularization prior, tends to be\nover-smooth and sub-optimal for inhomogeneous subjects, such as images with\nedges. Recently, Q-exponential process (Q-EP) has been proposed as an $L_q$\nrelaxation to GP and demonstrated with more desirable regularization properties\nthrough a parameter $q>0$ with $q=2$ corresponding to GP. Sharing the similar\ntractability of posterior and predictive distributions with GP, Q-EP can also\nbe stacked to improve its modeling flexibility. In this paper, we generalize\nQ-EP to deep Q-EP to enjoy both proper regularization and improved\nexpressiveness. The generalization is realized by introducing shallow Q-EP as a\nlatent variable model and then building a hierarchy of the shallow Q-EP layers.\nSparse approximation by inducing points and scalable variational strategy are\napplied to facilitate the inference. We demonstrate the numerical advantages of\nthe proposed deep Q-EP model by comparing with multiple state-of-the-art deep\nprobabilistic models."
                },
                "authors": [
                    {
                        "name": "Zhi Chang"
                    },
                    {
                        "name": "Chukwudi Obite"
                    },
                    {
                        "name": "Shuang Zhou"
                    },
                    {
                        "name": "Shiwei Lan"
                    }
                ],
                "author_detail": {
                    "name": "Shiwei Lan"
                },
                "author": "Shiwei Lan",
                "arxiv_comment": "21 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22119v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22119v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22118v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22118v1",
                "updated": "2024-10-29T15:19:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    19,
                    13,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T15:19:13Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    19,
                    13,
                    1,
                    303,
                    0
                ],
                "title": "The Impact of Inference Acceleration Strategies on Bias of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Inference Acceleration Strategies on Bias of LLMs"
                },
                "summary": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to deeply benefit a vast\narray of application domains. However, due to their immense size, performing\ninference with LLMs is both costly and slow. Consequently, a plethora of recent\nwork has proposed strategies to enhance inference efficiency, e.g.,\nquantization, pruning, and caching. These acceleration strategies reduce the\ninference cost and latency, often by several factors, while maintaining much of\nthe predictive performance measured via common benchmarks. In this work, we\nexplore another critical aspect of LLM performance: demographic bias in model\ngenerations due to inference acceleration optimizations. Using a wide range of\nmetrics, we probe bias in model outputs from a number of angles. Analysis of\noutputs before and after inference acceleration shows significant change in\nbias. Worryingly, these bias effects are complex and unpredictable. A\ncombination of an acceleration strategy and bias type may show little bias\nchange in one model but may lead to a large effect in another. Our results\nhighlight a need for in-depth and case-by-case evaluation of model bias after\nit has been modified to accelerate inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to deeply benefit a vast\narray of application domains. However, due to their immense size, performing\ninference with LLMs is both costly and slow. Consequently, a plethora of recent\nwork has proposed strategies to enhance inference efficiency, e.g.,\nquantization, pruning, and caching. These acceleration strategies reduce the\ninference cost and latency, often by several factors, while maintaining much of\nthe predictive performance measured via common benchmarks. In this work, we\nexplore another critical aspect of LLM performance: demographic bias in model\ngenerations due to inference acceleration optimizations. Using a wide range of\nmetrics, we probe bias in model outputs from a number of angles. Analysis of\noutputs before and after inference acceleration shows significant change in\nbias. Worryingly, these bias effects are complex and unpredictable. A\ncombination of an acceleration strategy and bias type may show little bias\nchange in one model but may lead to a large effect in another. Our results\nhighlight a need for in-depth and case-by-case evaluation of model bias after\nit has been modified to accelerate inference."
                },
                "authors": [
                    {
                        "name": "Elisabeth Kirsten"
                    },
                    {
                        "name": "Ivan Habernal"
                    },
                    {
                        "name": "Vedant Nanda"
                    },
                    {
                        "name": "Muhammad Bilal Zafar"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Bilal Zafar"
                },
                "author": "Muhammad Bilal Zafar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22118v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.06687v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.06687v3",
                "updated": "2024-10-29T15:12:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    12,
                    52,
                    1,
                    303,
                    0
                ],
                "published": "2024-01-12T16:51:02Z",
                "published_parsed": [
                    2024,
                    1,
                    12,
                    16,
                    51,
                    2,
                    4,
                    12,
                    0
                ],
                "title": "Proximal Causal Inference With Text Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proximal Causal Inference With Text Data"
                },
                "summary": "Recent text-based causal methods attempt to mitigate confounding bias by\nestimating proxies of confounding variables that are partially or imperfectly\nmeasured from unstructured text data. These approaches, however, assume\nanalysts have supervised labels of the confounders given text for a subset of\ninstances, a constraint that is sometimes infeasible due to data privacy or\nannotation costs. In this work, we address settings in which an important\nconfounding variable is completely unobserved. We propose a new causal\ninference method that uses two instances of pre-treatment text data, infers two\nproxies using two zero-shot models on the separate instances, and applies these\nproxies in the proximal g-formula. We prove, under certain assumptions about\nthe instances of text and accuracy of the zero-shot predictions, that our\nmethod of inferring text-based proxies satisfies identification conditions of\nthe proximal g-formula while other seemingly reasonable proposals do not. To\naddress untestable assumptions associated with our method and the proximal\ng-formula, we further propose an odds ratio falsification heuristic that flags\nwhen to proceed with downstream effect estimation using the inferred proxies.\nWe evaluate our method in synthetic and semi-synthetic settings -- the latter\nwith real-world clinical notes from MIMIC-III and open large language models\nfor zero-shot prediction -- and find that our method produces estimates with\nlow bias. We believe that this text-based design of proxies allows for the use\nof proximal causal inference in a wider range of scenarios, particularly those\nfor which obtaining suitable proxies from structured data is difficult.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent text-based causal methods attempt to mitigate confounding bias by\nestimating proxies of confounding variables that are partially or imperfectly\nmeasured from unstructured text data. These approaches, however, assume\nanalysts have supervised labels of the confounders given text for a subset of\ninstances, a constraint that is sometimes infeasible due to data privacy or\nannotation costs. In this work, we address settings in which an important\nconfounding variable is completely unobserved. We propose a new causal\ninference method that uses two instances of pre-treatment text data, infers two\nproxies using two zero-shot models on the separate instances, and applies these\nproxies in the proximal g-formula. We prove, under certain assumptions about\nthe instances of text and accuracy of the zero-shot predictions, that our\nmethod of inferring text-based proxies satisfies identification conditions of\nthe proximal g-formula while other seemingly reasonable proposals do not. To\naddress untestable assumptions associated with our method and the proximal\ng-formula, we further propose an odds ratio falsification heuristic that flags\nwhen to proceed with downstream effect estimation using the inferred proxies.\nWe evaluate our method in synthetic and semi-synthetic settings -- the latter\nwith real-world clinical notes from MIMIC-III and open large language models\nfor zero-shot prediction -- and find that our method produces estimates with\nlow bias. We believe that this text-based design of proxies allows for the use\nof proximal causal inference in a wider range of scenarios, particularly those\nfor which obtaining suitable proxies from structured data is difficult."
                },
                "authors": [
                    {
                        "name": "Jacob M. Chen"
                    },
                    {
                        "name": "Rohit Bhattacharya"
                    },
                    {
                        "name": "Katherine A. Keith"
                    }
                ],
                "author_detail": {
                    "name": "Katherine A. Keith"
                },
                "author": "Katherine A. Keith",
                "arxiv_comment": "35 pages; Accepted at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.06687v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.06687v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.08925v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.08925v3",
                "updated": "2024-10-30T03:04:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    3,
                    4,
                    21,
                    2,
                    304,
                    0
                ],
                "published": "2023-07-18T02:09:14Z",
                "published_parsed": [
                    2023,
                    7,
                    18,
                    2,
                    9,
                    14,
                    1,
                    199,
                    0
                ],
                "title": "Integration of Large Language Models and Federated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integration of Large Language Models and Federated Learning"
                },
                "summary": "As the parameter size of Large Language Models (LLMs) continues to expand,\nthere is an urgent need to address the scarcity of high-quality data. In\nresponse, existing research has attempted to make a breakthrough by\nincorporating Federated Learning (FL) into LLMs. Conversely, considering the\noutstanding performance of LLMs in task generalization, researchers have also\ntried applying LLMs within FL to tackle challenges in relevant domains. The\ncomplementarity between LLMs and FL has already ignited widespread research\ninterest. In this paper, we aim to deeply explore the integration of LLMs and\nFL. We propose a research framework, dividing the fusion of LLMs and FL into\nthree parts: the combination of LLM sub-technologies with FL, the integration\nof FL sub-technologies with LLMs, and the overall merger of LLMs and FL. We\nfirst provide a comprehensive review of the current state of research in the\ndomain of LLMs combined with FL, including their typical applications,\nintegration advantages, challenges faced, and future directions for resolution.\nSubsequently, we discuss the practical applications of the combination of LLMs\nand FL in critical scenarios such as healthcare, finance, and education, and\nprovide new perspectives and insights into future research directions for LLMs\nand FL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the parameter size of Large Language Models (LLMs) continues to expand,\nthere is an urgent need to address the scarcity of high-quality data. In\nresponse, existing research has attempted to make a breakthrough by\nincorporating Federated Learning (FL) into LLMs. Conversely, considering the\noutstanding performance of LLMs in task generalization, researchers have also\ntried applying LLMs within FL to tackle challenges in relevant domains. The\ncomplementarity between LLMs and FL has already ignited widespread research\ninterest. In this paper, we aim to deeply explore the integration of LLMs and\nFL. We propose a research framework, dividing the fusion of LLMs and FL into\nthree parts: the combination of LLM sub-technologies with FL, the integration\nof FL sub-technologies with LLMs, and the overall merger of LLMs and FL. We\nfirst provide a comprehensive review of the current state of research in the\ndomain of LLMs combined with FL, including their typical applications,\nintegration advantages, challenges faced, and future directions for resolution.\nSubsequently, we discuss the practical applications of the combination of LLMs\nand FL in critical scenarios such as healthcare, finance, and education, and\nprovide new perspectives and insights into future research directions for LLMs\nand FL."
                },
                "authors": [
                    {
                        "name": "Chaochao Chen"
                    },
                    {
                        "name": "Xiaohua Feng"
                    },
                    {
                        "name": "Yuyuan Li"
                    },
                    {
                        "name": "Lingjuan Lyu"
                    },
                    {
                        "name": "Jun Zhou"
                    },
                    {
                        "name": "Xiaolin Zheng"
                    },
                    {
                        "name": "Jianwei Yin"
                    }
                ],
                "author_detail": {
                    "name": "Jianwei Yin"
                },
                "author": "Jianwei Yin",
                "arxiv_comment": "Accepted by Cell Patterns",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2307.08925v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.08925v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22108v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22108v1",
                "updated": "2024-10-29T15:07:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    7,
                    23,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T15:07:23Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    7,
                    23,
                    1,
                    303,
                    0
                ],
                "title": "Protecting Privacy in Multimodal Large Language Models with MLLMU-Bench",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Protecting Privacy in Multimodal Large Language Models with MLLMU-Bench"
                },
                "summary": "Generative models such as Large Language Models (LLM) and Multimodal Large\nLanguage models (MLLMs) trained on massive web corpora can memorize and\ndisclose individuals' confidential and private data, raising legal and ethical\nconcerns. While many previous works have addressed this issue in LLM via\nmachine unlearning, it remains largely unexplored for MLLMs. To tackle this\nchallenge, we introduce Multimodal Large Language Model Unlearning Benchmark\n(MLLMU-Bench), a novel benchmark aimed at advancing the understanding of\nmultimodal machine unlearning. MLLMU-Bench consists of 500 fictitious profiles\nand 153 profiles for public celebrities, each profile feature over 14\ncustomized question-answer pairs, evaluated from both multimodal (image+text)\nand unimodal (text) perspectives. The benchmark is divided into four sets to\nassess unlearning algorithms in terms of efficacy, generalizability, and model\nutility. Finally, we provide baseline results using existing generative model\nunlearning algorithms. Surprisingly, our experiments show that unimodal\nunlearning algorithms excel in generation and cloze tasks, while multimodal\nunlearning approaches perform better in classification tasks with multimodal\ninputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative models such as Large Language Models (LLM) and Multimodal Large\nLanguage models (MLLMs) trained on massive web corpora can memorize and\ndisclose individuals' confidential and private data, raising legal and ethical\nconcerns. While many previous works have addressed this issue in LLM via\nmachine unlearning, it remains largely unexplored for MLLMs. To tackle this\nchallenge, we introduce Multimodal Large Language Model Unlearning Benchmark\n(MLLMU-Bench), a novel benchmark aimed at advancing the understanding of\nmultimodal machine unlearning. MLLMU-Bench consists of 500 fictitious profiles\nand 153 profiles for public celebrities, each profile feature over 14\ncustomized question-answer pairs, evaluated from both multimodal (image+text)\nand unimodal (text) perspectives. The benchmark is divided into four sets to\nassess unlearning algorithms in terms of efficacy, generalizability, and model\nutility. Finally, we provide baseline results using existing generative model\nunlearning algorithms. Surprisingly, our experiments show that unimodal\nunlearning algorithms excel in generation and cloze tasks, while multimodal\nunlearning approaches perform better in classification tasks with multimodal\ninputs."
                },
                "authors": [
                    {
                        "name": "Zheyuan Liu"
                    },
                    {
                        "name": "Guangyao Dou"
                    },
                    {
                        "name": "Mengzhao Jia"
                    },
                    {
                        "name": "Zhaoxuan Tan"
                    },
                    {
                        "name": "Qingkai Zeng"
                    },
                    {
                        "name": "Yongle Yuan"
                    },
                    {
                        "name": "Meng Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Meng Jiang"
                },
                "author": "Meng Jiang",
                "arxiv_comment": "30 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22108v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22108v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22103v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22103v1",
                "updated": "2024-10-29T15:00:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    0,
                    40,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T15:00:40Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    0,
                    40,
                    1,
                    303,
                    0
                ],
                "title": "Joint Extraction and Classification of Danish Competences for Job\n  Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Extraction and Classification of Danish Competences for Job\n  Matching"
                },
                "summary": "The matching of competences, such as skills, occupations or knowledges, is a\nkey desiderata for candidates to be fit for jobs. Automatic extraction of\ncompetences from CVs and Jobs can greatly promote recruiters' productivity in\nlocating relevant candidates for job vacancies. This work presents the first\nmodel that jointly extracts and classifies competence from Danish job postings.\nDifferent from existing works on skill extraction and skill classification, our\nmodel is trained on a large volume of annotated Danish corpora and is capable\nof extracting a wide range of Danish competences, including skills, occupations\nand knowledges of different categories. More importantly, as a single BERT-like\narchitecture for joint extraction and classification, our model is lightweight\nand efficient at inference. On a real-scenario job matching dataset, our model\nbeats the state-of-the-art models in the overall performance of Danish\ncompetence extraction and classification, and saves over 50% time at inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The matching of competences, such as skills, occupations or knowledges, is a\nkey desiderata for candidates to be fit for jobs. Automatic extraction of\ncompetences from CVs and Jobs can greatly promote recruiters' productivity in\nlocating relevant candidates for job vacancies. This work presents the first\nmodel that jointly extracts and classifies competence from Danish job postings.\nDifferent from existing works on skill extraction and skill classification, our\nmodel is trained on a large volume of annotated Danish corpora and is capable\nof extracting a wide range of Danish competences, including skills, occupations\nand knowledges of different categories. More importantly, as a single BERT-like\narchitecture for joint extraction and classification, our model is lightweight\nand efficient at inference. On a real-scenario job matching dataset, our model\nbeats the state-of-the-art models in the overall performance of Danish\ncompetence extraction and classification, and saves over 50% time at inference."
                },
                "authors": [
                    {
                        "name": "Qiuchi Li"
                    },
                    {
                        "name": "Christina Lioma"
                    }
                ],
                "author_detail": {
                    "name": "Christina Lioma"
                },
                "author": "Christina Lioma",
                "arxiv_doi": "10.1007/978-3-031-28238-6_38",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-28238-6_38",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.22103v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22103v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Advances in Information Retrieval. ECIR 2023.Lecture Notes in\n  Computer Science, vol 13981. Springer, Cham",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22099v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22099v1",
                "updated": "2024-10-29T14:53:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    14,
                    53,
                    10,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T14:53:10Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    14,
                    53,
                    10,
                    1,
                    303,
                    0
                ],
                "title": "TractShapeNet: Efficient Multi-Shape Learning with 3D Tractography Point\n  Clouds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TractShapeNet: Efficient Multi-Shape Learning with 3D Tractography Point\n  Clouds"
                },
                "summary": "Brain imaging studies have demonstrated that diffusion MRI tractography\ngeometric shape descriptors can inform the study of the brain's white matter\npathways and their relationship to brain function. In this work, we investigate\nthe possibility of utilizing a deep learning model to compute shape measures of\nthe brain's white matter connections. We introduce a novel framework,\nTractShapeNet, that leverages a point cloud representation of tractography to\ncompute five shape measures: length, span, volume, total surface area, and\nirregularity. We assess the performance of the method on a large dataset\nincluding 1065 healthy young adults. Experiments for shape measure computation\ndemonstrate that our proposed TractShapeNet outperforms other point cloud-based\nneural network models in both the Pearson correlation coefficient and\nnormalized error metrics. We compare the inference runtime results with the\nconventional shape computation tool DSI-Studio. Our results demonstrate that a\ndeep learning approach enables faster and more efficient shape measure\ncomputation. We also conduct experiments on two downstream language cognition\nprediction tasks, showing that shape measures from TractShapeNet perform\nsimilarly to those computed by DSI-Studio. Our code will be available at:\nhttps://github.com/SlicerDMRI/TractShapeNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Brain imaging studies have demonstrated that diffusion MRI tractography\ngeometric shape descriptors can inform the study of the brain's white matter\npathways and their relationship to brain function. In this work, we investigate\nthe possibility of utilizing a deep learning model to compute shape measures of\nthe brain's white matter connections. We introduce a novel framework,\nTractShapeNet, that leverages a point cloud representation of tractography to\ncompute five shape measures: length, span, volume, total surface area, and\nirregularity. We assess the performance of the method on a large dataset\nincluding 1065 healthy young adults. Experiments for shape measure computation\ndemonstrate that our proposed TractShapeNet outperforms other point cloud-based\nneural network models in both the Pearson correlation coefficient and\nnormalized error metrics. We compare the inference runtime results with the\nconventional shape computation tool DSI-Studio. Our results demonstrate that a\ndeep learning approach enables faster and more efficient shape measure\ncomputation. We also conduct experiments on two downstream language cognition\nprediction tasks, showing that shape measures from TractShapeNet perform\nsimilarly to those computed by DSI-Studio. Our code will be available at:\nhttps://github.com/SlicerDMRI/TractShapeNet."
                },
                "authors": [
                    {
                        "name": "Yui Lo"
                    },
                    {
                        "name": "Yuqian Chen"
                    },
                    {
                        "name": "Dongnan Liu"
                    },
                    {
                        "name": "Jon Haitz Legarreta"
                    },
                    {
                        "name": "Leo Zekelman"
                    },
                    {
                        "name": "Fan Zhang"
                    },
                    {
                        "name": "Jarrett Rushmore"
                    },
                    {
                        "name": "Yogesh Rathi"
                    },
                    {
                        "name": "Nikos Makris"
                    },
                    {
                        "name": "Alexandra J. Golby"
                    },
                    {
                        "name": "Weidong Cai"
                    },
                    {
                        "name": "Lauren J. O'Donnell"
                    }
                ],
                "author_detail": {
                    "name": "Lauren J. O'Donnell"
                },
                "author": "Lauren J. O'Donnell",
                "arxiv_comment": "10 pages, 2 figures, 4 tables. This work has been submitted to the\n  IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22099v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22099v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22086v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22086v1",
                "updated": "2024-10-29T14:41:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    14,
                    41,
                    44,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T14:41:44Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    14,
                    41,
                    44,
                    1,
                    303,
                    0
                ],
                "title": "Unlearning as multi-task optimization: A normalized gradient difference\n  approach with an adaptive learning rate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlearning as multi-task optimization: A normalized gradient difference\n  approach with an adaptive learning rate"
                },
                "summary": "Machine unlearning has been used to remove unwanted knowledge acquired by\nlarge language models (LLMs). In this paper, we examine machine unlearning from\nan optimization perspective, framing it as a regularized multi-task\noptimization problem, where one task optimizes a forgetting objective and\nanother optimizes the model performance. In particular, we introduce a\nnormalized gradient difference (NGDiff) algorithm, enabling us to have better\ncontrol over the trade-off between the objectives, while integrating a new,\nautomatic learning rate scheduler. We provide a theoretical analysis and\nempirically demonstrate the superior performance of NGDiff among\nstate-of-the-art unlearning methods on the TOFU and MUSE datasets while\nexhibiting stable training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine unlearning has been used to remove unwanted knowledge acquired by\nlarge language models (LLMs). In this paper, we examine machine unlearning from\nan optimization perspective, framing it as a regularized multi-task\noptimization problem, where one task optimizes a forgetting objective and\nanother optimizes the model performance. In particular, we introduce a\nnormalized gradient difference (NGDiff) algorithm, enabling us to have better\ncontrol over the trade-off between the objectives, while integrating a new,\nautomatic learning rate scheduler. We provide a theoretical analysis and\nempirically demonstrate the superior performance of NGDiff among\nstate-of-the-art unlearning methods on the TOFU and MUSE datasets while\nexhibiting stable training."
                },
                "authors": [
                    {
                        "name": "Zhiqi Bu"
                    },
                    {
                        "name": "Xiaomeng Jin"
                    },
                    {
                        "name": "Bhanukiran Vinzamuri"
                    },
                    {
                        "name": "Anil Ramakrishna"
                    },
                    {
                        "name": "Kai-Wei Chang"
                    },
                    {
                        "name": "Volkan Cevher"
                    },
                    {
                        "name": "Mingyi Hong"
                    }
                ],
                "author_detail": {
                    "name": "Mingyi Hong"
                },
                "author": "Mingyi Hong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22086v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22086v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03523v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03523v2",
                "updated": "2024-10-29T14:39:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    14,
                    39,
                    9,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-04T15:44:23Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    15,
                    44,
                    23,
                    4,
                    278,
                    0
                ],
                "title": "A Probabilistic Perspective on Unlearning and Alignment for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Probabilistic Perspective on Unlearning and Alignment for Large\n  Language Models"
                },
                "summary": "Comprehensive evaluation of Large Language Models (LLMs) is an open research\nproblem. Existing evaluations rely on deterministic point estimates generated\nvia greedy decoding. However, we find that deterministic evaluations fail to\ncapture the whole output distribution of a model, yielding inaccurate\nestimations of model capabilities. This is particularly problematic in critical\ncontexts such as unlearning and alignment, where precise model evaluations are\ncrucial. To remedy this, we introduce the first formal probabilistic evaluation\nframework in LLMs. Namely, we derive novel metrics with high-probability\nguarantees concerning the output distribution of a model. Our metrics are\napplication-independent and allow practitioners to make more reliable estimates\nabout model capabilities before deployment. Through a case study focused on\nunlearning, we reveal that deterministic evaluations falsely indicate\nsuccessful unlearning, whereas our probabilistic evaluations demonstrate that\nmost if not all of the supposedly unlearned information remains accessible in\nthese models. Additionally, we propose a novel unlearning loss based on entropy\noptimization and adaptive temperature scaling, which significantly improves\nunlearning in probabilistic settings on recent benchmarks. Our proposed shift\nfrom point estimates to probabilistic evaluations of output distributions\nrepresents an important step toward comprehensive evaluations of LLMs. Code\navailable at https://github.com/yascho/probabilistic-unlearning",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comprehensive evaluation of Large Language Models (LLMs) is an open research\nproblem. Existing evaluations rely on deterministic point estimates generated\nvia greedy decoding. However, we find that deterministic evaluations fail to\ncapture the whole output distribution of a model, yielding inaccurate\nestimations of model capabilities. This is particularly problematic in critical\ncontexts such as unlearning and alignment, where precise model evaluations are\ncrucial. To remedy this, we introduce the first formal probabilistic evaluation\nframework in LLMs. Namely, we derive novel metrics with high-probability\nguarantees concerning the output distribution of a model. Our metrics are\napplication-independent and allow practitioners to make more reliable estimates\nabout model capabilities before deployment. Through a case study focused on\nunlearning, we reveal that deterministic evaluations falsely indicate\nsuccessful unlearning, whereas our probabilistic evaluations demonstrate that\nmost if not all of the supposedly unlearned information remains accessible in\nthese models. Additionally, we propose a novel unlearning loss based on entropy\noptimization and adaptive temperature scaling, which significantly improves\nunlearning in probabilistic settings on recent benchmarks. Our proposed shift\nfrom point estimates to probabilistic evaluations of output distributions\nrepresents an important step toward comprehensive evaluations of LLMs. Code\navailable at https://github.com/yascho/probabilistic-unlearning"
                },
                "authors": [
                    {
                        "name": "Yan Scholten"
                    },
                    {
                        "name": "Stephan Gnnemann"
                    },
                    {
                        "name": "Leo Schwinn"
                    }
                ],
                "author_detail": {
                    "name": "Leo Schwinn"
                },
                "author": "Leo Schwinn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03523v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03523v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22074v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22074v1",
                "updated": "2024-10-29T14:33:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    14,
                    33,
                    52,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T14:33:52Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    14,
                    33,
                    52,
                    1,
                    303,
                    0
                ],
                "title": "Variational inference for pile-up removal at hadron colliders with\n  diffusion models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variational inference for pile-up removal at hadron colliders with\n  diffusion models"
                },
                "summary": "In this paper, we present a novel method for pile-up removal of pp\ninteractions using variational inference with diffusion models, called Vipr.\nInstead of using classification methods to identify which particles are from\nthe primary collision, a generative model is trained to predict the\nconstituents of the hard-scatter particle jets with pile-up removed. This\nresults in an estimate of the full posterior over hard-scatter jet\nconstituents, which has not yet been explored in the context of pile-up\nremoval. We evaluate the performance of Vipr in a sample of jets from simulated\n$t\\bar{t}$ events overlain with pile-up contamination. Vipr outperforms\nSoftDrop in predicting the substructure of the hard-scatter jets over a wide\nrange of pile-up scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present a novel method for pile-up removal of pp\ninteractions using variational inference with diffusion models, called Vipr.\nInstead of using classification methods to identify which particles are from\nthe primary collision, a generative model is trained to predict the\nconstituents of the hard-scatter particle jets with pile-up removed. This\nresults in an estimate of the full posterior over hard-scatter jet\nconstituents, which has not yet been explored in the context of pile-up\nremoval. We evaluate the performance of Vipr in a sample of jets from simulated\n$t\\bar{t}$ events overlain with pile-up contamination. Vipr outperforms\nSoftDrop in predicting the substructure of the hard-scatter jets over a wide\nrange of pile-up scenarios."
                },
                "authors": [
                    {
                        "name": "Malte Algren"
                    },
                    {
                        "name": "Christopher Pollard"
                    },
                    {
                        "name": "John Andrew Raine"
                    },
                    {
                        "name": "Tobias Golling"
                    }
                ],
                "author_detail": {
                    "name": "Tobias Golling"
                },
                "author": "Tobias Golling",
                "arxiv_comment": "19 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22074v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22074v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22071v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22071v1",
                "updated": "2024-10-29T14:31:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    14,
                    31,
                    33,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T14:31:33Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    14,
                    31,
                    33,
                    1,
                    303,
                    0
                ],
                "title": "Distinguishing Ignorance from Error in LLM Hallucinations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distinguishing Ignorance from Error in LLM Hallucinations"
                },
                "summary": "Large language models (LLMs) are susceptible to hallucinations-outputs that\nare ungrounded, factually incorrect, or inconsistent with prior generations. We\nfocus on close-book Question Answering (CBQA), where previous work has not\nfully addressed the distinction between two possible kinds of hallucinations,\nnamely, whether the model (1) does not hold the correct answer in its\nparameters or (2) answers incorrectly despite having the required knowledge. We\nargue that distinguishing these cases is crucial for detecting and mitigating\nhallucinations. Specifically, case (2) may be mitigated by intervening in the\nmodel's internal computation, as the knowledge resides within the model's\nparameters. In contrast, in case (1) there is no parametric knowledge to\nleverage for mitigation, so it should be addressed by resorting to an external\nknowledge source or abstaining. To help distinguish between the two cases, we\nintroduce Wrong Answer despite having Correct Knowledge (WACK), an approach for\nconstructing model-specific datasets for the second hallucination type. Our\nprobing experiments indicate that the two kinds of hallucinations are\nrepresented differently in the model's inner states. Next, we show that\ndatasets constructed using WACK exhibit variations across models, demonstrating\nthat even when models share knowledge of certain facts, they still vary in the\nspecific examples that lead to hallucinations. Finally, we show that training a\nprobe on our WACK datasets leads to better hallucination detection of case (2)\nhallucinations than using the common generic one-size-fits-all datasets. The\ncode is available at\nhttps://github.com/technion-cs-nlp/hallucination-mitigation .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are susceptible to hallucinations-outputs that\nare ungrounded, factually incorrect, or inconsistent with prior generations. We\nfocus on close-book Question Answering (CBQA), where previous work has not\nfully addressed the distinction between two possible kinds of hallucinations,\nnamely, whether the model (1) does not hold the correct answer in its\nparameters or (2) answers incorrectly despite having the required knowledge. We\nargue that distinguishing these cases is crucial for detecting and mitigating\nhallucinations. Specifically, case (2) may be mitigated by intervening in the\nmodel's internal computation, as the knowledge resides within the model's\nparameters. In contrast, in case (1) there is no parametric knowledge to\nleverage for mitigation, so it should be addressed by resorting to an external\nknowledge source or abstaining. To help distinguish between the two cases, we\nintroduce Wrong Answer despite having Correct Knowledge (WACK), an approach for\nconstructing model-specific datasets for the second hallucination type. Our\nprobing experiments indicate that the two kinds of hallucinations are\nrepresented differently in the model's inner states. Next, we show that\ndatasets constructed using WACK exhibit variations across models, demonstrating\nthat even when models share knowledge of certain facts, they still vary in the\nspecific examples that lead to hallucinations. Finally, we show that training a\nprobe on our WACK datasets leads to better hallucination detection of case (2)\nhallucinations than using the common generic one-size-fits-all datasets. The\ncode is available at\nhttps://github.com/technion-cs-nlp/hallucination-mitigation ."
                },
                "authors": [
                    {
                        "name": "Adi Simhi"
                    },
                    {
                        "name": "Jonathan Herzig"
                    },
                    {
                        "name": "Idan Szpektor"
                    },
                    {
                        "name": "Yonatan Belinkov"
                    }
                ],
                "author_detail": {
                    "name": "Yonatan Belinkov"
                },
                "author": "Yonatan Belinkov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22071v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22071v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15938v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15938v3",
                "updated": "2024-10-29T14:28:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    14,
                    28,
                    24,
                    1,
                    303,
                    0
                ],
                "published": "2024-06-22T20:57:12Z",
                "published_parsed": [
                    2024,
                    6,
                    22,
                    20,
                    57,
                    12,
                    5,
                    174,
                    0
                ],
                "title": "RuleR: Improving LLM Controllability by Rule-based Data Recycling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RuleR: Improving LLM Controllability by Rule-based Data Recycling"
                },
                "summary": "Despite the remarkable advancement of Large language models (LLMs), they\nstill lack delicate controllability under sophisticated constraints, which is\ncritical to enhancing their response quality and the user experience. While\nconditional supervised fine-tuning (SFT) can potentially improve LLM\ncontrollability, curating new SFT data to fulfill the constraints usually\nrelies on human experts or proprietary LLMs, which is time-consuming and\nexpensive. To bridge this gap, we propose Rule-based Data Recycling (RuleR), a\nhuman/LLM-free data augmentation method incorporating multiple constraints into\nthe original SFT data. Instead of creating new responses from scratch, RuleR\nintegrates linguistic or formatting rules into the original instructions and\nmodifies the responses to fulfill the rule-defined constraints. Training on the\n\"recycled\" data consolidates LLMs capability to generate constrained outputs.\nExtensive experiments demonstrate RuleR's effectiveness in improving LLM\ncontrollability while maintaining general instruction-following performance.\nRuleR's code is released on https://github.com/tianyi-lab/RuleR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable advancement of Large language models (LLMs), they\nstill lack delicate controllability under sophisticated constraints, which is\ncritical to enhancing their response quality and the user experience. While\nconditional supervised fine-tuning (SFT) can potentially improve LLM\ncontrollability, curating new SFT data to fulfill the constraints usually\nrelies on human experts or proprietary LLMs, which is time-consuming and\nexpensive. To bridge this gap, we propose Rule-based Data Recycling (RuleR), a\nhuman/LLM-free data augmentation method incorporating multiple constraints into\nthe original SFT data. Instead of creating new responses from scratch, RuleR\nintegrates linguistic or formatting rules into the original instructions and\nmodifies the responses to fulfill the rule-defined constraints. Training on the\n\"recycled\" data consolidates LLMs capability to generate constrained outputs.\nExtensive experiments demonstrate RuleR's effectiveness in improving LLM\ncontrollability while maintaining general instruction-following performance.\nRuleR's code is released on https://github.com/tianyi-lab/RuleR."
                },
                "authors": [
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Han Chen"
                    },
                    {
                        "name": "Chenguang Wang"
                    },
                    {
                        "name": "Dang Nguyen"
                    },
                    {
                        "name": "Dianqi Li"
                    },
                    {
                        "name": "Tianyi Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Tianyi Zhou"
                },
                "author": "Tianyi Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15938v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15938v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22066v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22066v1",
                "updated": "2024-10-29T14:23:56Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    14,
                    23,
                    56,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T14:23:56Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    14,
                    23,
                    56,
                    1,
                    303,
                    0
                ],
                "title": "Sing it, Narrate it: Quality Musical Lyrics Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sing it, Narrate it: Quality Musical Lyrics Translation"
                },
                "summary": "Translating lyrics for musicals presents unique challenges due to the need to\nensure high translation quality while adhering to singability requirements such\nas length and rhyme. Existing song translation approaches often prioritize\nthese singability constraints at the expense of translation quality, which is\ncrucial for musicals. This paper aims to enhance translation quality while\nmaintaining key singability features. Our method consists of three main\ncomponents. First, we create a dataset to train reward models for the automatic\nevaluation of translation quality. Second, to enhance both singability and\ntranslation quality, we implement a two-stage training process with filtering\ntechniques. Finally, we introduce an inference-time optimization framework for\ntranslating entire songs. Extensive experiments, including both automatic and\nhuman evaluations, demonstrate significant improvements over baseline methods\nand validate the effectiveness of each component in our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Translating lyrics for musicals presents unique challenges due to the need to\nensure high translation quality while adhering to singability requirements such\nas length and rhyme. Existing song translation approaches often prioritize\nthese singability constraints at the expense of translation quality, which is\ncrucial for musicals. This paper aims to enhance translation quality while\nmaintaining key singability features. Our method consists of three main\ncomponents. First, we create a dataset to train reward models for the automatic\nevaluation of translation quality. Second, to enhance both singability and\ntranslation quality, we implement a two-stage training process with filtering\ntechniques. Finally, we introduce an inference-time optimization framework for\ntranslating entire songs. Extensive experiments, including both automatic and\nhuman evaluations, demonstrate significant improvements over baseline methods\nand validate the effectiveness of each component in our approach."
                },
                "authors": [
                    {
                        "name": "Zhuorui Ye"
                    },
                    {
                        "name": "Jinhan Li"
                    },
                    {
                        "name": "Rongwu Xu"
                    }
                ],
                "author_detail": {
                    "name": "Rongwu Xu"
                },
                "author": "Rongwu Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22066v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22066v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22065v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22065v1",
                "updated": "2024-10-29T14:23:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    14,
                    23,
                    42,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T14:23:42Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    14,
                    23,
                    42,
                    1,
                    303,
                    0
                ],
                "title": "Hamiltonian Monte Carlo on ReLU Neural Networks is Inefficient",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hamiltonian Monte Carlo on ReLU Neural Networks is Inefficient"
                },
                "summary": "We analyze the error rates of the Hamiltonian Monte Carlo algorithm with\nleapfrog integrator for Bayesian neural network inference. We show that due to\nthe non-differentiability of activation functions in the ReLU family, leapfrog\nHMC for networks with these activation functions has a large local error rate\nof $\\Omega(\\epsilon)$ rather than the classical error rate of $O(\\epsilon^3)$.\nThis leads to a higher rejection rate of the proposals, making the method\ninefficient. We then verify our theoretical findings through empirical\nsimulations as well as experiments on a real-world dataset that highlight the\ninefficiency of HMC inference on ReLU-based neural networks compared to\nanalytical networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We analyze the error rates of the Hamiltonian Monte Carlo algorithm with\nleapfrog integrator for Bayesian neural network inference. We show that due to\nthe non-differentiability of activation functions in the ReLU family, leapfrog\nHMC for networks with these activation functions has a large local error rate\nof $\\Omega(\\epsilon)$ rather than the classical error rate of $O(\\epsilon^3)$.\nThis leads to a higher rejection rate of the proposals, making the method\ninefficient. We then verify our theoretical findings through empirical\nsimulations as well as experiments on a real-world dataset that highlight the\ninefficiency of HMC inference on ReLU-based neural networks compared to\nanalytical networks."
                },
                "authors": [
                    {
                        "name": "Vu C. Dinh"
                    },
                    {
                        "name": "Lam Si Tung Ho"
                    },
                    {
                        "name": "Cuong V. Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Cuong V. Nguyen"
                },
                "author": "Cuong V. Nguyen",
                "arxiv_comment": "Paper published at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22065v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22065v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02680v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02680v4",
                "updated": "2024-10-29T14:08:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    14,
                    8,
                    34,
                    1,
                    303,
                    0
                ],
                "published": "2024-07-02T21:44:22Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    21,
                    44,
                    22,
                    1,
                    184,
                    0
                ],
                "title": "KGym: A Platform and Dataset to Benchmark Large Language Models on Linux\n  Kernel Crash Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KGym: A Platform and Dataset to Benchmark Large Language Models on Linux\n  Kernel Crash Resolution"
                },
                "summary": "Large Language Models (LLMs) are consistently improving at increasingly\nrealistic software engineering (SE) tasks. In real-world software stacks,\nsignificant SE effort is spent developing foundational system software like the\nLinux kernel. Unlike application-level software, a systems codebase like Linux\nis multilingual (low-level C/Assembly/Bash/Rust); gigantic (>20 million lines);\ncritical (impacting billions of devices worldwide), and highly concurrent\n(involving complex multi-threading). To evaluate if ML models are useful while\ndeveloping such large-scale systems-level software, we introduce kGym (a\nplatform) and kBench (a dataset). The kGym platform provides a SE environment\nfor large-scale experiments on the Linux kernel, including compiling and\nrunning kernels in parallel across several virtual machines, detecting\noperations and crashes, inspecting logs, and querying and patching the code\nbase. We use kGym to facilitate evaluation on kBench, a crash resolution\nbenchmark drawn from real-world Linux kernel bugs. An example bug in kBench\ncontains crashing stack traces, a bug-reproducer file, a developer-written fix,\nand other associated data. To understand current performance, we conduct\nbaseline experiments by prompting LLMs to resolve Linux kernel crashes. Our\ninitial evaluations reveal that the best performing LLM achieves 0.72% and\n5.38% in the unassisted and assisted (i.e., buggy files disclosed to the model)\nsettings, respectively. These results highlight the need for further research\nto enhance model performance in SE tasks. Improving performance on kBench\nrequires models to master new learning skills, including understanding the\ncause of crashes and repairing faults, writing memory-safe and hardware-aware\ncode, and understanding concurrency. As a result, this work opens up multiple\navenues of research at the intersection of machine learning and systems\nsoftware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are consistently improving at increasingly\nrealistic software engineering (SE) tasks. In real-world software stacks,\nsignificant SE effort is spent developing foundational system software like the\nLinux kernel. Unlike application-level software, a systems codebase like Linux\nis multilingual (low-level C/Assembly/Bash/Rust); gigantic (>20 million lines);\ncritical (impacting billions of devices worldwide), and highly concurrent\n(involving complex multi-threading). To evaluate if ML models are useful while\ndeveloping such large-scale systems-level software, we introduce kGym (a\nplatform) and kBench (a dataset). The kGym platform provides a SE environment\nfor large-scale experiments on the Linux kernel, including compiling and\nrunning kernels in parallel across several virtual machines, detecting\noperations and crashes, inspecting logs, and querying and patching the code\nbase. We use kGym to facilitate evaluation on kBench, a crash resolution\nbenchmark drawn from real-world Linux kernel bugs. An example bug in kBench\ncontains crashing stack traces, a bug-reproducer file, a developer-written fix,\nand other associated data. To understand current performance, we conduct\nbaseline experiments by prompting LLMs to resolve Linux kernel crashes. Our\ninitial evaluations reveal that the best performing LLM achieves 0.72% and\n5.38% in the unassisted and assisted (i.e., buggy files disclosed to the model)\nsettings, respectively. These results highlight the need for further research\nto enhance model performance in SE tasks. Improving performance on kBench\nrequires models to master new learning skills, including understanding the\ncause of crashes and repairing faults, writing memory-safe and hardware-aware\ncode, and understanding concurrency. As a result, this work opens up multiple\navenues of research at the intersection of machine learning and systems\nsoftware."
                },
                "authors": [
                    {
                        "name": "Alex Mathai"
                    },
                    {
                        "name": "Chenxi Huang"
                    },
                    {
                        "name": "Petros Maniatis"
                    },
                    {
                        "name": "Aleksandr Nogikh"
                    },
                    {
                        "name": "Franjo Ivancic"
                    },
                    {
                        "name": "Junfeng Yang"
                    },
                    {
                        "name": "Baishakhi Ray"
                    }
                ],
                "author_detail": {
                    "name": "Baishakhi Ray"
                },
                "author": "Baishakhi Ray",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02680v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02680v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.02425v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.02425v4",
                "updated": "2024-10-29T13:49:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    13,
                    49,
                    59,
                    1,
                    303,
                    0
                ],
                "published": "2024-02-04T09:45:35Z",
                "published_parsed": [
                    2024,
                    2,
                    4,
                    9,
                    45,
                    35,
                    6,
                    35,
                    0
                ],
                "title": "DeepLag: Discovering Deep Lagrangian Dynamics for Intuitive Fluid\n  Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepLag: Discovering Deep Lagrangian Dynamics for Intuitive Fluid\n  Prediction"
                },
                "summary": "Accurately predicting the future fluid is vital to extensive areas such as\nmeteorology, oceanology, and aerodynamics. However, since the fluid is usually\nobserved from the Eulerian perspective, its moving and intricate dynamics are\nseriously obscured and confounded in static grids, bringing thorny challenges\nto the prediction. This paper introduces a new Lagrangian-Eulerian combined\nparadigm to tackle the tanglesome fluid dynamics. Instead of solely predicting\nthe future based on Eulerian observations, we propose DeepLag to discover\nhidden Lagrangian dynamics within the fluid by tracking the movements of\nadaptively sampled key particles. Further, DeepLag presents a new paradigm for\nfluid prediction, where the Lagrangian movement of the tracked particles is\ninferred from Eulerian observations, and their accumulated Lagrangian dynamics\ninformation is incorporated into global Eulerian evolving features to guide\nfuture prediction respectively. Tracking key particles not only provides a\ntransparent and interpretable clue for fluid dynamics but also makes our model\nfree from modeling complex correlations among massive grids for better\nefficiency. Experimentally, DeepLag excels in three challenging fluid\nprediction tasks covering 2D and 3D, simulated and real-world fluids. Code is\navailable at this repository: https://github.com/thuml/DeepLag.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately predicting the future fluid is vital to extensive areas such as\nmeteorology, oceanology, and aerodynamics. However, since the fluid is usually\nobserved from the Eulerian perspective, its moving and intricate dynamics are\nseriously obscured and confounded in static grids, bringing thorny challenges\nto the prediction. This paper introduces a new Lagrangian-Eulerian combined\nparadigm to tackle the tanglesome fluid dynamics. Instead of solely predicting\nthe future based on Eulerian observations, we propose DeepLag to discover\nhidden Lagrangian dynamics within the fluid by tracking the movements of\nadaptively sampled key particles. Further, DeepLag presents a new paradigm for\nfluid prediction, where the Lagrangian movement of the tracked particles is\ninferred from Eulerian observations, and their accumulated Lagrangian dynamics\ninformation is incorporated into global Eulerian evolving features to guide\nfuture prediction respectively. Tracking key particles not only provides a\ntransparent and interpretable clue for fluid dynamics but also makes our model\nfree from modeling complex correlations among massive grids for better\nefficiency. Experimentally, DeepLag excels in three challenging fluid\nprediction tasks covering 2D and 3D, simulated and real-world fluids. Code is\navailable at this repository: https://github.com/thuml/DeepLag."
                },
                "authors": [
                    {
                        "name": "Qilong Ma"
                    },
                    {
                        "name": "Haixu Wu"
                    },
                    {
                        "name": "Lanxiang Xing"
                    },
                    {
                        "name": "Shangchen Miao"
                    },
                    {
                        "name": "Mingsheng Long"
                    }
                ],
                "author_detail": {
                    "name": "Mingsheng Long"
                },
                "author": "Mingsheng Long",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.02425v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.02425v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22041v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22041v1",
                "updated": "2024-10-29T13:46:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    13,
                    46,
                    52,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T13:46:52Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    13,
                    46,
                    52,
                    1,
                    303,
                    0
                ],
                "title": "An LLM-based Simulation Framework for Embodied Conversational Agents in\n  Psychological Counseling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An LLM-based Simulation Framework for Embodied Conversational Agents in\n  Psychological Counseling"
                },
                "summary": "Simulation is crucial for validating algorithmic strategies in real-world\nscenarios. While LLM-based social simulation shows promise as a mainstream\ntool, simulating complex scenarios like psychological counseling remains\nchallenging. We present ECAs (short for Embodied Conversational Agents), a\nframework for simulating psychological counseling clients' embodied memory,\nintegrating embodied cognition and counseling theories. We formulate six design\ngoals based on a comprehensive review of psychological counseling theories.\nUsing LLMs, we expand real counseling case data into a nuanced embodied\ncognitive memory space and generate dialogues based on high-frequency\ncounseling questions. We validate our framework using the D4 dataset, with\nevaluations by licensed counselors. Results show our approach significantly\noutperforms baselines in simulation authenticity and necessity. To demonstrate\nscalability, we created a public ECAs dataset through batch simulations. This\nresearch provides valuable insights for future social simulation studies in\npsychological counseling and Embodied Counseling Agents research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulation is crucial for validating algorithmic strategies in real-world\nscenarios. While LLM-based social simulation shows promise as a mainstream\ntool, simulating complex scenarios like psychological counseling remains\nchallenging. We present ECAs (short for Embodied Conversational Agents), a\nframework for simulating psychological counseling clients' embodied memory,\nintegrating embodied cognition and counseling theories. We formulate six design\ngoals based on a comprehensive review of psychological counseling theories.\nUsing LLMs, we expand real counseling case data into a nuanced embodied\ncognitive memory space and generate dialogues based on high-frequency\ncounseling questions. We validate our framework using the D4 dataset, with\nevaluations by licensed counselors. Results show our approach significantly\noutperforms baselines in simulation authenticity and necessity. To demonstrate\nscalability, we created a public ECAs dataset through batch simulations. This\nresearch provides valuable insights for future social simulation studies in\npsychological counseling and Embodied Counseling Agents research."
                },
                "authors": [
                    {
                        "name": "Lixiu Wu"
                    },
                    {
                        "name": "Yuanrong Tang"
                    },
                    {
                        "name": "Qisen Pan"
                    },
                    {
                        "name": "Xianyang Zhan"
                    },
                    {
                        "name": "Yucheng Han"
                    },
                    {
                        "name": "Mingyang You"
                    },
                    {
                        "name": "Lanxi Xiao"
                    },
                    {
                        "name": "Tianhong Wang"
                    },
                    {
                        "name": "Chen Zhong"
                    },
                    {
                        "name": "Jiangtao Gong"
                    }
                ],
                "author_detail": {
                    "name": "Jiangtao Gong"
                },
                "author": "Jiangtao Gong",
                "arxiv_comment": "7 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22041v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22041v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.09066v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.09066v3",
                "updated": "2024-10-29T13:43:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    13,
                    43,
                    58,
                    1,
                    303,
                    0
                ],
                "published": "2024-04-13T19:30:58Z",
                "published_parsed": [
                    2024,
                    4,
                    13,
                    19,
                    30,
                    58,
                    5,
                    104,
                    0
                ],
                "title": "CodeCloak: A Method for Evaluating and Mitigating Code Leakage by LLM\n  Code Assistants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeCloak: A Method for Evaluating and Mitigating Code Leakage by LLM\n  Code Assistants"
                },
                "summary": "LLM-based code assistants are becoming increasingly popular among developers.\nThese tools help developers improve their coding efficiency and reduce errors\nby providing real-time suggestions based on the developer's codebase. While\nbeneficial, the use of these tools can inadvertently expose the developer's\nproprietary code to the code assistant service provider during the development\nprocess. In this work, we propose a method to mitigate the risk of code leakage\nwhen using LLM-based code assistants. CodeCloak is a novel deep reinforcement\nlearning agent that manipulates the prompts before sending them to the code\nassistant service. CodeCloak aims to achieve the following two contradictory\ngoals: (i) minimizing code leakage, while (ii) preserving relevant and useful\nsuggestions for the developer. Our evaluation, employing StarCoder and Code\nLlama, LLM-based code assistants models, demonstrates CodeCloak's effectiveness\non a diverse set of code repositories of varying sizes, as well as its\ntransferability across different models. We also designed a method for\nreconstructing the developer's original codebase from code segments sent to the\ncode assistant service (i.e., prompts) during the development process, to\nthoroughly analyze code leakage risks and evaluate the effectiveness of\nCodeCloak under practical development scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based code assistants are becoming increasingly popular among developers.\nThese tools help developers improve their coding efficiency and reduce errors\nby providing real-time suggestions based on the developer's codebase. While\nbeneficial, the use of these tools can inadvertently expose the developer's\nproprietary code to the code assistant service provider during the development\nprocess. In this work, we propose a method to mitigate the risk of code leakage\nwhen using LLM-based code assistants. CodeCloak is a novel deep reinforcement\nlearning agent that manipulates the prompts before sending them to the code\nassistant service. CodeCloak aims to achieve the following two contradictory\ngoals: (i) minimizing code leakage, while (ii) preserving relevant and useful\nsuggestions for the developer. Our evaluation, employing StarCoder and Code\nLlama, LLM-based code assistants models, demonstrates CodeCloak's effectiveness\non a diverse set of code repositories of varying sizes, as well as its\ntransferability across different models. We also designed a method for\nreconstructing the developer's original codebase from code segments sent to the\ncode assistant service (i.e., prompts) during the development process, to\nthoroughly analyze code leakage risks and evaluate the effectiveness of\nCodeCloak under practical development scenarios."
                },
                "authors": [
                    {
                        "name": "Amit Finkman Noah"
                    },
                    {
                        "name": "Avishag Shapira"
                    },
                    {
                        "name": "Eden Bar Kochva"
                    },
                    {
                        "name": "Inbar Maimon"
                    },
                    {
                        "name": "Dudu Mimran"
                    },
                    {
                        "name": "Yuval Elovici"
                    },
                    {
                        "name": "Asaf Shabtai"
                    }
                ],
                "author_detail": {
                    "name": "Asaf Shabtai"
                },
                "author": "Asaf Shabtai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.09066v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.09066v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.13943v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.13943v2",
                "updated": "2024-10-29T13:42:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    13,
                    42,
                    40,
                    1,
                    303,
                    0
                ],
                "published": "2024-05-22T19:17:58Z",
                "published_parsed": [
                    2024,
                    5,
                    22,
                    19,
                    17,
                    58,
                    2,
                    143,
                    0
                ],
                "title": "DOGS: Distributed-Oriented Gaussian Splatting for Large-Scale 3D\n  Reconstruction Via Gaussian Consensus",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DOGS: Distributed-Oriented Gaussian Splatting for Large-Scale 3D\n  Reconstruction Via Gaussian Consensus"
                },
                "summary": "The recent advances in 3D Gaussian Splatting (3DGS) show promising results on\nthe novel view synthesis (NVS) task. With its superior rendering performance\nand high-fidelity rendering quality, 3DGS is excelling at its previous NeRF\ncounterparts. The most recent 3DGS method focuses either on improving the\ninstability of rendering efficiency or reducing the model size. On the other\nhand, the training efficiency of 3DGS on large-scale scenes has not gained much\nattention. In this work, we propose DoGaussian, a method that trains 3DGS\ndistributedly. Our method first decomposes a scene into K blocks and then\nintroduces the Alternating Direction Method of Multipliers (ADMM) into the\ntraining procedure of 3DGS. During training, our DOGS maintains one global 3DGS\nmodel on the master node and K local 3DGS models on the slave nodes. The K\nlocal 3DGS models are dropped after training and we only query the global 3DGS\nmodel during inference. The training time is reduced by scene decomposition,\nand the training convergence and stability are guaranteed through the consensus\non the shared 3D Gaussians. Our method accelerates the training of 3DGS by 6+\ntimes when evaluated on large-scale scenes while concurrently achieving\nstate-of-the-art rendering quality. Our code is publicly available at\nhttps://github.com/AIBluefisher/DOGS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent advances in 3D Gaussian Splatting (3DGS) show promising results on\nthe novel view synthesis (NVS) task. With its superior rendering performance\nand high-fidelity rendering quality, 3DGS is excelling at its previous NeRF\ncounterparts. The most recent 3DGS method focuses either on improving the\ninstability of rendering efficiency or reducing the model size. On the other\nhand, the training efficiency of 3DGS on large-scale scenes has not gained much\nattention. In this work, we propose DoGaussian, a method that trains 3DGS\ndistributedly. Our method first decomposes a scene into K blocks and then\nintroduces the Alternating Direction Method of Multipliers (ADMM) into the\ntraining procedure of 3DGS. During training, our DOGS maintains one global 3DGS\nmodel on the master node and K local 3DGS models on the slave nodes. The K\nlocal 3DGS models are dropped after training and we only query the global 3DGS\nmodel during inference. The training time is reduced by scene decomposition,\nand the training convergence and stability are guaranteed through the consensus\non the shared 3D Gaussians. Our method accelerates the training of 3DGS by 6+\ntimes when evaluated on large-scale scenes while concurrently achieving\nstate-of-the-art rendering quality. Our code is publicly available at\nhttps://github.com/AIBluefisher/DOGS."
                },
                "authors": [
                    {
                        "name": "Yu Chen"
                    },
                    {
                        "name": "Gim Hee Lee"
                    }
                ],
                "author_detail": {
                    "name": "Gim Hee Lee"
                },
                "author": "Gim Hee Lee",
                "arxiv_comment": "Accepted to NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.13943v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.13943v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13895v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13895v3",
                "updated": "2024-10-29T13:32:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    13,
                    32,
                    46,
                    1,
                    303,
                    0
                ],
                "published": "2024-08-25T17:17:52Z",
                "published_parsed": [
                    2024,
                    8,
                    25,
                    17,
                    17,
                    52,
                    6,
                    238,
                    0
                ],
                "title": "ESG Rating Disagreement and Corporate Total Factor\n  Productivity:Inference and Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ESG Rating Disagreement and Corporate Total Factor\n  Productivity:Inference and Prediction"
                },
                "summary": "This paper examines how ESG rating disagreement (Dis) affects corporate total\nfactor productivity (TFP) in China based on data of A-share listed companies\nfrom 2015 to 2022. We find that Dis reduces TFP, especially in state-owned,\nnon-capital-intensive, and low-pollution firms. Mechanism analysis shows that\ngreen innovation strengthens the dampening effect of Dis on TFP, and that Dis\nlowers corporate TFP by increasing financing constraints. Furthermore, XGBoost\nregression demonstrates that Dis plays a significant role in predicting TFP,\nwith SHAP showing that the dampening effect of ESG rating disagreement on TFP\nis still pronounced in firms with large Dis values.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper examines how ESG rating disagreement (Dis) affects corporate total\nfactor productivity (TFP) in China based on data of A-share listed companies\nfrom 2015 to 2022. We find that Dis reduces TFP, especially in state-owned,\nnon-capital-intensive, and low-pollution firms. Mechanism analysis shows that\ngreen innovation strengthens the dampening effect of Dis on TFP, and that Dis\nlowers corporate TFP by increasing financing constraints. Furthermore, XGBoost\nregression demonstrates that Dis plays a significant role in predicting TFP,\nwith SHAP showing that the dampening effect of ESG rating disagreement on TFP\nis still pronounced in firms with large Dis values."
                },
                "authors": [
                    {
                        "name": "Zhanli Li"
                    },
                    {
                        "name": "Zichao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Zichao Yang"
                },
                "author": "Zichao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13895v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13895v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09713v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09713v2",
                "updated": "2024-10-29T13:19:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    13,
                    19,
                    12,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-13T03:45:24Z",
                "published_parsed": [
                    2024,
                    10,
                    13,
                    3,
                    45,
                    24,
                    6,
                    287,
                    0
                ],
                "title": "Agentic Information Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic Information Retrieval"
                },
                "summary": "What will information entry look like in the next generation of digital\nproducts? Since the 1970s, user access to relevant information has relied on\ndomain-specific architectures of information retrieval (IR). Over the past two\ndecades, the advent of modern IR systems, including web search engines and\npersonalized recommender systems, has greatly improved the efficiency of\nretrieving relevant information from vast data corpora. However, the core\nparadigm of these IR systems remains largely unchanged, relying on filtering a\npredefined set of candidate items. Since 2022, breakthroughs in large language\nmodels (LLMs) have begun transforming how information is accessed, establishing\na new technical paradigm. In this position paper, we introduce Agentic\nInformation Retrieval (Agentic IR), a novel IR paradigm shaped by the\ncapabilities of LLM agents. Agentic IR expands the scope of accessible tasks\nand leverages a suite of new techniques to redefine information retrieval. We\ndiscuss three types of cutting-edge applications of agentic IR and the\nchallenges faced. We propose that agentic IR holds promise for generating\ninnovative applications, potentially becoming a central information entry point\nin future digital ecosystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What will information entry look like in the next generation of digital\nproducts? Since the 1970s, user access to relevant information has relied on\ndomain-specific architectures of information retrieval (IR). Over the past two\ndecades, the advent of modern IR systems, including web search engines and\npersonalized recommender systems, has greatly improved the efficiency of\nretrieving relevant information from vast data corpora. However, the core\nparadigm of these IR systems remains largely unchanged, relying on filtering a\npredefined set of candidate items. Since 2022, breakthroughs in large language\nmodels (LLMs) have begun transforming how information is accessed, establishing\na new technical paradigm. In this position paper, we introduce Agentic\nInformation Retrieval (Agentic IR), a novel IR paradigm shaped by the\ncapabilities of LLM agents. Agentic IR expands the scope of accessible tasks\nand leverages a suite of new techniques to redefine information retrieval. We\ndiscuss three types of cutting-edge applications of agentic IR and the\nchallenges faced. We propose that agentic IR holds promise for generating\ninnovative applications, potentially becoming a central information entry point\nin future digital ecosystems."
                },
                "authors": [
                    {
                        "name": "Weinan Zhang"
                    },
                    {
                        "name": "Junwei Liao"
                    },
                    {
                        "name": "Ning Li"
                    },
                    {
                        "name": "Kounianhua Du"
                    }
                ],
                "author_detail": {
                    "name": "Kounianhua Du"
                },
                "author": "Kounianhua Du",
                "arxiv_comment": "11 pages, position paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09713v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09713v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.06384v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.06384v3",
                "updated": "2024-10-29T13:16:39Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    13,
                    16,
                    39,
                    1,
                    303,
                    0
                ],
                "published": "2023-03-11T11:32:31Z",
                "published_parsed": [
                    2023,
                    3,
                    11,
                    11,
                    32,
                    31,
                    5,
                    70,
                    0
                ],
                "title": "Measuring Information Transfer Between Nodes in a Brain Network through\n  Spectral Transfer Entropy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring Information Transfer Between Nodes in a Brain Network through\n  Spectral Transfer Entropy"
                },
                "summary": "Brain connectivity characterizes interactions between different regions of a\nbrain network during resting-state or performance of a cognitive task. In\nstudying brain signals such as electroencephalograms (EEG), one formal approach\nto investigating connectivity is through an information-theoretic causal\nmeasure called transfer entropy (TE). To enhance the functionality of TE in\nbrain signal analysis, we propose a novel methodology that captures\ncross-channel information transfer in the frequency domain. Specifically, we\nintroduce a new measure, the spectral transfer entropy (STE), to quantify the\nmagnitude and direction of information flow from a band-specific oscillation of\none channel to another band-specific oscillation of another channel. The main\nadvantage of our proposed approach is that it formulates TE in a novel way to\nperform inference on band-specific oscillations while maintaining robustness to\nthe inherent problems associated with filtering. In addition, an advantage of\nSTE is that it allows adjustments for multiple comparisons to control false\npositive rates. Another novel contribution is a simple yet efficient method for\nestimating STE using vine copula theory. This method can produce an exact zero\nestimate of STE (which is the boundary point of the parameter space) without\nthe need for bias adjustments. With the vine copula representation, a null\ncopula model, which exhibits zero STE, is defined, thus enabling\nstraightforward significance testing through standard resampling. Lastly, we\ndemonstrate the advantage of the proposed STE measure through numerical\nexperiments and provide interesting and novel findings on the analysis of EEG\ndata in a visual-memory experiment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Brain connectivity characterizes interactions between different regions of a\nbrain network during resting-state or performance of a cognitive task. In\nstudying brain signals such as electroencephalograms (EEG), one formal approach\nto investigating connectivity is through an information-theoretic causal\nmeasure called transfer entropy (TE). To enhance the functionality of TE in\nbrain signal analysis, we propose a novel methodology that captures\ncross-channel information transfer in the frequency domain. Specifically, we\nintroduce a new measure, the spectral transfer entropy (STE), to quantify the\nmagnitude and direction of information flow from a band-specific oscillation of\none channel to another band-specific oscillation of another channel. The main\nadvantage of our proposed approach is that it formulates TE in a novel way to\nperform inference on band-specific oscillations while maintaining robustness to\nthe inherent problems associated with filtering. In addition, an advantage of\nSTE is that it allows adjustments for multiple comparisons to control false\npositive rates. Another novel contribution is a simple yet efficient method for\nestimating STE using vine copula theory. This method can produce an exact zero\nestimate of STE (which is the boundary point of the parameter space) without\nthe need for bias adjustments. With the vine copula representation, a null\ncopula model, which exhibits zero STE, is defined, thus enabling\nstraightforward significance testing through standard resampling. Lastly, we\ndemonstrate the advantage of the proposed STE measure through numerical\nexperiments and provide interesting and novel findings on the analysis of EEG\ndata in a visual-memory experiment."
                },
                "authors": [
                    {
                        "name": "Paolo Victor Redondo"
                    },
                    {
                        "name": "Raphael Huser"
                    },
                    {
                        "name": "Hernando Ombao"
                    }
                ],
                "author_detail": {
                    "name": "Hernando Ombao"
                },
                "author": "Hernando Ombao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2303.06384v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.06384v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22016v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22016v1",
                "updated": "2024-10-29T13:06:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    13,
                    6,
                    46,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T13:06:46Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    13,
                    6,
                    46,
                    1,
                    303,
                    0
                ],
                "title": "Training via quantum superposition circumventing local minima and\n  vanishing gradient of sinusoidal neural network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training via quantum superposition circumventing local minima and\n  vanishing gradient of sinusoidal neural network"
                },
                "summary": "Deep neural networks have been very successful in applications ranging from\ncomputer vision and natural language processing to strategy optimization in\ngames. Recently neural networks with sinusoidal activation functions (SinNN)\nwere found to be ideally suited for representing complex natural signals and\ntheir fine spatial and temporal details, which makes them effective\nrepresentations of images, sound, and video, and good solvers of differential\nequations. However, training SinNN via gradient descent often results in bad\nlocal minima, posing a significant challenge when optimizing their weights.\nFurthermore, when the weights are discretized for better memory and inference\nefficiency on small devices, we find that a vanishing gradient problem appears\non the resulting discrete SinNN (DSinNN). Brute force search provides an\nalternative way to find the best weights for DSinNN but is intractable for a\nlarge number of parameters. We here provide a qualitatively different training\nmethod: an algorithm for quantum training of DSinNNs. The quantum training\nevolves an initially uniform superposition over weight values to one that is\nguaranteed to peak on the best weights. We demonstrate the algorithm on toy\nexamples and show that it indeed outperforms gradient descent in optimizing the\nloss function and outperforms brute force search in the time required.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep neural networks have been very successful in applications ranging from\ncomputer vision and natural language processing to strategy optimization in\ngames. Recently neural networks with sinusoidal activation functions (SinNN)\nwere found to be ideally suited for representing complex natural signals and\ntheir fine spatial and temporal details, which makes them effective\nrepresentations of images, sound, and video, and good solvers of differential\nequations. However, training SinNN via gradient descent often results in bad\nlocal minima, posing a significant challenge when optimizing their weights.\nFurthermore, when the weights are discretized for better memory and inference\nefficiency on small devices, we find that a vanishing gradient problem appears\non the resulting discrete SinNN (DSinNN). Brute force search provides an\nalternative way to find the best weights for DSinNN but is intractable for a\nlarge number of parameters. We here provide a qualitatively different training\nmethod: an algorithm for quantum training of DSinNNs. The quantum training\nevolves an initially uniform superposition over weight values to one that is\nguaranteed to peak on the best weights. We demonstrate the algorithm on toy\nexamples and show that it indeed outperforms gradient descent in optimizing the\nloss function and outperforms brute force search in the time required."
                },
                "authors": [
                    {
                        "name": "Zujin Wen"
                    },
                    {
                        "name": "Jin-Long Huang"
                    },
                    {
                        "name": "Oscar Dahlsten"
                    }
                ],
                "author_detail": {
                    "name": "Oscar Dahlsten"
                },
                "author": "Oscar Dahlsten",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22016v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22016v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.09526v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.09526v2",
                "updated": "2024-10-29T13:04:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    13,
                    4,
                    42,
                    1,
                    303,
                    0
                ],
                "published": "2024-04-15T07:45:04Z",
                "published_parsed": [
                    2024,
                    4,
                    15,
                    7,
                    45,
                    4,
                    0,
                    106,
                    0
                ],
                "title": "LoongServe: Efficiently Serving Long-Context Large Language Models with\n  Elastic Sequence Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoongServe: Efficiently Serving Long-Context Large Language Models with\n  Elastic Sequence Parallelism"
                },
                "summary": "The context window of large language models (LLMs) is rapidly increasing,\nleading to a huge variance in resource usage between different requests as well\nas between different phases of the same request. Restricted by static\nparallelism strategies, existing LLM serving systems cannot efficiently utilize\nthe underlying resources to serve variable-length requests in different phases.\nTo address this problem, we propose a new parallelism paradigm, elastic\nsequence parallelism (ESP), to elastically adapt to the variance between\ndifferent requests and phases. Based on ESP, we design and build LoongServe, an\nLLM serving system that (1) improves computation efficiency by elastically\nadjusting the degree of parallelism in real-time, (2) improves communication\nefficiency by reducing key-value cache migration overhead and overlapping\npartial decoding communication with computation, and (3) improves GPU memory\nefficiency by reducing key-value cache fragmentation across instances. Our\nevaluation under diverse real-world datasets shows that LoongServe improves the\nmaximum throughput by up to 3.85$\\times$ compared to the chunked prefill and\n5.81$\\times$ compared to the prefill-decoding disaggregation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The context window of large language models (LLMs) is rapidly increasing,\nleading to a huge variance in resource usage between different requests as well\nas between different phases of the same request. Restricted by static\nparallelism strategies, existing LLM serving systems cannot efficiently utilize\nthe underlying resources to serve variable-length requests in different phases.\nTo address this problem, we propose a new parallelism paradigm, elastic\nsequence parallelism (ESP), to elastically adapt to the variance between\ndifferent requests and phases. Based on ESP, we design and build LoongServe, an\nLLM serving system that (1) improves computation efficiency by elastically\nadjusting the degree of parallelism in real-time, (2) improves communication\nefficiency by reducing key-value cache migration overhead and overlapping\npartial decoding communication with computation, and (3) improves GPU memory\nefficiency by reducing key-value cache fragmentation across instances. Our\nevaluation under diverse real-world datasets shows that LoongServe improves the\nmaximum throughput by up to 3.85$\\times$ compared to the chunked prefill and\n5.81$\\times$ compared to the prefill-decoding disaggregation."
                },
                "authors": [
                    {
                        "name": "Bingyang Wu"
                    },
                    {
                        "name": "Shengyu Liu"
                    },
                    {
                        "name": "Yinmin Zhong"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Xuanzhe Liu"
                    },
                    {
                        "name": "Xin Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xin Jin"
                },
                "author": "Xin Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.09526v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.09526v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.11265v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.11265v2",
                "updated": "2024-10-29T13:01:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    13,
                    1,
                    18,
                    1,
                    303,
                    0
                ],
                "published": "2024-03-17T16:36:26Z",
                "published_parsed": [
                    2024,
                    3,
                    17,
                    16,
                    36,
                    26,
                    6,
                    77,
                    0
                ],
                "title": "Forging the Forger: An Attempt to Improve Authorship Verification via\n  Data Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forging the Forger: An Attempt to Improve Authorship Verification via\n  Data Augmentation"
                },
                "summary": "Authorship Verification (AV) is a text classification task concerned with\ninferring whether a candidate text has been written by one specific author or\nby someone else. It has been shown that many AV systems are vulnerable to\nadversarial attacks, where a malicious author actively tries to fool the\nclassifier by either concealing their writing style, or by imitating the style\nof another author. In this paper, we investigate the potential benefits of\naugmenting the classifier training set with (negative) synthetic examples.\nThese synthetic examples are generated to imitate the style of the author of\ninterest. We analyze the improvements in classifier prediction that this\naugmentation brings to bear in the task of AV in an adversarial setting. In\nparticular, we experiment with three different generator architectures (one\nbased on Recurrent Neural Networks, another based on small-scale transformers,\nand another based on the popular GPT model) and with two training strategies\n(one inspired by standard Language Models, and another inspired by Wasserstein\nGenerative Adversarial Networks). We evaluate our hypothesis on five datasets\n(three of which have been specifically collected to represent an adversarial\nsetting) and using two learning algorithms for the AV classifier (Support\nVector Machines and Convolutional Neural Networks). This experimentation has\nyielded negative results, revealing that, although our methodology proves\neffective in many adversarial settings, its benefits are too sporadic for a\npragmatical application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Authorship Verification (AV) is a text classification task concerned with\ninferring whether a candidate text has been written by one specific author or\nby someone else. It has been shown that many AV systems are vulnerable to\nadversarial attacks, where a malicious author actively tries to fool the\nclassifier by either concealing their writing style, or by imitating the style\nof another author. In this paper, we investigate the potential benefits of\naugmenting the classifier training set with (negative) synthetic examples.\nThese synthetic examples are generated to imitate the style of the author of\ninterest. We analyze the improvements in classifier prediction that this\naugmentation brings to bear in the task of AV in an adversarial setting. In\nparticular, we experiment with three different generator architectures (one\nbased on Recurrent Neural Networks, another based on small-scale transformers,\nand another based on the popular GPT model) and with two training strategies\n(one inspired by standard Language Models, and another inspired by Wasserstein\nGenerative Adversarial Networks). We evaluate our hypothesis on five datasets\n(three of which have been specifically collected to represent an adversarial\nsetting) and using two learning algorithms for the AV classifier (Support\nVector Machines and Convolutional Neural Networks). This experimentation has\nyielded negative results, revealing that, although our methodology proves\neffective in many adversarial settings, its benefits are too sporadic for a\npragmatical application."
                },
                "authors": [
                    {
                        "name": "Silvia Corbara"
                    },
                    {
                        "name": "Alejandro Moreo"
                    }
                ],
                "author_detail": {
                    "name": "Alejandro Moreo"
                },
                "author": "Alejandro Moreo",
                "arxiv_doi": "10.1109/ACCESS.2024.3481161",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ACCESS.2024.3481161",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.11265v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.11265v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Access (2024)",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04070v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04070v4",
                "updated": "2024-10-29T12:51:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    12,
                    51,
                    33,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-05T08:00:55Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    8,
                    0,
                    55,
                    5,
                    279,
                    0
                ],
                "title": "PAD: Personalized Alignment at Decoding-Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PAD: Personalized Alignment at Decoding-Time"
                },
                "summary": "Aligning with personalized preferences, which vary significantly across\ncultural, educational, and political differences, poses a significant challenge\ndue to the computational costs and data demands of traditional alignment\nmethods. In response, this paper presents Personalized Alignment at\nDecoding-time (PAD), a novel framework designed to align LLM outputs with\ndiverse personalized preferences during the inference phase, eliminating the\nneed for additional training. By introducing a unique personalized reward\nmodeling strategy, this framework decouples the text generation process from\npersonalized preferences, facilitating the generation of generalizable\ntoken-level personalized rewards. The PAD algorithm leverages these rewards to\nguide the decoding process, dynamically tailoring the base model's predictions\nto personalized preferences. Extensive experimental results demonstrate that\nPAD not only outperforms existing training-based alignment methods in terms of\naligning with diverse preferences but also shows significant generalizability\nto preferences unseen during training and scalability across different base\nmodels. This work advances the capability of LLMs to meet user needs in\nreal-time applications, presenting a substantial step forward in personalized\nLLM alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning with personalized preferences, which vary significantly across\ncultural, educational, and political differences, poses a significant challenge\ndue to the computational costs and data demands of traditional alignment\nmethods. In response, this paper presents Personalized Alignment at\nDecoding-time (PAD), a novel framework designed to align LLM outputs with\ndiverse personalized preferences during the inference phase, eliminating the\nneed for additional training. By introducing a unique personalized reward\nmodeling strategy, this framework decouples the text generation process from\npersonalized preferences, facilitating the generation of generalizable\ntoken-level personalized rewards. The PAD algorithm leverages these rewards to\nguide the decoding process, dynamically tailoring the base model's predictions\nto personalized preferences. Extensive experimental results demonstrate that\nPAD not only outperforms existing training-based alignment methods in terms of\naligning with diverse preferences but also shows significant generalizability\nto preferences unseen during training and scalability across different base\nmodels. This work advances the capability of LLMs to meet user needs in\nreal-time applications, presenting a substantial step forward in personalized\nLLM alignment."
                },
                "authors": [
                    {
                        "name": "Ruizhe Chen"
                    },
                    {
                        "name": "Xiaotian Zhang"
                    },
                    {
                        "name": "Meng Luo"
                    },
                    {
                        "name": "Wenhao Chai"
                    },
                    {
                        "name": "Zuozhu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zuozhu Liu"
                },
                "author": "Zuozhu Liu",
                "arxiv_comment": "This paper presents Personalized Alignment at Decoding-time (PAD), a\n  novel framework designed to align LLM outputs with diverse personalized\n  preferences during the inference phase",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04070v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04070v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22001v1",
                "updated": "2024-10-29T12:44:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    12,
                    44,
                    27,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T12:44:27Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    12,
                    44,
                    27,
                    1,
                    303,
                    0
                ],
                "title": "Markov Stochastic Choice",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Markov Stochastic Choice"
                },
                "summary": "We examine the effect of item arrangement on choices using a novel\ndecision-making model based on the Markovian exploration of choice sets. This\nmodel is inspired by experimental evidence suggesting that the decision-making\nprocess involves sequential search through rapid stochastic pairwise\ncomparisons. Our findings show that decision-makers following a reversible\nprocess are unaffected by item rearrangements, and further demonstrate that\nthis property can be inferred from their choice behavior. Additionally, we\nprovide a characterization of the class of Markovian models in which the agent\nmakes all possible pairwise comparisons with positive probability. The\nintersection of reversible models and those allowing all pairwise comparisons\nis observationally equivalent to the well-known Luce model. Finally, we\ncharacterize the class of Markovian models for which the initial fixation does\nnot impact the final choice and show that choice data reveals the existence and\ncomposition of consideration sets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We examine the effect of item arrangement on choices using a novel\ndecision-making model based on the Markovian exploration of choice sets. This\nmodel is inspired by experimental evidence suggesting that the decision-making\nprocess involves sequential search through rapid stochastic pairwise\ncomparisons. Our findings show that decision-makers following a reversible\nprocess are unaffected by item rearrangements, and further demonstrate that\nthis property can be inferred from their choice behavior. Additionally, we\nprovide a characterization of the class of Markovian models in which the agent\nmakes all possible pairwise comparisons with positive probability. The\nintersection of reversible models and those allowing all pairwise comparisons\nis observationally equivalent to the well-known Luce model. Finally, we\ncharacterize the class of Markovian models for which the initial fixation does\nnot impact the final choice and show that choice data reveals the existence and\ncomposition of consideration sets."
                },
                "authors": [
                    {
                        "name": "Kremena Valkanova"
                    }
                ],
                "author_detail": {
                    "name": "Kremena Valkanova"
                },
                "author": "Kremena Valkanova",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.TH",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "91B06",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05821v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05821v4",
                "updated": "2024-10-29T12:28:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    12,
                    28,
                    58,
                    1,
                    303,
                    0
                ],
                "published": "2023-12-10T08:41:24Z",
                "published_parsed": [
                    2023,
                    12,
                    10,
                    8,
                    41,
                    24,
                    6,
                    344,
                    0
                ],
                "title": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models"
                },
                "summary": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank decomposition, and find that the challenges of this task\nstem from the distribution variance in the LLM activations and the sensitivity\ndifference among various kinds of layers. To address these issues, we propose a\ntraining-free approach called Activation-aware Singular Value Decomposition\n(ASVD). Specifically, ASVD manages activation outliers by transforming the\nweight matrix based on the activation distribution. This transformation allows\nthe outliers in the activation matrix to be absorbed into the transformed\nweight matrix, thereby enhancing decomposition accuracy. Additionally, we\npropose an efficient iterative calibration process to optimize layer-specific\ndecomposition by addressing the varying sensitivity of different LLM layers. In\nthis way, ASVD can compress a network by 10%-30%. Based on the success of the\nlow-rank decomposition of projection matrices in the self-attention module, we\nfurther introduce ASVD to compress the KV cache. By reducing the channel\ndimension of KV activations, memory requirements for KV cache can be largely\nreduced. ASVD can further achieve 50% KV cache reductions without performance\ndrop in a training-free manner. Code is anonymously available in supplementary\nmaterials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank decomposition, and find that the challenges of this task\nstem from the distribution variance in the LLM activations and the sensitivity\ndifference among various kinds of layers. To address these issues, we propose a\ntraining-free approach called Activation-aware Singular Value Decomposition\n(ASVD). Specifically, ASVD manages activation outliers by transforming the\nweight matrix based on the activation distribution. This transformation allows\nthe outliers in the activation matrix to be absorbed into the transformed\nweight matrix, thereby enhancing decomposition accuracy. Additionally, we\npropose an efficient iterative calibration process to optimize layer-specific\ndecomposition by addressing the varying sensitivity of different LLM layers. In\nthis way, ASVD can compress a network by 10%-30%. Based on the success of the\nlow-rank decomposition of projection matrices in the self-attention module, we\nfurther introduce ASVD to compress the KV cache. By reducing the channel\ndimension of KV activations, memory requirements for KV cache can be largely\nreduced. ASVD can further achieve 50% KV cache reductions without performance\ndrop in a training-free manner. Code is anonymously available in supplementary\nmaterials."
                },
                "authors": [
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Yuzhang Shang"
                    },
                    {
                        "name": "Yue Song"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Yan Yan"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.05821v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05821v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21991v1",
                "updated": "2024-10-29T12:22:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    12,
                    22,
                    7,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T12:22:07Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    12,
                    22,
                    7,
                    1,
                    303,
                    0
                ],
                "title": "From Explicit Rules to Implicit Reasoning in an Interpretable Violence\n  Monitoring System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Explicit Rules to Implicit Reasoning in an Interpretable Violence\n  Monitoring System"
                },
                "summary": "Recently, research based on pre-trained models has demonstrated outstanding\nperformance in violence surveillance tasks. However, these black-box systems\nface challenges regarding explainability during training and inference\nprocesses. An important question is how to incorporate explicit knowledge into\nthese implicit models, thereby designing expert-driven and interpretable\nviolence surveillance systems. This paper proposes a new paradigm for weakly\nsupervised violence monitoring (WSVM) called Rule base Violence monitoring\n(RuleVM). The proposed RuleVM uses a dual-branch structure for different\ndesigns for images and text. One of the branches is called the implicit branch,\nwhich uses only visual features for coarse-grained binary classification. In\nthis branch, image feature extraction is divided into two channels: one\nresponsible for extracting scene frames and the other focusing on extracting\nactions. The other branch is called the explicit branch, which utilizes\nlanguage-image alignment to perform fine-grained classification. For the\nlanguage channel design in the explicit branch, the proposed RuleCLIP uses the\nstate-of-the-art YOLO-World model to detect objects and actions in video\nframes, and association rules are identified through data mining methods as\ndescriptions of the video. Leveraging the dual?branch architecture, RuleVM\nachieves interpretable coarse?grained and fine-grained violence surveillance.\nExtensive experiments were conducted on two commonly used benchmarks, and the\nresults show that RuleCLIP achieved the best performance in both coarse-grained\nand fine-grained detection, significantly outperforming existing\nstate-of-the-art methods. Moreover, interpretability experiments uncovered some\ninteresting rules, such as the observation that as the number of people\nincreases, the risk level of violent behavior also rises.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, research based on pre-trained models has demonstrated outstanding\nperformance in violence surveillance tasks. However, these black-box systems\nface challenges regarding explainability during training and inference\nprocesses. An important question is how to incorporate explicit knowledge into\nthese implicit models, thereby designing expert-driven and interpretable\nviolence surveillance systems. This paper proposes a new paradigm for weakly\nsupervised violence monitoring (WSVM) called Rule base Violence monitoring\n(RuleVM). The proposed RuleVM uses a dual-branch structure for different\ndesigns for images and text. One of the branches is called the implicit branch,\nwhich uses only visual features for coarse-grained binary classification. In\nthis branch, image feature extraction is divided into two channels: one\nresponsible for extracting scene frames and the other focusing on extracting\nactions. The other branch is called the explicit branch, which utilizes\nlanguage-image alignment to perform fine-grained classification. For the\nlanguage channel design in the explicit branch, the proposed RuleCLIP uses the\nstate-of-the-art YOLO-World model to detect objects and actions in video\nframes, and association rules are identified through data mining methods as\ndescriptions of the video. Leveraging the dual?branch architecture, RuleVM\nachieves interpretable coarse?grained and fine-grained violence surveillance.\nExtensive experiments were conducted on two commonly used benchmarks, and the\nresults show that RuleCLIP achieved the best performance in both coarse-grained\nand fine-grained detection, significantly outperforming existing\nstate-of-the-art methods. Moreover, interpretability experiments uncovered some\ninteresting rules, such as the observation that as the number of people\nincreases, the risk level of violent behavior also rises."
                },
                "authors": [
                    {
                        "name": "Wen-Dong Jiang"
                    },
                    {
                        "name": "Chih-Yung Chang"
                    },
                    {
                        "name": "Hsiang-Chuan Chang"
                    },
                    {
                        "name": "Diptendu Sinha Roy"
                    }
                ],
                "author_detail": {
                    "name": "Diptendu Sinha Roy"
                },
                "author": "Diptendu Sinha Roy",
                "arxiv_comment": "12 pages,7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10329v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10329v3",
                "updated": "2024-10-29T12:10:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    12,
                    10,
                    41,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-14T09:40:52Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    9,
                    40,
                    52,
                    0,
                    288,
                    0
                ],
                "title": "GraphCLIP: Enhancing Transferability in Graph Foundation Models for\n  Text-Attributed Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphCLIP: Enhancing Transferability in Graph Foundation Models for\n  Text-Attributed Graphs"
                },
                "summary": "Recently, research on Text-Attributed Graphs (TAGs) has gained significant\nattention due to the prevalence of free-text node features in real-world\napplications and the advancements in Large Language Models (LLMs) that bolster\nTAG methodologies. However, current TAG approaches face two primary challenges:\n(i) Heavy reliance on label information and (ii) Limited cross-domain\nzero/few-shot transferability. These issues constrain the scaling of both data\nand model size, owing to high labor costs and scaling laws, complicating the\ndevelopment of graph foundation models with strong transferability. In this\nwork, we propose the GraphCLIP framework to address these challenges by\nlearning graph foundation models with strong cross-domain zero/few-shot\ntransferability through a self-supervised contrastive graph-summary pretraining\nmethod. Specifically, we generate and curate large-scale graph-summary pair\ndata with the assistance of LLMs, and introduce a novel graph-summary\npretraining method, combined with invariant learning, to enhance graph\nfoundation models with strong cross-domain zero-shot transferability. For\nfew-shot learning, we propose a novel graph prompt tuning technique aligned\nwith our pretraining objective to mitigate catastrophic forgetting and minimize\nlearning costs. Extensive experiments show the superiority of GraphCLIP in both\nzero-shot and few-shot settings, while evaluations across various downstream\ntasks confirm the versatility of GraphCLIP. Our code is available at:\nhttps://github.com/ZhuYun97/GraphCLIP",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, research on Text-Attributed Graphs (TAGs) has gained significant\nattention due to the prevalence of free-text node features in real-world\napplications and the advancements in Large Language Models (LLMs) that bolster\nTAG methodologies. However, current TAG approaches face two primary challenges:\n(i) Heavy reliance on label information and (ii) Limited cross-domain\nzero/few-shot transferability. These issues constrain the scaling of both data\nand model size, owing to high labor costs and scaling laws, complicating the\ndevelopment of graph foundation models with strong transferability. In this\nwork, we propose the GraphCLIP framework to address these challenges by\nlearning graph foundation models with strong cross-domain zero/few-shot\ntransferability through a self-supervised contrastive graph-summary pretraining\nmethod. Specifically, we generate and curate large-scale graph-summary pair\ndata with the assistance of LLMs, and introduce a novel graph-summary\npretraining method, combined with invariant learning, to enhance graph\nfoundation models with strong cross-domain zero-shot transferability. For\nfew-shot learning, we propose a novel graph prompt tuning technique aligned\nwith our pretraining objective to mitigate catastrophic forgetting and minimize\nlearning costs. Extensive experiments show the superiority of GraphCLIP in both\nzero-shot and few-shot settings, while evaluations across various downstream\ntasks confirm the versatility of GraphCLIP. Our code is available at:\nhttps://github.com/ZhuYun97/GraphCLIP"
                },
                "authors": [
                    {
                        "name": "Yun Zhu"
                    },
                    {
                        "name": "Haizhou Shi"
                    },
                    {
                        "name": "Xiaotang Wang"
                    },
                    {
                        "name": "Yongchao Liu"
                    },
                    {
                        "name": "Yaoke Wang"
                    },
                    {
                        "name": "Boci Peng"
                    },
                    {
                        "name": "Chuntao Hong"
                    },
                    {
                        "name": "Siliang Tang"
                    }
                ],
                "author_detail": {
                    "name": "Siliang Tang"
                },
                "author": "Siliang Tang",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10329v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10329v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21980v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21980v1",
                "updated": "2024-10-29T12:10:20Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    12,
                    10,
                    20,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T12:10:20Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    12,
                    10,
                    20,
                    1,
                    303,
                    0
                ],
                "title": "Impact of cosmology dependence of baryonic feedback in weak lensing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Impact of cosmology dependence of baryonic feedback in weak lensing"
                },
                "summary": "Robust modeling of non-linear scales is critical for accurate cosmological\ninference in Stage IV surveys. For weak lensing analyses in particular, a key\nchallenge arises from the incomplete understanding of how non-gravitational\nprocesses, such as supernovae and active galactic nuclei - collectively known\nas baryonic feedback - affect the matter distribution. Several existing methods\nfor modeling baryonic feedback treat it independently from the underlying\ncosmology, an assumption which has been found to be inaccurate by\nhydrodynamical simulations. In this work, we examine the impact of this\ncoupling between baryonic feedback and cosmology on parameter inference at LSST\nY1 precision. We build mock 3$\\times$2pt data vectors using the Magneticum\nsuite of hydrodynamical simulations, which span a wide range of cosmologies\nwhile keeping subgrid parameters fixed. We perform simulated likelihood\nanalyses for two baryon mitigation techniques: (i) the Principal Component\nAnalysis (PCA) method which identifies eigenmodes for capturing the effect\nbaryonic feedback on the data vector and (ii) HMCode2020 (Mead et al. 2021)\nwhich analytically models the modification in the matter distribution using a\nhalo model approach. Our results show that the PCA method is robust to the\ncoupling between cosmology and baryonic feedback, whereas, when using\nHMCode2020 there can be up to $0.5\\sigma$ bias in $\\Omega_\\text{m}$-$S_8$. For\nHMCode2020, the bias also correlates with the input cosmology while for PCA we\nfind no such correlation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust modeling of non-linear scales is critical for accurate cosmological\ninference in Stage IV surveys. For weak lensing analyses in particular, a key\nchallenge arises from the incomplete understanding of how non-gravitational\nprocesses, such as supernovae and active galactic nuclei - collectively known\nas baryonic feedback - affect the matter distribution. Several existing methods\nfor modeling baryonic feedback treat it independently from the underlying\ncosmology, an assumption which has been found to be inaccurate by\nhydrodynamical simulations. In this work, we examine the impact of this\ncoupling between baryonic feedback and cosmology on parameter inference at LSST\nY1 precision. We build mock 3$\\times$2pt data vectors using the Magneticum\nsuite of hydrodynamical simulations, which span a wide range of cosmologies\nwhile keeping subgrid parameters fixed. We perform simulated likelihood\nanalyses for two baryon mitigation techniques: (i) the Principal Component\nAnalysis (PCA) method which identifies eigenmodes for capturing the effect\nbaryonic feedback on the data vector and (ii) HMCode2020 (Mead et al. 2021)\nwhich analytically models the modification in the matter distribution using a\nhalo model approach. Our results show that the PCA method is robust to the\ncoupling between cosmology and baryonic feedback, whereas, when using\nHMCode2020 there can be up to $0.5\\sigma$ bias in $\\Omega_\\text{m}$-$S_8$. For\nHMCode2020, the bias also correlates with the input cosmology while for PCA we\nfind no such correlation."
                },
                "authors": [
                    {
                        "name": "Pranjal R. S."
                    },
                    {
                        "name": "Elisabeth Krause"
                    },
                    {
                        "name": "Klaus Dolag"
                    },
                    {
                        "name": "Karim Benabed"
                    },
                    {
                        "name": "Tim Eifler"
                    },
                    {
                        "name": "Emma Ayoberry"
                    },
                    {
                        "name": "Yohan Dubois"
                    }
                ],
                "author_detail": {
                    "name": "Yohan Dubois"
                },
                "author": "Yohan Dubois",
                "arxiv_comment": "Comments welcome!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21980v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21980v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15677v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15677v2",
                "updated": "2024-10-29T11:58:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    11,
                    58,
                    42,
                    1,
                    303,
                    0
                ],
                "published": "2024-05-24T16:17:35Z",
                "published_parsed": [
                    2024,
                    5,
                    24,
                    16,
                    17,
                    35,
                    4,
                    145,
                    0
                ],
                "title": "SMART: Scalable Multi-agent Real-time Generation via Next-token\n  Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SMART: Scalable Multi-agent Real-time Generation via Next-token\n  Prediction"
                },
                "summary": "Data-driven autonomous driving motion generation tasks are frequently\nimpacted by the limitations of dataset size and the domain gap between\ndatasets, which precludes their extensive application in real-world scenarios.\nTo address this issue, we introduce SMART, a novel autonomous driving motion\ngeneration paradigm that models vectorized map and agent trajectory data into\ndiscrete sequence tokens. These tokens are then processed through a\ndecoder-only transformer architecture to train for the next token prediction\ntask across spatial-temporal series. This GPT-style method allows the model to\nlearn the motion distribution in real driving scenarios. SMART achieves\nstate-of-the-art performance across most of the metrics on the generative Sim\nAgents challenge, ranking 1st on the leaderboards of Waymo Open Motion Dataset\n(WOMD), demonstrating remarkable inference speed. Moreover, SMART represents\nthe generative model in the autonomous driving motion domain, exhibiting\nzero-shot generalization capabilities: Using only the NuPlan dataset for\ntraining and WOMD for validation, SMART achieved a competitive score of 0.72 on\nthe Sim Agents challenge. Lastly, we have collected over 1 billion motion\ntokens from multiple datasets, validating the model's scalability. These\nresults suggest that SMART has initially emulated two important properties:\nscalability and zero-shot generalization, and preliminarily meets the needs of\nlarge-scale real-time simulation applications. We have released all the code to\npromote the exploration of models for motion generation in the autonomous\ndriving field. The source code is available at\nhttps://github.com/rainmaker22/SMART.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-driven autonomous driving motion generation tasks are frequently\nimpacted by the limitations of dataset size and the domain gap between\ndatasets, which precludes their extensive application in real-world scenarios.\nTo address this issue, we introduce SMART, a novel autonomous driving motion\ngeneration paradigm that models vectorized map and agent trajectory data into\ndiscrete sequence tokens. These tokens are then processed through a\ndecoder-only transformer architecture to train for the next token prediction\ntask across spatial-temporal series. This GPT-style method allows the model to\nlearn the motion distribution in real driving scenarios. SMART achieves\nstate-of-the-art performance across most of the metrics on the generative Sim\nAgents challenge, ranking 1st on the leaderboards of Waymo Open Motion Dataset\n(WOMD), demonstrating remarkable inference speed. Moreover, SMART represents\nthe generative model in the autonomous driving motion domain, exhibiting\nzero-shot generalization capabilities: Using only the NuPlan dataset for\ntraining and WOMD for validation, SMART achieved a competitive score of 0.72 on\nthe Sim Agents challenge. Lastly, we have collected over 1 billion motion\ntokens from multiple datasets, validating the model's scalability. These\nresults suggest that SMART has initially emulated two important properties:\nscalability and zero-shot generalization, and preliminarily meets the needs of\nlarge-scale real-time simulation applications. We have released all the code to\npromote the exploration of models for motion generation in the autonomous\ndriving field. The source code is available at\nhttps://github.com/rainmaker22/SMART."
                },
                "authors": [
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Xiaoxin Feng"
                    },
                    {
                        "name": "Ziyan Gao"
                    },
                    {
                        "name": "Yuheng Kan"
                    }
                ],
                "author_detail": {
                    "name": "Yuheng Kan"
                },
                "author": "Yuheng Kan",
                "arxiv_comment": "Accepted by NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15677v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15677v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17758v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17758v3",
                "updated": "2024-10-29T11:56:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    11,
                    56,
                    27,
                    1,
                    303,
                    0
                ],
                "published": "2024-06-25T17:42:25Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    17,
                    42,
                    25,
                    1,
                    177,
                    0
                ],
                "title": "MotionBooth: Motion-Aware Customized Text-to-Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MotionBooth: Motion-Aware Customized Text-to-Video Generation"
                },
                "summary": "In this work, we present MotionBooth, an innovative framework designed for\nanimating customized subjects with precise control over both object and camera\nmovements. By leveraging a few images of a specific object, we efficiently\nfine-tune a text-to-video model to capture the object's shape and attributes\naccurately. Our approach presents subject region loss and video preservation\nloss to enhance the subject's learning performance, along with a subject token\ncross-attention loss to integrate the customized subject with motion control\nsignals. Additionally, we propose training-free techniques for managing subject\nand camera motions during inference. In particular, we utilize cross-attention\nmap manipulation to govern subject motion and introduce a novel latent shift\nmodule for camera movement control as well. MotionBooth excels in preserving\nthe appearance of subjects while simultaneously controlling the motions in\ngenerated videos. Extensive quantitative and qualitative evaluations\ndemonstrate the superiority and effectiveness of our method. Our project page\nis at https://jianzongwu.github.io/projects/motionbooth",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we present MotionBooth, an innovative framework designed for\nanimating customized subjects with precise control over both object and camera\nmovements. By leveraging a few images of a specific object, we efficiently\nfine-tune a text-to-video model to capture the object's shape and attributes\naccurately. Our approach presents subject region loss and video preservation\nloss to enhance the subject's learning performance, along with a subject token\ncross-attention loss to integrate the customized subject with motion control\nsignals. Additionally, we propose training-free techniques for managing subject\nand camera motions during inference. In particular, we utilize cross-attention\nmap manipulation to govern subject motion and introduce a novel latent shift\nmodule for camera movement control as well. MotionBooth excels in preserving\nthe appearance of subjects while simultaneously controlling the motions in\ngenerated videos. Extensive quantitative and qualitative evaluations\ndemonstrate the superiority and effectiveness of our method. Our project page\nis at https://jianzongwu.github.io/projects/motionbooth"
                },
                "authors": [
                    {
                        "name": "Jianzong Wu"
                    },
                    {
                        "name": "Xiangtai Li"
                    },
                    {
                        "name": "Yanhong Zeng"
                    },
                    {
                        "name": "Jiangning Zhang"
                    },
                    {
                        "name": "Qianyu Zhou"
                    },
                    {
                        "name": "Yining Li"
                    },
                    {
                        "name": "Yunhai Tong"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "arxiv_comment": "(NeurIPS 2024 Spotlight) Project page at\n  https://jianzongwu.github.io/projects/motionbooth",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17758v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17758v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21965v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21965v1",
                "updated": "2024-10-29T11:47:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    11,
                    47,
                    1,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T11:47:01Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    11,
                    47,
                    1,
                    1,
                    303,
                    0
                ],
                "title": "SG-Bench: Evaluating LLM Safety Generalization Across Diverse Tasks and\n  Prompt Types",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SG-Bench: Evaluating LLM Safety Generalization Across Diverse Tasks and\n  Prompt Types"
                },
                "summary": "Ensuring the safety of large language model (LLM) applications is essential\nfor developing trustworthy artificial intelligence. Current LLM safety\nbenchmarks have two limitations. First, they focus solely on either\ndiscriminative or generative evaluation paradigms while ignoring their\ninterconnection. Second, they rely on standardized inputs, overlooking the\neffects of widespread prompting techniques, such as system prompts, few-shot\ndemonstrations, and chain-of-thought prompting. To overcome these issues, we\ndeveloped SG-Bench, a novel benchmark to assess the generalization of LLM\nsafety across various tasks and prompt types. This benchmark integrates both\ngenerative and discriminative evaluation tasks and includes extended data to\nexamine the impact of prompt engineering and jailbreak on LLM safety. Our\nassessment of 3 advanced proprietary LLMs and 10 open-source LLMs with the\nbenchmark reveals that most LLMs perform worse on discriminative tasks than\ngenerative ones, and are highly susceptible to prompts, indicating poor\ngeneralization in safety alignment. We also explain these findings\nquantitatively and qualitatively to provide insights for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring the safety of large language model (LLM) applications is essential\nfor developing trustworthy artificial intelligence. Current LLM safety\nbenchmarks have two limitations. First, they focus solely on either\ndiscriminative or generative evaluation paradigms while ignoring their\ninterconnection. Second, they rely on standardized inputs, overlooking the\neffects of widespread prompting techniques, such as system prompts, few-shot\ndemonstrations, and chain-of-thought prompting. To overcome these issues, we\ndeveloped SG-Bench, a novel benchmark to assess the generalization of LLM\nsafety across various tasks and prompt types. This benchmark integrates both\ngenerative and discriminative evaluation tasks and includes extended data to\nexamine the impact of prompt engineering and jailbreak on LLM safety. Our\nassessment of 3 advanced proprietary LLMs and 10 open-source LLMs with the\nbenchmark reveals that most LLMs perform worse on discriminative tasks than\ngenerative ones, and are highly susceptible to prompts, indicating poor\ngeneralization in safety alignment. We also explain these findings\nquantitatively and qualitatively to provide insights for future research."
                },
                "authors": [
                    {
                        "name": "Yutao Mou"
                    },
                    {
                        "name": "Shikun Zhang"
                    },
                    {
                        "name": "Wei Ye"
                    }
                ],
                "author_detail": {
                    "name": "Wei Ye"
                },
                "author": "Wei Ye",
                "arxiv_comment": "Accepted by NeurIPS2024 (Dataset and Benchmark Track)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21965v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21965v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13282v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13282v2",
                "updated": "2024-10-29T11:29:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    11,
                    29,
                    31,
                    1,
                    303,
                    0
                ],
                "published": "2024-06-19T07:23:33Z",
                "published_parsed": [
                    2024,
                    6,
                    19,
                    7,
                    23,
                    33,
                    2,
                    171,
                    0
                ],
                "title": "Understanding the RoPE Extensions of Long-Context LLMs: An Attention\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the RoPE Extensions of Long-Context LLMs: An Attention\n  Perspective"
                },
                "summary": "Enabling LLMs to handle lengthy context is currently a research hotspot. Most\nLLMs are built upon rotary position embedding (RoPE), a popular position\nencoding method. Therefore, a prominent path is to extrapolate the RoPE trained\non comparably short texts to far longer texts. A heavy bunch of efforts have\nbeen dedicated to boosting the extrapolation via extending the formulations of\nthe RoPE, however, few of them have attempted to showcase their inner workings\ncomprehensively. In this paper, we are driven to offer a straightforward yet\nin-depth understanding of RoPE extensions from an attention perspective and on\ntwo benchmarking tasks. A broad array of experiments reveals several valuable\nfindings: 1) Maintaining attention patterns to those at the pretrained length\nimproves extrapolation; 2) Large attention uncertainty leads to retrieval\nerrors; 3) Using longer continual pretraining lengths for RoPE extensions could\nreduce attention uncertainty and significantly enhance extrapolation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling LLMs to handle lengthy context is currently a research hotspot. Most\nLLMs are built upon rotary position embedding (RoPE), a popular position\nencoding method. Therefore, a prominent path is to extrapolate the RoPE trained\non comparably short texts to far longer texts. A heavy bunch of efforts have\nbeen dedicated to boosting the extrapolation via extending the formulations of\nthe RoPE, however, few of them have attempted to showcase their inner workings\ncomprehensively. In this paper, we are driven to offer a straightforward yet\nin-depth understanding of RoPE extensions from an attention perspective and on\ntwo benchmarking tasks. A broad array of experiments reveals several valuable\nfindings: 1) Maintaining attention patterns to those at the pretrained length\nimproves extrapolation; 2) Large attention uncertainty leads to retrieval\nerrors; 3) Using longer continual pretraining lengths for RoPE extensions could\nreduce attention uncertainty and significantly enhance extrapolation."
                },
                "authors": [
                    {
                        "name": "Meizhi Zhong"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Yikun Lei"
                    },
                    {
                        "name": "Xikai Liu"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13282v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13282v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21958v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21958v1",
                "updated": "2024-10-29T11:23:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    11,
                    23,
                    9,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T11:23:09Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    11,
                    23,
                    9,
                    1,
                    303,
                    0
                ],
                "title": "Spatio-temporal Transformers for Action Unit Classification with Event\n  Cameras",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatio-temporal Transformers for Action Unit Classification with Event\n  Cameras"
                },
                "summary": "Face analysis has been studied from different angles to infer emotion, poses,\nshapes, and landmarks. Traditionally RGB cameras are used, yet for fine-grained\ntasks standard sensors might not be up to the task due to their latency, making\nit impossible to record and detect micro-movements that carry a highly\ninformative signal, which is necessary for inferring the true emotions of a\nsubject. Event cameras have been increasingly gaining interest as a possible\nsolution to this and similar high-frame rate tasks. We propose a novel\nspatiotemporal Vision Transformer model that uses Shifted Patch Tokenization\n(SPT) and Locality Self-Attention (LSA) to enhance the accuracy of Action Unit\nclassification from event streams. We also address the lack of labeled event\ndata in the literature, which can be considered one of the main causes of an\nexisting gap between the maturity of RGB and neuromorphic vision models.\nGathering data is harder in the event domain since it cannot be crawled from\nthe web and labeling frames should take into account event aggregation rates\nand the fact that static parts might not be visible in certain frames. To this\nend, we present FACEMORPHIC, a temporally synchronized multimodal face dataset\ncomposed of RGB videos and event streams. The dataset is annotated at a video\nlevel with facial Action Units and contains streams collected with various\npossible applications, ranging from 3D shape estimation to lip-reading. We then\nshow how temporal synchronization can allow effective neuromorphic face\nanalysis without the need to manually annotate videos: we instead leverage\ncross-modal supervision bridging the domain gap by representing face shapes in\na 3D space. Our proposed model outperforms baseline methods by effectively\ncapturing spatial and temporal information, crucial for recognizing subtle\nfacial micro-expressions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Face analysis has been studied from different angles to infer emotion, poses,\nshapes, and landmarks. Traditionally RGB cameras are used, yet for fine-grained\ntasks standard sensors might not be up to the task due to their latency, making\nit impossible to record and detect micro-movements that carry a highly\ninformative signal, which is necessary for inferring the true emotions of a\nsubject. Event cameras have been increasingly gaining interest as a possible\nsolution to this and similar high-frame rate tasks. We propose a novel\nspatiotemporal Vision Transformer model that uses Shifted Patch Tokenization\n(SPT) and Locality Self-Attention (LSA) to enhance the accuracy of Action Unit\nclassification from event streams. We also address the lack of labeled event\ndata in the literature, which can be considered one of the main causes of an\nexisting gap between the maturity of RGB and neuromorphic vision models.\nGathering data is harder in the event domain since it cannot be crawled from\nthe web and labeling frames should take into account event aggregation rates\nand the fact that static parts might not be visible in certain frames. To this\nend, we present FACEMORPHIC, a temporally synchronized multimodal face dataset\ncomposed of RGB videos and event streams. The dataset is annotated at a video\nlevel with facial Action Units and contains streams collected with various\npossible applications, ranging from 3D shape estimation to lip-reading. We then\nshow how temporal synchronization can allow effective neuromorphic face\nanalysis without the need to manually annotate videos: we instead leverage\ncross-modal supervision bridging the domain gap by representing face shapes in\na 3D space. Our proposed model outperforms baseline methods by effectively\ncapturing spatial and temporal information, crucial for recognizing subtle\nfacial micro-expressions."
                },
                "authors": [
                    {
                        "name": "Luca Cultrera"
                    },
                    {
                        "name": "Federico Becattini"
                    },
                    {
                        "name": "Lorenzo Berlincioni"
                    },
                    {
                        "name": "Claudio Ferrari"
                    },
                    {
                        "name": "Alberto Del Bimbo"
                    }
                ],
                "author_detail": {
                    "name": "Alberto Del Bimbo"
                },
                "author": "Alberto Del Bimbo",
                "arxiv_comment": "Under review at CVIU. arXiv admin note: substantial text overlap with\n  arXiv:2409.10213",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21958v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21958v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16122v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16122v2",
                "updated": "2024-10-29T11:17:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    11,
                    17,
                    44,
                    1,
                    303,
                    0
                ],
                "published": "2024-05-25T08:23:05Z",
                "published_parsed": [
                    2024,
                    5,
                    25,
                    8,
                    23,
                    5,
                    5,
                    146,
                    0
                ],
                "title": "Prompt Optimization with EASE? Efficient Ordering-aware Automated\n  Selection of Exemplars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt Optimization with EASE? Efficient Ordering-aware Automated\n  Selection of Exemplars"
                },
                "summary": "Large language models (LLMs) have shown impressive capabilities in real-world\napplications. The capability of in-context learning (ICL) allows us to adapt an\nLLM to downstream tasks by including input-label exemplars in the prompt\nwithout model fine-tuning. However, the quality of these exemplars in the\nprompt greatly impacts performance, highlighting the need for an effective\nautomated exemplar selection method. Recent studies have explored\nretrieval-based approaches to select exemplars tailored to individual test\nqueries, which can be undesirable due to extra test-time computation and an\nincreased risk of data exposure. Moreover, existing methods fail to adequately\naccount for the impact of exemplar ordering on the performance. On the other\nhand, the impact of the instruction, another essential component in the prompt\ngiven to the LLM, is often overlooked in existing exemplar selection methods.\nTo address these challenges, we propose a novel method named EASE, which\nleverages the hidden embedding from a pre-trained language model to represent\nordered sets of exemplars and uses a neural bandit algorithm to optimize the\nsets of exemplars while accounting for exemplar ordering. Our EASE can\nefficiently find an ordered set of exemplars that performs well for all test\nqueries from a given task, thereby eliminating test-time computation.\nImportantly, EASE can be readily extended to jointly optimize both the\nexemplars and the instruction. Through extensive empirical evaluations\n(including novel tasks), we demonstrate the superiority of EASE over existing\nmethods, and reveal practical insights about the impact of exemplar selection\non ICL, which may be of independent interest. Our code is available at\nhttps://github.com/ZhaoxuanWu/EASE-Prompt-Optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown impressive capabilities in real-world\napplications. The capability of in-context learning (ICL) allows us to adapt an\nLLM to downstream tasks by including input-label exemplars in the prompt\nwithout model fine-tuning. However, the quality of these exemplars in the\nprompt greatly impacts performance, highlighting the need for an effective\nautomated exemplar selection method. Recent studies have explored\nretrieval-based approaches to select exemplars tailored to individual test\nqueries, which can be undesirable due to extra test-time computation and an\nincreased risk of data exposure. Moreover, existing methods fail to adequately\naccount for the impact of exemplar ordering on the performance. On the other\nhand, the impact of the instruction, another essential component in the prompt\ngiven to the LLM, is often overlooked in existing exemplar selection methods.\nTo address these challenges, we propose a novel method named EASE, which\nleverages the hidden embedding from a pre-trained language model to represent\nordered sets of exemplars and uses a neural bandit algorithm to optimize the\nsets of exemplars while accounting for exemplar ordering. Our EASE can\nefficiently find an ordered set of exemplars that performs well for all test\nqueries from a given task, thereby eliminating test-time computation.\nImportantly, EASE can be readily extended to jointly optimize both the\nexemplars and the instruction. Through extensive empirical evaluations\n(including novel tasks), we demonstrate the superiority of EASE over existing\nmethods, and reveal practical insights about the impact of exemplar selection\non ICL, which may be of independent interest. Our code is available at\nhttps://github.com/ZhaoxuanWu/EASE-Prompt-Optimization."
                },
                "authors": [
                    {
                        "name": "Zhaoxuan Wu"
                    },
                    {
                        "name": "Xiaoqiang Lin"
                    },
                    {
                        "name": "Zhongxiang Dai"
                    },
                    {
                        "name": "Wenyang Hu"
                    },
                    {
                        "name": "Yao Shu"
                    },
                    {
                        "name": "See-Kiong Ng"
                    },
                    {
                        "name": "Patrick Jaillet"
                    },
                    {
                        "name": "Bryan Kian Hsiang Low"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Kian Hsiang Low"
                },
                "author": "Bryan Kian Hsiang Low",
                "arxiv_comment": "28 pages, 1 figure, 35 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16122v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16122v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13708v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13708v2",
                "updated": "2024-10-29T11:14:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    11,
                    14,
                    46,
                    1,
                    303,
                    0
                ],
                "published": "2024-09-06T14:26:18Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    14,
                    26,
                    18,
                    4,
                    250,
                    0
                ],
                "title": "Towards Safe Multilingual Frontier AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Safe Multilingual Frontier AI"
                },
                "summary": "Linguistically inclusive LLMs -- which maintain good performance regardless\nof the language with which they are prompted -- are necessary for the diffusion\nof AI benefits around the world. Multilingual jailbreaks that rely on language\ntranslation to evade safety measures undermine the safe and inclusive\ndeployment of AI systems. We provide policy recommendations to enhance the\nmultilingual capabilities of AI while mitigating the risks of multilingual\njailbreaks. We examine how a language's level of resourcing relates to how\nvulnerable LLMs are to multilingual jailbreaks in that language. We do this by\ntesting five advanced AI models across 24 official languages of the EU.\nBuilding on prior research, we propose policy actions that align with the EU\nlegal landscape and institutional framework to address multilingual jailbreaks,\nwhile promoting linguistic inclusivity. These include mandatory assessments of\nmultilingual capabilities and vulnerabilities, public opinion research, and\nstate support for multilingual AI development. The measures aim to improve AI\nsafety and functionality through EU policy initiatives, guiding the\nimplementation of the EU AI Act and informing regulatory efforts of the\nEuropean AI Office.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linguistically inclusive LLMs -- which maintain good performance regardless\nof the language with which they are prompted -- are necessary for the diffusion\nof AI benefits around the world. Multilingual jailbreaks that rely on language\ntranslation to evade safety measures undermine the safe and inclusive\ndeployment of AI systems. We provide policy recommendations to enhance the\nmultilingual capabilities of AI while mitigating the risks of multilingual\njailbreaks. We examine how a language's level of resourcing relates to how\nvulnerable LLMs are to multilingual jailbreaks in that language. We do this by\ntesting five advanced AI models across 24 official languages of the EU.\nBuilding on prior research, we propose policy actions that align with the EU\nlegal landscape and institutional framework to address multilingual jailbreaks,\nwhile promoting linguistic inclusivity. These include mandatory assessments of\nmultilingual capabilities and vulnerabilities, public opinion research, and\nstate support for multilingual AI development. The measures aim to improve AI\nsafety and functionality through EU policy initiatives, guiding the\nimplementation of the EU AI Act and informing regulatory efforts of the\nEuropean AI Office."
                },
                "authors": [
                    {
                        "name": "Artrs Kanepajs"
                    },
                    {
                        "name": "Vladimir Ivanov"
                    },
                    {
                        "name": "Richard Moulange"
                    }
                ],
                "author_detail": {
                    "name": "Richard Moulange"
                },
                "author": "Richard Moulange",
                "arxiv_comment": "23 pages; 1 figure and 10 supplementary figures; Accepted (spotlight\n  presentation) at NeurIPS 2024 SoLaR workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13708v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13708v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21954v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21954v1",
                "updated": "2024-10-29T11:12:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    11,
                    12,
                    58,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T11:12:58Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    11,
                    12,
                    58,
                    1,
                    303,
                    0
                ],
                "title": "Inference of a Susceptible-Infectious stochastic model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference of a Susceptible-Infectious stochastic model"
                },
                "summary": "We consider a time-inhomogeneous diffusion process able to describe the\ndynamics of infected people in a susceptible-infectious epidemic model in which\nthe transmission intensity function is time-dependent. Such a model is well\nsuited to describe some classes of micro-parasitic infections in which\nindividuals never acquire lasting immunity and over the course of the epidemic\neveryone eventually becomes infected. The stochastic process related to the\ndeterministic model is transformable into a non homogeneous Wiener process so\nthe probability distribution can be obtained. Here we focus on the inference\nfor such process, by providing an estimation procedure for the involved\nparameters. We point out that the time dependence in the infinitesimal moments\nof the diffusion process makes classical inference methods inapplicable. The\nproposed procedure is based on Generalized Method of Moments in order to find\nsuitable estimate for the infinitesimal drift and variance of the transformed\nprocess. Several simulation studies are conduced to test the procedure, these\ninclude the time homogeneous case, for which a comparison with the results\nobtained by applying the MLE is made, and cases in which the intensity function\nare time dependent with particular attention to periodic cases. Finally, we\napply the estimation procedure to a real dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a time-inhomogeneous diffusion process able to describe the\ndynamics of infected people in a susceptible-infectious epidemic model in which\nthe transmission intensity function is time-dependent. Such a model is well\nsuited to describe some classes of micro-parasitic infections in which\nindividuals never acquire lasting immunity and over the course of the epidemic\neveryone eventually becomes infected. The stochastic process related to the\ndeterministic model is transformable into a non homogeneous Wiener process so\nthe probability distribution can be obtained. Here we focus on the inference\nfor such process, by providing an estimation procedure for the involved\nparameters. We point out that the time dependence in the infinitesimal moments\nof the diffusion process makes classical inference methods inapplicable. The\nproposed procedure is based on Generalized Method of Moments in order to find\nsuitable estimate for the infinitesimal drift and variance of the transformed\nprocess. Several simulation studies are conduced to test the procedure, these\ninclude the time homogeneous case, for which a comparison with the results\nobtained by applying the MLE is made, and cases in which the intensity function\nare time dependent with particular attention to periodic cases. Finally, we\napply the estimation procedure to a real dataset."
                },
                "authors": [
                    {
                        "name": "Giuseppina Albano"
                    },
                    {
                        "name": "Virginia Giorno"
                    },
                    {
                        "name": "Francisco Torres-Ruiz"
                    }
                ],
                "author_detail": {
                    "name": "Francisco Torres-Ruiz"
                },
                "author": "Francisco Torres-Ruiz",
                "arxiv_doi": "10.3934/mbe.2024310",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3934/mbe.2024310",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.21954v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21954v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "20 pages, 6 figures",
                "arxiv_journal_ref": "Mathematical Biosciences and Engineering, 21(9), 7067-7083, 2024",
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21952v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21952v1",
                "updated": "2024-10-29T11:12:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    11,
                    12,
                    44,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T11:12:44Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    11,
                    12,
                    44,
                    1,
                    303,
                    0
                ],
                "title": "On the Robustness of Adversarial Training Against Uncertainty Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Robustness of Adversarial Training Against Uncertainty Attacks"
                },
                "summary": "In learning problems, the noise inherent to the task at hand hinders the\npossibility to infer without a certain degree of uncertainty. Quantifying this\nuncertainty, regardless of its wide use, assumes high relevance for\nsecurity-sensitive applications. Within these scenarios, it becomes fundamental\nto guarantee good (i.e., trustworthy) uncertainty measures, which downstream\nmodules can securely employ to drive the final decision-making process.\nHowever, an attacker may be interested in forcing the system to produce either\n(i) highly uncertain outputs jeopardizing the system's availability or (ii) low\nuncertainty estimates, making the system accept uncertain samples that would\ninstead require a careful inspection (e.g., human intervention). Therefore, it\nbecomes fundamental to understand how to obtain robust uncertainty estimates\nagainst these kinds of attacks. In this work, we reveal both empirically and\ntheoretically that defending against adversarial examples, i.e., carefully\nperturbed samples that cause misclassification, additionally guarantees a more\nsecure, trustworthy uncertainty estimate under common attack scenarios without\nthe need for an ad-hoc defense strategy. To support our claims, we evaluate\nmultiple adversarial-robust models from the publicly available benchmark\nRobustBench on the CIFAR-10 and ImageNet datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In learning problems, the noise inherent to the task at hand hinders the\npossibility to infer without a certain degree of uncertainty. Quantifying this\nuncertainty, regardless of its wide use, assumes high relevance for\nsecurity-sensitive applications. Within these scenarios, it becomes fundamental\nto guarantee good (i.e., trustworthy) uncertainty measures, which downstream\nmodules can securely employ to drive the final decision-making process.\nHowever, an attacker may be interested in forcing the system to produce either\n(i) highly uncertain outputs jeopardizing the system's availability or (ii) low\nuncertainty estimates, making the system accept uncertain samples that would\ninstead require a careful inspection (e.g., human intervention). Therefore, it\nbecomes fundamental to understand how to obtain robust uncertainty estimates\nagainst these kinds of attacks. In this work, we reveal both empirically and\ntheoretically that defending against adversarial examples, i.e., carefully\nperturbed samples that cause misclassification, additionally guarantees a more\nsecure, trustworthy uncertainty estimate under common attack scenarios without\nthe need for an ad-hoc defense strategy. To support our claims, we evaluate\nmultiple adversarial-robust models from the publicly available benchmark\nRobustBench on the CIFAR-10 and ImageNet datasets."
                },
                "authors": [
                    {
                        "name": "Emanuele Ledda"
                    },
                    {
                        "name": "Giovanni Scodeller"
                    },
                    {
                        "name": "Daniele Angioni"
                    },
                    {
                        "name": "Giorgio Piras"
                    },
                    {
                        "name": "Antonio Emanuele Cin"
                    },
                    {
                        "name": "Giorgio Fumera"
                    },
                    {
                        "name": "Battista Biggio"
                    },
                    {
                        "name": "Fabio Roli"
                    }
                ],
                "author_detail": {
                    "name": "Fabio Roli"
                },
                "author": "Fabio Roli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21952v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21952v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21951v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21951v1",
                "updated": "2024-10-29T11:12:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    11,
                    12,
                    1,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T11:12:01Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    11,
                    12,
                    1,
                    1,
                    303,
                    0
                ],
                "title": "Fast and High-Quality Auto-Regressive Speech Synthesis via Speculative\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and High-Quality Auto-Regressive Speech Synthesis via Speculative\n  Decoding"
                },
                "summary": "The auto-regressive architecture, like GPTs, is widely used in modern\nText-to-Speech (TTS) systems. However, it incurs substantial inference time,\nparticularly due to the challenges in the next-token prediction posed by\nlengthy sequences of speech tokens. In this work, we introduce VADUSA, one of\nthe first approaches to accelerate auto-regressive TTS through speculative\ndecoding. Our results show that VADUSA not only significantly improves\ninference speed but also enhances performance by incorporating draft heads to\npredict future speech content auto-regressively. Furthermore, the inclusion of\na tolerance mechanism during sampling accelerates inference without\ncompromising quality. Our approach demonstrates strong generalization across\nlarge datasets and various types of speech tokens.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The auto-regressive architecture, like GPTs, is widely used in modern\nText-to-Speech (TTS) systems. However, it incurs substantial inference time,\nparticularly due to the challenges in the next-token prediction posed by\nlengthy sequences of speech tokens. In this work, we introduce VADUSA, one of\nthe first approaches to accelerate auto-regressive TTS through speculative\ndecoding. Our results show that VADUSA not only significantly improves\ninference speed but also enhances performance by incorporating draft heads to\npredict future speech content auto-regressively. Furthermore, the inclusion of\na tolerance mechanism during sampling accelerates inference without\ncompromising quality. Our approach demonstrates strong generalization across\nlarge datasets and various types of speech tokens."
                },
                "authors": [
                    {
                        "name": "Bohan Li"
                    },
                    {
                        "name": "Hankun Wang"
                    },
                    {
                        "name": "Situo Zhang"
                    },
                    {
                        "name": "Yiwei Guo"
                    },
                    {
                        "name": "Kai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yu"
                },
                "author": "Kai Yu",
                "arxiv_comment": "5 pages, 3 figures, 3 tables. Submitted to ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21951v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21951v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00456v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00456v2",
                "updated": "2024-10-29T11:09:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    11,
                    9,
                    12,
                    1,
                    303,
                    0
                ],
                "published": "2024-03-30T19:20:06Z",
                "published_parsed": [
                    2024,
                    3,
                    30,
                    19,
                    20,
                    6,
                    5,
                    90,
                    0
                ],
                "title": "QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs"
                },
                "summary": "We introduce QuaRot, a new Quantization scheme based on Rotations, which is\nable to quantize LLMs end-to-end, including all weights, activations, and KV\ncache in 4 bits. QuaRot rotates LLMs in a way that removes outliers from the\nhidden state without changing the output, making quantization easier. This\ncomputational invariance is applied to the hidden state (residual) of the LLM,\nas well as to the activations of the feed-forward components, aspects of the\nattention mechanism, and to the KV cache. The result is a quantized model where\nall matrix multiplications are performed in 4 bits, without any channels\nidentified for retention in higher precision. Our 4-bit quantized LLaMa2-70B\nmodel has losses of at most 0.47 WikiText-2 perplexity and retains 99% of the\nzero-shot performance. We also show that QuaRot can provide lossless 6 and 8\nbit LLaMa2 models without any calibration data using round-to-nearest\nquantization. Code is available at: https://github.com/spcl/QuaRot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce QuaRot, a new Quantization scheme based on Rotations, which is\nable to quantize LLMs end-to-end, including all weights, activations, and KV\ncache in 4 bits. QuaRot rotates LLMs in a way that removes outliers from the\nhidden state without changing the output, making quantization easier. This\ncomputational invariance is applied to the hidden state (residual) of the LLM,\nas well as to the activations of the feed-forward components, aspects of the\nattention mechanism, and to the KV cache. The result is a quantized model where\nall matrix multiplications are performed in 4 bits, without any channels\nidentified for retention in higher precision. Our 4-bit quantized LLaMa2-70B\nmodel has losses of at most 0.47 WikiText-2 perplexity and retains 99% of the\nzero-shot performance. We also show that QuaRot can provide lossless 6 and 8\nbit LLaMa2 models without any calibration data using round-to-nearest\nquantization. Code is available at: https://github.com/spcl/QuaRot."
                },
                "authors": [
                    {
                        "name": "Saleh Ashkboos"
                    },
                    {
                        "name": "Amirkeivan Mohtashami"
                    },
                    {
                        "name": "Maximilian L. Croci"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Pashmina Cameron"
                    },
                    {
                        "name": "Martin Jaggi"
                    },
                    {
                        "name": "Dan Alistarh"
                    },
                    {
                        "name": "Torsten Hoefler"
                    },
                    {
                        "name": "James Hensman"
                    }
                ],
                "author_detail": {
                    "name": "James Hensman"
                },
                "author": "James Hensman",
                "arxiv_comment": "21 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00456v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00456v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21943v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21943v1",
                "updated": "2024-10-29T11:03:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    11,
                    3,
                    31,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T11:03:31Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    11,
                    3,
                    31,
                    1,
                    303,
                    0
                ],
                "title": "Beyond Text: Optimizing RAG with Multimodal Inputs for Industrial\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Text: Optimizing RAG with Multimodal Inputs for Industrial\n  Applications"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nanswering questions, but they lack domain-specific knowledge and are prone to\nhallucinations. Retrieval Augmented Generation (RAG) is one approach to address\nthese challenges, while multimodal models are emerging as promising AI\nassistants for processing both text and images. In this paper we describe a\nseries of experiments aimed at determining how to best integrate multimodal\nmodels into RAG systems for the industrial domain. The purpose of the\nexperiments is to determine whether including images alongside text from\ndocuments within the industrial domain increases RAG performance and to find\nthe optimal configuration for such a multimodal RAG system. Our experiments\ninclude two approaches for image processing and retrieval, as well as two LLMs\n(GPT4-Vision and LLaVA) for answer synthesis. These image processing strategies\ninvolve the use of multimodal embeddings and the generation of textual\nsummaries from images. We evaluate our experiments with an LLM-as-a-Judge\napproach. Our results reveal that multimodal RAG can outperform single-modality\nRAG settings, although image retrieval poses a greater challenge than text\nretrieval. Additionally, leveraging textual summaries from images presents a\nmore promising approach compared to the use of multimodal embeddings, providing\nmore opportunities for future advancements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nanswering questions, but they lack domain-specific knowledge and are prone to\nhallucinations. Retrieval Augmented Generation (RAG) is one approach to address\nthese challenges, while multimodal models are emerging as promising AI\nassistants for processing both text and images. In this paper we describe a\nseries of experiments aimed at determining how to best integrate multimodal\nmodels into RAG systems for the industrial domain. The purpose of the\nexperiments is to determine whether including images alongside text from\ndocuments within the industrial domain increases RAG performance and to find\nthe optimal configuration for such a multimodal RAG system. Our experiments\ninclude two approaches for image processing and retrieval, as well as two LLMs\n(GPT4-Vision and LLaVA) for answer synthesis. These image processing strategies\ninvolve the use of multimodal embeddings and the generation of textual\nsummaries from images. We evaluate our experiments with an LLM-as-a-Judge\napproach. Our results reveal that multimodal RAG can outperform single-modality\nRAG settings, although image retrieval poses a greater challenge than text\nretrieval. Additionally, leveraging textual summaries from images presents a\nmore promising approach compared to the use of multimodal embeddings, providing\nmore opportunities for future advancements."
                },
                "authors": [
                    {
                        "name": "Monica Riedler"
                    },
                    {
                        "name": "Stefan Langer"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Langer"
                },
                "author": "Stefan Langer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21943v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21943v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21939v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21939v1",
                "updated": "2024-10-29T10:57:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    10,
                    57,
                    11,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T10:57:11Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    10,
                    57,
                    11,
                    1,
                    303,
                    0
                ],
                "title": "Benchmarking OpenAI o1 in Cyber Security",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking OpenAI o1 in Cyber Security"
                },
                "summary": "We evaluate OpenAI's o1-preview and o1-mini models, benchmarking their\nperformance against the earlier GPT-4o model. Our evaluation focuses on their\nability to detect vulnerabilities in real-world software by generating\nstructured inputs that trigger known sanitizers. Using DARPA's AI Cyber\nChallenge (AIxCC) framework and the Nginx challenge project--a deliberately\nmodified version of the widely-used Nginx web server--we create a well-defined\nyet complex environment for testing LLMs on automated vulnerability detection\n(AVD) tasks. Our results show that the o1-preview model significantly\noutperforms GPT-4o in both success rate and efficiency, especially in more\ncomplex scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We evaluate OpenAI's o1-preview and o1-mini models, benchmarking their\nperformance against the earlier GPT-4o model. Our evaluation focuses on their\nability to detect vulnerabilities in real-world software by generating\nstructured inputs that trigger known sanitizers. Using DARPA's AI Cyber\nChallenge (AIxCC) framework and the Nginx challenge project--a deliberately\nmodified version of the widely-used Nginx web server--we create a well-defined\nyet complex environment for testing LLMs on automated vulnerability detection\n(AVD) tasks. Our results show that the o1-preview model significantly\noutperforms GPT-4o in both success rate and efficiency, especially in more\ncomplex scenarios."
                },
                "authors": [
                    {
                        "name": "Dan Ristea"
                    },
                    {
                        "name": "Vasilios Mavroudis"
                    },
                    {
                        "name": "Chris Hicks"
                    }
                ],
                "author_detail": {
                    "name": "Chris Hicks"
                },
                "author": "Chris Hicks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21939v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21939v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11430v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11430v3",
                "updated": "2024-10-29T10:52:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    10,
                    52,
                    35,
                    1,
                    303,
                    0
                ],
                "published": "2024-06-17T11:35:16Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    11,
                    35,
                    16,
                    0,
                    169,
                    0
                ],
                "title": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression"
                },
                "summary": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability."
                },
                "authors": [
                    {
                        "name": "Alessio Devoto"
                    },
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Simone Scardapane"
                    },
                    {
                        "name": "Pasquale Minervini"
                    }
                ],
                "author_detail": {
                    "name": "Pasquale Minervini"
                },
                "author": "Pasquale Minervini",
                "arxiv_comment": "This is an extended version of a paper published in the proceedings\n  of the 2024 Conference on Empirical Methods in Natural Language Processing\n  (EMNLP 2024); this version was presented at the 4th NeurIPS Workshop on\n  Efficient Natural Language and Speech Processing (ENLSP-IV)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11430v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11430v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19572v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19572v2",
                "updated": "2024-10-29T10:40:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    10,
                    40,
                    54,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-25T14:07:53Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    14,
                    7,
                    53,
                    4,
                    299,
                    0
                ],
                "title": "ChunkRAG: Novel LLM-Chunk Filtering Method for RAG Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkRAG: Novel LLM-Chunk Filtering Method for RAG Systems"
                },
                "summary": "Retrieval-Augmented Generation (RAG) systems using large language models\n(LLMs) often generate inaccurate responses due to the retrieval of irrelevant\nor loosely related information. Existing methods, which operate at the document\nlevel, fail to effectively filter out such content. We propose LLM-driven chunk\nfiltering, ChunkRAG, a framework that enhances RAG systems by evaluating and\nfiltering retrieved information at the chunk level. Our approach employs\nsemantic chunking to divide documents into coherent sections and utilizes\nLLM-based relevance scoring to assess each chunk's alignment with the user's\nquery. By filtering out less pertinent chunks before the generation phase, we\nsignificantly reduce hallucinations and improve factual accuracy. Experiments\nshow that our method outperforms existing RAG models, achieving higher accuracy\non tasks requiring precise information retrieval. This advancement enhances the\nreliability of RAG systems, making them particularly beneficial for\napplications like fact-checking and multi-hop reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) systems using large language models\n(LLMs) often generate inaccurate responses due to the retrieval of irrelevant\nor loosely related information. Existing methods, which operate at the document\nlevel, fail to effectively filter out such content. We propose LLM-driven chunk\nfiltering, ChunkRAG, a framework that enhances RAG systems by evaluating and\nfiltering retrieved information at the chunk level. Our approach employs\nsemantic chunking to divide documents into coherent sections and utilizes\nLLM-based relevance scoring to assess each chunk's alignment with the user's\nquery. By filtering out less pertinent chunks before the generation phase, we\nsignificantly reduce hallucinations and improve factual accuracy. Experiments\nshow that our method outperforms existing RAG models, achieving higher accuracy\non tasks requiring precise information retrieval. This advancement enhances the\nreliability of RAG systems, making them particularly beneficial for\napplications like fact-checking and multi-hop reasoning."
                },
                "authors": [
                    {
                        "name": "Ritvik Aggarwal Ishneet Sukhvinder Singh Ibrahim Allahverdiyev"
                    },
                    {
                        "name": "Muhammad Taha"
                    },
                    {
                        "name": "Aslihan Akalin"
                    },
                    {
                        "name": "Kevin Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Zhu"
                },
                "author": "Kevin Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19572v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19572v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21925v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21925v1",
                "updated": "2024-10-29T10:35:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    10,
                    35,
                    13,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T10:35:13Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    10,
                    35,
                    13,
                    1,
                    303,
                    0
                ],
                "title": "Redshift-Space Distortion constraints on neutrino mass and models to\n  alleviate the Hubble tension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Redshift-Space Distortion constraints on neutrino mass and models to\n  alleviate the Hubble tension"
                },
                "summary": "We discuss the neutrino mass and Hubble tension solutions and examine their\neffects on the Redshift-Space Distortion (RSD) observations. An analysis with\nRSD data indicates smaller amplitude of perturbation. Including RSD data\nresults in a slightly weaker upper limit on the neutrino mass than that derived\nfor data without RSD, which is common in other extended models too. We have\nevaluated the impacts of RSD observations on some extended models, including\nthe varying electron mass model, a time-dependent dark energy model with two\nparameter equations of state (EOS), and a model where the number of neutrino\nspecies is free. When we estimate the cosmological parameters for data\nincluding RSD, we found that the EOS parameter for dark energy is larger than\nthat of the cosmological constant, and the effective number of neutrino species\nis smaller than the standard value, which infers a smaller present Hubble\nparameter $H_0$. From the viewpoint of cosmological tensions, the varying\nelectron mass model with non-zero neutrino mass option looks promising to relax\nthe Hubble tension and the $S_8$ tension simultaneously.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We discuss the neutrino mass and Hubble tension solutions and examine their\neffects on the Redshift-Space Distortion (RSD) observations. An analysis with\nRSD data indicates smaller amplitude of perturbation. Including RSD data\nresults in a slightly weaker upper limit on the neutrino mass than that derived\nfor data without RSD, which is common in other extended models too. We have\nevaluated the impacts of RSD observations on some extended models, including\nthe varying electron mass model, a time-dependent dark energy model with two\nparameter equations of state (EOS), and a model where the number of neutrino\nspecies is free. When we estimate the cosmological parameters for data\nincluding RSD, we found that the EOS parameter for dark energy is larger than\nthat of the cosmological constant, and the effective number of neutrino species\nis smaller than the standard value, which infers a smaller present Hubble\nparameter $H_0$. From the viewpoint of cosmological tensions, the varying\nelectron mass model with non-zero neutrino mass option looks promising to relax\nthe Hubble tension and the $S_8$ tension simultaneously."
                },
                "authors": [
                    {
                        "name": "Yo Toda"
                    },
                    {
                        "name": "Osamu Seto"
                    }
                ],
                "author_detail": {
                    "name": "Osamu Seto"
                },
                "author": "Osamu Seto",
                "arxiv_comment": "22 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21925v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21925v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21920v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21920v1",
                "updated": "2024-10-29T10:24:43Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    10,
                    24,
                    43,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T10:24:43Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    10,
                    24,
                    43,
                    1,
                    303,
                    0
                ],
                "title": "Online Test of a Neural Network Deep Convection Parameterization in\n  ARP-GEM1",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Test of a Neural Network Deep Convection Parameterization in\n  ARP-GEM1"
                },
                "summary": "In this study, we present the integration of a neural network-based\nparameterization into the global atmospheric model ARP-GEM1, leveraging the\nPython interface of the OASIS coupler. This approach facilitates the exchange\nof fields between the Fortran-based ARP-GEM1 model and a Python component\nresponsible for neural network inference. As a proof-of-concept experiment, we\ntrained a neural network to emulate the deep convection parameterization of\nARP-GEM1. Using the flexible Fortran/Python interface, we have successfully\nreplaced ARP-GEM1's deep convection scheme with a neural network emulator. To\nassess the performance of the neural network deep convection scheme, we have\nrun a 5-years ARP-GEM1 simulation using the neural network emulator. The\nevaluation of averaged fields showed good agreement with output from an\nARP-GEM1 simulation using the physics-based deep convection scheme. The Python\ncomponent was deployed on a separate partition from the general circulation\nmodel, using GPUs to increase inference speed of the neural network.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we present the integration of a neural network-based\nparameterization into the global atmospheric model ARP-GEM1, leveraging the\nPython interface of the OASIS coupler. This approach facilitates the exchange\nof fields between the Fortran-based ARP-GEM1 model and a Python component\nresponsible for neural network inference. As a proof-of-concept experiment, we\ntrained a neural network to emulate the deep convection parameterization of\nARP-GEM1. Using the flexible Fortran/Python interface, we have successfully\nreplaced ARP-GEM1's deep convection scheme with a neural network emulator. To\nassess the performance of the neural network deep convection scheme, we have\nrun a 5-years ARP-GEM1 simulation using the neural network emulator. The\nevaluation of averaged fields showed good agreement with output from an\nARP-GEM1 simulation using the physics-based deep convection scheme. The Python\ncomponent was deployed on a separate partition from the general circulation\nmodel, using GPUs to increase inference speed of the neural network."
                },
                "authors": [
                    {
                        "name": "Blanka Balogh"
                    },
                    {
                        "name": "David Saint-Martin"
                    },
                    {
                        "name": "Olivier Geoffroy"
                    }
                ],
                "author_detail": {
                    "name": "Olivier Geoffroy"
                },
                "author": "Olivier Geoffroy",
                "arxiv_comment": "10 pages, 5 figures, submitted to Artificial Intelligence for the\n  Earth Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21920v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21920v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ao-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21917v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21917v2",
                "updated": "2024-10-30T05:46:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    5,
                    46,
                    38,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-29T10:15:56Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    10,
                    15,
                    56,
                    1,
                    303,
                    0
                ],
                "title": "Identifiability Analysis of Linear ODE Systems with Hidden Confounders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifiability Analysis of Linear ODE Systems with Hidden Confounders"
                },
                "summary": "The identifiability analysis of linear Ordinary Differential Equation (ODE)\nsystems is a necessary prerequisite for making reliable causal inferences about\nthese systems. While identifiability has been well studied in scenarios where\nthe system is fully observable, the conditions for identifiability remain\nunexplored when latent variables interact with the system. This paper aims to\naddress this gap by presenting a systematic analysis of identifiability in\nlinear ODE systems incorporating hidden confounders. Specifically, we\ninvestigate two cases of such systems. In the first case, latent confounders\nexhibit no causal relationships, yet their evolution adheres to specific\nfunctional forms, such as polynomial functions of time $t$. Subsequently, we\nextend this analysis to encompass scenarios where hidden confounders exhibit\ncausal dependencies, with the causal structure of latent variables described by\na Directed Acyclic Graph (DAG). The second case represents a more intricate\nvariation of the first case, prompting a more comprehensive identifiability\nanalysis. Accordingly, we conduct detailed identifiability analyses of the\nsecond system under various observation conditions, including both continuous\nand discrete observations from single or multiple trajectories. To validate our\ntheoretical results, we perform a series of simulations, which support and\nsubstantiate our findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The identifiability analysis of linear Ordinary Differential Equation (ODE)\nsystems is a necessary prerequisite for making reliable causal inferences about\nthese systems. While identifiability has been well studied in scenarios where\nthe system is fully observable, the conditions for identifiability remain\nunexplored when latent variables interact with the system. This paper aims to\naddress this gap by presenting a systematic analysis of identifiability in\nlinear ODE systems incorporating hidden confounders. Specifically, we\ninvestigate two cases of such systems. In the first case, latent confounders\nexhibit no causal relationships, yet their evolution adheres to specific\nfunctional forms, such as polynomial functions of time $t$. Subsequently, we\nextend this analysis to encompass scenarios where hidden confounders exhibit\ncausal dependencies, with the causal structure of latent variables described by\na Directed Acyclic Graph (DAG). The second case represents a more intricate\nvariation of the first case, prompting a more comprehensive identifiability\nanalysis. Accordingly, we conduct detailed identifiability analyses of the\nsecond system under various observation conditions, including both continuous\nand discrete observations from single or multiple trajectories. To validate our\ntheoretical results, we perform a series of simulations, which support and\nsubstantiate our findings."
                },
                "authors": [
                    {
                        "name": "Yuanyuan Wang"
                    },
                    {
                        "name": "Biwei Huang"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Xi Geng"
                    },
                    {
                        "name": "Mingming Gong"
                    }
                ],
                "author_detail": {
                    "name": "Mingming Gong"
                },
                "author": "Mingming Gong",
                "arxiv_comment": "38th Conference on Neural Information Processing Systems (NeurIPS\n  2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21917v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21917v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21914v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21914v1",
                "updated": "2024-10-29T10:12:48Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    10,
                    12,
                    48,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T10:12:48Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    10,
                    12,
                    48,
                    1,
                    303,
                    0
                ],
                "title": "Bayesian Stability Selection and Inference on Inclusion Probabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Stability Selection and Inference on Inclusion Probabilities"
                },
                "summary": "Stability selection is a versatile framework for structure estimation and\nvariable selection in high-dimensional setting, primarily grounded in\nfrequentist principles. In this paper, we propose an enhanced methodology that\nintegrates Bayesian analysis to refine the inference of inclusion probabilities\nwithin the stability selection framework. Traditional approaches rely on\nselection frequencies for decision-making, often disregarding domain-specific\nknowledge and failing to account for the inherent uncertainty in the variable\nselection process. Our methodology uses prior information to derive posterior\ndistributions of inclusion probabilities, thereby improving both inference and\ndecision-making. We present a two-step process for engaging with domain\nexperts, enabling statisticians to elucidate prior distributions informed by\nexpert knowledge while allowing experts to control the weight of their input on\nthe final results. Using posterior distributions, we offer Bayesian credible\nintervals to quantify uncertainty in the variable selection process. In\naddition, we highlight how selection frequencies can be uninformative or even\nmisleading when covariates are correlated with each other, and demonstrate how\ndomain expertise can alleviate such issues. Our approach preserves the\nversatility of stability selection and is suitable for a broad range of\nstructure estimation challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stability selection is a versatile framework for structure estimation and\nvariable selection in high-dimensional setting, primarily grounded in\nfrequentist principles. In this paper, we propose an enhanced methodology that\nintegrates Bayesian analysis to refine the inference of inclusion probabilities\nwithin the stability selection framework. Traditional approaches rely on\nselection frequencies for decision-making, often disregarding domain-specific\nknowledge and failing to account for the inherent uncertainty in the variable\nselection process. Our methodology uses prior information to derive posterior\ndistributions of inclusion probabilities, thereby improving both inference and\ndecision-making. We present a two-step process for engaging with domain\nexperts, enabling statisticians to elucidate prior distributions informed by\nexpert knowledge while allowing experts to control the weight of their input on\nthe final results. Using posterior distributions, we offer Bayesian credible\nintervals to quantify uncertainty in the variable selection process. In\naddition, we highlight how selection frequencies can be uninformative or even\nmisleading when covariates are correlated with each other, and demonstrate how\ndomain expertise can alleviate such issues. Our approach preserves the\nversatility of stability selection and is suitable for a broad range of\nstructure estimation challenges."
                },
                "authors": [
                    {
                        "name": "Mahdi Nouraie"
                    },
                    {
                        "name": "Connor Smith"
                    },
                    {
                        "name": "Samuel Muller"
                    }
                ],
                "author_detail": {
                    "name": "Samuel Muller"
                },
                "author": "Samuel Muller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21914v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21914v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02982v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02982v3",
                "updated": "2024-10-29T10:02:50Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    10,
                    2,
                    50,
                    1,
                    303,
                    0
                ],
                "published": "2024-04-03T18:07:02Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    18,
                    7,
                    2,
                    2,
                    94,
                    0
                ],
                "title": "Spatio-temporal count autoregression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatio-temporal count autoregression"
                },
                "summary": "We study the problem of modeling and inference for spatio-temporal count\nprocesses. Our approach uses parsimonious parameterisations of multivariate\nautoregressive count time series models, including possible regression on\ncovariates. We control the number of parameters by specifying spatial\nneighbourhood structures for possibly huge matrices that take into account\nspatio-temporal dependencies. This work is motivated by real data applications\nwhich call for suitable models. Extensive simulation studies show that our\napproach yields reliable estimators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the problem of modeling and inference for spatio-temporal count\nprocesses. Our approach uses parsimonious parameterisations of multivariate\nautoregressive count time series models, including possible regression on\ncovariates. We control the number of parameters by specifying spatial\nneighbourhood structures for possibly huge matrices that take into account\nspatio-temporal dependencies. This work is motivated by real data applications\nwhich call for suitable models. Extensive simulation studies show that our\napproach yields reliable estimators."
                },
                "authors": [
                    {
                        "name": "Steffen Maletz"
                    },
                    {
                        "name": "Konstantinos Fokianos"
                    },
                    {
                        "name": "Roland Fried"
                    }
                ],
                "author_detail": {
                    "name": "Roland Fried"
                },
                "author": "Roland Fried",
                "arxiv_comment": "24 pages, 16 figures and 22 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02982v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02982v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21909v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21909v1",
                "updated": "2024-10-29T10:01:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    10,
                    1,
                    40,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T10:01:40Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    10,
                    1,
                    40,
                    1,
                    303,
                    0
                ],
                "title": "SceneGenAgent: Precise Industrial Scene Generation with Coding Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SceneGenAgent: Precise Industrial Scene Generation with Coding Agent"
                },
                "summary": "The modeling of industrial scenes is essential for simulations in industrial\nmanufacturing. While large language models (LLMs) have shown significant\nprogress in generating general 3D scenes from textual descriptions, generating\nindustrial scenes with LLMs poses a unique challenge due to their demand for\nprecise measurements and positioning, requiring complex planning over spatial\narrangement. To address this challenge, we introduce SceneGenAgent, an\nLLM-based agent for generating industrial scenes through C# code. SceneGenAgent\nensures precise layout planning through a structured and calculable format,\nlayout verification, and iterative refinement to meet the quantitative\nrequirements of industrial scenarios. Experiment results demonstrate that LLMs\npowered by SceneGenAgent exceed their original performance, reaching up to\n81.0% success rate in real-world industrial scene generation tasks and\neffectively meeting most scene generation requirements. To further enhance\naccessibility, we construct SceneInstruct, a dataset designed for fine-tuning\nopen-source LLMs to integrate into SceneGenAgent. Experiments show that\nfine-tuning open-source LLMs on SceneInstruct yields significant performance\nimprovements, with Llama3.1-70B approaching the capabilities of GPT-4o. Our\ncode and data are available at https://github.com/THUDM/SceneGenAgent .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The modeling of industrial scenes is essential for simulations in industrial\nmanufacturing. While large language models (LLMs) have shown significant\nprogress in generating general 3D scenes from textual descriptions, generating\nindustrial scenes with LLMs poses a unique challenge due to their demand for\nprecise measurements and positioning, requiring complex planning over spatial\narrangement. To address this challenge, we introduce SceneGenAgent, an\nLLM-based agent for generating industrial scenes through C# code. SceneGenAgent\nensures precise layout planning through a structured and calculable format,\nlayout verification, and iterative refinement to meet the quantitative\nrequirements of industrial scenarios. Experiment results demonstrate that LLMs\npowered by SceneGenAgent exceed their original performance, reaching up to\n81.0% success rate in real-world industrial scene generation tasks and\neffectively meeting most scene generation requirements. To further enhance\naccessibility, we construct SceneInstruct, a dataset designed for fine-tuning\nopen-source LLMs to integrate into SceneGenAgent. Experiments show that\nfine-tuning open-source LLMs on SceneInstruct yields significant performance\nimprovements, with Llama3.1-70B approaching the capabilities of GPT-4o. Our\ncode and data are available at https://github.com/THUDM/SceneGenAgent ."
                },
                "authors": [
                    {
                        "name": "Xiao Xia"
                    },
                    {
                        "name": "Dan Zhang"
                    },
                    {
                        "name": "Zibo Liao"
                    },
                    {
                        "name": "Zhenyu Hou"
                    },
                    {
                        "name": "Tianrui Sun"
                    },
                    {
                        "name": "Jing Li"
                    },
                    {
                        "name": "Ling Fu"
                    },
                    {
                        "name": "Yuxiao Dong"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiao Dong"
                },
                "author": "Yuxiao Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21909v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21909v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.07350v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.07350v3",
                "updated": "2024-10-29T09:31:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    9,
                    31,
                    22,
                    1,
                    303,
                    0
                ],
                "published": "2024-03-12T06:16:33Z",
                "published_parsed": [
                    2024,
                    3,
                    12,
                    6,
                    16,
                    33,
                    1,
                    72,
                    0
                ],
                "title": "VLKEB: A Large Vision-Language Model Knowledge Editing Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLKEB: A Large Vision-Language Model Knowledge Editing Benchmark"
                },
                "summary": "Recently, knowledge editing on large language models (LLMs) has received\nconsiderable attention. Compared to this, editing Large Vision-Language Models\n(LVLMs) faces extra challenges from diverse data modalities and complicated\nmodel components, and data for LVLMs editing are limited. The existing LVLM\nediting benchmark, which comprises three metrics (Reliability, Locality, and\nGenerality), falls short in the quality of synthesized evaluation images and\ncannot assess whether models apply edited knowledge in relevant content.\nTherefore, we employ more reliable data collection methods to construct a new\nLarge $\\textbf{V}$ision-$\\textbf{L}$anguage Model $\\textbf{K}$nowledge\n$\\textbf{E}$diting $\\textbf{B}$enchmark, $\\textbf{VLKEB}$, and extend the\nPortability metric for more comprehensive evaluation. Leveraging a multi-modal\nknowledge graph, our image data are bound with knowledge entities. This can be\nfurther used to extract entity-related knowledge, which constitutes the base of\nediting data. We conduct experiments of different editing methods on five\nLVLMs, and thoroughly analyze how do they impact the models. The results reveal\nstrengths and deficiencies of these methods and hopefully provide insights for\nfuture research. The codes and dataset are available at:\nhttps://github.com/VLKEB/VLKEB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, knowledge editing on large language models (LLMs) has received\nconsiderable attention. Compared to this, editing Large Vision-Language Models\n(LVLMs) faces extra challenges from diverse data modalities and complicated\nmodel components, and data for LVLMs editing are limited. The existing LVLM\nediting benchmark, which comprises three metrics (Reliability, Locality, and\nGenerality), falls short in the quality of synthesized evaluation images and\ncannot assess whether models apply edited knowledge in relevant content.\nTherefore, we employ more reliable data collection methods to construct a new\nLarge $\\textbf{V}$ision-$\\textbf{L}$anguage Model $\\textbf{K}$nowledge\n$\\textbf{E}$diting $\\textbf{B}$enchmark, $\\textbf{VLKEB}$, and extend the\nPortability metric for more comprehensive evaluation. Leveraging a multi-modal\nknowledge graph, our image data are bound with knowledge entities. This can be\nfurther used to extract entity-related knowledge, which constitutes the base of\nediting data. We conduct experiments of different editing methods on five\nLVLMs, and thoroughly analyze how do they impact the models. The results reveal\nstrengths and deficiencies of these methods and hopefully provide insights for\nfuture research. The codes and dataset are available at:\nhttps://github.com/VLKEB/VLKEB."
                },
                "authors": [
                    {
                        "name": "Han Huang"
                    },
                    {
                        "name": "Haitian Zhong"
                    },
                    {
                        "name": "Tao Yu"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Shu Wu"
                    },
                    {
                        "name": "Liang Wang"
                    },
                    {
                        "name": "Tieniu Tan"
                    }
                ],
                "author_detail": {
                    "name": "Tieniu Tan"
                },
                "author": "Tieniu Tan",
                "arxiv_comment": "NeurIPS 2024, Datasets and Benchmarks Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.07350v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.07350v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02795v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02795v4",
                "updated": "2024-10-29T09:27:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    9,
                    27,
                    15,
                    1,
                    303,
                    0
                ],
                "published": "2024-09-04T15:11:55Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    15,
                    11,
                    55,
                    2,
                    248,
                    0
                ],
                "title": "Towards a Unified View of Preference Learning for Large Language Models:\n  A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a Unified View of Preference Learning for Large Language Models:\n  A Survey"
                },
                "summary": "Large Language Models (LLMs) exhibit remarkably powerful capabilities. One of\nthe crucial factors to achieve success is aligning the LLM's output with human\npreferences. This alignment process often requires only a small amount of data\nto efficiently enhance the LLM's performance. While effective, research in this\narea spans multiple domains, and the methods involved are relatively complex to\nunderstand. The relationships between different methods have been\nunder-explored, limiting the development of the preference alignment. In light\nof this, we break down the existing popular alignment strategies into different\ncomponents and provide a unified framework to study the current alignment\nstrategies, thereby establishing connections among them. In this survey, we\ndecompose all the strategies in preference learning into four components:\nmodel, data, feedback, and algorithm. This unified view offers an in-depth\nunderstanding of existing alignment algorithms and also opens up possibilities\nto synergize the strengths of different strategies. Furthermore, we present\ndetailed working examples of prevalent existing algorithms to facilitate a\ncomprehensive understanding for the readers. Finally, based on our unified\nperspective, we explore the challenges and future research directions for\naligning large language models with human preferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit remarkably powerful capabilities. One of\nthe crucial factors to achieve success is aligning the LLM's output with human\npreferences. This alignment process often requires only a small amount of data\nto efficiently enhance the LLM's performance. While effective, research in this\narea spans multiple domains, and the methods involved are relatively complex to\nunderstand. The relationships between different methods have been\nunder-explored, limiting the development of the preference alignment. In light\nof this, we break down the existing popular alignment strategies into different\ncomponents and provide a unified framework to study the current alignment\nstrategies, thereby establishing connections among them. In this survey, we\ndecompose all the strategies in preference learning into four components:\nmodel, data, feedback, and algorithm. This unified view offers an in-depth\nunderstanding of existing alignment algorithms and also opens up possibilities\nto synergize the strengths of different strategies. Furthermore, we present\ndetailed working examples of prevalent existing algorithms to facilitate a\ncomprehensive understanding for the readers. Finally, based on our unified\nperspective, we explore the challenges and future research directions for\naligning large language models with human preferences."
                },
                "authors": [
                    {
                        "name": "Bofei Gao"
                    },
                    {
                        "name": "Feifan Song"
                    },
                    {
                        "name": "Yibo Miao"
                    },
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Zhe Yang"
                    },
                    {
                        "name": "Liang Chen"
                    },
                    {
                        "name": "Helan Hu"
                    },
                    {
                        "name": "Runxin Xu"
                    },
                    {
                        "name": "Qingxiu Dong"
                    },
                    {
                        "name": "Ce Zheng"
                    },
                    {
                        "name": "Shanghaoran Quan"
                    },
                    {
                        "name": "Wen Xiao"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Daoguang Zan"
                    },
                    {
                        "name": "Keming Lu"
                    },
                    {
                        "name": "Bowen Yu"
                    },
                    {
                        "name": "Dayiheng Liu"
                    },
                    {
                        "name": "Zeyu Cui"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Lei Sha"
                    },
                    {
                        "name": "Houfeng Wang"
                    },
                    {
                        "name": "Zhifang Sui"
                    },
                    {
                        "name": "Peiyi Wang"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Baobao Chang"
                    }
                ],
                "author_detail": {
                    "name": "Baobao Chang"
                },
                "author": "Baobao Chang",
                "arxiv_comment": "23 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02795v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02795v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.09142v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.09142v2",
                "updated": "2024-10-29T09:13:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    9,
                    13,
                    49,
                    1,
                    303,
                    0
                ],
                "published": "2024-03-14T07:40:54Z",
                "published_parsed": [
                    2024,
                    3,
                    14,
                    7,
                    40,
                    54,
                    3,
                    74,
                    0
                ],
                "title": "USimAgent: Large Language Models for Simulating Search Users",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "USimAgent: Large Language Models for Simulating Search Users"
                },
                "summary": "Due to the advantages in the cost-efficiency and reproducibility, user\nsimulation has become a promising solution to the user-centric evaluation of\ninformation retrieval systems. Nonetheless, accurately simulating user search\nbehaviors has long been a challenge, because users' actions in search are\nhighly complex and driven by intricate cognitive processes such as learning,\nreasoning, and planning. Recently, Large Language Models (LLMs) have\ndemonstrated remarked potential in simulating human-level intelligence and have\nbeen used in building autonomous agents for various tasks. However, the\npotential of using LLMs in simulating search behaviors has not yet been fully\nexplored. In this paper, we introduce a LLM-based user search behavior\nsimulator, USimAgent. The proposed simulator can simulate users' querying,\nclicking, and stopping behaviors during search, and thus, is capable of\ngenerating complete search sessions for specific search tasks. Empirical\ninvestigation on a real user behavior dataset shows that the proposed simulator\noutperforms existing methods in query generation and is comparable to\ntraditional methods in predicting user clicks and stopping behaviors. These\nresults not only validate the effectiveness of using LLMs for user simulation\nbut also shed light on the development of a more robust and generic user\nsimulators. The code and data are accessible at\nhttps://github.com/Meow-E/USimAgent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to the advantages in the cost-efficiency and reproducibility, user\nsimulation has become a promising solution to the user-centric evaluation of\ninformation retrieval systems. Nonetheless, accurately simulating user search\nbehaviors has long been a challenge, because users' actions in search are\nhighly complex and driven by intricate cognitive processes such as learning,\nreasoning, and planning. Recently, Large Language Models (LLMs) have\ndemonstrated remarked potential in simulating human-level intelligence and have\nbeen used in building autonomous agents for various tasks. However, the\npotential of using LLMs in simulating search behaviors has not yet been fully\nexplored. In this paper, we introduce a LLM-based user search behavior\nsimulator, USimAgent. The proposed simulator can simulate users' querying,\nclicking, and stopping behaviors during search, and thus, is capable of\ngenerating complete search sessions for specific search tasks. Empirical\ninvestigation on a real user behavior dataset shows that the proposed simulator\noutperforms existing methods in query generation and is comparable to\ntraditional methods in predicting user clicks and stopping behaviors. These\nresults not only validate the effectiveness of using LLMs for user simulation\nbut also shed light on the development of a more robust and generic user\nsimulators. The code and data are accessible at\nhttps://github.com/Meow-E/USimAgent."
                },
                "authors": [
                    {
                        "name": "Erhan Zhang"
                    },
                    {
                        "name": "Xingzhu Wang"
                    },
                    {
                        "name": "Peiyuan Gong"
                    },
                    {
                        "name": "Yankai Lin"
                    },
                    {
                        "name": "Jiaxin Mao"
                    }
                ],
                "author_detail": {
                    "name": "Jiaxin Mao"
                },
                "author": "Jiaxin Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.09142v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.09142v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2410.22318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22318v1",
                "updated": "2024-10-29T17:55:14Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    55,
                    14,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T17:55:14Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    55,
                    14,
                    1,
                    303,
                    0
                ],
                "title": "Online Detecting LLM-Generated Texts via Sequential Hypothesis Testing\n  by Betting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Detecting LLM-Generated Texts via Sequential Hypothesis Testing\n  by Betting"
                },
                "summary": "Developing algorithms to differentiate between machine-generated texts and\nhuman-written texts has garnered substantial attention in recent years.\nExisting methods in this direction typically concern an offline setting where a\ndataset containing a mix of real and machine-generated texts is given upfront,\nand the task is to determine whether each sample in the dataset is from a large\nlanguage model (LLM) or a human. However, in many practical scenarios, sources\nsuch as news websites, social media accounts, or on other forums publish\ncontent in a streaming fashion. Therefore, in this online scenario, how to\nquickly and accurately determine whether the source is an LLM with strong\nstatistical guarantees is crucial for these media or platforms to function\neffectively and prevent the spread of misinformation and other potential misuse\nof LLMs. To tackle the problem of online detection, we develop an algorithm\nbased on the techniques of sequential hypothesis testing by betting that not\nonly builds upon and complements existing offline detection techniques but also\nenjoys statistical guarantees, which include a controlled false positive rate\nand the expected time to correctly identify a source as an LLM. Experiments\nwere conducted to demonstrate the effectiveness of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing algorithms to differentiate between machine-generated texts and\nhuman-written texts has garnered substantial attention in recent years.\nExisting methods in this direction typically concern an offline setting where a\ndataset containing a mix of real and machine-generated texts is given upfront,\nand the task is to determine whether each sample in the dataset is from a large\nlanguage model (LLM) or a human. However, in many practical scenarios, sources\nsuch as news websites, social media accounts, or on other forums publish\ncontent in a streaming fashion. Therefore, in this online scenario, how to\nquickly and accurately determine whether the source is an LLM with strong\nstatistical guarantees is crucial for these media or platforms to function\neffectively and prevent the spread of misinformation and other potential misuse\nof LLMs. To tackle the problem of online detection, we develop an algorithm\nbased on the techniques of sequential hypothesis testing by betting that not\nonly builds upon and complements existing offline detection techniques but also\nenjoys statistical guarantees, which include a controlled false positive rate\nand the expected time to correctly identify a source as an LLM. Experiments\nwere conducted to demonstrate the effectiveness of our method."
                },
                "authors": [
                    {
                        "name": "Can Chen"
                    },
                    {
                        "name": "Jun-Kun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun-Kun Wang"
                },
                "author": "Jun-Kun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22316v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22316v1",
                "updated": "2024-10-29T17:55:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    55,
                    0,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T17:55:00Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    55,
                    0,
                    1,
                    303,
                    0
                ],
                "title": "Understanding Synthetic Context Extension via Retrieval Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Synthetic Context Extension via Retrieval Heads"
                },
                "summary": "Long-context LLMs are increasingly in demand for applications such as\nretrieval-augmented generation. To defray the cost of pretraining LLMs over\nlong contexts, recent work takes an approach of synthetic context extension:\nfine-tuning LLMs with synthetically generated long-context data in a\npost-training stage. However, it remains unclear how and why this synthetic\ncontext extension imparts abilities for downstream long-context tasks. In this\npaper, we investigate fine-tuning on synthetic data for three long-context\ntasks that require retrieval and reasoning. We vary the realism of \"needle\"\nconcepts to be retrieved and diversity of the surrounding \"haystack\" context,\nfrom using LLMs to construct synthetic documents to using templated relations\nand creating symbolic datasets. We find that models trained on synthetic data\nfall short of the real data, but surprisingly, the mismatch can be interpreted\nand even predicted in terms of a special set of attention heads that are\nresponsible for retrieval over long context: retrieval heads (Wu et al., 2024).\nThe retrieval heads learned on synthetic data are mostly subsets of the\nretrieval heads learned on real data, and there is a strong correlation between\nthe recall of heads learned and the downstream performance of a model.\nFurthermore, with attention knockout and activation patching, we\nmechanistically show that retrieval heads are necessary and explain model\nperformance, although they are not totally sufficient. Our results shed light\non how to interpret synthetic data fine-tuning performance and how to approach\ncreating better data for learning real-world capabilities over long contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context LLMs are increasingly in demand for applications such as\nretrieval-augmented generation. To defray the cost of pretraining LLMs over\nlong contexts, recent work takes an approach of synthetic context extension:\nfine-tuning LLMs with synthetically generated long-context data in a\npost-training stage. However, it remains unclear how and why this synthetic\ncontext extension imparts abilities for downstream long-context tasks. In this\npaper, we investigate fine-tuning on synthetic data for three long-context\ntasks that require retrieval and reasoning. We vary the realism of \"needle\"\nconcepts to be retrieved and diversity of the surrounding \"haystack\" context,\nfrom using LLMs to construct synthetic documents to using templated relations\nand creating symbolic datasets. We find that models trained on synthetic data\nfall short of the real data, but surprisingly, the mismatch can be interpreted\nand even predicted in terms of a special set of attention heads that are\nresponsible for retrieval over long context: retrieval heads (Wu et al., 2024).\nThe retrieval heads learned on synthetic data are mostly subsets of the\nretrieval heads learned on real data, and there is a strong correlation between\nthe recall of heads learned and the downstream performance of a model.\nFurthermore, with attention knockout and activation patching, we\nmechanistically show that retrieval heads are necessary and explain model\nperformance, although they are not totally sufficient. Our results shed light\non how to interpret synthetic data fine-tuning performance and how to approach\ncreating better data for learning real-world capabilities over long contexts."
                },
                "authors": [
                    {
                        "name": "Xinyu Zhao"
                    },
                    {
                        "name": "Fangcong Yin"
                    },
                    {
                        "name": "Greg Durrett"
                    }
                ],
                "author_detail": {
                    "name": "Greg Durrett"
                },
                "author": "Greg Durrett",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22316v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22316v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22315v1",
                "updated": "2024-10-29T17:54:17Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    54,
                    17,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T17:54:17Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    54,
                    17,
                    1,
                    303,
                    0
                ],
                "title": "Natural Language Inference Improves Compositionality in Vision-Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language Inference Improves Compositionality in Vision-Language\n  Models"
                },
                "summary": "Compositional reasoning in Vision-Language Models (VLMs) remains challenging\nas these models often struggle to relate objects, attributes, and spatial\nrelationships. Recent methods aim to address these limitations by relying on\nthe semantics of the textual description, using Large Language Models (LLMs) to\nbreak them down into subsets of questions and answers. However, these methods\nprimarily operate on the surface level, failing to incorporate deeper lexical\nunderstanding while introducing incorrect assumptions generated by the LLM. In\nresponse to these issues, we present Caption Expansion with Contradictions and\nEntailments (CECE), a principled approach that leverages Natural Language\nInference (NLI) to generate entailments and contradictions from a given\npremise. CECE produces lexically diverse sentences while maintaining their core\nmeaning. Through extensive experiments, we show that CECE enhances\ninterpretability and reduces overreliance on biased or superficial features. By\nbalancing CECE along the original premise, we achieve significant improvements\nover previous methods without requiring additional fine-tuning, producing\nstate-of-the-art results on benchmarks that score agreement with human\njudgments for image-text alignment, and achieving an increase in performance on\nWinoground of +19.2% (group score) and +12.9% on EqBen (group score) over the\nbest prior work (finetuned with targeted data).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compositional reasoning in Vision-Language Models (VLMs) remains challenging\nas these models often struggle to relate objects, attributes, and spatial\nrelationships. Recent methods aim to address these limitations by relying on\nthe semantics of the textual description, using Large Language Models (LLMs) to\nbreak them down into subsets of questions and answers. However, these methods\nprimarily operate on the surface level, failing to incorporate deeper lexical\nunderstanding while introducing incorrect assumptions generated by the LLM. In\nresponse to these issues, we present Caption Expansion with Contradictions and\nEntailments (CECE), a principled approach that leverages Natural Language\nInference (NLI) to generate entailments and contradictions from a given\npremise. CECE produces lexically diverse sentences while maintaining their core\nmeaning. Through extensive experiments, we show that CECE enhances\ninterpretability and reduces overreliance on biased or superficial features. By\nbalancing CECE along the original premise, we achieve significant improvements\nover previous methods without requiring additional fine-tuning, producing\nstate-of-the-art results on benchmarks that score agreement with human\njudgments for image-text alignment, and achieving an increase in performance on\nWinoground of +19.2% (group score) and +12.9% on EqBen (group score) over the\nbest prior work (finetuned with targeted data)."
                },
                "authors": [
                    {
                        "name": "Paola Cascante-Bonilla"
                    },
                    {
                        "name": "Yu Hou"
                    },
                    {
                        "name": "Yang Trista Cao"
                    },
                    {
                        "name": "Hal Daum III"
                    },
                    {
                        "name": "Rachel Rudinger"
                    }
                ],
                "author_detail": {
                    "name": "Rachel Rudinger"
                },
                "author": "Rachel Rudinger",
                "arxiv_comment": "Project page: https://cece-vlm.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22309v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22309v1",
                "updated": "2024-10-29T17:53:10Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    53,
                    10,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T17:53:10Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    53,
                    10,
                    1,
                    303,
                    0
                ],
                "title": "GPT-4o reads the mind in the eyes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT-4o reads the mind in the eyes"
                },
                "summary": "Large Language Models (LLMs) are capable of reproducing human-like\ninferences, including inferences about emotions and mental states, from text.\nWhether this capability extends beyond text to other modalities remains\nunclear. Humans possess a sophisticated ability to read the mind in the eyes of\nother people. Here we tested whether this ability is also present in GPT-4o, a\nmultimodal LLM. Using two versions of a widely used theory of mind test, the\nReading the Mind in Eyes Test and the Multiracial Reading the Mind in the Eyes\nTest, we found that GPT-4o outperformed humans in interpreting mental states\nfrom upright faces but underperformed humans when faces were inverted. While\nhumans in our sample showed no difference between White and Non-white faces,\nGPT-4o's accuracy was higher for White than for Non-white faces. GPT-4o's\nerrors were not random but revealed a highly consistent, yet incorrect,\nprocessing of mental-state information across trials, with an\norientation-dependent error structure that qualitatively differed from that of\nhumans for inverted faces but not for upright faces. These findings highlight\nhow advanced mental state inference abilities and human-like face processing\nsignatures, such as inversion effects, coexist in GPT-4o alongside substantial\ndifferences in information processing compared to humans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are capable of reproducing human-like\ninferences, including inferences about emotions and mental states, from text.\nWhether this capability extends beyond text to other modalities remains\nunclear. Humans possess a sophisticated ability to read the mind in the eyes of\nother people. Here we tested whether this ability is also present in GPT-4o, a\nmultimodal LLM. Using two versions of a widely used theory of mind test, the\nReading the Mind in Eyes Test and the Multiracial Reading the Mind in the Eyes\nTest, we found that GPT-4o outperformed humans in interpreting mental states\nfrom upright faces but underperformed humans when faces were inverted. While\nhumans in our sample showed no difference between White and Non-white faces,\nGPT-4o's accuracy was higher for White than for Non-white faces. GPT-4o's\nerrors were not random but revealed a highly consistent, yet incorrect,\nprocessing of mental-state information across trials, with an\norientation-dependent error structure that qualitatively differed from that of\nhumans for inverted faces but not for upright faces. These findings highlight\nhow advanced mental state inference abilities and human-like face processing\nsignatures, such as inversion effects, coexist in GPT-4o alongside substantial\ndifferences in information processing compared to humans."
                },
                "authors": [
                    {
                        "name": "James W. A. Strachan"
                    },
                    {
                        "name": "Oriana Pansardi"
                    },
                    {
                        "name": "Eugenio Scaliti"
                    },
                    {
                        "name": "Marco Celotto"
                    },
                    {
                        "name": "Krati Saxena"
                    },
                    {
                        "name": "Chunzhi Yi"
                    },
                    {
                        "name": "Fabio Manzi"
                    },
                    {
                        "name": "Alessandro Rufo"
                    },
                    {
                        "name": "Guido Manzi"
                    },
                    {
                        "name": "Michael S. A. Graziano"
                    },
                    {
                        "name": "Stefano Panzeri"
                    },
                    {
                        "name": "Cristina Becchio"
                    }
                ],
                "author_detail": {
                    "name": "Cristina Becchio"
                },
                "author": "Cristina Becchio",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22309v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22309v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22307v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22307v1",
                "updated": "2024-10-29T17:52:45Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    52,
                    45,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T17:52:45Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    52,
                    45,
                    1,
                    303,
                    0
                ],
                "title": "SVIP: Towards Verifiable Inference of Open-source Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SVIP: Towards Verifiable Inference of Open-source Large Language Models"
                },
                "summary": "Open-source Large Language Models (LLMs) have recently demonstrated\nremarkable capabilities in natural language understanding and generation,\nleading to widespread adoption across various domains. However, their\nincreasing model sizes render local deployment impractical for individual\nusers, pushing many to rely on computing service providers for inference\nthrough a blackbox API. This reliance introduces a new risk: a computing\nprovider may stealthily substitute the requested LLM with a smaller, less\ncapable model without consent from users, thereby delivering inferior outputs\nwhile benefiting from cost savings. In this paper, we formalize the problem of\nverifiable inference for LLMs. Existing verifiable computing solutions based on\ncryptographic or game-theoretic techniques are either computationally\nuneconomical or rest on strong assumptions. We introduce SVIP, a secret-based\nverifiable LLM inference protocol that leverages intermediate outputs from LLM\nas unique model identifiers. By training a proxy task on these outputs and\nrequiring the computing provider to return both the generated text and the\nprocessed intermediate outputs, users can reliably verify whether the computing\nprovider is acting honestly. In addition, the integration of a secret mechanism\nfurther enhances the security of our protocol. We thoroughly analyze our\nprotocol under multiple strong and adaptive adversarial scenarios. Our\nextensive experiments demonstrate that SVIP is accurate, generalizable,\ncomputationally efficient, and resistant to various attacks. Notably, SVIP\nachieves false negative rates below 5% and false positive rates below 3%, while\nrequiring less than 0.01 seconds per query for verification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-source Large Language Models (LLMs) have recently demonstrated\nremarkable capabilities in natural language understanding and generation,\nleading to widespread adoption across various domains. However, their\nincreasing model sizes render local deployment impractical for individual\nusers, pushing many to rely on computing service providers for inference\nthrough a blackbox API. This reliance introduces a new risk: a computing\nprovider may stealthily substitute the requested LLM with a smaller, less\ncapable model without consent from users, thereby delivering inferior outputs\nwhile benefiting from cost savings. In this paper, we formalize the problem of\nverifiable inference for LLMs. Existing verifiable computing solutions based on\ncryptographic or game-theoretic techniques are either computationally\nuneconomical or rest on strong assumptions. We introduce SVIP, a secret-based\nverifiable LLM inference protocol that leverages intermediate outputs from LLM\nas unique model identifiers. By training a proxy task on these outputs and\nrequiring the computing provider to return both the generated text and the\nprocessed intermediate outputs, users can reliably verify whether the computing\nprovider is acting honestly. In addition, the integration of a secret mechanism\nfurther enhances the security of our protocol. We thoroughly analyze our\nprotocol under multiple strong and adaptive adversarial scenarios. Our\nextensive experiments demonstrate that SVIP is accurate, generalizable,\ncomputationally efficient, and resistant to various attacks. Notably, SVIP\nachieves false negative rates below 5% and false positive rates below 3%, while\nrequiring less than 0.01 seconds per query for verification."
                },
                "authors": [
                    {
                        "name": "Yifan Sun"
                    },
                    {
                        "name": "Yuhang Li"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Yuchen Jin"
                    },
                    {
                        "name": "Huan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Zhang"
                },
                "author": "Huan Zhang",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22307v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22307v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22304v1",
                "updated": "2024-10-29T17:50:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    50,
                    31,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T17:50:31Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    50,
                    31,
                    1,
                    303,
                    0
                ],
                "title": "Flow-DPO: Improving LLM Mathematical Reasoning through Online\n  Multi-Agent Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flow-DPO: Improving LLM Mathematical Reasoning through Online\n  Multi-Agent Learning"
                },
                "summary": "Mathematical reasoning is a crucial capability for Large Language Models\n(LLMs), yet generating detailed and accurate reasoning traces remains a\nsignificant challenge. This paper introduces a novel approach to produce\nhigh-quality reasoning traces for LLM fine-tuning using online learning\n\\textbf{Flows}. Our method employs an incremental output production Flow, where\ncomponent LLMs collaboratively construct solutions through iterative\ncommunication. We train the Flow using online Direct Preference Optimization\n(DPO) learning with rollouts, generating DPO pairs for each training example\nand updating models in real-time. We directly compare the quality of reasoning\ntraces generated by our method with those produced through direct model\ninference, demonstrating the effectiveness of our approach in improving LLM\nperformance in mathematical reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mathematical reasoning is a crucial capability for Large Language Models\n(LLMs), yet generating detailed and accurate reasoning traces remains a\nsignificant challenge. This paper introduces a novel approach to produce\nhigh-quality reasoning traces for LLM fine-tuning using online learning\n\\textbf{Flows}. Our method employs an incremental output production Flow, where\ncomponent LLMs collaboratively construct solutions through iterative\ncommunication. We train the Flow using online Direct Preference Optimization\n(DPO) learning with rollouts, generating DPO pairs for each training example\nand updating models in real-time. We directly compare the quality of reasoning\ntraces generated by our method with those produced through direct model\ninference, demonstrating the effectiveness of our approach in improving LLM\nperformance in mathematical reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Yihe Deng"
                    },
                    {
                        "name": "Paul Mineiro"
                    }
                ],
                "author_detail": {
                    "name": "Paul Mineiro"
                },
                "author": "Paul Mineiro",
                "arxiv_comment": "5 pages, 4 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22296v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22296v1",
                "updated": "2024-10-29T17:45:57Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    45,
                    57,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T17:45:57Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    45,
                    57,
                    1,
                    303,
                    0
                ],
                "title": "LLMs are Highly-Constrained Biophysical Sequence Optimizers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are Highly-Constrained Biophysical Sequence Optimizers"
                },
                "summary": "Large language models (LLMs) have recently shown significant potential in\nvarious biological tasks such as protein engineering and molecule design. These\ntasks typically involve black-box discrete sequence optimization, where the\nchallenge lies in generating sequences that are not only biologically feasible\nbut also adhere to hard fine-grained constraints. However, LLMs often struggle\nwith such constraints, especially in biological contexts where verifying\ncandidate solutions is costly and time-consuming. In this study, we explore the\npossibility of employing LLMs as highly-constrained bilevel optimizers through\na methodology we refer to as Language Model Optimization with Margin\nExpectation (LLOME). This approach combines both offline and online\noptimization, utilizing limited oracle evaluations to iteratively enhance the\nsequences generated by the LLM. We additionally propose a novel training\nobjective -- Margin-Aligned Expectation (MargE) -- that trains the LLM to\nsmoothly interpolate between the reward and reference distributions. Lastly, we\nintroduce a synthetic test suite that bears strong geometric similarity to real\nbiophysical problems and enables rapid evaluation of LLM optimizers without\ntime-consuming lab validation. Our findings reveal that, in comparison to\ngenetic algorithm baselines, LLMs achieve significantly lower regret solutions\nwhile requiring fewer test function evaluations. However, we also observe that\nLLMs exhibit moderate miscalibration, are susceptible to generator collapse,\nand have difficulty finding the optimal solution when no explicit ground truth\nrewards are available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have recently shown significant potential in\nvarious biological tasks such as protein engineering and molecule design. These\ntasks typically involve black-box discrete sequence optimization, where the\nchallenge lies in generating sequences that are not only biologically feasible\nbut also adhere to hard fine-grained constraints. However, LLMs often struggle\nwith such constraints, especially in biological contexts where verifying\ncandidate solutions is costly and time-consuming. In this study, we explore the\npossibility of employing LLMs as highly-constrained bilevel optimizers through\na methodology we refer to as Language Model Optimization with Margin\nExpectation (LLOME). This approach combines both offline and online\noptimization, utilizing limited oracle evaluations to iteratively enhance the\nsequences generated by the LLM. We additionally propose a novel training\nobjective -- Margin-Aligned Expectation (MargE) -- that trains the LLM to\nsmoothly interpolate between the reward and reference distributions. Lastly, we\nintroduce a synthetic test suite that bears strong geometric similarity to real\nbiophysical problems and enables rapid evaluation of LLM optimizers without\ntime-consuming lab validation. Our findings reveal that, in comparison to\ngenetic algorithm baselines, LLMs achieve significantly lower regret solutions\nwhile requiring fewer test function evaluations. However, we also observe that\nLLMs exhibit moderate miscalibration, are susceptible to generator collapse,\nand have difficulty finding the optimal solution when no explicit ground truth\nrewards are available."
                },
                "authors": [
                    {
                        "name": "Angelica Chen"
                    },
                    {
                        "name": "Samuel D. Stanton"
                    },
                    {
                        "name": "Robert G. Alberstein"
                    },
                    {
                        "name": "Andrew M. Watkins"
                    },
                    {
                        "name": "Richard Bonneau"
                    },
                    {
                        "name": "Vladimir Gligorijevi"
                    },
                    {
                        "name": "Kyunghyun Cho"
                    },
                    {
                        "name": "Nathan C. Frey"
                    }
                ],
                "author_detail": {
                    "name": "Nathan C. Frey"
                },
                "author": "Nathan C. Frey",
                "arxiv_comment": "Supercedes arXiv:2407.00236v1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22296v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22296v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19487v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19487v2",
                "updated": "2024-10-29T17:44:03Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    44,
                    3,
                    1,
                    303,
                    0
                ],
                "published": "2024-05-29T20:05:46Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    20,
                    5,
                    46,
                    2,
                    150,
                    0
                ],
                "title": "A Full-duplex Speech Dialogue Scheme Based On Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Full-duplex Speech Dialogue Scheme Based On Large Language Models"
                },
                "summary": "We present a generative dialogue system capable of operating in a full-duplex\nmanner, allowing for seamless interaction. It is based on a large language\nmodel (LLM) carefully aligned to be aware of a perception module, a motor\nfunction module, and the concept of a simple finite state machine (called\nneural FSM) with two states. The perception and motor function modules operate\nin tandem, allowing the system to speak and listen to the user simultaneously.\nThe LLM generates textual tokens for inquiry responses and makes autonomous\ndecisions to start responding to, wait for, or interrupt the user by emitting\ncontrol tokens to the neural FSM. All these tasks of the LLM are carried out as\nnext token prediction on a serialized view of the dialogue in real-time. In\nautomatic quality evaluations simulating real-life interaction, the proposed\nsystem reduces the average conversation response latency by more than threefold\ncompared with LLM-based half-duplex dialogue systems while responding within\nless than 500 milliseconds in more than 50% of evaluated interactions. Running\nan LLM with only 8 billion parameters, our system exhibits an 8% higher\ninterruption precision rate than the best available commercial LLM for\nvoice-based dialogue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a generative dialogue system capable of operating in a full-duplex\nmanner, allowing for seamless interaction. It is based on a large language\nmodel (LLM) carefully aligned to be aware of a perception module, a motor\nfunction module, and the concept of a simple finite state machine (called\nneural FSM) with two states. The perception and motor function modules operate\nin tandem, allowing the system to speak and listen to the user simultaneously.\nThe LLM generates textual tokens for inquiry responses and makes autonomous\ndecisions to start responding to, wait for, or interrupt the user by emitting\ncontrol tokens to the neural FSM. All these tasks of the LLM are carried out as\nnext token prediction on a serialized view of the dialogue in real-time. In\nautomatic quality evaluations simulating real-life interaction, the proposed\nsystem reduces the average conversation response latency by more than threefold\ncompared with LLM-based half-duplex dialogue systems while responding within\nless than 500 milliseconds in more than 50% of evaluated interactions. Running\nan LLM with only 8 billion parameters, our system exhibits an 8% higher\ninterruption precision rate than the best available commercial LLM for\nvoice-based dialogue."
                },
                "authors": [
                    {
                        "name": "Peng Wang"
                    },
                    {
                        "name": "Songshuo Lu"
                    },
                    {
                        "name": "Yaohua Tang"
                    },
                    {
                        "name": "Sijie Yan"
                    },
                    {
                        "name": "Wei Xia"
                    },
                    {
                        "name": "Yuanjun Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Yuanjun Xiong"
                },
                "author": "Yuanjun Xiong",
                "arxiv_comment": "Accepted to NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.19487v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19487v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22293v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22293v1",
                "updated": "2024-10-29T17:43:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    43,
                    6,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T17:43:06Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    43,
                    6,
                    1,
                    303,
                    0
                ],
                "title": "Fine-Tuning LLMs for Code Mutation: A New Era of Cyber Threats",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Tuning LLMs for Code Mutation: A New Era of Cyber Threats"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have significantly\nimproved their capabilities in natural language processing and code synthesis,\nenabling more complex applications across different fields. This paper explores\nthe application of LLMs in the context of code mutation, a process where the\nstructure of program code is altered without changing its functionality.\nTraditionally, code mutation has been employed to increase software robustness\nin mission-critical applications. Additionally, mutation engines have been\nexploited by malware developers to evade the signature-based detection methods\nemployed by malware detection systems. Existing code mutation engines, often\nused by such threat actors, typically result in only limited variations in the\nmalware, which can still be identified through static code analysis. However,\nthe agility demonstrated by an LLM-based code synthesizer could significantly\nchange this threat landscape by allowing for more complex code mutations that\nare not easily detected using static analysis. One can increase variations of\ncodes synthesized by a pre-trained LLM through fine-tuning and retraining. This\nprocess is what we refer to as code mutation training. In this paper, we\npropose a novel definition of code mutation training tailored for pre-trained\nLLM-based code synthesizers and demonstrate this training on a lightweight\npre-trained model. Our approach involves restructuring (i.e., mutating) code at\nthe subroutine level, which allows for more manageable mutations while\nmaintaining the semantic integrity verified through unit testing. Our\nexperimental results illustrate the effectiveness of our approach in improving\ncode mutation capabilities of LLM-based program synthesizers in producing\nvaried and functionally correct code solutions, showcasing their potential to\ntransform the landscape of code mutation and the threats associated with it.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have significantly\nimproved their capabilities in natural language processing and code synthesis,\nenabling more complex applications across different fields. This paper explores\nthe application of LLMs in the context of code mutation, a process where the\nstructure of program code is altered without changing its functionality.\nTraditionally, code mutation has been employed to increase software robustness\nin mission-critical applications. Additionally, mutation engines have been\nexploited by malware developers to evade the signature-based detection methods\nemployed by malware detection systems. Existing code mutation engines, often\nused by such threat actors, typically result in only limited variations in the\nmalware, which can still be identified through static code analysis. However,\nthe agility demonstrated by an LLM-based code synthesizer could significantly\nchange this threat landscape by allowing for more complex code mutations that\nare not easily detected using static analysis. One can increase variations of\ncodes synthesized by a pre-trained LLM through fine-tuning and retraining. This\nprocess is what we refer to as code mutation training. In this paper, we\npropose a novel definition of code mutation training tailored for pre-trained\nLLM-based code synthesizers and demonstrate this training on a lightweight\npre-trained model. Our approach involves restructuring (i.e., mutating) code at\nthe subroutine level, which allows for more manageable mutations while\nmaintaining the semantic integrity verified through unit testing. Our\nexperimental results illustrate the effectiveness of our approach in improving\ncode mutation capabilities of LLM-based program synthesizers in producing\nvaried and functionally correct code solutions, showcasing their potential to\ntransform the landscape of code mutation and the threats associated with it."
                },
                "authors": [
                    {
                        "name": "Mohammad Setak"
                    },
                    {
                        "name": "Pooria Madani"
                    }
                ],
                "author_detail": {
                    "name": "Pooria Madani"
                },
                "author": "Pooria Madani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22293v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22293v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22284v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22284v1",
                "updated": "2024-10-29T17:36:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    36,
                    59,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T17:36:59Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    36,
                    59,
                    1,
                    303,
                    0
                ],
                "title": "Embedding-based classifiers can detect prompt injection attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embedding-based classifiers can detect prompt injection attacks"
                },
                "summary": "Large Language Models (LLMs) are seeing significant adoption in every type of\norganization due to their exceptional generative capabilities. However, LLMs\nare found to be vulnerable to various adversarial attacks, particularly prompt\ninjection attacks, which trick them into producing harmful or inappropriate\ncontent. Adversaries execute such attacks by crafting malicious prompts to\ndeceive the LLMs. In this paper, we propose a novel approach based on\nembedding-based Machine Learning (ML) classifiers to protect LLM-based\napplications against this severe threat. We leverage three commonly used\nembedding models to generate embeddings of malicious and benign prompts and\nutilize ML classifiers to predict whether an input prompt is malicious. Out of\nseveral traditional ML methods, we achieve the best performance with\nclassifiers built using Random Forest and XGBoost. Our classifiers outperform\nstate-of-the-art prompt injection classifiers available in open-source\nimplementations, which use encoder-only neural networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are seeing significant adoption in every type of\norganization due to their exceptional generative capabilities. However, LLMs\nare found to be vulnerable to various adversarial attacks, particularly prompt\ninjection attacks, which trick them into producing harmful or inappropriate\ncontent. Adversaries execute such attacks by crafting malicious prompts to\ndeceive the LLMs. In this paper, we propose a novel approach based on\nembedding-based Machine Learning (ML) classifiers to protect LLM-based\napplications against this severe threat. We leverage three commonly used\nembedding models to generate embeddings of malicious and benign prompts and\nutilize ML classifiers to predict whether an input prompt is malicious. Out of\nseveral traditional ML methods, we achieve the best performance with\nclassifiers built using Random Forest and XGBoost. Our classifiers outperform\nstate-of-the-art prompt injection classifiers available in open-source\nimplementations, which use encoder-only neural networks."
                },
                "authors": [
                    {
                        "name": "Md. Ahsan Ayub"
                    },
                    {
                        "name": "Subhabrata Majumdar"
                    }
                ],
                "author_detail": {
                    "name": "Subhabrata Majumdar"
                },
                "author": "Subhabrata Majumdar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22284v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22284v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22282v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22282v1",
                "updated": "2024-10-29T17:35:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    35,
                    46,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T17:35:46Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    35,
                    46,
                    1,
                    303,
                    0
                ],
                "title": "Whose ChatGPT? Unveiling Real-World Educational Inequalities Introduced\n  by Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Whose ChatGPT? Unveiling Real-World Educational Inequalities Introduced\n  by Large Language Models"
                },
                "summary": "The universal availability of ChatGPT and other similar tools since late 2022\nhas prompted tremendous public excitement and experimental effort about the\npotential of large language models (LLMs) to improve learning experience and\noutcomes, especially for learners from disadvantaged backgrounds. However,\nlittle research has systematically examined the real-world impacts of LLM\navailability on educational equity beyond theoretical projections and\ncontrolled studies of innovative LLM applications. To depict trends of post-LLM\ninequalities, we analyze 1,140,328 academic writing submissions from 16,791\ncollege students across 2,391 courses between 2021 and 2024 at a public,\nminority-serving institution in the US. We find that students' overall writing\nquality gradually increased following the availability of LLMs and that the\nwriting quality gaps between linguistically advantaged and disadvantaged\nstudents became increasingly narrower. However, this equitizing effect was more\nconcentrated on students with higher socioeconomic status. These findings shed\nlight on the digital divides in the era of LLMs and raise questions about the\nequity benefits of LLMs in early stages and highlight the need for researchers\nand practitioners on developing responsible practices to improve educational\nequity through LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The universal availability of ChatGPT and other similar tools since late 2022\nhas prompted tremendous public excitement and experimental effort about the\npotential of large language models (LLMs) to improve learning experience and\noutcomes, especially for learners from disadvantaged backgrounds. However,\nlittle research has systematically examined the real-world impacts of LLM\navailability on educational equity beyond theoretical projections and\ncontrolled studies of innovative LLM applications. To depict trends of post-LLM\ninequalities, we analyze 1,140,328 academic writing submissions from 16,791\ncollege students across 2,391 courses between 2021 and 2024 at a public,\nminority-serving institution in the US. We find that students' overall writing\nquality gradually increased following the availability of LLMs and that the\nwriting quality gaps between linguistically advantaged and disadvantaged\nstudents became increasingly narrower. However, this equitizing effect was more\nconcentrated on students with higher socioeconomic status. These findings shed\nlight on the digital divides in the era of LLMs and raise questions about the\nequity benefits of LLMs in early stages and highlight the need for researchers\nand practitioners on developing responsible practices to improve educational\nequity through LLMs."
                },
                "authors": [
                    {
                        "name": "Renzhe Yu"
                    },
                    {
                        "name": "Zhen Xu"
                    },
                    {
                        "name": "Sky CH-Wang"
                    },
                    {
                        "name": "Richard Arum"
                    }
                ],
                "author_detail": {
                    "name": "Richard Arum"
                },
                "author": "Richard Arum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22282v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22282v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.09252v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.09252v3",
                "updated": "2024-10-29T17:34:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    34,
                    54,
                    1,
                    303,
                    0
                ],
                "published": "2024-07-12T13:30:44Z",
                "published_parsed": [
                    2024,
                    7,
                    12,
                    13,
                    30,
                    44,
                    4,
                    194,
                    0
                ],
                "title": "Context Embeddings for Efficient Answer Generation in RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context Embeddings for Efficient Answer Generation in RAG"
                },
                "summary": "Retrieval-Augmented Generation (RAG) allows overcoming the limited knowledge\nof LLMs by extending the input with external information. As a consequence, the\ncontextual inputs to the model become much longer which slows down decoding\ntime directly translating to the time a user has to wait for an answer. We\naddress this challenge by presenting COCOM, an effective context compression\nmethod, reducing long contexts to only a handful of Context Embeddings speeding\nup the generation time by a large margin. Our method allows for different\ncompression rates trading off decoding time for answer quality. Compared to\nearlier methods, COCOM allows for handling multiple contexts more effectively,\nsignificantly reducing decoding time for long inputs. Our method demonstrates a\nspeed-up of up to 5.69 $\\times$ while achieving higher performance compared to\nexisting efficient context compression methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) allows overcoming the limited knowledge\nof LLMs by extending the input with external information. As a consequence, the\ncontextual inputs to the model become much longer which slows down decoding\ntime directly translating to the time a user has to wait for an answer. We\naddress this challenge by presenting COCOM, an effective context compression\nmethod, reducing long contexts to only a handful of Context Embeddings speeding\nup the generation time by a large margin. Our method allows for different\ncompression rates trading off decoding time for answer quality. Compared to\nearlier methods, COCOM allows for handling multiple contexts more effectively,\nsignificantly reducing decoding time for long inputs. Our method demonstrates a\nspeed-up of up to 5.69 $\\times$ while achieving higher performance compared to\nexisting efficient context compression methods."
                },
                "authors": [
                    {
                        "name": "David Rau"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Herv Djean"
                    },
                    {
                        "name": "Stphane Clinchant"
                    }
                ],
                "author_detail": {
                    "name": "Stphane Clinchant"
                },
                "author": "Stphane Clinchant",
                "arxiv_comment": "10 pages",
                "arxiv_journal_ref": "WSDM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.09252v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.09252v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19274v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19274v2",
                "updated": "2024-10-29T17:33:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    33,
                    19,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-25T03:01:19Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    3,
                    1,
                    19,
                    4,
                    299,
                    0
                ],
                "title": "Ripple: Accelerating LLM Inference on Smartphones with Correlation-Aware\n  Neuron Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ripple: Accelerating LLM Inference on Smartphones with Correlation-Aware\n  Neuron Management"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains, yet deploying them on mobile devices remains an arduous challenge due\nto their extensive computational and memory demands. While lightweight LLMs\nhave been developed to fit mobile environments, they suffer from degraded model\naccuracy. In contrast, sparsity-based techniques minimize DRAM usage by\nselectively transferring only relevant neurons to DRAM while retaining the full\nmodel in external storage, such as flash. However, such approaches are\ncritically limited by numerous I/O operations, particularly on smartphones with\nsevere IOPS constraints.\n  In this paper, we propose Ripple, a novel approach that accelerates LLM\ninference on smartphones by optimizing neuron placement in flash memory. Ripple\nleverages the concept of Neuron Co-Activation, where neurons frequently\nactivated together are linked to facilitate continuous read access and optimize\ndata transfer efficiency. Our approach incorporates a two-stage solution: an\noffline stage that reorganizes neuron placement based on co-activation\npatterns, and an online stage that employs tailored data access and caching\nstrategies to align well with hardware characteristics. Evaluations conducted\non a variety of smartphones and LLMs demonstrate that Ripple achieves up to\n5.93x improvements in I/O latency compared to the state-of-the-art. As the\nfirst solution to optimize storage placement under sparsity, Ripple explores a\nnew optimization space at the intersection of sparsity-driven algorithm and\nstorage-level system co-design in LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains, yet deploying them on mobile devices remains an arduous challenge due\nto their extensive computational and memory demands. While lightweight LLMs\nhave been developed to fit mobile environments, they suffer from degraded model\naccuracy. In contrast, sparsity-based techniques minimize DRAM usage by\nselectively transferring only relevant neurons to DRAM while retaining the full\nmodel in external storage, such as flash. However, such approaches are\ncritically limited by numerous I/O operations, particularly on smartphones with\nsevere IOPS constraints.\n  In this paper, we propose Ripple, a novel approach that accelerates LLM\ninference on smartphones by optimizing neuron placement in flash memory. Ripple\nleverages the concept of Neuron Co-Activation, where neurons frequently\nactivated together are linked to facilitate continuous read access and optimize\ndata transfer efficiency. Our approach incorporates a two-stage solution: an\noffline stage that reorganizes neuron placement based on co-activation\npatterns, and an online stage that employs tailored data access and caching\nstrategies to align well with hardware characteristics. Evaluations conducted\non a variety of smartphones and LLMs demonstrate that Ripple achieves up to\n5.93x improvements in I/O latency compared to the state-of-the-art. As the\nfirst solution to optimize storage placement under sparsity, Ripple explores a\nnew optimization space at the intersection of sparsity-driven algorithm and\nstorage-level system co-design in LLM inference."
                },
                "authors": [
                    {
                        "name": "Tuowei Wang"
                    },
                    {
                        "name": "Ruwen Fan"
                    },
                    {
                        "name": "Minxing Huang"
                    },
                    {
                        "name": "Zixu Hao"
                    },
                    {
                        "name": "Kun Li"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Youyou Lu"
                    },
                    {
                        "name": "Yaoxue Zhang"
                    },
                    {
                        "name": "Ju Ren"
                    }
                ],
                "author_detail": {
                    "name": "Ju Ren"
                },
                "author": "Ju Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19274v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19274v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01878v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01878v3",
                "updated": "2024-10-29T17:29:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    29,
                    37,
                    1,
                    303,
                    0
                ],
                "published": "2024-07-02T01:37:56Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    1,
                    37,
                    56,
                    1,
                    184,
                    0
                ],
                "title": "Compare without Despair: Reliable Preference Evaluation with Generation\n  Separability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compare without Despair: Reliable Preference Evaluation with Generation\n  Separability"
                },
                "summary": "Human evaluation of generated language through pairwise preference judgments\nis pervasive. However, under common scenarios, such as when generations from a\nmodel pair are very similar, or when stochastic decoding results in large\nvariations in generations, it results in inconsistent preference ratings. We\naddress these challenges by introducing a meta-evaluation measure,\nseparability, which estimates how suitable a test instance is for pairwise\npreference evaluation. For a candidate test instance, separability samples\nmultiple generations from a pair of models, and measures how distinguishable\nthe two sets of generations are. Our experiments show that instances with high\nseparability values yield more consistent preference ratings from both human-\nand auto-raters. Further, the distribution of separability allows insights into\nwhich test benchmarks are more valuable for comparing models. Finally, we\nincorporate separability into ELO ratings, accounting for how suitable each\ntest instance might be for reliably ranking LLMs. Overall, separability has\nimplications for consistent, efficient and robust preference evaluation of LLMs\nwith both human- and auto-raters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human evaluation of generated language through pairwise preference judgments\nis pervasive. However, under common scenarios, such as when generations from a\nmodel pair are very similar, or when stochastic decoding results in large\nvariations in generations, it results in inconsistent preference ratings. We\naddress these challenges by introducing a meta-evaluation measure,\nseparability, which estimates how suitable a test instance is for pairwise\npreference evaluation. For a candidate test instance, separability samples\nmultiple generations from a pair of models, and measures how distinguishable\nthe two sets of generations are. Our experiments show that instances with high\nseparability values yield more consistent preference ratings from both human-\nand auto-raters. Further, the distribution of separability allows insights into\nwhich test benchmarks are more valuable for comparing models. Finally, we\nincorporate separability into ELO ratings, accounting for how suitable each\ntest instance might be for reliably ranking LLMs. Overall, separability has\nimplications for consistent, efficient and robust preference evaluation of LLMs\nwith both human- and auto-raters."
                },
                "authors": [
                    {
                        "name": "Sayan Ghosh"
                    },
                    {
                        "name": "Tejas Srinivasan"
                    },
                    {
                        "name": "Swabha Swayamdipta"
                    }
                ],
                "author_detail": {
                    "name": "Swabha Swayamdipta"
                },
                "author": "Swabha Swayamdipta",
                "arxiv_comment": "Corrected description of reference in Related Work; Findings of EMNLP\n  2024 Camera Ready Version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.01878v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01878v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01489v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01489v2",
                "updated": "2024-10-29T17:29:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    29,
                    27,
                    1,
                    303,
                    0
                ],
                "published": "2024-07-01T17:24:45Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    17,
                    24,
                    45,
                    0,
                    183,
                    0
                ],
                "title": "Agentless: Demystifying LLM-based Software Engineering Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentless: Demystifying LLM-based Software Engineering Agents"
                },
                "summary": "Recent advancements in large language models (LLMs) have significantly\nadvanced the automation of software development tasks, including code\nsynthesis, program repair, and test generation. More recently, researchers and\nindustry practitioners have developed various autonomous LLM agents to perform\nend-to-end software development tasks. These agents are equipped with the\nability to use tools, run commands, observe feedback from the environment, and\nplan for future actions. However, the complexity of these agent-based\napproaches, together with the limited abilities of current LLMs, raises the\nfollowing question: Do we really have to employ complex autonomous software\nagents? To attempt to answer this question, we build Agentless -- an agentless\napproach to automatically solve software development problems. Compared to the\nverbose and complex setup of agent-based approaches, Agentless employs a\nsimplistic three-phase process of localization, repair, and patch validation,\nwithout letting the LLM decide future actions or operate with complex tools.\nOur results on the popular SWE-bench Lite benchmark show that surprisingly the\nsimplistic Agentless is able to achieve both the highest performance (32.00%,\n96 correct fixes) and low cost ($0.70) compared with all existing open-source\nsoftware agents! Furthermore, we manually classified the problems in SWE-bench\nLite and found problems with exact ground truth patch or\ninsufficient/misleading issue descriptions. As such, we construct SWE-bench\nLite-S by excluding such problematic issues to perform more rigorous evaluation\nand comparison. Our work highlights the current overlooked potential of a\nsimple, interpretable technique in autonomous software development. We hope\nAgentless will help reset the baseline, starting point, and horizon for\nautonomous software agents, and inspire future work along this crucial\ndirection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have significantly\nadvanced the automation of software development tasks, including code\nsynthesis, program repair, and test generation. More recently, researchers and\nindustry practitioners have developed various autonomous LLM agents to perform\nend-to-end software development tasks. These agents are equipped with the\nability to use tools, run commands, observe feedback from the environment, and\nplan for future actions. However, the complexity of these agent-based\napproaches, together with the limited abilities of current LLMs, raises the\nfollowing question: Do we really have to employ complex autonomous software\nagents? To attempt to answer this question, we build Agentless -- an agentless\napproach to automatically solve software development problems. Compared to the\nverbose and complex setup of agent-based approaches, Agentless employs a\nsimplistic three-phase process of localization, repair, and patch validation,\nwithout letting the LLM decide future actions or operate with complex tools.\nOur results on the popular SWE-bench Lite benchmark show that surprisingly the\nsimplistic Agentless is able to achieve both the highest performance (32.00%,\n96 correct fixes) and low cost ($0.70) compared with all existing open-source\nsoftware agents! Furthermore, we manually classified the problems in SWE-bench\nLite and found problems with exact ground truth patch or\ninsufficient/misleading issue descriptions. As such, we construct SWE-bench\nLite-S by excluding such problematic issues to perform more rigorous evaluation\nand comparison. Our work highlights the current overlooked potential of a\nsimple, interpretable technique in autonomous software development. We hope\nAgentless will help reset the baseline, starting point, and horizon for\nautonomous software agents, and inspire future work along this crucial\ndirection."
                },
                "authors": [
                    {
                        "name": "Chunqiu Steven Xia"
                    },
                    {
                        "name": "Yinlin Deng"
                    },
                    {
                        "name": "Soren Dunn"
                    },
                    {
                        "name": "Lingming Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lingming Zhang"
                },
                "author": "Lingming Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.01489v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01489v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22269v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22269v1",
                "updated": "2024-10-29T17:27:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    27,
                    58,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T17:27:58Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    27,
                    58,
                    1,
                    303,
                    0
                ],
                "title": "Fourier Head: Helping Large Language Models Learn Complex Probability\n  Distributions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fourier Head: Helping Large Language Models Learn Complex Probability\n  Distributions"
                },
                "summary": "As the quality of large language models has improved, there has been\nincreased interest in using them to model non-linguistic tokens. For example,\nthe Decision Transformer recasts agentic decision making as a sequence modeling\nproblem, using a decoder-only LLM to model the distribution over the discrete\naction space for an Atari agent. However, when adapting LLMs to non-linguistic\ndomains, it remains unclear if softmax over discrete bins captures the\ncontinuous structure of the tokens and the potentially complex distributions\nneeded for high quality token generation. We introduce a neural network layer,\nconstructed using Fourier series, which we can easily substitute for any linear\nlayer if we want the outputs to have a more continuous structure. We perform\nextensive analysis on synthetic datasets, as well as on large-scale decision\nmaking and time series forecasting tasks. We also provide theoretical evidence\nthat this layer can better learn signal from data while ignoring high-frequency\nnoise. All of our results support the effectiveness of our proposed Fourier\nhead in scenarios where the underlying data distribution has a natural\ncontinuous structure. For example, the Fourier head improves a Decision\nTransformer agent's returns by 46% on the Atari Seaquest game, and increases a\nstate-of-the-art times series foundation model's forecasting performance by\n3.5% across 20 benchmarks unseen during training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the quality of large language models has improved, there has been\nincreased interest in using them to model non-linguistic tokens. For example,\nthe Decision Transformer recasts agentic decision making as a sequence modeling\nproblem, using a decoder-only LLM to model the distribution over the discrete\naction space for an Atari agent. However, when adapting LLMs to non-linguistic\ndomains, it remains unclear if softmax over discrete bins captures the\ncontinuous structure of the tokens and the potentially complex distributions\nneeded for high quality token generation. We introduce a neural network layer,\nconstructed using Fourier series, which we can easily substitute for any linear\nlayer if we want the outputs to have a more continuous structure. We perform\nextensive analysis on synthetic datasets, as well as on large-scale decision\nmaking and time series forecasting tasks. We also provide theoretical evidence\nthat this layer can better learn signal from data while ignoring high-frequency\nnoise. All of our results support the effectiveness of our proposed Fourier\nhead in scenarios where the underlying data distribution has a natural\ncontinuous structure. For example, the Fourier head improves a Decision\nTransformer agent's returns by 46% on the Atari Seaquest game, and increases a\nstate-of-the-art times series foundation model's forecasting performance by\n3.5% across 20 benchmarks unseen during training."
                },
                "authors": [
                    {
                        "name": "Nate Gillman"
                    },
                    {
                        "name": "Daksh Aggarwal"
                    },
                    {
                        "name": "Michael Freeman"
                    },
                    {
                        "name": "Saurabh Singh"
                    },
                    {
                        "name": "Chen Sun"
                    }
                ],
                "author_detail": {
                    "name": "Chen Sun"
                },
                "author": "Chen Sun",
                "arxiv_comment": "Project page and code are at https://nategillman.com/fourier-head",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22269v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22249v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22249v1",
                "updated": "2024-10-29T17:13:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    13,
                    54,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T17:13:54Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    13,
                    54,
                    1,
                    303,
                    0
                ],
                "title": "Pushing the Performance Envelope of DNN-based Recommendation Systems\n  Inference on GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pushing the Performance Envelope of DNN-based Recommendation Systems\n  Inference on GPUs"
                },
                "summary": "Personalized recommendation is a ubiquitous application on the internet, with\nmany industries and hyperscalers extensively leveraging Deep Learning\nRecommendation Models (DLRMs) for their personalization needs (like ad serving\nor movie suggestions). With growing model and dataset sizes pushing computation\nand memory requirements, GPUs are being increasingly preferred for executing\nDLRM inference. However, serving newer DLRMs, while meeting acceptable\nlatencies, continues to remain challenging, making traditional deployments\nincreasingly more GPU-hungry, resulting in higher inference serving costs. In\nthis paper, we show that the embedding stage continues to be the primary\nbottleneck in the GPU inference pipeline, leading up to a 3.2x embedding-only\nperformance slowdown.\n  To thoroughly grasp the problem, we conduct a detailed microarchitecture\ncharacterization and highlight the presence of low occupancy in the standard\nembedding kernels. By leveraging direct compiler optimizations, we achieve\noptimal occupancy, pushing the performance by up to 53%. Yet, long memory\nlatency stalls continue to exist. To tackle this challenge, we propose\nspecialized plug-and-play-based software prefetching and L2 pinning techniques,\nwhich help in hiding and decreasing the latencies. Further, we propose\ncombining them, as they complement each other. Experimental evaluations using\nA100 GPUs with large models and datasets show that our proposed techniques\nimprove performance by up to 103% for the embedding stage, and up to 77% for\nthe overall DLRM inference pipeline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized recommendation is a ubiquitous application on the internet, with\nmany industries and hyperscalers extensively leveraging Deep Learning\nRecommendation Models (DLRMs) for their personalization needs (like ad serving\nor movie suggestions). With growing model and dataset sizes pushing computation\nand memory requirements, GPUs are being increasingly preferred for executing\nDLRM inference. However, serving newer DLRMs, while meeting acceptable\nlatencies, continues to remain challenging, making traditional deployments\nincreasingly more GPU-hungry, resulting in higher inference serving costs. In\nthis paper, we show that the embedding stage continues to be the primary\nbottleneck in the GPU inference pipeline, leading up to a 3.2x embedding-only\nperformance slowdown.\n  To thoroughly grasp the problem, we conduct a detailed microarchitecture\ncharacterization and highlight the presence of low occupancy in the standard\nembedding kernels. By leveraging direct compiler optimizations, we achieve\noptimal occupancy, pushing the performance by up to 53%. Yet, long memory\nlatency stalls continue to exist. To tackle this challenge, we propose\nspecialized plug-and-play-based software prefetching and L2 pinning techniques,\nwhich help in hiding and decreasing the latencies. Further, we propose\ncombining them, as they complement each other. Experimental evaluations using\nA100 GPUs with large models and datasets show that our proposed techniques\nimprove performance by up to 103% for the embedding stage, and up to 77% for\nthe overall DLRM inference pipeline."
                },
                "authors": [
                    {
                        "name": "Rishabh Jain"
                    },
                    {
                        "name": "Vivek M. Bhasi"
                    },
                    {
                        "name": "Adwait Jog"
                    },
                    {
                        "name": "Anand Sivasubramaniam"
                    },
                    {
                        "name": "Mahmut T. Kandemir"
                    },
                    {
                        "name": "Chita R. Das"
                    }
                ],
                "author_detail": {
                    "name": "Chita R. Das"
                },
                "author": "Chita R. Das",
                "arxiv_comment": "This work has been accepted in the 57th MICRO\n  (https://microarch.org/micro57/program/). Please check appendix for details\n  on reproducing our work including codebase and steps",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22249v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22249v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22246v1",
                "updated": "2024-10-29T17:10:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    10,
                    23,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T17:10:23Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    10,
                    23,
                    1,
                    303,
                    0
                ],
                "title": "Optimizing and Managing Wireless Backhaul for Resilient Next-Generation\n  Cellular Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing and Managing Wireless Backhaul for Resilient Next-Generation\n  Cellular Networks"
                },
                "summary": "Next-generation wireless networks target high network availability,\nubiquitous coverage, and extremely high data rates for mobile users. This\nrequires exploring new frequency bands, e.g., mmWaves, moving toward\nultra-dense deployments in urban locations, and providing ad hoc, resilient\nconnectivity in rural scenarios. The design of the backhaul network plays a key\nrole in advancing how the access part of the wireless system supports\nnext-generation use cases. Wireless backhauling, such as the newly introduced\nIntegrated Access and Backhaul (IAB) concept in 5G, provides a promising\nsolution, also leveraging the mmWave technology and steerable beams to mitigate\ninterference and scalability issues. At the same time, however, managing and\noptimizing a complex wireless backhaul introduces additional challenges for the\noperation of cellular systems. This paper presents a strategy for the optimal\ncreation of the backhaul network considering various constraints related to\nnetwork topology, robustness, and flow management. We evaluate its feasibility\nand efficiency using synthetic and realistic network scenarios based on 3D\nmodeling of buildings and ray tracing. We implement and prototype our solution\nas a dynamic IAB control framework based on the Open Radio Access Network (RAN)\narchitecture, and demonstrate its functionality in Colosseum, a large-scale\nwireless network emulator with hardware in the loop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next-generation wireless networks target high network availability,\nubiquitous coverage, and extremely high data rates for mobile users. This\nrequires exploring new frequency bands, e.g., mmWaves, moving toward\nultra-dense deployments in urban locations, and providing ad hoc, resilient\nconnectivity in rural scenarios. The design of the backhaul network plays a key\nrole in advancing how the access part of the wireless system supports\nnext-generation use cases. Wireless backhauling, such as the newly introduced\nIntegrated Access and Backhaul (IAB) concept in 5G, provides a promising\nsolution, also leveraging the mmWave technology and steerable beams to mitigate\ninterference and scalability issues. At the same time, however, managing and\noptimizing a complex wireless backhaul introduces additional challenges for the\noperation of cellular systems. This paper presents a strategy for the optimal\ncreation of the backhaul network considering various constraints related to\nnetwork topology, robustness, and flow management. We evaluate its feasibility\nand efficiency using synthetic and realistic network scenarios based on 3D\nmodeling of buildings and ray tracing. We implement and prototype our solution\nas a dynamic IAB control framework based on the Open Radio Access Network (RAN)\narchitecture, and demonstrate its functionality in Colosseum, a large-scale\nwireless network emulator with hardware in the loop."
                },
                "authors": [
                    {
                        "name": "Gabriele Gemmi"
                    },
                    {
                        "name": "Michele Polese"
                    },
                    {
                        "name": "Tommaso Melodia"
                    },
                    {
                        "name": "Leonardo Maccari"
                    }
                ],
                "author_detail": {
                    "name": "Leonardo Maccari"
                },
                "author": "Leonardo Maccari",
                "arxiv_comment": "9 pages, 7 figures, conference, Published version at CNSM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22240v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22240v1",
                "updated": "2024-10-29T17:05:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    5,
                    25,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T17:05:25Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    17,
                    5,
                    25,
                    1,
                    303,
                    0
                ],
                "title": "Are Decoder-Only Large Language Models the Silver Bullet for Code\n  Search?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Decoder-Only Large Language Models the Silver Bullet for Code\n  Search?"
                },
                "summary": "Code search is crucial for code reuse, enabling developers to efficiently\nlocate relevant snippets. Current methods rely on encoder-based models, which\nsuffer from limitations such as poor generalization and restricted input\nlengths. Decoder-only large language models (LLMs), with their extensive\npre-training, larger size, and longer input capabilities, offer potential\nsolutions to these issues, yet their effectiveness in code search remains\nunderexplored. To fill this gap, our study presents the first systematic\nexploration of decoder-only LLMs for code search. We evaluate nine\nstate-of-the-art decoder-only models using two fine-tuning methods, two\ndatasets (CSN and CoSQA$^+$), and three model sizes. Our findings reveal that\nfine-tuned CodeGemma significantly outperforms encoder-only models like\nUniXcoder, achieving a 5.57% improvement in MRR on CSN and a 49.6% increase in\nMAP on CoSQA$^+$ compared to zero-shot UniXcoder. These results highlight the\nsuperior performance and adaptability of decoder-only models. Additionally, we\nprovide valuable insights into optimizing these models for code search,\ncovering aspects such as model selection, fine-tuning methods, training data,\nand model size, and discussing their strengths and limitations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code search is crucial for code reuse, enabling developers to efficiently\nlocate relevant snippets. Current methods rely on encoder-based models, which\nsuffer from limitations such as poor generalization and restricted input\nlengths. Decoder-only large language models (LLMs), with their extensive\npre-training, larger size, and longer input capabilities, offer potential\nsolutions to these issues, yet their effectiveness in code search remains\nunderexplored. To fill this gap, our study presents the first systematic\nexploration of decoder-only LLMs for code search. We evaluate nine\nstate-of-the-art decoder-only models using two fine-tuning methods, two\ndatasets (CSN and CoSQA$^+$), and three model sizes. Our findings reveal that\nfine-tuned CodeGemma significantly outperforms encoder-only models like\nUniXcoder, achieving a 5.57% improvement in MRR on CSN and a 49.6% increase in\nMAP on CoSQA$^+$ compared to zero-shot UniXcoder. These results highlight the\nsuperior performance and adaptability of decoder-only models. Additionally, we\nprovide valuable insights into optimizing these models for code search,\ncovering aspects such as model selection, fine-tuning methods, training data,\nand model size, and discussing their strengths and limitations."
                },
                "authors": [
                    {
                        "name": "Yuxuan Chen"
                    },
                    {
                        "name": "Guangsheng Ou"
                    },
                    {
                        "name": "Mingwei Liu"
                    },
                    {
                        "name": "Yanlin Wang"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22240v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22240v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22225v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22225v1",
                "updated": "2024-10-29T16:54:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    16,
                    54,
                    15,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T16:54:15Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    16,
                    54,
                    15,
                    1,
                    303,
                    0
                ],
                "title": "CaStL: Constraints as Specifications through LLM Translation for\n  Long-Horizon Task and Motion Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CaStL: Constraints as Specifications through LLM Translation for\n  Long-Horizon Task and Motion Planning"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable ability in\nlong-horizon Task and Motion Planning (TAMP) by translating clear and\nstraightforward natural language problems into formal specifications such as\nthe Planning Domain Definition Language (PDDL). However, real-world problems\nare often ambiguous and involve many complex constraints. In this paper, we\nintroduce Constraints as Specifications through LLMs (CaStL), a framework that\nidentifies constraints such as goal conditions, action ordering, and action\nblocking from natural language in multiple stages. CaStL translates these\nconstraints into PDDL and Python scripts, which are solved using an custom PDDL\nsolver. Tested across three PDDL domains, CaStL significantly improves\nconstraint handling and planning success rates from natural language\nspecification in complex scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable ability in\nlong-horizon Task and Motion Planning (TAMP) by translating clear and\nstraightforward natural language problems into formal specifications such as\nthe Planning Domain Definition Language (PDDL). However, real-world problems\nare often ambiguous and involve many complex constraints. In this paper, we\nintroduce Constraints as Specifications through LLMs (CaStL), a framework that\nidentifies constraints such as goal conditions, action ordering, and action\nblocking from natural language in multiple stages. CaStL translates these\nconstraints into PDDL and Python scripts, which are solved using an custom PDDL\nsolver. Tested across three PDDL domains, CaStL significantly improves\nconstraint handling and planning success rates from natural language\nspecification in complex scenarios."
                },
                "authors": [
                    {
                        "name": "Weihang Guo"
                    },
                    {
                        "name": "Zachary Kingston"
                    },
                    {
                        "name": "Lydia E. Kavraki"
                    }
                ],
                "author_detail": {
                    "name": "Lydia E. Kavraki"
                },
                "author": "Lydia E. Kavraki",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22225v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22225v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22217v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22217v1",
                "updated": "2024-10-29T16:48:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    16,
                    48,
                    22,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T16:48:22Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    16,
                    48,
                    22,
                    1,
                    303,
                    0
                ],
                "title": "Towards Unifying Understanding and Generation in the Era of Vision\n  Foundation Models: A Survey from the Autoregression Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Unifying Understanding and Generation in the Era of Vision\n  Foundation Models: A Survey from the Autoregression Perspective"
                },
                "summary": "Autoregression in large language models (LLMs) has shown impressive\nscalability by unifying all language tasks into the next token prediction\nparadigm. Recently, there is a growing interest in extending this success to\nvision foundation models. In this survey, we review the recent advances and\ndiscuss future directions for autoregressive vision foundation models. First,\nwe present the trend for next generation of vision foundation models, i.e.,\nunifying both understanding and generation in vision tasks. We then analyze the\nlimitations of existing vision foundation models, and present a formal\ndefinition of autoregression with its advantages. Later, we categorize\nautoregressive vision foundation models from their vision tokenizers and\nautoregression backbones. Finally, we discuss several promising research\nchallenges and directions. To the best of our knowledge, this is the first\nsurvey to comprehensively summarize autoregressive vision foundation models\nunder the trend of unifying understanding and generation. A collection of\nrelated resources is available at https://github.com/EmmaSRH/ARVFM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregression in large language models (LLMs) has shown impressive\nscalability by unifying all language tasks into the next token prediction\nparadigm. Recently, there is a growing interest in extending this success to\nvision foundation models. In this survey, we review the recent advances and\ndiscuss future directions for autoregressive vision foundation models. First,\nwe present the trend for next generation of vision foundation models, i.e.,\nunifying both understanding and generation in vision tasks. We then analyze the\nlimitations of existing vision foundation models, and present a formal\ndefinition of autoregression with its advantages. Later, we categorize\nautoregressive vision foundation models from their vision tokenizers and\nautoregression backbones. Finally, we discuss several promising research\nchallenges and directions. To the best of our knowledge, this is the first\nsurvey to comprehensively summarize autoregressive vision foundation models\nunder the trend of unifying understanding and generation. A collection of\nrelated resources is available at https://github.com/EmmaSRH/ARVFM."
                },
                "authors": [
                    {
                        "name": "Shenghao Xie"
                    },
                    {
                        "name": "Wenqiang Zu"
                    },
                    {
                        "name": "Mingyang Zhao"
                    },
                    {
                        "name": "Duo Su"
                    },
                    {
                        "name": "Shilong Liu"
                    },
                    {
                        "name": "Ruohua Shi"
                    },
                    {
                        "name": "Guoqi Li"
                    },
                    {
                        "name": "Shanghang Zhang"
                    },
                    {
                        "name": "Lei Ma"
                    }
                ],
                "author_detail": {
                    "name": "Lei Ma"
                },
                "author": "Lei Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22217v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22217v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19759v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19759v3",
                "updated": "2024-10-30T06:12:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    6,
                    12,
                    49,
                    2,
                    304,
                    0
                ],
                "published": "2024-09-29T20:14:50Z",
                "published_parsed": [
                    2024,
                    9,
                    29,
                    20,
                    14,
                    50,
                    6,
                    273,
                    0
                ],
                "title": "Balancing Cost and Effectiveness of Synthetic Data Generation Strategies\n  for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Balancing Cost and Effectiveness of Synthetic Data Generation Strategies\n  for LLMs"
                },
                "summary": "As large language models (LLMs) are applied to more use cases, creating high\nquality, task-specific datasets for fine-tuning becomes a bottleneck for model\nimprovement. Using high quality human data has been the most common approach to\nunlock model performance, but is prohibitively expensive in many scenarios.\nSeveral alternative methods have also emerged, such as generating synthetic or\nhybrid data, but the effectiveness of these approaches remain unclear,\nespecially in resource-constrained scenarios and tasks that are not easily\nverified. To investigate this, we group various synthetic data generation\nstrategies into three representative categories -- Answer Augmentation,\nQuestion Rephrase and New Question -- and study the performance of student LLMs\ntrained under various constraints, namely seed instruction set size and query\nbudget. We demonstrate that these strategies are not equally effective across\nsettings. Notably, the optimal data generation strategy depends strongly on the\nratio between the available teacher query budget and the size of the seed\ninstruction set. When this ratio is low, generating new answers to existing\nquestions proves most effective, but as this ratio increases, generating new\nquestions becomes optimal. Across all tasks, we find that choice of\naugmentation method and other design choices matter substantially more in low\nto mid data regimes than in high data regimes. We provide a practical framework\nfor selecting the appropriate augmentation method across settings, taking into\naccount additional factors such as the scalability of each method, the\nimportance of verifying synthetic data, and the use of different LLMs for\nsynthetic data generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) are applied to more use cases, creating high\nquality, task-specific datasets for fine-tuning becomes a bottleneck for model\nimprovement. Using high quality human data has been the most common approach to\nunlock model performance, but is prohibitively expensive in many scenarios.\nSeveral alternative methods have also emerged, such as generating synthetic or\nhybrid data, but the effectiveness of these approaches remain unclear,\nespecially in resource-constrained scenarios and tasks that are not easily\nverified. To investigate this, we group various synthetic data generation\nstrategies into three representative categories -- Answer Augmentation,\nQuestion Rephrase and New Question -- and study the performance of student LLMs\ntrained under various constraints, namely seed instruction set size and query\nbudget. We demonstrate that these strategies are not equally effective across\nsettings. Notably, the optimal data generation strategy depends strongly on the\nratio between the available teacher query budget and the size of the seed\ninstruction set. When this ratio is low, generating new answers to existing\nquestions proves most effective, but as this ratio increases, generating new\nquestions becomes optimal. Across all tasks, we find that choice of\naugmentation method and other design choices matter substantially more in low\nto mid data regimes than in high data regimes. We provide a practical framework\nfor selecting the appropriate augmentation method across settings, taking into\naccount additional factors such as the scalability of each method, the\nimportance of verifying synthetic data, and the use of different LLMs for\nsynthetic data generation."
                },
                "authors": [
                    {
                        "name": "Yung-Chieh Chan"
                    },
                    {
                        "name": "George Pu"
                    },
                    {
                        "name": "Apaar Shanker"
                    },
                    {
                        "name": "Parth Suresh"
                    },
                    {
                        "name": "Penn Jenks"
                    },
                    {
                        "name": "John Heyer"
                    },
                    {
                        "name": "Sam Denton"
                    }
                ],
                "author_detail": {
                    "name": "Sam Denton"
                },
                "author": "Sam Denton",
                "arxiv_comment": "NeurIPS '24 Workshop on Fine-Tuning in Modern Machine Learning:\n  Principles and Scalability",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19759v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19759v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22211v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22211v1",
                "updated": "2024-10-29T16:39:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    16,
                    39,
                    28,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T16:39:28Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    16,
                    39,
                    28,
                    1,
                    303,
                    0
                ],
                "title": "ProMQA: Question Answering Dataset for Multimodal Procedural Activity\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProMQA: Question Answering Dataset for Multimodal Procedural Activity\n  Understanding"
                },
                "summary": "Multimodal systems have great potential to assist humans in procedural\nactivities, where people follow instructions to achieve their goals. Despite\ndiverse application scenarios, systems are typically evaluated on traditional\nclassification tasks, e.g., action recognition or temporal action segmentation.\nIn this paper, we present a novel evaluation dataset, ProMQA, to measure system\nadvancements in application-oriented scenarios. ProMQA consists of 401\nmultimodal procedural QA pairs on user recording of procedural activities\ncoupled with their corresponding instruction. For QA annotation, we take a\ncost-effective human-LLM collaborative approach, where the existing annotation\nis augmented with LLM-generated QA pairs that are later verified by humans. We\nthen provide the benchmark results to set the baseline performance on ProMQA.\nOur experiment reveals a significant gap between human performance and that of\ncurrent systems, including competitive proprietary multimodal models. We hope\nour dataset sheds light on new aspects of models' multimodal understanding\ncapabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal systems have great potential to assist humans in procedural\nactivities, where people follow instructions to achieve their goals. Despite\ndiverse application scenarios, systems are typically evaluated on traditional\nclassification tasks, e.g., action recognition or temporal action segmentation.\nIn this paper, we present a novel evaluation dataset, ProMQA, to measure system\nadvancements in application-oriented scenarios. ProMQA consists of 401\nmultimodal procedural QA pairs on user recording of procedural activities\ncoupled with their corresponding instruction. For QA annotation, we take a\ncost-effective human-LLM collaborative approach, where the existing annotation\nis augmented with LLM-generated QA pairs that are later verified by humans. We\nthen provide the benchmark results to set the baseline performance on ProMQA.\nOur experiment reveals a significant gap between human performance and that of\ncurrent systems, including competitive proprietary multimodal models. We hope\nour dataset sheds light on new aspects of models' multimodal understanding\ncapabilities."
                },
                "authors": [
                    {
                        "name": "Kimihiro Hasegawa"
                    },
                    {
                        "name": "Wiradee Imrattanatrai"
                    },
                    {
                        "name": "Zhi-Qi Cheng"
                    },
                    {
                        "name": "Masaki Asada"
                    },
                    {
                        "name": "Susan Holm"
                    },
                    {
                        "name": "Yuran Wang"
                    },
                    {
                        "name": "Ken Fukuda"
                    },
                    {
                        "name": "Teruko Mitamura"
                    }
                ],
                "author_detail": {
                    "name": "Teruko Mitamura"
                },
                "author": "Teruko Mitamura",
                "arxiv_comment": "18 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22211v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22211v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22182v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22182v1",
                "updated": "2024-10-29T16:19:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    16,
                    19,
                    8,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T16:19:08Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    16,
                    19,
                    8,
                    1,
                    303,
                    0
                ],
                "title": "Synthetic Data Generation with Large Language Models for Personalized\n  Community Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic Data Generation with Large Language Models for Personalized\n  Community Question Answering"
                },
                "summary": "Personalization in Information Retrieval (IR) is a topic studied by the\nresearch community since a long time. However, there is still a lack of\ndatasets to conduct large-scale evaluations of personalized IR; this is mainly\ndue to the fact that collecting and curating high-quality user-related\ninformation requires significant costs and time investment. Furthermore, the\ncreation of datasets for Personalized IR (PIR) tasks is affected by both\nprivacy concerns and the need for accurate user-related data, which are often\nnot publicly available. Recently, researchers have started to explore the use\nof Large Language Models (LLMs) to generate synthetic datasets, which is a\npossible solution to generate data for low-resource tasks. In this paper, we\ninvestigate the potential of Large Language Models (LLMs) for generating\nsynthetic documents to train an IR system for a Personalized Community Question\nAnswering task. To study the effectiveness of IR models fine-tuned on\nLLM-generated data, we introduce a new dataset, named Sy-SE-PQA. We build\nSy-SE-PQA based on an existing dataset, SE-PQA, which consists of questions and\nanswers posted on the popular StackExchange communities. Starting from\nquestions in SE-PQA, we generate synthetic answers using different prompt\ntechniques and LLMs. Our findings suggest that LLMs have high potential in\ngenerating data tailored to users' needs. The synthetic data can replace\nhuman-written training data, even if the generated data may contain incorrect\ninformation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalization in Information Retrieval (IR) is a topic studied by the\nresearch community since a long time. However, there is still a lack of\ndatasets to conduct large-scale evaluations of personalized IR; this is mainly\ndue to the fact that collecting and curating high-quality user-related\ninformation requires significant costs and time investment. Furthermore, the\ncreation of datasets for Personalized IR (PIR) tasks is affected by both\nprivacy concerns and the need for accurate user-related data, which are often\nnot publicly available. Recently, researchers have started to explore the use\nof Large Language Models (LLMs) to generate synthetic datasets, which is a\npossible solution to generate data for low-resource tasks. In this paper, we\ninvestigate the potential of Large Language Models (LLMs) for generating\nsynthetic documents to train an IR system for a Personalized Community Question\nAnswering task. To study the effectiveness of IR models fine-tuned on\nLLM-generated data, we introduce a new dataset, named Sy-SE-PQA. We build\nSy-SE-PQA based on an existing dataset, SE-PQA, which consists of questions and\nanswers posted on the popular StackExchange communities. Starting from\nquestions in SE-PQA, we generate synthetic answers using different prompt\ntechniques and LLMs. Our findings suggest that LLMs have high potential in\ngenerating data tailored to users' needs. The synthetic data can replace\nhuman-written training data, even if the generated data may contain incorrect\ninformation."
                },
                "authors": [
                    {
                        "name": "Marco Braga"
                    },
                    {
                        "name": "Pranav Kasela"
                    },
                    {
                        "name": "Alessandro Raganato"
                    },
                    {
                        "name": "Gabriella Pasi"
                    }
                ],
                "author_detail": {
                    "name": "Gabriella Pasi"
                },
                "author": "Gabriella Pasi",
                "arxiv_comment": "Accepted in WI-IAT '24",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22182v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22182v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22177v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22177v1",
                "updated": "2024-10-29T16:15:59Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    16,
                    15,
                    59,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T16:15:59Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    16,
                    15,
                    59,
                    1,
                    303,
                    0
                ],
                "title": "Analyzing Multimodal Interaction Strategies for LLM-Assisted\n  Manipulation of 3D Scenes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing Multimodal Interaction Strategies for LLM-Assisted\n  Manipulation of 3D Scenes"
                },
                "summary": "As more applications of large language models (LLMs) for 3D content for\nimmersive environments emerge, it is crucial to study user behaviour to\nidentify interaction patterns and potential barriers to guide the future design\nof immersive content creation and editing systems which involve LLMs. In an\nempirical user study with 12 participants, we combine quantitative usage data\nwith post-experience questionnaire feedback to reveal common interaction\npatterns and key barriers in LLM-assisted 3D scene editing systems. We identify\nopportunities for improving natural language interfaces in 3D design tools and\npropose design recommendations for future LLM-integrated 3D content creation\nsystems. Through an empirical study, we demonstrate that LLM-assisted\ninteractive systems can be used productively in immersive environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As more applications of large language models (LLMs) for 3D content for\nimmersive environments emerge, it is crucial to study user behaviour to\nidentify interaction patterns and potential barriers to guide the future design\nof immersive content creation and editing systems which involve LLMs. In an\nempirical user study with 12 participants, we combine quantitative usage data\nwith post-experience questionnaire feedback to reveal common interaction\npatterns and key barriers in LLM-assisted 3D scene editing systems. We identify\nopportunities for improving natural language interfaces in 3D design tools and\npropose design recommendations for future LLM-integrated 3D content creation\nsystems. Through an empirical study, we demonstrate that LLM-assisted\ninteractive systems can be used productively in immersive environments."
                },
                "authors": [
                    {
                        "name": "Junlong Chen"
                    },
                    {
                        "name": "Jens Grubert"
                    },
                    {
                        "name": "Per Ola Kristensson"
                    }
                ],
                "author_detail": {
                    "name": "Per Ola Kristensson"
                },
                "author": "Per Ola Kristensson",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22177v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22177v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22159v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22159v2",
                "updated": "2024-10-30T06:11:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    6,
                    11,
                    54,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-29T15:54:09Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    54,
                    9,
                    1,
                    303,
                    0
                ],
                "title": "Training LLMs for Generating IEC 61131-3 Structured Text with Online\n  Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training LLMs for Generating IEC 61131-3 Structured Text with Online\n  Feedback"
                },
                "summary": "The advent of large language models (LLMs), such as GPT-4, has enabled\nsignificant advancements in generating code across various domains. However,\nthese models face unique challenges when generating IEC 61131-3 Structured Text\n(ST) code due to limited data in public training datasets and the complexity of\nST language syntax. This paper proposes a novel approach to training LLMs that\nemphasizes improving the quality of learning data through an online process\ninvolving compiler feedback and evaluation from a secondary LLM. In this\nframework, the primary LLM generates new training samples, which are\nsubsequently evaluated by a compiler for syntactical correctness and by a\nspecialized LLM that excels at assessing semantic accuracy, though it is not\noptimized for code generation itself. Through iterative refinement of the\ntraining data, this approach results in marked improvements for the trained\nLLM, leading to higher compilation success rates and better semantic precision.\nAs a result, the framework proves highly suitable for industrial automation\napplications and outperforms state-of-the-art models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of large language models (LLMs), such as GPT-4, has enabled\nsignificant advancements in generating code across various domains. However,\nthese models face unique challenges when generating IEC 61131-3 Structured Text\n(ST) code due to limited data in public training datasets and the complexity of\nST language syntax. This paper proposes a novel approach to training LLMs that\nemphasizes improving the quality of learning data through an online process\ninvolving compiler feedback and evaluation from a secondary LLM. In this\nframework, the primary LLM generates new training samples, which are\nsubsequently evaluated by a compiler for syntactical correctness and by a\nspecialized LLM that excels at assessing semantic accuracy, though it is not\noptimized for code generation itself. Through iterative refinement of the\ntraining data, this approach results in marked improvements for the trained\nLLM, leading to higher compilation success rates and better semantic precision.\nAs a result, the framework proves highly suitable for industrial automation\napplications and outperforms state-of-the-art models."
                },
                "authors": [
                    {
                        "name": "Aaron Haag"
                    },
                    {
                        "name": "Bertram Fuchs"
                    },
                    {
                        "name": "Altay Kacan"
                    },
                    {
                        "name": "Oliver Lohse"
                    }
                ],
                "author_detail": {
                    "name": "Oliver Lohse"
                },
                "author": "Oliver Lohse",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22159v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22159v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22153v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22153v1",
                "updated": "2024-10-29T15:51:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    51,
                    24,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T15:51:24Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    51,
                    24,
                    1,
                    303,
                    0
                ],
                "title": "Benchmarking LLM Guardrails in Handling Multilingual Toxicity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking LLM Guardrails in Handling Multilingual Toxicity"
                },
                "summary": "With the ubiquity of Large Language Models (LLMs), guardrails have become\ncrucial to detect and defend against toxic content. However, with the\nincreasing pervasiveness of LLMs in multilingual scenarios, their effectiveness\nin handling multilingual toxic inputs remains unclear. In this work, we\nintroduce a comprehensive multilingual test suite, spanning seven datasets and\nover ten languages, to benchmark the performance of state-of-the-art\nguardrails. We also investigates the resilience of guardrails against recent\njailbreaking techniques, and assess the impact of in-context safety policies\nand language resource availability on guardrails' performance. Our findings\nshow that existing guardrails are still ineffective at handling multilingual\ntoxicity and lack robustness against jailbreaking prompts. This work aims to\nidentify the limitations of guardrails and to build a more reliable and\ntrustworthy LLMs in multilingual scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the ubiquity of Large Language Models (LLMs), guardrails have become\ncrucial to detect and defend against toxic content. However, with the\nincreasing pervasiveness of LLMs in multilingual scenarios, their effectiveness\nin handling multilingual toxic inputs remains unclear. In this work, we\nintroduce a comprehensive multilingual test suite, spanning seven datasets and\nover ten languages, to benchmark the performance of state-of-the-art\nguardrails. We also investigates the resilience of guardrails against recent\njailbreaking techniques, and assess the impact of in-context safety policies\nand language resource availability on guardrails' performance. Our findings\nshow that existing guardrails are still ineffective at handling multilingual\ntoxicity and lack robustness against jailbreaking prompts. This work aims to\nidentify the limitations of guardrails and to build a more reliable and\ntrustworthy LLMs in multilingual scenarios."
                },
                "authors": [
                    {
                        "name": "Yahan Yang"
                    },
                    {
                        "name": "Soham Dan"
                    },
                    {
                        "name": "Dan Roth"
                    },
                    {
                        "name": "Insup Lee"
                    }
                ],
                "author_detail": {
                    "name": "Insup Lee"
                },
                "author": "Insup Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22153v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22143v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22143v1",
                "updated": "2024-10-29T15:40:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    40,
                    7,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T15:40:07Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    40,
                    7,
                    1,
                    303,
                    0
                ],
                "title": "AmpleGCG-Plus: A Strong Generative Model of Adversarial Suffixes to\n  Jailbreak LLMs with Higher Success Rates in Fewer Attempts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AmpleGCG-Plus: A Strong Generative Model of Adversarial Suffixes to\n  Jailbreak LLMs with Higher Success Rates in Fewer Attempts"
                },
                "summary": "Although large language models (LLMs) are typically aligned, they remain\nvulnerable to jailbreaking through either carefully crafted prompts in natural\nlanguage or, interestingly, gibberish adversarial suffixes. However, gibberish\ntokens have received relatively less attention despite their success in\nattacking aligned LLMs. Recent work, AmpleGCG~\\citep{liao2024amplegcg},\ndemonstrates that a generative model can quickly produce numerous customizable\ngibberish adversarial suffixes for any harmful query, exposing a range of\nalignment gaps in out-of-distribution (OOD) language spaces. To bring more\nattention to this area, we introduce AmpleGCG-Plus, an enhanced version that\nachieves better performance in fewer attempts. Through a series of exploratory\nexperiments, we identify several training strategies to improve the learning of\ngibberish suffixes. Our results, verified under a strict evaluation setting,\nshow that it outperforms AmpleGCG on both open-weight and closed-source models,\nachieving increases in attack success rate (ASR) of up to 17\\% in the white-box\nsetting against Llama-2-7B-chat, and more than tripling ASR in the black-box\nsetting against GPT-4. Notably, AmpleGCG-Plus jailbreaks the newer GPT-4o\nseries of models at similar rates to GPT-4, and, uncovers vulnerabilities\nagainst the recently proposed circuit breakers defense. We publicly release\nAmpleGCG-Plus along with our collected training datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although large language models (LLMs) are typically aligned, they remain\nvulnerable to jailbreaking through either carefully crafted prompts in natural\nlanguage or, interestingly, gibberish adversarial suffixes. However, gibberish\ntokens have received relatively less attention despite their success in\nattacking aligned LLMs. Recent work, AmpleGCG~\\citep{liao2024amplegcg},\ndemonstrates that a generative model can quickly produce numerous customizable\ngibberish adversarial suffixes for any harmful query, exposing a range of\nalignment gaps in out-of-distribution (OOD) language spaces. To bring more\nattention to this area, we introduce AmpleGCG-Plus, an enhanced version that\nachieves better performance in fewer attempts. Through a series of exploratory\nexperiments, we identify several training strategies to improve the learning of\ngibberish suffixes. Our results, verified under a strict evaluation setting,\nshow that it outperforms AmpleGCG on both open-weight and closed-source models,\nachieving increases in attack success rate (ASR) of up to 17\\% in the white-box\nsetting against Llama-2-7B-chat, and more than tripling ASR in the black-box\nsetting against GPT-4. Notably, AmpleGCG-Plus jailbreaks the newer GPT-4o\nseries of models at similar rates to GPT-4, and, uncovers vulnerabilities\nagainst the recently proposed circuit breakers defense. We publicly release\nAmpleGCG-Plus along with our collected training datasets."
                },
                "authors": [
                    {
                        "name": "Vishal Kumar"
                    },
                    {
                        "name": "Zeyi Liao"
                    },
                    {
                        "name": "Jaylen Jones"
                    },
                    {
                        "name": "Huan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Huan Sun"
                },
                "author": "Huan Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22143v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22143v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.11208v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.11208v2",
                "updated": "2024-10-29T15:32:04Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    32,
                    4,
                    1,
                    303,
                    0
                ],
                "published": "2024-02-17T06:48:45Z",
                "published_parsed": [
                    2024,
                    2,
                    17,
                    6,
                    48,
                    45,
                    5,
                    48,
                    0
                ],
                "title": "Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based\n  Agents"
                },
                "summary": "Driven by the rapid development of Large Language Models (LLMs), LLM-based\nagents have been developed to handle various real-world applications, including\nfinance, healthcare, and shopping, etc. It is crucial to ensure the reliability\nand security of LLM-based agents during applications. However, the safety\nissues of LLM-based agents are currently under-explored. In this work, we take\nthe first step to investigate one of the typical safety threats, backdoor\nattack, to LLM-based agents. We first formulate a general framework of agent\nbackdoor attacks, then we present a thorough analysis of different forms of\nagent backdoor attacks. Specifically, compared with traditional backdoor\nattacks on LLMs that are only able to manipulate the user inputs and model\noutputs, agent backdoor attacks exhibit more diverse and covert forms: (1) From\nthe perspective of the final attacking outcomes, the agent backdoor attacker\ncan not only choose to manipulate the final output distribution, but also\nintroduce the malicious behavior in an intermediate reasoning step only, while\nkeeping the final output correct. (2) Furthermore, the former category can be\ndivided into two subcategories based on trigger locations, in which the\nbackdoor trigger can either be hidden in the user query or appear in an\nintermediate observation returned by the external environment. We implement the\nabove variations of agent backdoor attacks on two typical agent tasks including\nweb shopping and tool utilization. Extensive experiments show that LLM-based\nagents suffer severely from backdoor attacks and such backdoor vulnerability\ncannot be easily mitigated by current textual backdoor defense algorithms. This\nindicates an urgent need for further research on the development of targeted\ndefenses against backdoor attacks on LLM-based agents. Warning: This paper may\ncontain biased content.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Driven by the rapid development of Large Language Models (LLMs), LLM-based\nagents have been developed to handle various real-world applications, including\nfinance, healthcare, and shopping, etc. It is crucial to ensure the reliability\nand security of LLM-based agents during applications. However, the safety\nissues of LLM-based agents are currently under-explored. In this work, we take\nthe first step to investigate one of the typical safety threats, backdoor\nattack, to LLM-based agents. We first formulate a general framework of agent\nbackdoor attacks, then we present a thorough analysis of different forms of\nagent backdoor attacks. Specifically, compared with traditional backdoor\nattacks on LLMs that are only able to manipulate the user inputs and model\noutputs, agent backdoor attacks exhibit more diverse and covert forms: (1) From\nthe perspective of the final attacking outcomes, the agent backdoor attacker\ncan not only choose to manipulate the final output distribution, but also\nintroduce the malicious behavior in an intermediate reasoning step only, while\nkeeping the final output correct. (2) Furthermore, the former category can be\ndivided into two subcategories based on trigger locations, in which the\nbackdoor trigger can either be hidden in the user query or appear in an\nintermediate observation returned by the external environment. We implement the\nabove variations of agent backdoor attacks on two typical agent tasks including\nweb shopping and tool utilization. Extensive experiments show that LLM-based\nagents suffer severely from backdoor attacks and such backdoor vulnerability\ncannot be easily mitigated by current textual backdoor defense algorithms. This\nindicates an urgent need for further research on the development of targeted\ndefenses against backdoor attacks on LLM-based agents. Warning: This paper may\ncontain biased content."
                },
                "authors": [
                    {
                        "name": "Wenkai Yang"
                    },
                    {
                        "name": "Xiaohan Bi"
                    },
                    {
                        "name": "Yankai Lin"
                    },
                    {
                        "name": "Sishuo Chen"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Xu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Xu Sun"
                },
                "author": "Xu Sun",
                "arxiv_comment": "Accepted at NeurIPS 2024, camera ready version. Code and data are\n  available at https://github.com/lancopku/agent-backdoor-attacks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.11208v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.11208v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22134v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22134v1",
                "updated": "2024-10-29T15:31:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    31,
                    27,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T15:31:27Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    31,
                    27,
                    1,
                    303,
                    0
                ],
                "title": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching"
                },
                "summary": "The promising applications of large language models are often constrained by\nthe limited GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help mitigate this issue by activating only a subset of the\nmodel's parameters during computation, allowing the unused parameters to be\noffloaded to host memory and reducing overall GPU memory demand. However,\nexisting cache-based offloading solutions handle cache misses reactively and\nsignificantly impact system performance. In this paper, we propose ProMoE, a\nnovel proactive caching system that leverages intermediate model results to\npredict subsequent parameter usage. By proactively fetching experts in advance,\nProMoE removes the loading time from the critical path and diminishes the\nperformance overhead of offloading. Our evaluations demonstrate that ProMoE\nachieves an average speedup of 2.13x and 2.84x in the prefill and decode stages\nrespectively, compared to existing offloading solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The promising applications of large language models are often constrained by\nthe limited GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help mitigate this issue by activating only a subset of the\nmodel's parameters during computation, allowing the unused parameters to be\noffloaded to host memory and reducing overall GPU memory demand. However,\nexisting cache-based offloading solutions handle cache misses reactively and\nsignificantly impact system performance. In this paper, we propose ProMoE, a\nnovel proactive caching system that leverages intermediate model results to\npredict subsequent parameter usage. By proactively fetching experts in advance,\nProMoE removes the loading time from the critical path and diminishes the\nperformance overhead of offloading. Our evaluations demonstrate that ProMoE\nachieves an average speedup of 2.13x and 2.84x in the prefill and decode stages\nrespectively, compared to existing offloading solutions."
                },
                "authors": [
                    {
                        "name": "Xiaoniu Song"
                    },
                    {
                        "name": "Zihang Zhong"
                    },
                    {
                        "name": "Rong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Rong Chen"
                },
                "author": "Rong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22134v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22134v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22129v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22129v1",
                "updated": "2024-10-29T15:28:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    28,
                    19,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T15:28:19Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    28,
                    19,
                    1,
                    303,
                    0
                ],
                "title": "Improving Performance of Commercially Available AI Products in a\n  Multi-Agent Configuration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Performance of Commercially Available AI Products in a\n  Multi-Agent Configuration"
                },
                "summary": "In recent years, with the rapid advancement of large language models (LLMs),\nmulti-agent systems have become increasingly more capable of practical\napplication. At the same time, the software development industry has had a\nnumber of new AI-powered tools developed that improve the software development\nlifecycle (SDLC). Academically, much attention has been paid to the role of\nmulti-agent systems to the SDLC. And, while single-agent systems have\nfrequently been examined in real-world applications, we have seen comparatively\nfew real-world examples of publicly available commercial tools working together\nin a multi-agent system with measurable improvements. In this experiment we\ntest context sharing between Crowdbotics PRD AI, a tool for generating software\nrequirements using AI, and GitHub Copilot, an AI pair-programming tool. By\nsharing business requirements from PRD AI, we improve the code suggestion\ncapabilities of GitHub Copilot by 13.8% and developer task success rate by\n24.5% -- demonstrating a real-world example of commercially-available AI\nsystems working together with improved outcomes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, with the rapid advancement of large language models (LLMs),\nmulti-agent systems have become increasingly more capable of practical\napplication. At the same time, the software development industry has had a\nnumber of new AI-powered tools developed that improve the software development\nlifecycle (SDLC). Academically, much attention has been paid to the role of\nmulti-agent systems to the SDLC. And, while single-agent systems have\nfrequently been examined in real-world applications, we have seen comparatively\nfew real-world examples of publicly available commercial tools working together\nin a multi-agent system with measurable improvements. In this experiment we\ntest context sharing between Crowdbotics PRD AI, a tool for generating software\nrequirements using AI, and GitHub Copilot, an AI pair-programming tool. By\nsharing business requirements from PRD AI, we improve the code suggestion\ncapabilities of GitHub Copilot by 13.8% and developer task success rate by\n24.5% -- demonstrating a real-world example of commercially-available AI\nsystems working together with improved outcomes."
                },
                "authors": [
                    {
                        "name": "Cory Hymel"
                    },
                    {
                        "name": "Sida Peng"
                    },
                    {
                        "name": "Kevin Xu"
                    },
                    {
                        "name": "Charath Ranganathan"
                    }
                ],
                "author_detail": {
                    "name": "Charath Ranganathan"
                },
                "author": "Charath Ranganathan",
                "arxiv_comment": "7 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22129v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22129v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12629v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12629v3",
                "updated": "2024-10-29T15:21:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    21,
                    40,
                    1,
                    303,
                    0
                ],
                "published": "2024-06-18T13:55:13Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    13,
                    55,
                    13,
                    1,
                    170,
                    0
                ],
                "title": "SeTAR: Out-of-Distribution Detection with Selective Low-Rank\n  Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SeTAR: Out-of-Distribution Detection with Selective Low-Rank\n  Approximation"
                },
                "summary": "Out-of-distribution (OOD) detection is crucial for the safe deployment of\nneural networks. Existing CLIP-based approaches perform OOD detection by\ndevising novel scoring functions or sophisticated fine-tuning methods. In this\nwork, we propose SeTAR, a novel, training-free OOD detection method that\nleverages selective low-rank approximation of weight matrices in\nvision-language and vision-only models. SeTAR enhances OOD detection via\npost-hoc modification of the model's weight matrices using a simple greedy\nsearch algorithm. Based on SeTAR, we further propose SeTAR+FT, a fine-tuning\nextension optimizing model performance for OOD detection tasks. Extensive\nevaluations on ImageNet1K and Pascal-VOC benchmarks show SeTAR's superior\nperformance, reducing the relatively false positive rate by up to 18.95% and\n36.80% compared to zero-shot and fine-tuning baselines. Ablation studies\nfurther validate SeTAR's effectiveness, robustness, and generalizability across\ndifferent model backbones. Our work offers a scalable, efficient solution for\nOOD detection, setting a new state-of-the-art in this area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Out-of-distribution (OOD) detection is crucial for the safe deployment of\nneural networks. Existing CLIP-based approaches perform OOD detection by\ndevising novel scoring functions or sophisticated fine-tuning methods. In this\nwork, we propose SeTAR, a novel, training-free OOD detection method that\nleverages selective low-rank approximation of weight matrices in\nvision-language and vision-only models. SeTAR enhances OOD detection via\npost-hoc modification of the model's weight matrices using a simple greedy\nsearch algorithm. Based on SeTAR, we further propose SeTAR+FT, a fine-tuning\nextension optimizing model performance for OOD detection tasks. Extensive\nevaluations on ImageNet1K and Pascal-VOC benchmarks show SeTAR's superior\nperformance, reducing the relatively false positive rate by up to 18.95% and\n36.80% compared to zero-shot and fine-tuning baselines. Ablation studies\nfurther validate SeTAR's effectiveness, robustness, and generalizability across\ndifferent model backbones. Our work offers a scalable, efficient solution for\nOOD detection, setting a new state-of-the-art in this area."
                },
                "authors": [
                    {
                        "name": "Yixia Li"
                    },
                    {
                        "name": "Boya Xiong"
                    },
                    {
                        "name": "Guanhua Chen"
                    },
                    {
                        "name": "Yun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yun Chen"
                },
                "author": "Yun Chen",
                "arxiv_comment": "Accepted by NeurIPS 2024. Project page is live at\n  https://SeTAR-OOD.github.io. Code are available at\n  https://github.com/X1AOX1A/SeTAR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12629v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12629v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22118v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22118v1",
                "updated": "2024-10-29T15:19:13Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    19,
                    13,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T15:19:13Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    19,
                    13,
                    1,
                    303,
                    0
                ],
                "title": "The Impact of Inference Acceleration Strategies on Bias of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Inference Acceleration Strategies on Bias of LLMs"
                },
                "summary": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to deeply benefit a vast\narray of application domains. However, due to their immense size, performing\ninference with LLMs is both costly and slow. Consequently, a plethora of recent\nwork has proposed strategies to enhance inference efficiency, e.g.,\nquantization, pruning, and caching. These acceleration strategies reduce the\ninference cost and latency, often by several factors, while maintaining much of\nthe predictive performance measured via common benchmarks. In this work, we\nexplore another critical aspect of LLM performance: demographic bias in model\ngenerations due to inference acceleration optimizations. Using a wide range of\nmetrics, we probe bias in model outputs from a number of angles. Analysis of\noutputs before and after inference acceleration shows significant change in\nbias. Worryingly, these bias effects are complex and unpredictable. A\ncombination of an acceleration strategy and bias type may show little bias\nchange in one model but may lead to a large effect in another. Our results\nhighlight a need for in-depth and case-by-case evaluation of model bias after\nit has been modified to accelerate inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to deeply benefit a vast\narray of application domains. However, due to their immense size, performing\ninference with LLMs is both costly and slow. Consequently, a plethora of recent\nwork has proposed strategies to enhance inference efficiency, e.g.,\nquantization, pruning, and caching. These acceleration strategies reduce the\ninference cost and latency, often by several factors, while maintaining much of\nthe predictive performance measured via common benchmarks. In this work, we\nexplore another critical aspect of LLM performance: demographic bias in model\ngenerations due to inference acceleration optimizations. Using a wide range of\nmetrics, we probe bias in model outputs from a number of angles. Analysis of\noutputs before and after inference acceleration shows significant change in\nbias. Worryingly, these bias effects are complex and unpredictable. A\ncombination of an acceleration strategy and bias type may show little bias\nchange in one model but may lead to a large effect in another. Our results\nhighlight a need for in-depth and case-by-case evaluation of model bias after\nit has been modified to accelerate inference."
                },
                "authors": [
                    {
                        "name": "Elisabeth Kirsten"
                    },
                    {
                        "name": "Ivan Habernal"
                    },
                    {
                        "name": "Vedant Nanda"
                    },
                    {
                        "name": "Muhammad Bilal Zafar"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Bilal Zafar"
                },
                "author": "Muhammad Bilal Zafar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22118v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.08925v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.08925v3",
                "updated": "2024-10-30T03:04:21Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    3,
                    4,
                    21,
                    2,
                    304,
                    0
                ],
                "published": "2023-07-18T02:09:14Z",
                "published_parsed": [
                    2023,
                    7,
                    18,
                    2,
                    9,
                    14,
                    1,
                    199,
                    0
                ],
                "title": "Integration of Large Language Models and Federated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integration of Large Language Models and Federated Learning"
                },
                "summary": "As the parameter size of Large Language Models (LLMs) continues to expand,\nthere is an urgent need to address the scarcity of high-quality data. In\nresponse, existing research has attempted to make a breakthrough by\nincorporating Federated Learning (FL) into LLMs. Conversely, considering the\noutstanding performance of LLMs in task generalization, researchers have also\ntried applying LLMs within FL to tackle challenges in relevant domains. The\ncomplementarity between LLMs and FL has already ignited widespread research\ninterest. In this paper, we aim to deeply explore the integration of LLMs and\nFL. We propose a research framework, dividing the fusion of LLMs and FL into\nthree parts: the combination of LLM sub-technologies with FL, the integration\nof FL sub-technologies with LLMs, and the overall merger of LLMs and FL. We\nfirst provide a comprehensive review of the current state of research in the\ndomain of LLMs combined with FL, including their typical applications,\nintegration advantages, challenges faced, and future directions for resolution.\nSubsequently, we discuss the practical applications of the combination of LLMs\nand FL in critical scenarios such as healthcare, finance, and education, and\nprovide new perspectives and insights into future research directions for LLMs\nand FL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the parameter size of Large Language Models (LLMs) continues to expand,\nthere is an urgent need to address the scarcity of high-quality data. In\nresponse, existing research has attempted to make a breakthrough by\nincorporating Federated Learning (FL) into LLMs. Conversely, considering the\noutstanding performance of LLMs in task generalization, researchers have also\ntried applying LLMs within FL to tackle challenges in relevant domains. The\ncomplementarity between LLMs and FL has already ignited widespread research\ninterest. In this paper, we aim to deeply explore the integration of LLMs and\nFL. We propose a research framework, dividing the fusion of LLMs and FL into\nthree parts: the combination of LLM sub-technologies with FL, the integration\nof FL sub-technologies with LLMs, and the overall merger of LLMs and FL. We\nfirst provide a comprehensive review of the current state of research in the\ndomain of LLMs combined with FL, including their typical applications,\nintegration advantages, challenges faced, and future directions for resolution.\nSubsequently, we discuss the practical applications of the combination of LLMs\nand FL in critical scenarios such as healthcare, finance, and education, and\nprovide new perspectives and insights into future research directions for LLMs\nand FL."
                },
                "authors": [
                    {
                        "name": "Chaochao Chen"
                    },
                    {
                        "name": "Xiaohua Feng"
                    },
                    {
                        "name": "Yuyuan Li"
                    },
                    {
                        "name": "Lingjuan Lyu"
                    },
                    {
                        "name": "Jun Zhou"
                    },
                    {
                        "name": "Xiaolin Zheng"
                    },
                    {
                        "name": "Jianwei Yin"
                    }
                ],
                "author_detail": {
                    "name": "Jianwei Yin"
                },
                "author": "Jianwei Yin",
                "arxiv_comment": "Accepted by Cell Patterns",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2307.08925v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.08925v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.01750v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.01750v2",
                "updated": "2024-10-29T15:10:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    10,
                    0,
                    1,
                    303,
                    0
                ],
                "published": "2024-05-02T21:35:45Z",
                "published_parsed": [
                    2024,
                    5,
                    2,
                    21,
                    35,
                    45,
                    3,
                    123,
                    0
                ],
                "title": "PointCompress3D: A Point Cloud Compression Framework for Roadside LiDARs\n  in Intelligent Transportation Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PointCompress3D: A Point Cloud Compression Framework for Roadside LiDARs\n  in Intelligent Transportation Systems"
                },
                "summary": "In the context of Intelligent Transportation Systems (ITS), efficient data\ncompression is crucial for managing large-scale point cloud data acquired by\nroadside LiDAR sensors. The demand for efficient storage, streaming, and\nreal-time object detection capabilities for point cloud data is substantial.\nThis work introduces PointCompress3D, a novel point cloud compression framework\ntailored specifically for roadside LiDARs. Our framework addresses the\nchallenges of compressing high-resolution point clouds while maintaining\naccuracy and compatibility with roadside LiDAR sensors. We adapt, extend,\nintegrate, and evaluate three cutting-edge compression methods using our\nreal-world-based TUMTraf dataset family. We achieve a frame rate of 10 FPS\nwhile keeping compression sizes below 105 Kb, a reduction of 50 times, and\nmaintaining object detection performance on par with the original data. In\nextensive experiments and ablation studies, we finally achieved a PSNR d2 of\n94.46 and a BPP of 6.54 on our dataset. Future work includes the deployment on\nthe live system. The code is available on our project website:\nhttps://pointcompress3d.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the context of Intelligent Transportation Systems (ITS), efficient data\ncompression is crucial for managing large-scale point cloud data acquired by\nroadside LiDAR sensors. The demand for efficient storage, streaming, and\nreal-time object detection capabilities for point cloud data is substantial.\nThis work introduces PointCompress3D, a novel point cloud compression framework\ntailored specifically for roadside LiDARs. Our framework addresses the\nchallenges of compressing high-resolution point clouds while maintaining\naccuracy and compatibility with roadside LiDAR sensors. We adapt, extend,\nintegrate, and evaluate three cutting-edge compression methods using our\nreal-world-based TUMTraf dataset family. We achieve a frame rate of 10 FPS\nwhile keeping compression sizes below 105 Kb, a reduction of 50 times, and\nmaintaining object detection performance on par with the original data. In\nextensive experiments and ablation studies, we finally achieved a PSNR d2 of\n94.46 and a BPP of 6.54 on our dataset. Future work includes the deployment on\nthe live system. The code is available on our project website:\nhttps://pointcompress3d.github.io."
                },
                "authors": [
                    {
                        "name": "Walter Zimmer"
                    },
                    {
                        "name": "Ramandika Pranamulia"
                    },
                    {
                        "name": "Xingcheng Zhou"
                    },
                    {
                        "name": "Mingyu Liu"
                    },
                    {
                        "name": "Alois C. Knoll"
                    }
                ],
                "author_detail": {
                    "name": "Alois C. Knoll"
                },
                "author": "Alois C. Knoll",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.01750v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.01750v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22108v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22108v1",
                "updated": "2024-10-29T15:07:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    7,
                    23,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T15:07:23Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    7,
                    23,
                    1,
                    303,
                    0
                ],
                "title": "Protecting Privacy in Multimodal Large Language Models with MLLMU-Bench",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Protecting Privacy in Multimodal Large Language Models with MLLMU-Bench"
                },
                "summary": "Generative models such as Large Language Models (LLM) and Multimodal Large\nLanguage models (MLLMs) trained on massive web corpora can memorize and\ndisclose individuals' confidential and private data, raising legal and ethical\nconcerns. While many previous works have addressed this issue in LLM via\nmachine unlearning, it remains largely unexplored for MLLMs. To tackle this\nchallenge, we introduce Multimodal Large Language Model Unlearning Benchmark\n(MLLMU-Bench), a novel benchmark aimed at advancing the understanding of\nmultimodal machine unlearning. MLLMU-Bench consists of 500 fictitious profiles\nand 153 profiles for public celebrities, each profile feature over 14\ncustomized question-answer pairs, evaluated from both multimodal (image+text)\nand unimodal (text) perspectives. The benchmark is divided into four sets to\nassess unlearning algorithms in terms of efficacy, generalizability, and model\nutility. Finally, we provide baseline results using existing generative model\nunlearning algorithms. Surprisingly, our experiments show that unimodal\nunlearning algorithms excel in generation and cloze tasks, while multimodal\nunlearning approaches perform better in classification tasks with multimodal\ninputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative models such as Large Language Models (LLM) and Multimodal Large\nLanguage models (MLLMs) trained on massive web corpora can memorize and\ndisclose individuals' confidential and private data, raising legal and ethical\nconcerns. While many previous works have addressed this issue in LLM via\nmachine unlearning, it remains largely unexplored for MLLMs. To tackle this\nchallenge, we introduce Multimodal Large Language Model Unlearning Benchmark\n(MLLMU-Bench), a novel benchmark aimed at advancing the understanding of\nmultimodal machine unlearning. MLLMU-Bench consists of 500 fictitious profiles\nand 153 profiles for public celebrities, each profile feature over 14\ncustomized question-answer pairs, evaluated from both multimodal (image+text)\nand unimodal (text) perspectives. The benchmark is divided into four sets to\nassess unlearning algorithms in terms of efficacy, generalizability, and model\nutility. Finally, we provide baseline results using existing generative model\nunlearning algorithms. Surprisingly, our experiments show that unimodal\nunlearning algorithms excel in generation and cloze tasks, while multimodal\nunlearning approaches perform better in classification tasks with multimodal\ninputs."
                },
                "authors": [
                    {
                        "name": "Zheyuan Liu"
                    },
                    {
                        "name": "Guangyao Dou"
                    },
                    {
                        "name": "Mengzhao Jia"
                    },
                    {
                        "name": "Zhaoxuan Tan"
                    },
                    {
                        "name": "Qingkai Zeng"
                    },
                    {
                        "name": "Yongle Yuan"
                    },
                    {
                        "name": "Meng Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Meng Jiang"
                },
                "author": "Meng Jiang",
                "arxiv_comment": "30 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22108v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22108v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22086v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22086v1",
                "updated": "2024-10-29T14:41:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    14,
                    41,
                    44,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T14:41:44Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    14,
                    41,
                    44,
                    1,
                    303,
                    0
                ],
                "title": "Unlearning as multi-task optimization: A normalized gradient difference\n  approach with an adaptive learning rate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlearning as multi-task optimization: A normalized gradient difference\n  approach with an adaptive learning rate"
                },
                "summary": "Machine unlearning has been used to remove unwanted knowledge acquired by\nlarge language models (LLMs). In this paper, we examine machine unlearning from\nan optimization perspective, framing it as a regularized multi-task\noptimization problem, where one task optimizes a forgetting objective and\nanother optimizes the model performance. In particular, we introduce a\nnormalized gradient difference (NGDiff) algorithm, enabling us to have better\ncontrol over the trade-off between the objectives, while integrating a new,\nautomatic learning rate scheduler. We provide a theoretical analysis and\nempirically demonstrate the superior performance of NGDiff among\nstate-of-the-art unlearning methods on the TOFU and MUSE datasets while\nexhibiting stable training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine unlearning has been used to remove unwanted knowledge acquired by\nlarge language models (LLMs). In this paper, we examine machine unlearning from\nan optimization perspective, framing it as a regularized multi-task\noptimization problem, where one task optimizes a forgetting objective and\nanother optimizes the model performance. In particular, we introduce a\nnormalized gradient difference (NGDiff) algorithm, enabling us to have better\ncontrol over the trade-off between the objectives, while integrating a new,\nautomatic learning rate scheduler. We provide a theoretical analysis and\nempirically demonstrate the superior performance of NGDiff among\nstate-of-the-art unlearning methods on the TOFU and MUSE datasets while\nexhibiting stable training."
                },
                "authors": [
                    {
                        "name": "Zhiqi Bu"
                    },
                    {
                        "name": "Xiaomeng Jin"
                    },
                    {
                        "name": "Bhanukiran Vinzamuri"
                    },
                    {
                        "name": "Anil Ramakrishna"
                    },
                    {
                        "name": "Kai-Wei Chang"
                    },
                    {
                        "name": "Volkan Cevher"
                    },
                    {
                        "name": "Mingyi Hong"
                    }
                ],
                "author_detail": {
                    "name": "Mingyi Hong"
                },
                "author": "Mingyi Hong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22086v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22086v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03523v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03523v2",
                "updated": "2024-10-29T14:39:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    14,
                    39,
                    9,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-04T15:44:23Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    15,
                    44,
                    23,
                    4,
                    278,
                    0
                ],
                "title": "A Probabilistic Perspective on Unlearning and Alignment for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Probabilistic Perspective on Unlearning and Alignment for Large\n  Language Models"
                },
                "summary": "Comprehensive evaluation of Large Language Models (LLMs) is an open research\nproblem. Existing evaluations rely on deterministic point estimates generated\nvia greedy decoding. However, we find that deterministic evaluations fail to\ncapture the whole output distribution of a model, yielding inaccurate\nestimations of model capabilities. This is particularly problematic in critical\ncontexts such as unlearning and alignment, where precise model evaluations are\ncrucial. To remedy this, we introduce the first formal probabilistic evaluation\nframework in LLMs. Namely, we derive novel metrics with high-probability\nguarantees concerning the output distribution of a model. Our metrics are\napplication-independent and allow practitioners to make more reliable estimates\nabout model capabilities before deployment. Through a case study focused on\nunlearning, we reveal that deterministic evaluations falsely indicate\nsuccessful unlearning, whereas our probabilistic evaluations demonstrate that\nmost if not all of the supposedly unlearned information remains accessible in\nthese models. Additionally, we propose a novel unlearning loss based on entropy\noptimization and adaptive temperature scaling, which significantly improves\nunlearning in probabilistic settings on recent benchmarks. Our proposed shift\nfrom point estimates to probabilistic evaluations of output distributions\nrepresents an important step toward comprehensive evaluations of LLMs. Code\navailable at https://github.com/yascho/probabilistic-unlearning",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comprehensive evaluation of Large Language Models (LLMs) is an open research\nproblem. Existing evaluations rely on deterministic point estimates generated\nvia greedy decoding. However, we find that deterministic evaluations fail to\ncapture the whole output distribution of a model, yielding inaccurate\nestimations of model capabilities. This is particularly problematic in critical\ncontexts such as unlearning and alignment, where precise model evaluations are\ncrucial. To remedy this, we introduce the first formal probabilistic evaluation\nframework in LLMs. Namely, we derive novel metrics with high-probability\nguarantees concerning the output distribution of a model. Our metrics are\napplication-independent and allow practitioners to make more reliable estimates\nabout model capabilities before deployment. Through a case study focused on\nunlearning, we reveal that deterministic evaluations falsely indicate\nsuccessful unlearning, whereas our probabilistic evaluations demonstrate that\nmost if not all of the supposedly unlearned information remains accessible in\nthese models. Additionally, we propose a novel unlearning loss based on entropy\noptimization and adaptive temperature scaling, which significantly improves\nunlearning in probabilistic settings on recent benchmarks. Our proposed shift\nfrom point estimates to probabilistic evaluations of output distributions\nrepresents an important step toward comprehensive evaluations of LLMs. Code\navailable at https://github.com/yascho/probabilistic-unlearning"
                },
                "authors": [
                    {
                        "name": "Yan Scholten"
                    },
                    {
                        "name": "Stephan Gnnemann"
                    },
                    {
                        "name": "Leo Schwinn"
                    }
                ],
                "author_detail": {
                    "name": "Leo Schwinn"
                },
                "author": "Leo Schwinn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03523v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03523v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22071v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22071v1",
                "updated": "2024-10-29T14:31:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    14,
                    31,
                    33,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T14:31:33Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    14,
                    31,
                    33,
                    1,
                    303,
                    0
                ],
                "title": "Distinguishing Ignorance from Error in LLM Hallucinations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distinguishing Ignorance from Error in LLM Hallucinations"
                },
                "summary": "Large language models (LLMs) are susceptible to hallucinations-outputs that\nare ungrounded, factually incorrect, or inconsistent with prior generations. We\nfocus on close-book Question Answering (CBQA), where previous work has not\nfully addressed the distinction between two possible kinds of hallucinations,\nnamely, whether the model (1) does not hold the correct answer in its\nparameters or (2) answers incorrectly despite having the required knowledge. We\nargue that distinguishing these cases is crucial for detecting and mitigating\nhallucinations. Specifically, case (2) may be mitigated by intervening in the\nmodel's internal computation, as the knowledge resides within the model's\nparameters. In contrast, in case (1) there is no parametric knowledge to\nleverage for mitigation, so it should be addressed by resorting to an external\nknowledge source or abstaining. To help distinguish between the two cases, we\nintroduce Wrong Answer despite having Correct Knowledge (WACK), an approach for\nconstructing model-specific datasets for the second hallucination type. Our\nprobing experiments indicate that the two kinds of hallucinations are\nrepresented differently in the model's inner states. Next, we show that\ndatasets constructed using WACK exhibit variations across models, demonstrating\nthat even when models share knowledge of certain facts, they still vary in the\nspecific examples that lead to hallucinations. Finally, we show that training a\nprobe on our WACK datasets leads to better hallucination detection of case (2)\nhallucinations than using the common generic one-size-fits-all datasets. The\ncode is available at\nhttps://github.com/technion-cs-nlp/hallucination-mitigation .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are susceptible to hallucinations-outputs that\nare ungrounded, factually incorrect, or inconsistent with prior generations. We\nfocus on close-book Question Answering (CBQA), where previous work has not\nfully addressed the distinction between two possible kinds of hallucinations,\nnamely, whether the model (1) does not hold the correct answer in its\nparameters or (2) answers incorrectly despite having the required knowledge. We\nargue that distinguishing these cases is crucial for detecting and mitigating\nhallucinations. Specifically, case (2) may be mitigated by intervening in the\nmodel's internal computation, as the knowledge resides within the model's\nparameters. In contrast, in case (1) there is no parametric knowledge to\nleverage for mitigation, so it should be addressed by resorting to an external\nknowledge source or abstaining. To help distinguish between the two cases, we\nintroduce Wrong Answer despite having Correct Knowledge (WACK), an approach for\nconstructing model-specific datasets for the second hallucination type. Our\nprobing experiments indicate that the two kinds of hallucinations are\nrepresented differently in the model's inner states. Next, we show that\ndatasets constructed using WACK exhibit variations across models, demonstrating\nthat even when models share knowledge of certain facts, they still vary in the\nspecific examples that lead to hallucinations. Finally, we show that training a\nprobe on our WACK datasets leads to better hallucination detection of case (2)\nhallucinations than using the common generic one-size-fits-all datasets. The\ncode is available at\nhttps://github.com/technion-cs-nlp/hallucination-mitigation ."
                },
                "authors": [
                    {
                        "name": "Adi Simhi"
                    },
                    {
                        "name": "Jonathan Herzig"
                    },
                    {
                        "name": "Idan Szpektor"
                    },
                    {
                        "name": "Yonatan Belinkov"
                    }
                ],
                "author_detail": {
                    "name": "Yonatan Belinkov"
                },
                "author": "Yonatan Belinkov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22071v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22071v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15938v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15938v3",
                "updated": "2024-10-29T14:28:24Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    14,
                    28,
                    24,
                    1,
                    303,
                    0
                ],
                "published": "2024-06-22T20:57:12Z",
                "published_parsed": [
                    2024,
                    6,
                    22,
                    20,
                    57,
                    12,
                    5,
                    174,
                    0
                ],
                "title": "RuleR: Improving LLM Controllability by Rule-based Data Recycling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RuleR: Improving LLM Controllability by Rule-based Data Recycling"
                },
                "summary": "Despite the remarkable advancement of Large language models (LLMs), they\nstill lack delicate controllability under sophisticated constraints, which is\ncritical to enhancing their response quality and the user experience. While\nconditional supervised fine-tuning (SFT) can potentially improve LLM\ncontrollability, curating new SFT data to fulfill the constraints usually\nrelies on human experts or proprietary LLMs, which is time-consuming and\nexpensive. To bridge this gap, we propose Rule-based Data Recycling (RuleR), a\nhuman/LLM-free data augmentation method incorporating multiple constraints into\nthe original SFT data. Instead of creating new responses from scratch, RuleR\nintegrates linguistic or formatting rules into the original instructions and\nmodifies the responses to fulfill the rule-defined constraints. Training on the\n\"recycled\" data consolidates LLMs capability to generate constrained outputs.\nExtensive experiments demonstrate RuleR's effectiveness in improving LLM\ncontrollability while maintaining general instruction-following performance.\nRuleR's code is released on https://github.com/tianyi-lab/RuleR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable advancement of Large language models (LLMs), they\nstill lack delicate controllability under sophisticated constraints, which is\ncritical to enhancing their response quality and the user experience. While\nconditional supervised fine-tuning (SFT) can potentially improve LLM\ncontrollability, curating new SFT data to fulfill the constraints usually\nrelies on human experts or proprietary LLMs, which is time-consuming and\nexpensive. To bridge this gap, we propose Rule-based Data Recycling (RuleR), a\nhuman/LLM-free data augmentation method incorporating multiple constraints into\nthe original SFT data. Instead of creating new responses from scratch, RuleR\nintegrates linguistic or formatting rules into the original instructions and\nmodifies the responses to fulfill the rule-defined constraints. Training on the\n\"recycled\" data consolidates LLMs capability to generate constrained outputs.\nExtensive experiments demonstrate RuleR's effectiveness in improving LLM\ncontrollability while maintaining general instruction-following performance.\nRuleR's code is released on https://github.com/tianyi-lab/RuleR."
                },
                "authors": [
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Han Chen"
                    },
                    {
                        "name": "Chenguang Wang"
                    },
                    {
                        "name": "Dang Nguyen"
                    },
                    {
                        "name": "Dianqi Li"
                    },
                    {
                        "name": "Tianyi Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Tianyi Zhou"
                },
                "author": "Tianyi Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15938v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15938v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02680v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02680v4",
                "updated": "2024-10-29T14:08:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    14,
                    8,
                    34,
                    1,
                    303,
                    0
                ],
                "published": "2024-07-02T21:44:22Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    21,
                    44,
                    22,
                    1,
                    184,
                    0
                ],
                "title": "KGym: A Platform and Dataset to Benchmark Large Language Models on Linux\n  Kernel Crash Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KGym: A Platform and Dataset to Benchmark Large Language Models on Linux\n  Kernel Crash Resolution"
                },
                "summary": "Large Language Models (LLMs) are consistently improving at increasingly\nrealistic software engineering (SE) tasks. In real-world software stacks,\nsignificant SE effort is spent developing foundational system software like the\nLinux kernel. Unlike application-level software, a systems codebase like Linux\nis multilingual (low-level C/Assembly/Bash/Rust); gigantic (>20 million lines);\ncritical (impacting billions of devices worldwide), and highly concurrent\n(involving complex multi-threading). To evaluate if ML models are useful while\ndeveloping such large-scale systems-level software, we introduce kGym (a\nplatform) and kBench (a dataset). The kGym platform provides a SE environment\nfor large-scale experiments on the Linux kernel, including compiling and\nrunning kernels in parallel across several virtual machines, detecting\noperations and crashes, inspecting logs, and querying and patching the code\nbase. We use kGym to facilitate evaluation on kBench, a crash resolution\nbenchmark drawn from real-world Linux kernel bugs. An example bug in kBench\ncontains crashing stack traces, a bug-reproducer file, a developer-written fix,\nand other associated data. To understand current performance, we conduct\nbaseline experiments by prompting LLMs to resolve Linux kernel crashes. Our\ninitial evaluations reveal that the best performing LLM achieves 0.72% and\n5.38% in the unassisted and assisted (i.e., buggy files disclosed to the model)\nsettings, respectively. These results highlight the need for further research\nto enhance model performance in SE tasks. Improving performance on kBench\nrequires models to master new learning skills, including understanding the\ncause of crashes and repairing faults, writing memory-safe and hardware-aware\ncode, and understanding concurrency. As a result, this work opens up multiple\navenues of research at the intersection of machine learning and systems\nsoftware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are consistently improving at increasingly\nrealistic software engineering (SE) tasks. In real-world software stacks,\nsignificant SE effort is spent developing foundational system software like the\nLinux kernel. Unlike application-level software, a systems codebase like Linux\nis multilingual (low-level C/Assembly/Bash/Rust); gigantic (>20 million lines);\ncritical (impacting billions of devices worldwide), and highly concurrent\n(involving complex multi-threading). To evaluate if ML models are useful while\ndeveloping such large-scale systems-level software, we introduce kGym (a\nplatform) and kBench (a dataset). The kGym platform provides a SE environment\nfor large-scale experiments on the Linux kernel, including compiling and\nrunning kernels in parallel across several virtual machines, detecting\noperations and crashes, inspecting logs, and querying and patching the code\nbase. We use kGym to facilitate evaluation on kBench, a crash resolution\nbenchmark drawn from real-world Linux kernel bugs. An example bug in kBench\ncontains crashing stack traces, a bug-reproducer file, a developer-written fix,\nand other associated data. To understand current performance, we conduct\nbaseline experiments by prompting LLMs to resolve Linux kernel crashes. Our\ninitial evaluations reveal that the best performing LLM achieves 0.72% and\n5.38% in the unassisted and assisted (i.e., buggy files disclosed to the model)\nsettings, respectively. These results highlight the need for further research\nto enhance model performance in SE tasks. Improving performance on kBench\nrequires models to master new learning skills, including understanding the\ncause of crashes and repairing faults, writing memory-safe and hardware-aware\ncode, and understanding concurrency. As a result, this work opens up multiple\navenues of research at the intersection of machine learning and systems\nsoftware."
                },
                "authors": [
                    {
                        "name": "Alex Mathai"
                    },
                    {
                        "name": "Chenxi Huang"
                    },
                    {
                        "name": "Petros Maniatis"
                    },
                    {
                        "name": "Aleksandr Nogikh"
                    },
                    {
                        "name": "Franjo Ivancic"
                    },
                    {
                        "name": "Junfeng Yang"
                    },
                    {
                        "name": "Baishakhi Ray"
                    }
                ],
                "author_detail": {
                    "name": "Baishakhi Ray"
                },
                "author": "Baishakhi Ray",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02680v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02680v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16607v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16607v3",
                "updated": "2024-10-29T13:59:32Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    13,
                    59,
                    32,
                    1,
                    303,
                    0
                ],
                "published": "2024-04-25T13:47:37Z",
                "published_parsed": [
                    2024,
                    4,
                    25,
                    13,
                    47,
                    37,
                    3,
                    116,
                    0
                ],
                "title": "A Comprehensive Design Framework for UE-side and BS-Side RIS Deployments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Design Framework for UE-side and BS-Side RIS Deployments"
                },
                "summary": "Integrating reconfigurable intelligent surfaces (RISs) in emerging\ncommunication systems is a fast-growing research field that has recently earned\nmuch attention. While implementing RISs near the base station (BS), i.e.,\nBS-side RIS, or user equipment (UE), i.e., UE-side RIS, exhibits optimum\nperformance, understanding the differences between these two deployments in\nterms of the system design perspective needs to be clarified. Critical design\nparameters, such as RIS size, phase shift adjustment, control link, and element\ntype (passive/active), require greater clarity across these scenarios.\nOverlooking the intricacies of such critical design parameters in light of 6G\ndemands endangers practical implementation, widening the gap between\ntheoretical insights and practical applications. In this regard, our study\ninvestigates the impact of each RIS deployment strategy on the anticipated 6G\nrequirements and offers tailored RIS design recommendations to fulfill these\nforward-looking requirements. Through this, we clarify the practical\ndistinctions and propose a comprehensive framework for differentiating between\nBS-side and UE-side RIS scenarios in terms of their design parameters.\nHighlighting the unique needs of each and the potential challenges ahead, we\naim to fuse the theoretical underpinnings of RIS with tangible implementation\nconsiderations, propelling progress in both the academic sphere and the\nindustry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating reconfigurable intelligent surfaces (RISs) in emerging\ncommunication systems is a fast-growing research field that has recently earned\nmuch attention. While implementing RISs near the base station (BS), i.e.,\nBS-side RIS, or user equipment (UE), i.e., UE-side RIS, exhibits optimum\nperformance, understanding the differences between these two deployments in\nterms of the system design perspective needs to be clarified. Critical design\nparameters, such as RIS size, phase shift adjustment, control link, and element\ntype (passive/active), require greater clarity across these scenarios.\nOverlooking the intricacies of such critical design parameters in light of 6G\ndemands endangers practical implementation, widening the gap between\ntheoretical insights and practical applications. In this regard, our study\ninvestigates the impact of each RIS deployment strategy on the anticipated 6G\nrequirements and offers tailored RIS design recommendations to fulfill these\nforward-looking requirements. Through this, we clarify the practical\ndistinctions and propose a comprehensive framework for differentiating between\nBS-side and UE-side RIS scenarios in terms of their design parameters.\nHighlighting the unique needs of each and the potential challenges ahead, we\naim to fuse the theoretical underpinnings of RIS with tangible implementation\nconsiderations, propelling progress in both the academic sphere and the\nindustry."
                },
                "authors": [
                    {
                        "name": "Mahmoud Raeisi"
                    },
                    {
                        "name": "Aymen Khaleel"
                    },
                    {
                        "name": "Mehmet Cagri Ilter"
                    },
                    {
                        "name": "Majid Gerami"
                    },
                    {
                        "name": "Ertugrul Basar"
                    }
                ],
                "author_detail": {
                    "name": "Ertugrul Basar"
                },
                "author": "Ertugrul Basar",
                "arxiv_comment": "Submitted in IEEE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16607v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16607v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22041v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22041v1",
                "updated": "2024-10-29T13:46:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    13,
                    46,
                    52,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T13:46:52Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    13,
                    46,
                    52,
                    1,
                    303,
                    0
                ],
                "title": "An LLM-based Simulation Framework for Embodied Conversational Agents in\n  Psychological Counseling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An LLM-based Simulation Framework for Embodied Conversational Agents in\n  Psychological Counseling"
                },
                "summary": "Simulation is crucial for validating algorithmic strategies in real-world\nscenarios. While LLM-based social simulation shows promise as a mainstream\ntool, simulating complex scenarios like psychological counseling remains\nchallenging. We present ECAs (short for Embodied Conversational Agents), a\nframework for simulating psychological counseling clients' embodied memory,\nintegrating embodied cognition and counseling theories. We formulate six design\ngoals based on a comprehensive review of psychological counseling theories.\nUsing LLMs, we expand real counseling case data into a nuanced embodied\ncognitive memory space and generate dialogues based on high-frequency\ncounseling questions. We validate our framework using the D4 dataset, with\nevaluations by licensed counselors. Results show our approach significantly\noutperforms baselines in simulation authenticity and necessity. To demonstrate\nscalability, we created a public ECAs dataset through batch simulations. This\nresearch provides valuable insights for future social simulation studies in\npsychological counseling and Embodied Counseling Agents research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulation is crucial for validating algorithmic strategies in real-world\nscenarios. While LLM-based social simulation shows promise as a mainstream\ntool, simulating complex scenarios like psychological counseling remains\nchallenging. We present ECAs (short for Embodied Conversational Agents), a\nframework for simulating psychological counseling clients' embodied memory,\nintegrating embodied cognition and counseling theories. We formulate six design\ngoals based on a comprehensive review of psychological counseling theories.\nUsing LLMs, we expand real counseling case data into a nuanced embodied\ncognitive memory space and generate dialogues based on high-frequency\ncounseling questions. We validate our framework using the D4 dataset, with\nevaluations by licensed counselors. Results show our approach significantly\noutperforms baselines in simulation authenticity and necessity. To demonstrate\nscalability, we created a public ECAs dataset through batch simulations. This\nresearch provides valuable insights for future social simulation studies in\npsychological counseling and Embodied Counseling Agents research."
                },
                "authors": [
                    {
                        "name": "Lixiu Wu"
                    },
                    {
                        "name": "Yuanrong Tang"
                    },
                    {
                        "name": "Qisen Pan"
                    },
                    {
                        "name": "Xianyang Zhan"
                    },
                    {
                        "name": "Yucheng Han"
                    },
                    {
                        "name": "Mingyang You"
                    },
                    {
                        "name": "Lanxi Xiao"
                    },
                    {
                        "name": "Tianhong Wang"
                    },
                    {
                        "name": "Chen Zhong"
                    },
                    {
                        "name": "Jiangtao Gong"
                    }
                ],
                "author_detail": {
                    "name": "Jiangtao Gong"
                },
                "author": "Jiangtao Gong",
                "arxiv_comment": "7 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22041v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22041v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.09066v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.09066v3",
                "updated": "2024-10-29T13:43:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    13,
                    43,
                    58,
                    1,
                    303,
                    0
                ],
                "published": "2024-04-13T19:30:58Z",
                "published_parsed": [
                    2024,
                    4,
                    13,
                    19,
                    30,
                    58,
                    5,
                    104,
                    0
                ],
                "title": "CodeCloak: A Method for Evaluating and Mitigating Code Leakage by LLM\n  Code Assistants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeCloak: A Method for Evaluating and Mitigating Code Leakage by LLM\n  Code Assistants"
                },
                "summary": "LLM-based code assistants are becoming increasingly popular among developers.\nThese tools help developers improve their coding efficiency and reduce errors\nby providing real-time suggestions based on the developer's codebase. While\nbeneficial, the use of these tools can inadvertently expose the developer's\nproprietary code to the code assistant service provider during the development\nprocess. In this work, we propose a method to mitigate the risk of code leakage\nwhen using LLM-based code assistants. CodeCloak is a novel deep reinforcement\nlearning agent that manipulates the prompts before sending them to the code\nassistant service. CodeCloak aims to achieve the following two contradictory\ngoals: (i) minimizing code leakage, while (ii) preserving relevant and useful\nsuggestions for the developer. Our evaluation, employing StarCoder and Code\nLlama, LLM-based code assistants models, demonstrates CodeCloak's effectiveness\non a diverse set of code repositories of varying sizes, as well as its\ntransferability across different models. We also designed a method for\nreconstructing the developer's original codebase from code segments sent to the\ncode assistant service (i.e., prompts) during the development process, to\nthoroughly analyze code leakage risks and evaluate the effectiveness of\nCodeCloak under practical development scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based code assistants are becoming increasingly popular among developers.\nThese tools help developers improve their coding efficiency and reduce errors\nby providing real-time suggestions based on the developer's codebase. While\nbeneficial, the use of these tools can inadvertently expose the developer's\nproprietary code to the code assistant service provider during the development\nprocess. In this work, we propose a method to mitigate the risk of code leakage\nwhen using LLM-based code assistants. CodeCloak is a novel deep reinforcement\nlearning agent that manipulates the prompts before sending them to the code\nassistant service. CodeCloak aims to achieve the following two contradictory\ngoals: (i) minimizing code leakage, while (ii) preserving relevant and useful\nsuggestions for the developer. Our evaluation, employing StarCoder and Code\nLlama, LLM-based code assistants models, demonstrates CodeCloak's effectiveness\non a diverse set of code repositories of varying sizes, as well as its\ntransferability across different models. We also designed a method for\nreconstructing the developer's original codebase from code segments sent to the\ncode assistant service (i.e., prompts) during the development process, to\nthoroughly analyze code leakage risks and evaluate the effectiveness of\nCodeCloak under practical development scenarios."
                },
                "authors": [
                    {
                        "name": "Amit Finkman Noah"
                    },
                    {
                        "name": "Avishag Shapira"
                    },
                    {
                        "name": "Eden Bar Kochva"
                    },
                    {
                        "name": "Inbar Maimon"
                    },
                    {
                        "name": "Dudu Mimran"
                    },
                    {
                        "name": "Yuval Elovici"
                    },
                    {
                        "name": "Asaf Shabtai"
                    }
                ],
                "author_detail": {
                    "name": "Asaf Shabtai"
                },
                "author": "Asaf Shabtai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.09066v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.09066v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09713v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09713v2",
                "updated": "2024-10-29T13:19:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    13,
                    19,
                    12,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-13T03:45:24Z",
                "published_parsed": [
                    2024,
                    10,
                    13,
                    3,
                    45,
                    24,
                    6,
                    287,
                    0
                ],
                "title": "Agentic Information Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic Information Retrieval"
                },
                "summary": "What will information entry look like in the next generation of digital\nproducts? Since the 1970s, user access to relevant information has relied on\ndomain-specific architectures of information retrieval (IR). Over the past two\ndecades, the advent of modern IR systems, including web search engines and\npersonalized recommender systems, has greatly improved the efficiency of\nretrieving relevant information from vast data corpora. However, the core\nparadigm of these IR systems remains largely unchanged, relying on filtering a\npredefined set of candidate items. Since 2022, breakthroughs in large language\nmodels (LLMs) have begun transforming how information is accessed, establishing\na new technical paradigm. In this position paper, we introduce Agentic\nInformation Retrieval (Agentic IR), a novel IR paradigm shaped by the\ncapabilities of LLM agents. Agentic IR expands the scope of accessible tasks\nand leverages a suite of new techniques to redefine information retrieval. We\ndiscuss three types of cutting-edge applications of agentic IR and the\nchallenges faced. We propose that agentic IR holds promise for generating\ninnovative applications, potentially becoming a central information entry point\nin future digital ecosystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What will information entry look like in the next generation of digital\nproducts? Since the 1970s, user access to relevant information has relied on\ndomain-specific architectures of information retrieval (IR). Over the past two\ndecades, the advent of modern IR systems, including web search engines and\npersonalized recommender systems, has greatly improved the efficiency of\nretrieving relevant information from vast data corpora. However, the core\nparadigm of these IR systems remains largely unchanged, relying on filtering a\npredefined set of candidate items. Since 2022, breakthroughs in large language\nmodels (LLMs) have begun transforming how information is accessed, establishing\na new technical paradigm. In this position paper, we introduce Agentic\nInformation Retrieval (Agentic IR), a novel IR paradigm shaped by the\ncapabilities of LLM agents. Agentic IR expands the scope of accessible tasks\nand leverages a suite of new techniques to redefine information retrieval. We\ndiscuss three types of cutting-edge applications of agentic IR and the\nchallenges faced. We propose that agentic IR holds promise for generating\ninnovative applications, potentially becoming a central information entry point\nin future digital ecosystems."
                },
                "authors": [
                    {
                        "name": "Weinan Zhang"
                    },
                    {
                        "name": "Junwei Liao"
                    },
                    {
                        "name": "Ning Li"
                    },
                    {
                        "name": "Kounianhua Du"
                    }
                ],
                "author_detail": {
                    "name": "Kounianhua Du"
                },
                "author": "Kounianhua Du",
                "arxiv_comment": "11 pages, position paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09713v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09713v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.09526v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.09526v2",
                "updated": "2024-10-29T13:04:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    13,
                    4,
                    42,
                    1,
                    303,
                    0
                ],
                "published": "2024-04-15T07:45:04Z",
                "published_parsed": [
                    2024,
                    4,
                    15,
                    7,
                    45,
                    4,
                    0,
                    106,
                    0
                ],
                "title": "LoongServe: Efficiently Serving Long-Context Large Language Models with\n  Elastic Sequence Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoongServe: Efficiently Serving Long-Context Large Language Models with\n  Elastic Sequence Parallelism"
                },
                "summary": "The context window of large language models (LLMs) is rapidly increasing,\nleading to a huge variance in resource usage between different requests as well\nas between different phases of the same request. Restricted by static\nparallelism strategies, existing LLM serving systems cannot efficiently utilize\nthe underlying resources to serve variable-length requests in different phases.\nTo address this problem, we propose a new parallelism paradigm, elastic\nsequence parallelism (ESP), to elastically adapt to the variance between\ndifferent requests and phases. Based on ESP, we design and build LoongServe, an\nLLM serving system that (1) improves computation efficiency by elastically\nadjusting the degree of parallelism in real-time, (2) improves communication\nefficiency by reducing key-value cache migration overhead and overlapping\npartial decoding communication with computation, and (3) improves GPU memory\nefficiency by reducing key-value cache fragmentation across instances. Our\nevaluation under diverse real-world datasets shows that LoongServe improves the\nmaximum throughput by up to 3.85$\\times$ compared to the chunked prefill and\n5.81$\\times$ compared to the prefill-decoding disaggregation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The context window of large language models (LLMs) is rapidly increasing,\nleading to a huge variance in resource usage between different requests as well\nas between different phases of the same request. Restricted by static\nparallelism strategies, existing LLM serving systems cannot efficiently utilize\nthe underlying resources to serve variable-length requests in different phases.\nTo address this problem, we propose a new parallelism paradigm, elastic\nsequence parallelism (ESP), to elastically adapt to the variance between\ndifferent requests and phases. Based on ESP, we design and build LoongServe, an\nLLM serving system that (1) improves computation efficiency by elastically\nadjusting the degree of parallelism in real-time, (2) improves communication\nefficiency by reducing key-value cache migration overhead and overlapping\npartial decoding communication with computation, and (3) improves GPU memory\nefficiency by reducing key-value cache fragmentation across instances. Our\nevaluation under diverse real-world datasets shows that LoongServe improves the\nmaximum throughput by up to 3.85$\\times$ compared to the chunked prefill and\n5.81$\\times$ compared to the prefill-decoding disaggregation."
                },
                "authors": [
                    {
                        "name": "Bingyang Wu"
                    },
                    {
                        "name": "Shengyu Liu"
                    },
                    {
                        "name": "Yinmin Zhong"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Xuanzhe Liu"
                    },
                    {
                        "name": "Xin Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xin Jin"
                },
                "author": "Xin Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.09526v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.09526v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04070v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04070v4",
                "updated": "2024-10-29T12:51:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    12,
                    51,
                    33,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-05T08:00:55Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    8,
                    0,
                    55,
                    5,
                    279,
                    0
                ],
                "title": "PAD: Personalized Alignment at Decoding-Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PAD: Personalized Alignment at Decoding-Time"
                },
                "summary": "Aligning with personalized preferences, which vary significantly across\ncultural, educational, and political differences, poses a significant challenge\ndue to the computational costs and data demands of traditional alignment\nmethods. In response, this paper presents Personalized Alignment at\nDecoding-time (PAD), a novel framework designed to align LLM outputs with\ndiverse personalized preferences during the inference phase, eliminating the\nneed for additional training. By introducing a unique personalized reward\nmodeling strategy, this framework decouples the text generation process from\npersonalized preferences, facilitating the generation of generalizable\ntoken-level personalized rewards. The PAD algorithm leverages these rewards to\nguide the decoding process, dynamically tailoring the base model's predictions\nto personalized preferences. Extensive experimental results demonstrate that\nPAD not only outperforms existing training-based alignment methods in terms of\naligning with diverse preferences but also shows significant generalizability\nto preferences unseen during training and scalability across different base\nmodels. This work advances the capability of LLMs to meet user needs in\nreal-time applications, presenting a substantial step forward in personalized\nLLM alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning with personalized preferences, which vary significantly across\ncultural, educational, and political differences, poses a significant challenge\ndue to the computational costs and data demands of traditional alignment\nmethods. In response, this paper presents Personalized Alignment at\nDecoding-time (PAD), a novel framework designed to align LLM outputs with\ndiverse personalized preferences during the inference phase, eliminating the\nneed for additional training. By introducing a unique personalized reward\nmodeling strategy, this framework decouples the text generation process from\npersonalized preferences, facilitating the generation of generalizable\ntoken-level personalized rewards. The PAD algorithm leverages these rewards to\nguide the decoding process, dynamically tailoring the base model's predictions\nto personalized preferences. Extensive experimental results demonstrate that\nPAD not only outperforms existing training-based alignment methods in terms of\naligning with diverse preferences but also shows significant generalizability\nto preferences unseen during training and scalability across different base\nmodels. This work advances the capability of LLMs to meet user needs in\nreal-time applications, presenting a substantial step forward in personalized\nLLM alignment."
                },
                "authors": [
                    {
                        "name": "Ruizhe Chen"
                    },
                    {
                        "name": "Xiaotian Zhang"
                    },
                    {
                        "name": "Meng Luo"
                    },
                    {
                        "name": "Wenhao Chai"
                    },
                    {
                        "name": "Zuozhu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zuozhu Liu"
                },
                "author": "Zuozhu Liu",
                "arxiv_comment": "This paper presents Personalized Alignment at Decoding-time (PAD), a\n  novel framework designed to align LLM outputs with diverse personalized\n  preferences during the inference phase",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04070v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04070v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05821v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05821v4",
                "updated": "2024-10-29T12:28:58Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    12,
                    28,
                    58,
                    1,
                    303,
                    0
                ],
                "published": "2023-12-10T08:41:24Z",
                "published_parsed": [
                    2023,
                    12,
                    10,
                    8,
                    41,
                    24,
                    6,
                    344,
                    0
                ],
                "title": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models"
                },
                "summary": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank decomposition, and find that the challenges of this task\nstem from the distribution variance in the LLM activations and the sensitivity\ndifference among various kinds of layers. To address these issues, we propose a\ntraining-free approach called Activation-aware Singular Value Decomposition\n(ASVD). Specifically, ASVD manages activation outliers by transforming the\nweight matrix based on the activation distribution. This transformation allows\nthe outliers in the activation matrix to be absorbed into the transformed\nweight matrix, thereby enhancing decomposition accuracy. Additionally, we\npropose an efficient iterative calibration process to optimize layer-specific\ndecomposition by addressing the varying sensitivity of different LLM layers. In\nthis way, ASVD can compress a network by 10%-30%. Based on the success of the\nlow-rank decomposition of projection matrices in the self-attention module, we\nfurther introduce ASVD to compress the KV cache. By reducing the channel\ndimension of KV activations, memory requirements for KV cache can be largely\nreduced. ASVD can further achieve 50% KV cache reductions without performance\ndrop in a training-free manner. Code is anonymously available in supplementary\nmaterials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank decomposition, and find that the challenges of this task\nstem from the distribution variance in the LLM activations and the sensitivity\ndifference among various kinds of layers. To address these issues, we propose a\ntraining-free approach called Activation-aware Singular Value Decomposition\n(ASVD). Specifically, ASVD manages activation outliers by transforming the\nweight matrix based on the activation distribution. This transformation allows\nthe outliers in the activation matrix to be absorbed into the transformed\nweight matrix, thereby enhancing decomposition accuracy. Additionally, we\npropose an efficient iterative calibration process to optimize layer-specific\ndecomposition by addressing the varying sensitivity of different LLM layers. In\nthis way, ASVD can compress a network by 10%-30%. Based on the success of the\nlow-rank decomposition of projection matrices in the self-attention module, we\nfurther introduce ASVD to compress the KV cache. By reducing the channel\ndimension of KV activations, memory requirements for KV cache can be largely\nreduced. ASVD can further achieve 50% KV cache reductions without performance\ndrop in a training-free manner. Code is anonymously available in supplementary\nmaterials."
                },
                "authors": [
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Yuzhang Shang"
                    },
                    {
                        "name": "Yue Song"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Yan Yan"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.05821v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05821v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10329v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10329v3",
                "updated": "2024-10-29T12:10:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    12,
                    10,
                    41,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-14T09:40:52Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    9,
                    40,
                    52,
                    0,
                    288,
                    0
                ],
                "title": "GraphCLIP: Enhancing Transferability in Graph Foundation Models for\n  Text-Attributed Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphCLIP: Enhancing Transferability in Graph Foundation Models for\n  Text-Attributed Graphs"
                },
                "summary": "Recently, research on Text-Attributed Graphs (TAGs) has gained significant\nattention due to the prevalence of free-text node features in real-world\napplications and the advancements in Large Language Models (LLMs) that bolster\nTAG methodologies. However, current TAG approaches face two primary challenges:\n(i) Heavy reliance on label information and (ii) Limited cross-domain\nzero/few-shot transferability. These issues constrain the scaling of both data\nand model size, owing to high labor costs and scaling laws, complicating the\ndevelopment of graph foundation models with strong transferability. In this\nwork, we propose the GraphCLIP framework to address these challenges by\nlearning graph foundation models with strong cross-domain zero/few-shot\ntransferability through a self-supervised contrastive graph-summary pretraining\nmethod. Specifically, we generate and curate large-scale graph-summary pair\ndata with the assistance of LLMs, and introduce a novel graph-summary\npretraining method, combined with invariant learning, to enhance graph\nfoundation models with strong cross-domain zero-shot transferability. For\nfew-shot learning, we propose a novel graph prompt tuning technique aligned\nwith our pretraining objective to mitigate catastrophic forgetting and minimize\nlearning costs. Extensive experiments show the superiority of GraphCLIP in both\nzero-shot and few-shot settings, while evaluations across various downstream\ntasks confirm the versatility of GraphCLIP. Our code is available at:\nhttps://github.com/ZhuYun97/GraphCLIP",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, research on Text-Attributed Graphs (TAGs) has gained significant\nattention due to the prevalence of free-text node features in real-world\napplications and the advancements in Large Language Models (LLMs) that bolster\nTAG methodologies. However, current TAG approaches face two primary challenges:\n(i) Heavy reliance on label information and (ii) Limited cross-domain\nzero/few-shot transferability. These issues constrain the scaling of both data\nand model size, owing to high labor costs and scaling laws, complicating the\ndevelopment of graph foundation models with strong transferability. In this\nwork, we propose the GraphCLIP framework to address these challenges by\nlearning graph foundation models with strong cross-domain zero/few-shot\ntransferability through a self-supervised contrastive graph-summary pretraining\nmethod. Specifically, we generate and curate large-scale graph-summary pair\ndata with the assistance of LLMs, and introduce a novel graph-summary\npretraining method, combined with invariant learning, to enhance graph\nfoundation models with strong cross-domain zero-shot transferability. For\nfew-shot learning, we propose a novel graph prompt tuning technique aligned\nwith our pretraining objective to mitigate catastrophic forgetting and minimize\nlearning costs. Extensive experiments show the superiority of GraphCLIP in both\nzero-shot and few-shot settings, while evaluations across various downstream\ntasks confirm the versatility of GraphCLIP. Our code is available at:\nhttps://github.com/ZhuYun97/GraphCLIP"
                },
                "authors": [
                    {
                        "name": "Yun Zhu"
                    },
                    {
                        "name": "Haizhou Shi"
                    },
                    {
                        "name": "Xiaotang Wang"
                    },
                    {
                        "name": "Yongchao Liu"
                    },
                    {
                        "name": "Yaoke Wang"
                    },
                    {
                        "name": "Boci Peng"
                    },
                    {
                        "name": "Chuntao Hong"
                    },
                    {
                        "name": "Siliang Tang"
                    }
                ],
                "author_detail": {
                    "name": "Siliang Tang"
                },
                "author": "Siliang Tang",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10329v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10329v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21965v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21965v1",
                "updated": "2024-10-29T11:47:01Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    11,
                    47,
                    1,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T11:47:01Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    11,
                    47,
                    1,
                    1,
                    303,
                    0
                ],
                "title": "SG-Bench: Evaluating LLM Safety Generalization Across Diverse Tasks and\n  Prompt Types",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SG-Bench: Evaluating LLM Safety Generalization Across Diverse Tasks and\n  Prompt Types"
                },
                "summary": "Ensuring the safety of large language model (LLM) applications is essential\nfor developing trustworthy artificial intelligence. Current LLM safety\nbenchmarks have two limitations. First, they focus solely on either\ndiscriminative or generative evaluation paradigms while ignoring their\ninterconnection. Second, they rely on standardized inputs, overlooking the\neffects of widespread prompting techniques, such as system prompts, few-shot\ndemonstrations, and chain-of-thought prompting. To overcome these issues, we\ndeveloped SG-Bench, a novel benchmark to assess the generalization of LLM\nsafety across various tasks and prompt types. This benchmark integrates both\ngenerative and discriminative evaluation tasks and includes extended data to\nexamine the impact of prompt engineering and jailbreak on LLM safety. Our\nassessment of 3 advanced proprietary LLMs and 10 open-source LLMs with the\nbenchmark reveals that most LLMs perform worse on discriminative tasks than\ngenerative ones, and are highly susceptible to prompts, indicating poor\ngeneralization in safety alignment. We also explain these findings\nquantitatively and qualitatively to provide insights for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring the safety of large language model (LLM) applications is essential\nfor developing trustworthy artificial intelligence. Current LLM safety\nbenchmarks have two limitations. First, they focus solely on either\ndiscriminative or generative evaluation paradigms while ignoring their\ninterconnection. Second, they rely on standardized inputs, overlooking the\neffects of widespread prompting techniques, such as system prompts, few-shot\ndemonstrations, and chain-of-thought prompting. To overcome these issues, we\ndeveloped SG-Bench, a novel benchmark to assess the generalization of LLM\nsafety across various tasks and prompt types. This benchmark integrates both\ngenerative and discriminative evaluation tasks and includes extended data to\nexamine the impact of prompt engineering and jailbreak on LLM safety. Our\nassessment of 3 advanced proprietary LLMs and 10 open-source LLMs with the\nbenchmark reveals that most LLMs perform worse on discriminative tasks than\ngenerative ones, and are highly susceptible to prompts, indicating poor\ngeneralization in safety alignment. We also explain these findings\nquantitatively and qualitatively to provide insights for future research."
                },
                "authors": [
                    {
                        "name": "Yutao Mou"
                    },
                    {
                        "name": "Shikun Zhang"
                    },
                    {
                        "name": "Wei Ye"
                    }
                ],
                "author_detail": {
                    "name": "Wei Ye"
                },
                "author": "Wei Ye",
                "arxiv_comment": "Accepted by NeurIPS2024 (Dataset and Benchmark Track)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21965v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21965v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13282v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13282v2",
                "updated": "2024-10-29T11:29:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    11,
                    29,
                    31,
                    1,
                    303,
                    0
                ],
                "published": "2024-06-19T07:23:33Z",
                "published_parsed": [
                    2024,
                    6,
                    19,
                    7,
                    23,
                    33,
                    2,
                    171,
                    0
                ],
                "title": "Understanding the RoPE Extensions of Long-Context LLMs: An Attention\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the RoPE Extensions of Long-Context LLMs: An Attention\n  Perspective"
                },
                "summary": "Enabling LLMs to handle lengthy context is currently a research hotspot. Most\nLLMs are built upon rotary position embedding (RoPE), a popular position\nencoding method. Therefore, a prominent path is to extrapolate the RoPE trained\non comparably short texts to far longer texts. A heavy bunch of efforts have\nbeen dedicated to boosting the extrapolation via extending the formulations of\nthe RoPE, however, few of them have attempted to showcase their inner workings\ncomprehensively. In this paper, we are driven to offer a straightforward yet\nin-depth understanding of RoPE extensions from an attention perspective and on\ntwo benchmarking tasks. A broad array of experiments reveals several valuable\nfindings: 1) Maintaining attention patterns to those at the pretrained length\nimproves extrapolation; 2) Large attention uncertainty leads to retrieval\nerrors; 3) Using longer continual pretraining lengths for RoPE extensions could\nreduce attention uncertainty and significantly enhance extrapolation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling LLMs to handle lengthy context is currently a research hotspot. Most\nLLMs are built upon rotary position embedding (RoPE), a popular position\nencoding method. Therefore, a prominent path is to extrapolate the RoPE trained\non comparably short texts to far longer texts. A heavy bunch of efforts have\nbeen dedicated to boosting the extrapolation via extending the formulations of\nthe RoPE, however, few of them have attempted to showcase their inner workings\ncomprehensively. In this paper, we are driven to offer a straightforward yet\nin-depth understanding of RoPE extensions from an attention perspective and on\ntwo benchmarking tasks. A broad array of experiments reveals several valuable\nfindings: 1) Maintaining attention patterns to those at the pretrained length\nimproves extrapolation; 2) Large attention uncertainty leads to retrieval\nerrors; 3) Using longer continual pretraining lengths for RoPE extensions could\nreduce attention uncertainty and significantly enhance extrapolation."
                },
                "authors": [
                    {
                        "name": "Meizhi Zhong"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Yikun Lei"
                    },
                    {
                        "name": "Xikai Liu"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13282v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13282v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16122v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16122v2",
                "updated": "2024-10-29T11:17:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    11,
                    17,
                    44,
                    1,
                    303,
                    0
                ],
                "published": "2024-05-25T08:23:05Z",
                "published_parsed": [
                    2024,
                    5,
                    25,
                    8,
                    23,
                    5,
                    5,
                    146,
                    0
                ],
                "title": "Prompt Optimization with EASE? Efficient Ordering-aware Automated\n  Selection of Exemplars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt Optimization with EASE? Efficient Ordering-aware Automated\n  Selection of Exemplars"
                },
                "summary": "Large language models (LLMs) have shown impressive capabilities in real-world\napplications. The capability of in-context learning (ICL) allows us to adapt an\nLLM to downstream tasks by including input-label exemplars in the prompt\nwithout model fine-tuning. However, the quality of these exemplars in the\nprompt greatly impacts performance, highlighting the need for an effective\nautomated exemplar selection method. Recent studies have explored\nretrieval-based approaches to select exemplars tailored to individual test\nqueries, which can be undesirable due to extra test-time computation and an\nincreased risk of data exposure. Moreover, existing methods fail to adequately\naccount for the impact of exemplar ordering on the performance. On the other\nhand, the impact of the instruction, another essential component in the prompt\ngiven to the LLM, is often overlooked in existing exemplar selection methods.\nTo address these challenges, we propose a novel method named EASE, which\nleverages the hidden embedding from a pre-trained language model to represent\nordered sets of exemplars and uses a neural bandit algorithm to optimize the\nsets of exemplars while accounting for exemplar ordering. Our EASE can\nefficiently find an ordered set of exemplars that performs well for all test\nqueries from a given task, thereby eliminating test-time computation.\nImportantly, EASE can be readily extended to jointly optimize both the\nexemplars and the instruction. Through extensive empirical evaluations\n(including novel tasks), we demonstrate the superiority of EASE over existing\nmethods, and reveal practical insights about the impact of exemplar selection\non ICL, which may be of independent interest. Our code is available at\nhttps://github.com/ZhaoxuanWu/EASE-Prompt-Optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown impressive capabilities in real-world\napplications. The capability of in-context learning (ICL) allows us to adapt an\nLLM to downstream tasks by including input-label exemplars in the prompt\nwithout model fine-tuning. However, the quality of these exemplars in the\nprompt greatly impacts performance, highlighting the need for an effective\nautomated exemplar selection method. Recent studies have explored\nretrieval-based approaches to select exemplars tailored to individual test\nqueries, which can be undesirable due to extra test-time computation and an\nincreased risk of data exposure. Moreover, existing methods fail to adequately\naccount for the impact of exemplar ordering on the performance. On the other\nhand, the impact of the instruction, another essential component in the prompt\ngiven to the LLM, is often overlooked in existing exemplar selection methods.\nTo address these challenges, we propose a novel method named EASE, which\nleverages the hidden embedding from a pre-trained language model to represent\nordered sets of exemplars and uses a neural bandit algorithm to optimize the\nsets of exemplars while accounting for exemplar ordering. Our EASE can\nefficiently find an ordered set of exemplars that performs well for all test\nqueries from a given task, thereby eliminating test-time computation.\nImportantly, EASE can be readily extended to jointly optimize both the\nexemplars and the instruction. Through extensive empirical evaluations\n(including novel tasks), we demonstrate the superiority of EASE over existing\nmethods, and reveal practical insights about the impact of exemplar selection\non ICL, which may be of independent interest. Our code is available at\nhttps://github.com/ZhaoxuanWu/EASE-Prompt-Optimization."
                },
                "authors": [
                    {
                        "name": "Zhaoxuan Wu"
                    },
                    {
                        "name": "Xiaoqiang Lin"
                    },
                    {
                        "name": "Zhongxiang Dai"
                    },
                    {
                        "name": "Wenyang Hu"
                    },
                    {
                        "name": "Yao Shu"
                    },
                    {
                        "name": "See-Kiong Ng"
                    },
                    {
                        "name": "Patrick Jaillet"
                    },
                    {
                        "name": "Bryan Kian Hsiang Low"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Kian Hsiang Low"
                },
                "author": "Bryan Kian Hsiang Low",
                "arxiv_comment": "28 pages, 1 figure, 35 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16122v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16122v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13708v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13708v2",
                "updated": "2024-10-29T11:14:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    11,
                    14,
                    46,
                    1,
                    303,
                    0
                ],
                "published": "2024-09-06T14:26:18Z",
                "published_parsed": [
                    2024,
                    9,
                    6,
                    14,
                    26,
                    18,
                    4,
                    250,
                    0
                ],
                "title": "Towards Safe Multilingual Frontier AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Safe Multilingual Frontier AI"
                },
                "summary": "Linguistically inclusive LLMs -- which maintain good performance regardless\nof the language with which they are prompted -- are necessary for the diffusion\nof AI benefits around the world. Multilingual jailbreaks that rely on language\ntranslation to evade safety measures undermine the safe and inclusive\ndeployment of AI systems. We provide policy recommendations to enhance the\nmultilingual capabilities of AI while mitigating the risks of multilingual\njailbreaks. We examine how a language's level of resourcing relates to how\nvulnerable LLMs are to multilingual jailbreaks in that language. We do this by\ntesting five advanced AI models across 24 official languages of the EU.\nBuilding on prior research, we propose policy actions that align with the EU\nlegal landscape and institutional framework to address multilingual jailbreaks,\nwhile promoting linguistic inclusivity. These include mandatory assessments of\nmultilingual capabilities and vulnerabilities, public opinion research, and\nstate support for multilingual AI development. The measures aim to improve AI\nsafety and functionality through EU policy initiatives, guiding the\nimplementation of the EU AI Act and informing regulatory efforts of the\nEuropean AI Office.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linguistically inclusive LLMs -- which maintain good performance regardless\nof the language with which they are prompted -- are necessary for the diffusion\nof AI benefits around the world. Multilingual jailbreaks that rely on language\ntranslation to evade safety measures undermine the safe and inclusive\ndeployment of AI systems. We provide policy recommendations to enhance the\nmultilingual capabilities of AI while mitigating the risks of multilingual\njailbreaks. We examine how a language's level of resourcing relates to how\nvulnerable LLMs are to multilingual jailbreaks in that language. We do this by\ntesting five advanced AI models across 24 official languages of the EU.\nBuilding on prior research, we propose policy actions that align with the EU\nlegal landscape and institutional framework to address multilingual jailbreaks,\nwhile promoting linguistic inclusivity. These include mandatory assessments of\nmultilingual capabilities and vulnerabilities, public opinion research, and\nstate support for multilingual AI development. The measures aim to improve AI\nsafety and functionality through EU policy initiatives, guiding the\nimplementation of the EU AI Act and informing regulatory efforts of the\nEuropean AI Office."
                },
                "authors": [
                    {
                        "name": "Artrs Kanepajs"
                    },
                    {
                        "name": "Vladimir Ivanov"
                    },
                    {
                        "name": "Richard Moulange"
                    }
                ],
                "author_detail": {
                    "name": "Richard Moulange"
                },
                "author": "Richard Moulange",
                "arxiv_comment": "23 pages; 1 figure and 10 supplementary figures; Accepted (spotlight\n  presentation) at NeurIPS 2024 SoLaR workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13708v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13708v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00456v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00456v2",
                "updated": "2024-10-29T11:09:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    11,
                    9,
                    12,
                    1,
                    303,
                    0
                ],
                "published": "2024-03-30T19:20:06Z",
                "published_parsed": [
                    2024,
                    3,
                    30,
                    19,
                    20,
                    6,
                    5,
                    90,
                    0
                ],
                "title": "QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs"
                },
                "summary": "We introduce QuaRot, a new Quantization scheme based on Rotations, which is\nable to quantize LLMs end-to-end, including all weights, activations, and KV\ncache in 4 bits. QuaRot rotates LLMs in a way that removes outliers from the\nhidden state without changing the output, making quantization easier. This\ncomputational invariance is applied to the hidden state (residual) of the LLM,\nas well as to the activations of the feed-forward components, aspects of the\nattention mechanism, and to the KV cache. The result is a quantized model where\nall matrix multiplications are performed in 4 bits, without any channels\nidentified for retention in higher precision. Our 4-bit quantized LLaMa2-70B\nmodel has losses of at most 0.47 WikiText-2 perplexity and retains 99% of the\nzero-shot performance. We also show that QuaRot can provide lossless 6 and 8\nbit LLaMa2 models without any calibration data using round-to-nearest\nquantization. Code is available at: https://github.com/spcl/QuaRot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce QuaRot, a new Quantization scheme based on Rotations, which is\nable to quantize LLMs end-to-end, including all weights, activations, and KV\ncache in 4 bits. QuaRot rotates LLMs in a way that removes outliers from the\nhidden state without changing the output, making quantization easier. This\ncomputational invariance is applied to the hidden state (residual) of the LLM,\nas well as to the activations of the feed-forward components, aspects of the\nattention mechanism, and to the KV cache. The result is a quantized model where\nall matrix multiplications are performed in 4 bits, without any channels\nidentified for retention in higher precision. Our 4-bit quantized LLaMa2-70B\nmodel has losses of at most 0.47 WikiText-2 perplexity and retains 99% of the\nzero-shot performance. We also show that QuaRot can provide lossless 6 and 8\nbit LLaMa2 models without any calibration data using round-to-nearest\nquantization. Code is available at: https://github.com/spcl/QuaRot."
                },
                "authors": [
                    {
                        "name": "Saleh Ashkboos"
                    },
                    {
                        "name": "Amirkeivan Mohtashami"
                    },
                    {
                        "name": "Maximilian L. Croci"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Pashmina Cameron"
                    },
                    {
                        "name": "Martin Jaggi"
                    },
                    {
                        "name": "Dan Alistarh"
                    },
                    {
                        "name": "Torsten Hoefler"
                    },
                    {
                        "name": "James Hensman"
                    }
                ],
                "author_detail": {
                    "name": "James Hensman"
                },
                "author": "James Hensman",
                "arxiv_comment": "21 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00456v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00456v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21943v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21943v1",
                "updated": "2024-10-29T11:03:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    11,
                    3,
                    31,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T11:03:31Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    11,
                    3,
                    31,
                    1,
                    303,
                    0
                ],
                "title": "Beyond Text: Optimizing RAG with Multimodal Inputs for Industrial\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Text: Optimizing RAG with Multimodal Inputs for Industrial\n  Applications"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nanswering questions, but they lack domain-specific knowledge and are prone to\nhallucinations. Retrieval Augmented Generation (RAG) is one approach to address\nthese challenges, while multimodal models are emerging as promising AI\nassistants for processing both text and images. In this paper we describe a\nseries of experiments aimed at determining how to best integrate multimodal\nmodels into RAG systems for the industrial domain. The purpose of the\nexperiments is to determine whether including images alongside text from\ndocuments within the industrial domain increases RAG performance and to find\nthe optimal configuration for such a multimodal RAG system. Our experiments\ninclude two approaches for image processing and retrieval, as well as two LLMs\n(GPT4-Vision and LLaVA) for answer synthesis. These image processing strategies\ninvolve the use of multimodal embeddings and the generation of textual\nsummaries from images. We evaluate our experiments with an LLM-as-a-Judge\napproach. Our results reveal that multimodal RAG can outperform single-modality\nRAG settings, although image retrieval poses a greater challenge than text\nretrieval. Additionally, leveraging textual summaries from images presents a\nmore promising approach compared to the use of multimodal embeddings, providing\nmore opportunities for future advancements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nanswering questions, but they lack domain-specific knowledge and are prone to\nhallucinations. Retrieval Augmented Generation (RAG) is one approach to address\nthese challenges, while multimodal models are emerging as promising AI\nassistants for processing both text and images. In this paper we describe a\nseries of experiments aimed at determining how to best integrate multimodal\nmodels into RAG systems for the industrial domain. The purpose of the\nexperiments is to determine whether including images alongside text from\ndocuments within the industrial domain increases RAG performance and to find\nthe optimal configuration for such a multimodal RAG system. Our experiments\ninclude two approaches for image processing and retrieval, as well as two LLMs\n(GPT4-Vision and LLaVA) for answer synthesis. These image processing strategies\ninvolve the use of multimodal embeddings and the generation of textual\nsummaries from images. We evaluate our experiments with an LLM-as-a-Judge\napproach. Our results reveal that multimodal RAG can outperform single-modality\nRAG settings, although image retrieval poses a greater challenge than text\nretrieval. Additionally, leveraging textual summaries from images presents a\nmore promising approach compared to the use of multimodal embeddings, providing\nmore opportunities for future advancements."
                },
                "authors": [
                    {
                        "name": "Monica Riedler"
                    },
                    {
                        "name": "Stefan Langer"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Langer"
                },
                "author": "Stefan Langer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21943v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21943v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21939v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21939v1",
                "updated": "2024-10-29T10:57:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    10,
                    57,
                    11,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T10:57:11Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    10,
                    57,
                    11,
                    1,
                    303,
                    0
                ],
                "title": "Benchmarking OpenAI o1 in Cyber Security",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking OpenAI o1 in Cyber Security"
                },
                "summary": "We evaluate OpenAI's o1-preview and o1-mini models, benchmarking their\nperformance against the earlier GPT-4o model. Our evaluation focuses on their\nability to detect vulnerabilities in real-world software by generating\nstructured inputs that trigger known sanitizers. Using DARPA's AI Cyber\nChallenge (AIxCC) framework and the Nginx challenge project--a deliberately\nmodified version of the widely-used Nginx web server--we create a well-defined\nyet complex environment for testing LLMs on automated vulnerability detection\n(AVD) tasks. Our results show that the o1-preview model significantly\noutperforms GPT-4o in both success rate and efficiency, especially in more\ncomplex scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We evaluate OpenAI's o1-preview and o1-mini models, benchmarking their\nperformance against the earlier GPT-4o model. Our evaluation focuses on their\nability to detect vulnerabilities in real-world software by generating\nstructured inputs that trigger known sanitizers. Using DARPA's AI Cyber\nChallenge (AIxCC) framework and the Nginx challenge project--a deliberately\nmodified version of the widely-used Nginx web server--we create a well-defined\nyet complex environment for testing LLMs on automated vulnerability detection\n(AVD) tasks. Our results show that the o1-preview model significantly\noutperforms GPT-4o in both success rate and efficiency, especially in more\ncomplex scenarios."
                },
                "authors": [
                    {
                        "name": "Dan Ristea"
                    },
                    {
                        "name": "Vasilios Mavroudis"
                    },
                    {
                        "name": "Chris Hicks"
                    }
                ],
                "author_detail": {
                    "name": "Chris Hicks"
                },
                "author": "Chris Hicks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21939v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21939v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11430v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11430v3",
                "updated": "2024-10-29T10:52:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    10,
                    52,
                    35,
                    1,
                    303,
                    0
                ],
                "published": "2024-06-17T11:35:16Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    11,
                    35,
                    16,
                    0,
                    169,
                    0
                ],
                "title": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache\n  Compression"
                },
                "summary": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of large language models (LLMs) is often hindered by the\nextensive memory requirements of the Key-Value (KV) cache, especially as\ncontext lengths increase. Existing approaches to reduce the KV cache size\ninvolve either fine-tuning the model to learn a compression strategy or\nleveraging attention scores to reduce the sequence length. We analyse the\nattention distributions in decoder-only Transformers-based models and observe\nthat attention allocation patterns stay consistent across most layers.\nSurprisingly, we find a clear correlation between the $L_2$ and the attention\nscores over cached KV pairs, where a low $L_2$ of a key embedding usually leads\nto a high attention score during decoding. This finding indicates that the\ninfluence of a KV pair is potentially determined by the key embedding itself\nbefore being queried. Based on this observation, we compress the KV cache based\non the $L_2$ of key embeddings. Our experimental results show that this simple\nstrategy can reduce the KV cache size by 50% on language modelling and\nneedle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing\naccuracy. Moreover, without relying on the attention scores, this approach\nremains compatible with FlashAttention, enabling broader applicability."
                },
                "authors": [
                    {
                        "name": "Alessio Devoto"
                    },
                    {
                        "name": "Yu Zhao"
                    },
                    {
                        "name": "Simone Scardapane"
                    },
                    {
                        "name": "Pasquale Minervini"
                    }
                ],
                "author_detail": {
                    "name": "Pasquale Minervini"
                },
                "author": "Pasquale Minervini",
                "arxiv_comment": "This is an extended version of a paper published in the proceedings\n  of the 2024 Conference on Empirical Methods in Natural Language Processing\n  (EMNLP 2024); this version was presented at the 4th NeurIPS Workshop on\n  Efficient Natural Language and Speech Processing (ENLSP-IV)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11430v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11430v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19572v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19572v2",
                "updated": "2024-10-29T10:40:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    10,
                    40,
                    54,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-25T14:07:53Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    14,
                    7,
                    53,
                    4,
                    299,
                    0
                ],
                "title": "ChunkRAG: Novel LLM-Chunk Filtering Method for RAG Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkRAG: Novel LLM-Chunk Filtering Method for RAG Systems"
                },
                "summary": "Retrieval-Augmented Generation (RAG) systems using large language models\n(LLMs) often generate inaccurate responses due to the retrieval of irrelevant\nor loosely related information. Existing methods, which operate at the document\nlevel, fail to effectively filter out such content. We propose LLM-driven chunk\nfiltering, ChunkRAG, a framework that enhances RAG systems by evaluating and\nfiltering retrieved information at the chunk level. Our approach employs\nsemantic chunking to divide documents into coherent sections and utilizes\nLLM-based relevance scoring to assess each chunk's alignment with the user's\nquery. By filtering out less pertinent chunks before the generation phase, we\nsignificantly reduce hallucinations and improve factual accuracy. Experiments\nshow that our method outperforms existing RAG models, achieving higher accuracy\non tasks requiring precise information retrieval. This advancement enhances the\nreliability of RAG systems, making them particularly beneficial for\napplications like fact-checking and multi-hop reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) systems using large language models\n(LLMs) often generate inaccurate responses due to the retrieval of irrelevant\nor loosely related information. Existing methods, which operate at the document\nlevel, fail to effectively filter out such content. We propose LLM-driven chunk\nfiltering, ChunkRAG, a framework that enhances RAG systems by evaluating and\nfiltering retrieved information at the chunk level. Our approach employs\nsemantic chunking to divide documents into coherent sections and utilizes\nLLM-based relevance scoring to assess each chunk's alignment with the user's\nquery. By filtering out less pertinent chunks before the generation phase, we\nsignificantly reduce hallucinations and improve factual accuracy. Experiments\nshow that our method outperforms existing RAG models, achieving higher accuracy\non tasks requiring precise information retrieval. This advancement enhances the\nreliability of RAG systems, making them particularly beneficial for\napplications like fact-checking and multi-hop reasoning."
                },
                "authors": [
                    {
                        "name": "Ritvik Aggarwal Ishneet Sukhvinder Singh Ibrahim Allahverdiyev"
                    },
                    {
                        "name": "Muhammad Taha"
                    },
                    {
                        "name": "Aslihan Akalin"
                    },
                    {
                        "name": "Kevin Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Zhu"
                },
                "author": "Kevin Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19572v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19572v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01236v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01236v2",
                "updated": "2024-10-29T10:16:54Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    10,
                    16,
                    54,
                    1,
                    303,
                    0
                ],
                "published": "2024-09-02T13:11:38Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    13,
                    11,
                    38,
                    0,
                    246,
                    0
                ],
                "title": "Spatial-Aware Conformal Prediction for Trustworthy Hyperspectral Image\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatial-Aware Conformal Prediction for Trustworthy Hyperspectral Image\n  Classification"
                },
                "summary": "Hyperspectral image (HSI) classification involves assigning unique labels to\neach pixel to identify various land cover categories. While deep classifiers\nhave achieved high predictive accuracy in this field, they lack the ability to\nrigorously quantify confidence in their predictions. Quantifying the certainty\nof model predictions is crucial for the safe usage of predictive models, and\nthis limitation restricts their application in critical contexts where the cost\nof prediction errors is significant. To support the safe deployment of HSI\nclassifiers, we first provide a theoretical proof establishing the validity of\nthe emerging uncertainty quantification technique, conformal prediction, in the\ncontext of HSI classification. We then propose a conformal procedure that\nequips any trained HSI classifier with trustworthy prediction sets, ensuring\nthat these sets include the true labels with a user-specified probability\n(e.g., 95\\%). Building on this foundation, we introduce Spatial-Aware Conformal\nPrediction (\\texttt{SACP}), a conformal prediction framework specifically\ndesigned for HSI data. This method integrates essential spatial information\ninherent in HSIs by aggregating the non-conformity scores of pixels with high\nspatial correlation, which effectively enhances the efficiency of prediction\nsets. Both theoretical and empirical results validate the effectiveness of our\nproposed approach. The source code is available at\n\\url{https://github.com/J4ckLiu/SACP}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hyperspectral image (HSI) classification involves assigning unique labels to\neach pixel to identify various land cover categories. While deep classifiers\nhave achieved high predictive accuracy in this field, they lack the ability to\nrigorously quantify confidence in their predictions. Quantifying the certainty\nof model predictions is crucial for the safe usage of predictive models, and\nthis limitation restricts their application in critical contexts where the cost\nof prediction errors is significant. To support the safe deployment of HSI\nclassifiers, we first provide a theoretical proof establishing the validity of\nthe emerging uncertainty quantification technique, conformal prediction, in the\ncontext of HSI classification. We then propose a conformal procedure that\nequips any trained HSI classifier with trustworthy prediction sets, ensuring\nthat these sets include the true labels with a user-specified probability\n(e.g., 95\\%). Building on this foundation, we introduce Spatial-Aware Conformal\nPrediction (\\texttt{SACP}), a conformal prediction framework specifically\ndesigned for HSI data. This method integrates essential spatial information\ninherent in HSIs by aggregating the non-conformity scores of pixels with high\nspatial correlation, which effectively enhances the efficiency of prediction\nsets. Both theoretical and empirical results validate the effectiveness of our\nproposed approach. The source code is available at\n\\url{https://github.com/J4ckLiu/SACP}."
                },
                "authors": [
                    {
                        "name": "Kangdao Liu"
                    },
                    {
                        "name": "Tianhao Sun"
                    },
                    {
                        "name": "Hao Zeng"
                    },
                    {
                        "name": "Yongshan Zhang"
                    },
                    {
                        "name": "Chi-Man Pun"
                    },
                    {
                        "name": "Chi-Man Vong"
                    }
                ],
                "author_detail": {
                    "name": "Chi-Man Vong"
                },
                "author": "Chi-Man Vong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01236v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01236v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21909v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21909v1",
                "updated": "2024-10-29T10:01:40Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    10,
                    1,
                    40,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T10:01:40Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    10,
                    1,
                    40,
                    1,
                    303,
                    0
                ],
                "title": "SceneGenAgent: Precise Industrial Scene Generation with Coding Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SceneGenAgent: Precise Industrial Scene Generation with Coding Agent"
                },
                "summary": "The modeling of industrial scenes is essential for simulations in industrial\nmanufacturing. While large language models (LLMs) have shown significant\nprogress in generating general 3D scenes from textual descriptions, generating\nindustrial scenes with LLMs poses a unique challenge due to their demand for\nprecise measurements and positioning, requiring complex planning over spatial\narrangement. To address this challenge, we introduce SceneGenAgent, an\nLLM-based agent for generating industrial scenes through C# code. SceneGenAgent\nensures precise layout planning through a structured and calculable format,\nlayout verification, and iterative refinement to meet the quantitative\nrequirements of industrial scenarios. Experiment results demonstrate that LLMs\npowered by SceneGenAgent exceed their original performance, reaching up to\n81.0% success rate in real-world industrial scene generation tasks and\neffectively meeting most scene generation requirements. To further enhance\naccessibility, we construct SceneInstruct, a dataset designed for fine-tuning\nopen-source LLMs to integrate into SceneGenAgent. Experiments show that\nfine-tuning open-source LLMs on SceneInstruct yields significant performance\nimprovements, with Llama3.1-70B approaching the capabilities of GPT-4o. Our\ncode and data are available at https://github.com/THUDM/SceneGenAgent .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The modeling of industrial scenes is essential for simulations in industrial\nmanufacturing. While large language models (LLMs) have shown significant\nprogress in generating general 3D scenes from textual descriptions, generating\nindustrial scenes with LLMs poses a unique challenge due to their demand for\nprecise measurements and positioning, requiring complex planning over spatial\narrangement. To address this challenge, we introduce SceneGenAgent, an\nLLM-based agent for generating industrial scenes through C# code. SceneGenAgent\nensures precise layout planning through a structured and calculable format,\nlayout verification, and iterative refinement to meet the quantitative\nrequirements of industrial scenarios. Experiment results demonstrate that LLMs\npowered by SceneGenAgent exceed their original performance, reaching up to\n81.0% success rate in real-world industrial scene generation tasks and\neffectively meeting most scene generation requirements. To further enhance\naccessibility, we construct SceneInstruct, a dataset designed for fine-tuning\nopen-source LLMs to integrate into SceneGenAgent. Experiments show that\nfine-tuning open-source LLMs on SceneInstruct yields significant performance\nimprovements, with Llama3.1-70B approaching the capabilities of GPT-4o. Our\ncode and data are available at https://github.com/THUDM/SceneGenAgent ."
                },
                "authors": [
                    {
                        "name": "Xiao Xia"
                    },
                    {
                        "name": "Dan Zhang"
                    },
                    {
                        "name": "Zibo Liao"
                    },
                    {
                        "name": "Zhenyu Hou"
                    },
                    {
                        "name": "Tianrui Sun"
                    },
                    {
                        "name": "Jing Li"
                    },
                    {
                        "name": "Ling Fu"
                    },
                    {
                        "name": "Yuxiao Dong"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiao Dong"
                },
                "author": "Yuxiao Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21909v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21909v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.07350v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.07350v3",
                "updated": "2024-10-29T09:31:22Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    9,
                    31,
                    22,
                    1,
                    303,
                    0
                ],
                "published": "2024-03-12T06:16:33Z",
                "published_parsed": [
                    2024,
                    3,
                    12,
                    6,
                    16,
                    33,
                    1,
                    72,
                    0
                ],
                "title": "VLKEB: A Large Vision-Language Model Knowledge Editing Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLKEB: A Large Vision-Language Model Knowledge Editing Benchmark"
                },
                "summary": "Recently, knowledge editing on large language models (LLMs) has received\nconsiderable attention. Compared to this, editing Large Vision-Language Models\n(LVLMs) faces extra challenges from diverse data modalities and complicated\nmodel components, and data for LVLMs editing are limited. The existing LVLM\nediting benchmark, which comprises three metrics (Reliability, Locality, and\nGenerality), falls short in the quality of synthesized evaluation images and\ncannot assess whether models apply edited knowledge in relevant content.\nTherefore, we employ more reliable data collection methods to construct a new\nLarge $\\textbf{V}$ision-$\\textbf{L}$anguage Model $\\textbf{K}$nowledge\n$\\textbf{E}$diting $\\textbf{B}$enchmark, $\\textbf{VLKEB}$, and extend the\nPortability metric for more comprehensive evaluation. Leveraging a multi-modal\nknowledge graph, our image data are bound with knowledge entities. This can be\nfurther used to extract entity-related knowledge, which constitutes the base of\nediting data. We conduct experiments of different editing methods on five\nLVLMs, and thoroughly analyze how do they impact the models. The results reveal\nstrengths and deficiencies of these methods and hopefully provide insights for\nfuture research. The codes and dataset are available at:\nhttps://github.com/VLKEB/VLKEB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, knowledge editing on large language models (LLMs) has received\nconsiderable attention. Compared to this, editing Large Vision-Language Models\n(LVLMs) faces extra challenges from diverse data modalities and complicated\nmodel components, and data for LVLMs editing are limited. The existing LVLM\nediting benchmark, which comprises three metrics (Reliability, Locality, and\nGenerality), falls short in the quality of synthesized evaluation images and\ncannot assess whether models apply edited knowledge in relevant content.\nTherefore, we employ more reliable data collection methods to construct a new\nLarge $\\textbf{V}$ision-$\\textbf{L}$anguage Model $\\textbf{K}$nowledge\n$\\textbf{E}$diting $\\textbf{B}$enchmark, $\\textbf{VLKEB}$, and extend the\nPortability metric for more comprehensive evaluation. Leveraging a multi-modal\nknowledge graph, our image data are bound with knowledge entities. This can be\nfurther used to extract entity-related knowledge, which constitutes the base of\nediting data. We conduct experiments of different editing methods on five\nLVLMs, and thoroughly analyze how do they impact the models. The results reveal\nstrengths and deficiencies of these methods and hopefully provide insights for\nfuture research. The codes and dataset are available at:\nhttps://github.com/VLKEB/VLKEB."
                },
                "authors": [
                    {
                        "name": "Han Huang"
                    },
                    {
                        "name": "Haitian Zhong"
                    },
                    {
                        "name": "Tao Yu"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Shu Wu"
                    },
                    {
                        "name": "Liang Wang"
                    },
                    {
                        "name": "Tieniu Tan"
                    }
                ],
                "author_detail": {
                    "name": "Tieniu Tan"
                },
                "author": "Tieniu Tan",
                "arxiv_comment": "NeurIPS 2024, Datasets and Benchmarks Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.07350v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.07350v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.07524v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.07524v2",
                "updated": "2024-10-29T09:28:12Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    9,
                    28,
                    12,
                    1,
                    303,
                    0
                ],
                "published": "2023-03-13T23:24:30Z",
                "published_parsed": [
                    2023,
                    3,
                    13,
                    23,
                    24,
                    30,
                    0,
                    72,
                    0
                ],
                "title": "Integration of storage endpoints into a Rucio data lake, as an activity\n  to prototype a SKA Regional Centres Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integration of storage endpoints into a Rucio data lake, as an activity\n  to prototype a SKA Regional Centres Network"
                },
                "summary": "The Square Kilometre Array (SKA) infrastructure will consist of two radio\ntelescopes that will be the most sensitive telescopes on Earth. The SKA\ncommunity will have to process and manage near exascale data, which will be a\ntechnical challenge for the coming years. In this respect, the SKA Global\nNetwork of Regional Centres plays a key role in data distribution and\nmanagement. The SRCNet will provide distributed computing and data storage\ncapacity, as well as other important services for the network. Within the\nSRCNet, several teams have been set up for the research, design and development\nof 5 prototypes. One of these prototypes is related to data management and\ndistribution, where a data lake has been deployed using Rucio. In this paper we\nfocus on the tasks performed by several of the teams to deploy new storage\nendpoints within the SKAO data lake. In particular, we will describe the steps\nand deployment instructions for the services required to provide the Rucio data\nlake with a new Rucio Storage Element based on StoRM and WebDAV within the\nSpanish SRC prototype.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Square Kilometre Array (SKA) infrastructure will consist of two radio\ntelescopes that will be the most sensitive telescopes on Earth. The SKA\ncommunity will have to process and manage near exascale data, which will be a\ntechnical challenge for the coming years. In this respect, the SKA Global\nNetwork of Regional Centres plays a key role in data distribution and\nmanagement. The SRCNet will provide distributed computing and data storage\ncapacity, as well as other important services for the network. Within the\nSRCNet, several teams have been set up for the research, design and development\nof 5 prototypes. One of these prototypes is related to data management and\ndistribution, where a data lake has been deployed using Rucio. In this paper we\nfocus on the tasks performed by several of the teams to deploy new storage\nendpoints within the SKAO data lake. In particular, we will describe the steps\nand deployment instructions for the services required to provide the Rucio data\nlake with a new Rucio Storage Element based on StoRM and WebDAV within the\nSpanish SRC prototype."
                },
                "authors": [
                    {
                        "name": "Manuel Parra-Royn"
                    },
                    {
                        "name": "Jess Snchez-Castaeda"
                    },
                    {
                        "name": "Julin Garrido"
                    },
                    {
                        "name": "Susana Snchez-Expsito"
                    },
                    {
                        "name": "Rohini Joshi"
                    },
                    {
                        "name": "James Collinson"
                    },
                    {
                        "name": "Rob Barnsley"
                    },
                    {
                        "name": "Jess Salgado"
                    },
                    {
                        "name": "Lourdes Verdes-Montenegro"
                    }
                ],
                "author_detail": {
                    "name": "Lourdes Verdes-Montenegro"
                },
                "author": "Lourdes Verdes-Montenegro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2303.07524v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.07524v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02795v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02795v4",
                "updated": "2024-10-29T09:27:15Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    9,
                    27,
                    15,
                    1,
                    303,
                    0
                ],
                "published": "2024-09-04T15:11:55Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    15,
                    11,
                    55,
                    2,
                    248,
                    0
                ],
                "title": "Towards a Unified View of Preference Learning for Large Language Models:\n  A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a Unified View of Preference Learning for Large Language Models:\n  A Survey"
                },
                "summary": "Large Language Models (LLMs) exhibit remarkably powerful capabilities. One of\nthe crucial factors to achieve success is aligning the LLM's output with human\npreferences. This alignment process often requires only a small amount of data\nto efficiently enhance the LLM's performance. While effective, research in this\narea spans multiple domains, and the methods involved are relatively complex to\nunderstand. The relationships between different methods have been\nunder-explored, limiting the development of the preference alignment. In light\nof this, we break down the existing popular alignment strategies into different\ncomponents and provide a unified framework to study the current alignment\nstrategies, thereby establishing connections among them. In this survey, we\ndecompose all the strategies in preference learning into four components:\nmodel, data, feedback, and algorithm. This unified view offers an in-depth\nunderstanding of existing alignment algorithms and also opens up possibilities\nto synergize the strengths of different strategies. Furthermore, we present\ndetailed working examples of prevalent existing algorithms to facilitate a\ncomprehensive understanding for the readers. Finally, based on our unified\nperspective, we explore the challenges and future research directions for\naligning large language models with human preferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit remarkably powerful capabilities. One of\nthe crucial factors to achieve success is aligning the LLM's output with human\npreferences. This alignment process often requires only a small amount of data\nto efficiently enhance the LLM's performance. While effective, research in this\narea spans multiple domains, and the methods involved are relatively complex to\nunderstand. The relationships between different methods have been\nunder-explored, limiting the development of the preference alignment. In light\nof this, we break down the existing popular alignment strategies into different\ncomponents and provide a unified framework to study the current alignment\nstrategies, thereby establishing connections among them. In this survey, we\ndecompose all the strategies in preference learning into four components:\nmodel, data, feedback, and algorithm. This unified view offers an in-depth\nunderstanding of existing alignment algorithms and also opens up possibilities\nto synergize the strengths of different strategies. Furthermore, we present\ndetailed working examples of prevalent existing algorithms to facilitate a\ncomprehensive understanding for the readers. Finally, based on our unified\nperspective, we explore the challenges and future research directions for\naligning large language models with human preferences."
                },
                "authors": [
                    {
                        "name": "Bofei Gao"
                    },
                    {
                        "name": "Feifan Song"
                    },
                    {
                        "name": "Yibo Miao"
                    },
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Zhe Yang"
                    },
                    {
                        "name": "Liang Chen"
                    },
                    {
                        "name": "Helan Hu"
                    },
                    {
                        "name": "Runxin Xu"
                    },
                    {
                        "name": "Qingxiu Dong"
                    },
                    {
                        "name": "Ce Zheng"
                    },
                    {
                        "name": "Shanghaoran Quan"
                    },
                    {
                        "name": "Wen Xiao"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Daoguang Zan"
                    },
                    {
                        "name": "Keming Lu"
                    },
                    {
                        "name": "Bowen Yu"
                    },
                    {
                        "name": "Dayiheng Liu"
                    },
                    {
                        "name": "Zeyu Cui"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Lei Sha"
                    },
                    {
                        "name": "Houfeng Wang"
                    },
                    {
                        "name": "Zhifang Sui"
                    },
                    {
                        "name": "Peiyi Wang"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Baobao Chang"
                    }
                ],
                "author_detail": {
                    "name": "Baobao Chang"
                },
                "author": "Baobao Chang",
                "arxiv_comment": "23 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02795v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02795v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.09142v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.09142v2",
                "updated": "2024-10-29T09:13:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    9,
                    13,
                    49,
                    1,
                    303,
                    0
                ],
                "published": "2024-03-14T07:40:54Z",
                "published_parsed": [
                    2024,
                    3,
                    14,
                    7,
                    40,
                    54,
                    3,
                    74,
                    0
                ],
                "title": "USimAgent: Large Language Models for Simulating Search Users",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "USimAgent: Large Language Models for Simulating Search Users"
                },
                "summary": "Due to the advantages in the cost-efficiency and reproducibility, user\nsimulation has become a promising solution to the user-centric evaluation of\ninformation retrieval systems. Nonetheless, accurately simulating user search\nbehaviors has long been a challenge, because users' actions in search are\nhighly complex and driven by intricate cognitive processes such as learning,\nreasoning, and planning. Recently, Large Language Models (LLMs) have\ndemonstrated remarked potential in simulating human-level intelligence and have\nbeen used in building autonomous agents for various tasks. However, the\npotential of using LLMs in simulating search behaviors has not yet been fully\nexplored. In this paper, we introduce a LLM-based user search behavior\nsimulator, USimAgent. The proposed simulator can simulate users' querying,\nclicking, and stopping behaviors during search, and thus, is capable of\ngenerating complete search sessions for specific search tasks. Empirical\ninvestigation on a real user behavior dataset shows that the proposed simulator\noutperforms existing methods in query generation and is comparable to\ntraditional methods in predicting user clicks and stopping behaviors. These\nresults not only validate the effectiveness of using LLMs for user simulation\nbut also shed light on the development of a more robust and generic user\nsimulators. The code and data are accessible at\nhttps://github.com/Meow-E/USimAgent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to the advantages in the cost-efficiency and reproducibility, user\nsimulation has become a promising solution to the user-centric evaluation of\ninformation retrieval systems. Nonetheless, accurately simulating user search\nbehaviors has long been a challenge, because users' actions in search are\nhighly complex and driven by intricate cognitive processes such as learning,\nreasoning, and planning. Recently, Large Language Models (LLMs) have\ndemonstrated remarked potential in simulating human-level intelligence and have\nbeen used in building autonomous agents for various tasks. However, the\npotential of using LLMs in simulating search behaviors has not yet been fully\nexplored. In this paper, we introduce a LLM-based user search behavior\nsimulator, USimAgent. The proposed simulator can simulate users' querying,\nclicking, and stopping behaviors during search, and thus, is capable of\ngenerating complete search sessions for specific search tasks. Empirical\ninvestigation on a real user behavior dataset shows that the proposed simulator\noutperforms existing methods in query generation and is comparable to\ntraditional methods in predicting user clicks and stopping behaviors. These\nresults not only validate the effectiveness of using LLMs for user simulation\nbut also shed light on the development of a more robust and generic user\nsimulators. The code and data are accessible at\nhttps://github.com/Meow-E/USimAgent."
                },
                "authors": [
                    {
                        "name": "Erhan Zhang"
                    },
                    {
                        "name": "Xingzhu Wang"
                    },
                    {
                        "name": "Peiyuan Gong"
                    },
                    {
                        "name": "Yankai Lin"
                    },
                    {
                        "name": "Jiaxin Mao"
                    }
                ],
                "author_detail": {
                    "name": "Jiaxin Mao"
                },
                "author": "Jiaxin Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.09142v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.09142v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20941v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20941v2",
                "updated": "2024-10-29T09:08:19Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    9,
                    8,
                    19,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-28T11:49:58Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    11,
                    49,
                    58,
                    0,
                    302,
                    0
                ],
                "title": "Instruction-Tuned LLMs Succeed in Document-Level MT Without Fine-Tuning\n  -- But BLEU Turns a Blind Eye",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction-Tuned LLMs Succeed in Document-Level MT Without Fine-Tuning\n  -- But BLEU Turns a Blind Eye"
                },
                "summary": "Large language models (LLMs) have excelled in various NLP tasks, including\nmachine translation (MT), yet most studies focus on sentence-level translation.\nThis work investigates the inherent capability of instruction-tuned LLMs for\ndocument-level translation (docMT). Unlike prior approaches that require\nspecialized techniques, we evaluate LLMs by directly prompting them to\ntranslate entire documents in a single pass. Our results show that this method\nimproves translation quality compared to translating sentences separately, even\nwithout document-level fine-tuning. However, this advantage is not reflected in\nBLEU scores, which often favor sentence-based translations. We propose using\nthe LLM-as-a-judge paradigm for evaluation, where GPT-4 is used to assess\ndocument coherence, accuracy, and fluency in a more nuanced way than\nn-gram-based metrics. Overall, our work demonstrates that instruction-tuned\nLLMs can effectively leverage document context for translation. However, we\ncaution against using BLEU scores for evaluating docMT, as they often provide\nmisleading outcomes, failing to capture the quality of document-level\ntranslation. Code and data are available at\nhttps://github.com/EIT-NLP/BLEUless_DocMT",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have excelled in various NLP tasks, including\nmachine translation (MT), yet most studies focus on sentence-level translation.\nThis work investigates the inherent capability of instruction-tuned LLMs for\ndocument-level translation (docMT). Unlike prior approaches that require\nspecialized techniques, we evaluate LLMs by directly prompting them to\ntranslate entire documents in a single pass. Our results show that this method\nimproves translation quality compared to translating sentences separately, even\nwithout document-level fine-tuning. However, this advantage is not reflected in\nBLEU scores, which often favor sentence-based translations. We propose using\nthe LLM-as-a-judge paradigm for evaluation, where GPT-4 is used to assess\ndocument coherence, accuracy, and fluency in a more nuanced way than\nn-gram-based metrics. Overall, our work demonstrates that instruction-tuned\nLLMs can effectively leverage document context for translation. However, we\ncaution against using BLEU scores for evaluating docMT, as they often provide\nmisleading outcomes, failing to capture the quality of document-level\ntranslation. Code and data are available at\nhttps://github.com/EIT-NLP/BLEUless_DocMT"
                },
                "authors": [
                    {
                        "name": "Yirong Sun"
                    },
                    {
                        "name": "Dawei Zhu"
                    },
                    {
                        "name": "Yanjun Chen"
                    },
                    {
                        "name": "Erjia Xiao"
                    },
                    {
                        "name": "Xinghao Chen"
                    },
                    {
                        "name": "Xiaoyu Shen"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyu Shen"
                },
                "author": "Xiaoyu Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20941v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20941v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19920v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19920v2",
                "updated": "2024-10-29T09:07:45Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    9,
                    7,
                    45,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-25T18:25:35Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    18,
                    25,
                    35,
                    4,
                    299,
                    0
                ],
                "title": "Reinforcement Learning for Aligning Large Language Models Agents with\n  Interactive Environments: Quantifying and Mitigating Prompt Overfitting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning for Aligning Large Language Models Agents with\n  Interactive Environments: Quantifying and Mitigating Prompt Overfitting"
                },
                "summary": "Reinforcement learning (RL) is a promising approach for aligning large\nlanguage models (LLMs) knowledge with sequential decision-making tasks.\nHowever, few studies have thoroughly investigated the impact on LLM agents\ncapabilities of fine-tuning them with RL in a specific environment. In this\npaper, we propose a novel framework to analyze the sensitivity of LLMs to\nprompt formulations following RL training in a textual environment. Our\nfindings reveal that the performance of LLMs degrades when faced with prompt\nformulations different from those used during the RL training phase. Besides,\nwe analyze the source of this sensitivity by examining the model's internal\nrepresentations and salient tokens. Finally, we propose to use a contrastive\nloss to mitigate this sensitivity and improve the robustness and generalization\ncapabilities of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) is a promising approach for aligning large\nlanguage models (LLMs) knowledge with sequential decision-making tasks.\nHowever, few studies have thoroughly investigated the impact on LLM agents\ncapabilities of fine-tuning them with RL in a specific environment. In this\npaper, we propose a novel framework to analyze the sensitivity of LLMs to\nprompt formulations following RL training in a textual environment. Our\nfindings reveal that the performance of LLMs degrades when faced with prompt\nformulations different from those used during the RL training phase. Besides,\nwe analyze the source of this sensitivity by examining the model's internal\nrepresentations and salient tokens. Finally, we propose to use a contrastive\nloss to mitigate this sensitivity and improve the robustness and generalization\ncapabilities of LLMs."
                },
                "authors": [
                    {
                        "name": "Mohamed Salim Aissi"
                    },
                    {
                        "name": "Clement Romac"
                    },
                    {
                        "name": "Thomas Carta"
                    },
                    {
                        "name": "Sylvain Lamprier"
                    },
                    {
                        "name": "Pierre-Yves Oudeyer"
                    },
                    {
                        "name": "Olivier Sigaud"
                    },
                    {
                        "name": "Laure Soulier"
                    },
                    {
                        "name": "Nicolas Thome"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Thome"
                },
                "author": "Nicolas Thome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19920v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19920v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21868v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21868v1",
                "updated": "2024-10-29T09:02:37Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    9,
                    2,
                    37,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T09:02:37Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    9,
                    2,
                    37,
                    1,
                    303,
                    0
                ],
                "title": "Improving In-Context Learning with Small Language Model Ensembles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving In-Context Learning with Small Language Model Ensembles"
                },
                "summary": "Large language models (LLMs) have shown impressive capabilities across\nvarious tasks, but their performance on domain-specific tasks remains limited.\nWhile methods like retrieval augmented generation and fine-tuning can help to\naddress this, they require significant resources. In-context learning (ICL) is\na cheap and efficient alternative but cannot match the accuracies of advanced\nmethods. We present Ensemble SuperICL, a novel approach that enhances ICL by\nleveraging the expertise of multiple fine-tuned small language models (SLMs).\nEnsemble SuperICL achieves state of the art (SoTA) results on several natural\nlanguage understanding benchmarks. Additionally, we test it on a medical-domain\nlabelling task and showcase its practicality by using off-the-shelf SLMs\nfine-tuned on a general language task, achieving superior accuracy in\nlarge-scale data labelling compared to all baselines. Finally, we conduct an\nablation study and sensitivity analyses to elucidate the underlying mechanism\nof Ensemble SuperICL. Our research contributes to the growing demand for\nefficient domain specialisation methods in LLMs, offering a cheap and effective\nmethod for practitioners.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown impressive capabilities across\nvarious tasks, but their performance on domain-specific tasks remains limited.\nWhile methods like retrieval augmented generation and fine-tuning can help to\naddress this, they require significant resources. In-context learning (ICL) is\na cheap and efficient alternative but cannot match the accuracies of advanced\nmethods. We present Ensemble SuperICL, a novel approach that enhances ICL by\nleveraging the expertise of multiple fine-tuned small language models (SLMs).\nEnsemble SuperICL achieves state of the art (SoTA) results on several natural\nlanguage understanding benchmarks. Additionally, we test it on a medical-domain\nlabelling task and showcase its practicality by using off-the-shelf SLMs\nfine-tuned on a general language task, achieving superior accuracy in\nlarge-scale data labelling compared to all baselines. Finally, we conduct an\nablation study and sensitivity analyses to elucidate the underlying mechanism\nof Ensemble SuperICL. Our research contributes to the growing demand for\nefficient domain specialisation methods in LLMs, offering a cheap and effective\nmethod for practitioners."
                },
                "authors": [
                    {
                        "name": "M. Mehdi Mojarradi"
                    },
                    {
                        "name": "Lingyi Yang"
                    },
                    {
                        "name": "Robert McCraith"
                    },
                    {
                        "name": "Adam Mahdi"
                    }
                ],
                "author_detail": {
                    "name": "Adam Mahdi"
                },
                "author": "Adam Mahdi",
                "arxiv_comment": "Accepted to NeurIPS 2024 Workshop on Adaptive Foundation Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21868v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21868v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07457v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07457v4",
                "updated": "2024-10-29T08:49:11Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    8,
                    49,
                    11,
                    1,
                    303,
                    0
                ],
                "published": "2024-07-10T08:20:47Z",
                "published_parsed": [
                    2024,
                    7,
                    10,
                    8,
                    20,
                    47,
                    2,
                    192,
                    0
                ],
                "title": "GLBench: A Comprehensive Benchmark for Graph with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GLBench: A Comprehensive Benchmark for Graph with Large Language Models"
                },
                "summary": "The emergence of large language models (LLMs) has revolutionized the way we\ninteract with graphs, leading to a new paradigm called GraphLLM. Despite the\nrapid development of GraphLLM methods in recent years, the progress and\nunderstanding of this field remain unclear due to the lack of a benchmark with\nconsistent experimental protocols. To bridge this gap, we introduce GLBench,\nthe first comprehensive benchmark for evaluating GraphLLM methods in both\nsupervised and zero-shot scenarios. GLBench provides a fair and thorough\nevaluation of different categories of GraphLLM methods, along with traditional\nbaselines such as graph neural networks. Through extensive experiments on a\ncollection of real-world datasets with consistent data processing and splitting\nstrategies, we have uncovered several key findings. Firstly, GraphLLM methods\noutperform traditional baselines in supervised settings, with LLM-as-enhancers\nshowing the most robust performance. However, using LLMs as predictors is less\neffective and often leads to uncontrollable output issues. We also notice that\nno clear scaling laws exist for current GraphLLM methods. In addition, both\nstructures and semantics are crucial for effective zero-shot transfer, and our\nproposed simple baseline can even outperform several models tailored for\nzero-shot scenarios. The data and code of the benchmark can be found at\nhttps://github.com/NineAbyss/GLBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of large language models (LLMs) has revolutionized the way we\ninteract with graphs, leading to a new paradigm called GraphLLM. Despite the\nrapid development of GraphLLM methods in recent years, the progress and\nunderstanding of this field remain unclear due to the lack of a benchmark with\nconsistent experimental protocols. To bridge this gap, we introduce GLBench,\nthe first comprehensive benchmark for evaluating GraphLLM methods in both\nsupervised and zero-shot scenarios. GLBench provides a fair and thorough\nevaluation of different categories of GraphLLM methods, along with traditional\nbaselines such as graph neural networks. Through extensive experiments on a\ncollection of real-world datasets with consistent data processing and splitting\nstrategies, we have uncovered several key findings. Firstly, GraphLLM methods\noutperform traditional baselines in supervised settings, with LLM-as-enhancers\nshowing the most robust performance. However, using LLMs as predictors is less\neffective and often leads to uncontrollable output issues. We also notice that\nno clear scaling laws exist for current GraphLLM methods. In addition, both\nstructures and semantics are crucial for effective zero-shot transfer, and our\nproposed simple baseline can even outperform several models tailored for\nzero-shot scenarios. The data and code of the benchmark can be found at\nhttps://github.com/NineAbyss/GLBench."
                },
                "authors": [
                    {
                        "name": "Yuhan Li"
                    },
                    {
                        "name": "Peisong Wang"
                    },
                    {
                        "name": "Xiao Zhu"
                    },
                    {
                        "name": "Aochuan Chen"
                    },
                    {
                        "name": "Haiyun Jiang"
                    },
                    {
                        "name": "Deng Cai"
                    },
                    {
                        "name": "Victor Wai Kin Chan"
                    },
                    {
                        "name": "Jia Li"
                    }
                ],
                "author_detail": {
                    "name": "Jia Li"
                },
                "author": "Jia Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.07457v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07457v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.17010v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.17010v2",
                "updated": "2024-10-29T08:45:35Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    8,
                    45,
                    35,
                    1,
                    303,
                    0
                ],
                "published": "2024-02-26T20:35:32Z",
                "published_parsed": [
                    2024,
                    2,
                    26,
                    20,
                    35,
                    32,
                    0,
                    57,
                    0
                ],
                "title": "Generative Retrieval with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Retrieval with Large Language Models"
                },
                "summary": "When completing knowledge-intensive tasks, humans sometimes need not just an\nanswer but also a corresponding reference passage for auxiliary reading.\nPrevious methods required obtaining pre-segmented article chunks through\nadditional retrieval models. This paper explores leveraging the parameterized\nknowledge stored during the pre-training phase of large language models (LLMs)\nto independently recall reference passage from any starting position. We\npropose a two-stage framework that simulates the scenario of humans recalling\neasily forgotten references. Initially, the LLM is prompted to recall document\ntitle identifiers to obtain a coarse-grained document set. Then, based on the\nacquired coarse-grained document set, it recalls fine-grained passage. In the\ntwo-stage recall process, we use constrained decoding to ensure that content\noutside of the stored documents is not generated. To increase speed, we only\nrecall a short prefix in the second stage, then locate its position to retrieve\na complete passage. Experiments on KILT knowledge-sensitive tasks have verified\nthat LLMs can independently recall reference passage location in various task\nforms, and the obtained reference significantly assist downstream tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When completing knowledge-intensive tasks, humans sometimes need not just an\nanswer but also a corresponding reference passage for auxiliary reading.\nPrevious methods required obtaining pre-segmented article chunks through\nadditional retrieval models. This paper explores leveraging the parameterized\nknowledge stored during the pre-training phase of large language models (LLMs)\nto independently recall reference passage from any starting position. We\npropose a two-stage framework that simulates the scenario of humans recalling\neasily forgotten references. Initially, the LLM is prompted to recall document\ntitle identifiers to obtain a coarse-grained document set. Then, based on the\nacquired coarse-grained document set, it recalls fine-grained passage. In the\ntwo-stage recall process, we use constrained decoding to ensure that content\noutside of the stored documents is not generated. To increase speed, we only\nrecall a short prefix in the second stage, then locate its position to retrieve\na complete passage. Experiments on KILT knowledge-sensitive tasks have verified\nthat LLMs can independently recall reference passage location in various task\nforms, and the obtained reference significantly assist downstream tasks."
                },
                "authors": [
                    {
                        "name": "Ye Wang"
                    },
                    {
                        "name": "Xinrun Xu"
                    },
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Wenxin Hu"
                    },
                    {
                        "name": "Wei Ye"
                    }
                ],
                "author_detail": {
                    "name": "Wei Ye"
                },
                "author": "Wei Ye",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.17010v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.17010v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12673v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12673v2",
                "updated": "2024-10-29T08:40:25Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    8,
                    40,
                    25,
                    1,
                    303,
                    0
                ],
                "published": "2024-06-18T14:45:50Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    14,
                    45,
                    50,
                    1,
                    170,
                    0
                ],
                "title": "Estimating Knowledge in Large Language Models Without Generating a\n  Single Token",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating Knowledge in Large Language Models Without Generating a\n  Single Token"
                },
                "summary": "To evaluate knowledge in large language models (LLMs), current methods query\nthe model and then evaluate its generated responses. In this work, we ask\nwhether evaluation can be done before the model has generated any text.\nConcretely, is it possible to estimate how knowledgeable a model is about a\ncertain entity, only from its internal computation? We study this question with\ntwo tasks: given a subject entity, the goal is to predict (a) the ability of\nthe model to answer common questions about the entity, and (b) the factuality\nof open-ended responses generated by the model about the entity. Experiments\nwith a variety of LLMs show that KEEN, a simple probe trained over internal\nsubject representations, succeeds at both tasks - correlating with both the QA\naccuracy of the model per-subject and FActScore, a recent factuality metric in\nopen-ended generation. Moreover, KEEN naturally aligns with the model's hedging\nbehavior and faithfully reflects changes in the model's knowledge after\nfine-tuning. Lastly, we show a more interpretable yet equally performant\nvariant of KEEN, which highlights a small set of tokens indicative of clusters\nand gaps in the model's knowledge. Being simple and lightweight, KEEN can be\nleveraged to guide decisions such as when it is appropriate to apply further\ntraining or augment queries with retrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To evaluate knowledge in large language models (LLMs), current methods query\nthe model and then evaluate its generated responses. In this work, we ask\nwhether evaluation can be done before the model has generated any text.\nConcretely, is it possible to estimate how knowledgeable a model is about a\ncertain entity, only from its internal computation? We study this question with\ntwo tasks: given a subject entity, the goal is to predict (a) the ability of\nthe model to answer common questions about the entity, and (b) the factuality\nof open-ended responses generated by the model about the entity. Experiments\nwith a variety of LLMs show that KEEN, a simple probe trained over internal\nsubject representations, succeeds at both tasks - correlating with both the QA\naccuracy of the model per-subject and FActScore, a recent factuality metric in\nopen-ended generation. Moreover, KEEN naturally aligns with the model's hedging\nbehavior and faithfully reflects changes in the model's knowledge after\nfine-tuning. Lastly, we show a more interpretable yet equally performant\nvariant of KEEN, which highlights a small set of tokens indicative of clusters\nand gaps in the model's knowledge. Being simple and lightweight, KEEN can be\nleveraged to guide decisions such as when it is appropriate to apply further\ntraining or augment queries with retrieval."
                },
                "authors": [
                    {
                        "name": "Daniela Gottesman"
                    },
                    {
                        "name": "Mor Geva"
                    }
                ],
                "author_detail": {
                    "name": "Mor Geva"
                },
                "author": "Mor Geva",
                "arxiv_comment": "Accepted at EMNLP 2024 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12673v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12673v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02518v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02518v2",
                "updated": "2024-10-29T08:20:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    8,
                    20,
                    28,
                    1,
                    303,
                    0
                ],
                "published": "2024-06-23T15:55:07Z",
                "published_parsed": [
                    2024,
                    6,
                    23,
                    15,
                    55,
                    7,
                    6,
                    175,
                    0
                ],
                "title": "INDICT: Code Generation with Internal Dialogues of Critiques for Both\n  Security and Helpfulness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "INDICT: Code Generation with Internal Dialogues of Critiques for Both\n  Security and Helpfulness"
                },
                "summary": "Large language models (LLMs) for code are typically trained to align with\nnatural language instructions to closely follow their intentions and\nrequirements. However, in many practical scenarios, it becomes increasingly\nchallenging for these models to navigate the intricate boundary between\nhelpfulness and safety, especially against highly complex yet potentially\nmalicious instructions. In this work, we introduce INDICT: a new framework that\nempowers LLMs with Internal Dialogues of Critiques for both safety and\nhelpfulness guidance. The internal dialogue is a dual cooperative system\nbetween a safety-driven critic and a helpfulness-driven critic. Each critic\nprovides analysis against the given task and corresponding generated response,\nequipped with external knowledge queried through relevant code snippets and\ntools like web search and code interpreter. We engage the dual critic system in\nboth code generation stage as well as code execution stage, providing\npreemptive and post-hoc guidance respectively to LLMs. We evaluated INDICT on 8\ndiverse tasks across 8 programming languages from 5 benchmarks, using LLMs from\n7B to 70B parameters. We observed that our approach can provide an advanced\nlevel of critiques of both safety and helpfulness analysis, significantly\nimproving the quality of output codes ($+10\\%$ absolute improvements in all\nmodels).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) for code are typically trained to align with\nnatural language instructions to closely follow their intentions and\nrequirements. However, in many practical scenarios, it becomes increasingly\nchallenging for these models to navigate the intricate boundary between\nhelpfulness and safety, especially against highly complex yet potentially\nmalicious instructions. In this work, we introduce INDICT: a new framework that\nempowers LLMs with Internal Dialogues of Critiques for both safety and\nhelpfulness guidance. The internal dialogue is a dual cooperative system\nbetween a safety-driven critic and a helpfulness-driven critic. Each critic\nprovides analysis against the given task and corresponding generated response,\nequipped with external knowledge queried through relevant code snippets and\ntools like web search and code interpreter. We engage the dual critic system in\nboth code generation stage as well as code execution stage, providing\npreemptive and post-hoc guidance respectively to LLMs. We evaluated INDICT on 8\ndiverse tasks across 8 programming languages from 5 benchmarks, using LLMs from\n7B to 70B parameters. We observed that our approach can provide an advanced\nlevel of critiques of both safety and helpfulness analysis, significantly\nimproving the quality of output codes ($+10\\%$ absolute improvements in all\nmodels)."
                },
                "authors": [
                    {
                        "name": "Hung Le"
                    },
                    {
                        "name": "Yingbo Zhou"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Silvio Savarese"
                    },
                    {
                        "name": "Doyen Sahoo"
                    }
                ],
                "author_detail": {
                    "name": "Doyen Sahoo"
                },
                "author": "Doyen Sahoo",
                "arxiv_comment": "Accepted to The Thirty-Eighth Annual Conference on Neural Information\n  Processing Systems (NeurIPS 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02518v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02518v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21842v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21842v1",
                "updated": "2024-10-29T08:10:06Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    8,
                    10,
                    6,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T08:10:06Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    8,
                    10,
                    6,
                    1,
                    303,
                    0
                ],
                "title": "Diffusion as Reasoning: Enhancing Object Goal Navigation with LLM-Biased\n  Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion as Reasoning: Enhancing Object Goal Navigation with LLM-Biased\n  Diffusion Model"
                },
                "summary": "The Object Goal Navigation (ObjectNav) task requires the agent to navigate to\na specified target in an unseen environment. Since the environment layout is\nunknown, the agent needs to perform semantic reasoning to infer the potential\nlocation of the target, based on its accumulated memory of the environment\nduring the navigation process. Diffusion models have been shown to be able to\nlearn the distribution relationships between features in RGB images, and thus\ngenerate new realistic images.In this work, we propose a new approach to\nsolving the ObjectNav task, by training a diffusion model to learn the\nstatistical distribution patterns of objects in semantic maps, and using the\nmap of the explored regions during navigation as the condition to generate the\nmap of the unknown regions, thereby realizing the semantic reasoning of the\ntarget object, i.e., diffusion as reasoning (DAR). Meanwhile, we propose the\nglobal target bias and local LLM bias methods, where the former can constrain\nthe diffusion model to generate the target object more effectively, and the\nlatter utilizes the common sense knowledge extracted from the LLM to improve\nthe generalization of the reasoning process. Based on the generated map in the\nunknown region, the agent sets the predicted location of the target as the goal\nand moves towards it. Experiments on Gibson and MP3D show the effectiveness of\nour method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Object Goal Navigation (ObjectNav) task requires the agent to navigate to\na specified target in an unseen environment. Since the environment layout is\nunknown, the agent needs to perform semantic reasoning to infer the potential\nlocation of the target, based on its accumulated memory of the environment\nduring the navigation process. Diffusion models have been shown to be able to\nlearn the distribution relationships between features in RGB images, and thus\ngenerate new realistic images.In this work, we propose a new approach to\nsolving the ObjectNav task, by training a diffusion model to learn the\nstatistical distribution patterns of objects in semantic maps, and using the\nmap of the explored regions during navigation as the condition to generate the\nmap of the unknown regions, thereby realizing the semantic reasoning of the\ntarget object, i.e., diffusion as reasoning (DAR). Meanwhile, we propose the\nglobal target bias and local LLM bias methods, where the former can constrain\nthe diffusion model to generate the target object more effectively, and the\nlatter utilizes the common sense knowledge extracted from the LLM to improve\nthe generalization of the reasoning process. Based on the generated map in the\nunknown region, the agent sets the predicted location of the target as the goal\nand moves towards it. Experiments on Gibson and MP3D show the effectiveness of\nour method."
                },
                "authors": [
                    {
                        "name": "Yiming Ji"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Zhengpu Wang"
                    },
                    {
                        "name": "Boyu Ma"
                    },
                    {
                        "name": "Zongwu Xie"
                    },
                    {
                        "name": "Hong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hong Liu"
                },
                "author": "Hong Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21842v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21842v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21819v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21819v1",
                "updated": "2024-10-29T07:42:18Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    7,
                    42,
                    18,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T07:42:18Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    7,
                    42,
                    18,
                    1,
                    303,
                    0
                ],
                "title": "Self-Preference Bias in LLM-as-a-Judge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Preference Bias in LLM-as-a-Judge"
                },
                "summary": "Automated evaluation leveraging large language models (LLMs), commonly\nreferred to as LLM evaluators or LLM-as-a-judge, has been widely used in\nmeasuring the performance of dialogue systems. However, the self-preference\nbias in LLMs has posed significant risks, including promoting specific styles\nor policies intrinsic to the LLMs. Despite the importance of this issue, there\nis a lack of established methods to measure the self-preference bias\nquantitatively, and its underlying causes are poorly understood. In this paper,\nwe introduce a novel quantitative metric to measure the self-preference bias.\nOur experimental results demonstrate that GPT-4 exhibits a significant degree\nof self-preference bias. To explore the causes, we hypothesize that LLMs may\nfavor outputs that are more familiar to them, as indicated by lower perplexity.\nWe analyze the relationship between LLM evaluations and the perplexities of\noutputs. Our findings reveal that LLMs assign significantly higher evaluations\nto outputs with lower perplexity than human evaluators, regardless of whether\nthe outputs were self-generated. This suggests that the essence of the bias\nlies in perplexity and that the self-preference bias exists because LLMs prefer\ntexts more familiar to them.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated evaluation leveraging large language models (LLMs), commonly\nreferred to as LLM evaluators or LLM-as-a-judge, has been widely used in\nmeasuring the performance of dialogue systems. However, the self-preference\nbias in LLMs has posed significant risks, including promoting specific styles\nor policies intrinsic to the LLMs. Despite the importance of this issue, there\nis a lack of established methods to measure the self-preference bias\nquantitatively, and its underlying causes are poorly understood. In this paper,\nwe introduce a novel quantitative metric to measure the self-preference bias.\nOur experimental results demonstrate that GPT-4 exhibits a significant degree\nof self-preference bias. To explore the causes, we hypothesize that LLMs may\nfavor outputs that are more familiar to them, as indicated by lower perplexity.\nWe analyze the relationship between LLM evaluations and the perplexities of\noutputs. Our findings reveal that LLMs assign significantly higher evaluations\nto outputs with lower perplexity than human evaluators, regardless of whether\nthe outputs were self-generated. This suggests that the essence of the bias\nlies in perplexity and that the self-preference bias exists because LLMs prefer\ntexts more familiar to them."
                },
                "authors": [
                    {
                        "name": "Koki Wataoka"
                    },
                    {
                        "name": "Tsubasa Takahashi"
                    },
                    {
                        "name": "Ryokan Ri"
                    }
                ],
                "author_detail": {
                    "name": "Ryokan Ri"
                },
                "author": "Ryokan Ri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21819v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21819v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21806v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21806v2",
                "updated": "2024-10-30T02:28:41Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    2,
                    28,
                    41,
                    2,
                    304,
                    0
                ],
                "published": "2024-10-29T07:23:43Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    7,
                    23,
                    43,
                    1,
                    303,
                    0
                ],
                "title": "Large Language Models Based JSON Parser Fuzzing for Bug Discovery and\n  Behavioral Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Based JSON Parser Fuzzing for Bug Discovery and\n  Behavioral Analysis"
                },
                "summary": "Fuzzing has been incredibly successful in uncovering bugs and vulnerabilities\nacross diverse software systems. JSON parsers play a vital role in modern\nsoftware development, and ensuring their reliability is of great importance.\nThis research project focuses on leveraging Large Language Models (LLMs) to\nenhance JSON parser testing. The primary objectives are to generate test cases\nand mutants using LLMs for the discovery of potential bugs in open-source JSON\nparsers and the identification of behavioral diversities among them. We aim to\nuncover underlying bugs, plus discovering (and overcoming) behavioral\ndiversities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fuzzing has been incredibly successful in uncovering bugs and vulnerabilities\nacross diverse software systems. JSON parsers play a vital role in modern\nsoftware development, and ensuring their reliability is of great importance.\nThis research project focuses on leveraging Large Language Models (LLMs) to\nenhance JSON parser testing. The primary objectives are to generate test cases\nand mutants using LLMs for the discovery of potential bugs in open-source JSON\nparsers and the identification of behavioral diversities among them. We aim to\nuncover underlying bugs, plus discovering (and overcoming) behavioral\ndiversities."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Zhong"
                    },
                    {
                        "name": "Zhezhen Cao"
                    },
                    {
                        "name": "Zhanwei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhanwei Zhang"
                },
                "author": "Zhanwei Zhang",
                "arxiv_comment": "This submission was a test to evaluate the arXiv submission process\n  and is being withdrawn as it was not intended for formal publication. No\n  research findings are included, and no errors or corrections apply",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21806v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21806v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03949v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03949v2",
                "updated": "2024-10-29T07:17:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    7,
                    17,
                    8,
                    1,
                    303,
                    0
                ],
                "published": "2024-06-06T10:50:26Z",
                "published_parsed": [
                    2024,
                    6,
                    6,
                    10,
                    50,
                    26,
                    3,
                    158,
                    0
                ],
                "title": "UltraMedical: Building Specialized Generalists in Biomedicine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UltraMedical: Building Specialized Generalists in Biomedicine"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious domains and are moving towards more specialized areas. Recent advanced\nproprietary models such as GPT-4 and Gemini have achieved significant\nadvancements in biomedicine, which have also raised privacy and security\nchallenges. The construction of specialized generalists hinges largely on\nhigh-quality datasets, enhanced by techniques like supervised fine-tuning and\nreinforcement learning from human or AI feedback, and direct preference\noptimization. However, these leading technologies (e.g., preference learning)\nare still significantly limited in the open source community due to the\nscarcity of specialized data. In this paper, we present the UltraMedical\ncollections, which consist of high-quality manual and synthetic datasets in the\nbiomedicine domain, featuring preference annotations across multiple advanced\nLLMs. By utilizing these datasets, we fine-tune a suite of specialized medical\nmodels based on Llama-3 series, demonstrating breathtaking capabilities across\nvarious medical benchmarks. Moreover, we develop powerful reward models skilled\nin biomedical and general reward benchmark, enhancing further online preference\nlearning within the biomedical LLM community. Datasets and models are available\nat https://github.com/TsinghuaC3I/UltraMedical",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious domains and are moving towards more specialized areas. Recent advanced\nproprietary models such as GPT-4 and Gemini have achieved significant\nadvancements in biomedicine, which have also raised privacy and security\nchallenges. The construction of specialized generalists hinges largely on\nhigh-quality datasets, enhanced by techniques like supervised fine-tuning and\nreinforcement learning from human or AI feedback, and direct preference\noptimization. However, these leading technologies (e.g., preference learning)\nare still significantly limited in the open source community due to the\nscarcity of specialized data. In this paper, we present the UltraMedical\ncollections, which consist of high-quality manual and synthetic datasets in the\nbiomedicine domain, featuring preference annotations across multiple advanced\nLLMs. By utilizing these datasets, we fine-tune a suite of specialized medical\nmodels based on Llama-3 series, demonstrating breathtaking capabilities across\nvarious medical benchmarks. Moreover, we develop powerful reward models skilled\nin biomedical and general reward benchmark, enhancing further online preference\nlearning within the biomedical LLM community. Datasets and models are available\nat https://github.com/TsinghuaC3I/UltraMedical"
                },
                "authors": [
                    {
                        "name": "Kaiyan Zhang"
                    },
                    {
                        "name": "Sihang Zeng"
                    },
                    {
                        "name": "Ermo Hua"
                    },
                    {
                        "name": "Ning Ding"
                    },
                    {
                        "name": "Zhang-Ren Chen"
                    },
                    {
                        "name": "Zhiyuan Ma"
                    },
                    {
                        "name": "Haoxin Li"
                    },
                    {
                        "name": "Ganqu Cui"
                    },
                    {
                        "name": "Biqing Qi"
                    },
                    {
                        "name": "Xuekai Zhu"
                    },
                    {
                        "name": "Xingtai Lv"
                    },
                    {
                        "name": "Hu Jinfang"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Bowen Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Bowen Zhou"
                },
                "author": "Bowen Zhou",
                "arxiv_comment": "Camera ready version for NeurIPS 2024 D&B Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03949v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03949v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10248v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10248v4",
                "updated": "2024-10-30T09:48:52Z",
                "updated_parsed": [
                    2024,
                    10,
                    30,
                    9,
                    48,
                    52,
                    2,
                    304,
                    0
                ],
                "published": "2024-06-08T13:40:38Z",
                "published_parsed": [
                    2024,
                    6,
                    8,
                    13,
                    40,
                    38,
                    5,
                    160,
                    0
                ],
                "title": "On the Worst Prompt Performance of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Worst Prompt Performance of Large Language Models"
                },
                "summary": "The performance of large language models (LLMs) is acutely sensitive to the\nphrasing of prompts, which raises significant concerns about their reliability\nin real-world scenarios. Existing studies often divide prompts into task-level\ninstructions and case-level inputs and primarily focus on evaluating and\nimproving robustness against variations in tasks-level instructions. However,\nthis setup fails to fully address the diversity of real-world user queries and\nassumes the existence of task-specific datasets. To address these limitations,\nwe introduce RobustAlpacaEval, a new benchmark that consists of semantically\nequivalent case-level queries and emphasizes the importance of using the worst\nprompt performance to gauge the lower bound of model performance. Extensive\nexperiments on RobustAlpacaEval with ChatGPT and six open-source LLMs from the\nLlama, Mistral, and Gemma families uncover substantial variability in model\nperformance; for instance, a difference of 45.48% between the worst and best\nperformance for the Llama-2-70B-chat model, with its worst performance dipping\nas low as 9.38%. We further illustrate the difficulty in identifying the worst\nprompt from both model-agnostic and model-dependent perspectives, emphasizing\nthe absence of a shortcut to characterize the worst prompt. We also attempt to\nenhance the worst prompt performance using existing prompt engineering and\nprompt consistency methods, but find that their impact is limited. These\nfindings underscore the need to create more resilient LLMs that can maintain\nhigh performance across diverse prompts. Data and code are available at\nhttps://github.com/cbwbuaa/On-the-Worst-Prompt- Performance-of-LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The performance of large language models (LLMs) is acutely sensitive to the\nphrasing of prompts, which raises significant concerns about their reliability\nin real-world scenarios. Existing studies often divide prompts into task-level\ninstructions and case-level inputs and primarily focus on evaluating and\nimproving robustness against variations in tasks-level instructions. However,\nthis setup fails to fully address the diversity of real-world user queries and\nassumes the existence of task-specific datasets. To address these limitations,\nwe introduce RobustAlpacaEval, a new benchmark that consists of semantically\nequivalent case-level queries and emphasizes the importance of using the worst\nprompt performance to gauge the lower bound of model performance. Extensive\nexperiments on RobustAlpacaEval with ChatGPT and six open-source LLMs from the\nLlama, Mistral, and Gemma families uncover substantial variability in model\nperformance; for instance, a difference of 45.48% between the worst and best\nperformance for the Llama-2-70B-chat model, with its worst performance dipping\nas low as 9.38%. We further illustrate the difficulty in identifying the worst\nprompt from both model-agnostic and model-dependent perspectives, emphasizing\nthe absence of a shortcut to characterize the worst prompt. We also attempt to\nenhance the worst prompt performance using existing prompt engineering and\nprompt consistency methods, but find that their impact is limited. These\nfindings underscore the need to create more resilient LLMs that can maintain\nhigh performance across diverse prompts. Data and code are available at\nhttps://github.com/cbwbuaa/On-the-Worst-Prompt- Performance-of-LLMs."
                },
                "authors": [
                    {
                        "name": "Bowen Cao"
                    },
                    {
                        "name": "Deng Cai"
                    },
                    {
                        "name": "Zhisong Zhang"
                    },
                    {
                        "name": "Yuexian Zou"
                    },
                    {
                        "name": "Wai Lam"
                    }
                ],
                "author_detail": {
                    "name": "Wai Lam"
                },
                "author": "Wai Lam",
                "arxiv_comment": "Accepted at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10248v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10248v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14753v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14753v2",
                "updated": "2024-10-29T07:03:36Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    7,
                    3,
                    36,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-18T04:04:51Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    4,
                    4,
                    51,
                    4,
                    292,
                    0
                ],
                "title": "Collaboratively adding new knowledge to an LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaboratively adding new knowledge to an LLM"
                },
                "summary": "We address the question of how to successively add new knowledge to an LLM\nwhilst retaining previously-added knowledge. We consider two settings,\nsemi-cooperative and fully-cooperative. Overall, LoRA performs better in most\ncases than full-fine tuning of all parameters when both new knowledge\nacquisition and retention of old, including recent, knowledge are taken into\naccount. In the semi-cooperative setting, where datasets are not available\nafter training, MOE mixing, model merging, and LoRA-based orthogonal subspace\nsequential learning, using a small weight on the orthogonality term, perform\nwell. In the fully-cooperative setting where datasets remain available, joint\ntraining and sequential training with replay are both effective approaches with\nLoRA training generally preferable to full fine-tuning. The codes needed to\nreproduce the results are provided in an open source repository.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the question of how to successively add new knowledge to an LLM\nwhilst retaining previously-added knowledge. We consider two settings,\nsemi-cooperative and fully-cooperative. Overall, LoRA performs better in most\ncases than full-fine tuning of all parameters when both new knowledge\nacquisition and retention of old, including recent, knowledge are taken into\naccount. In the semi-cooperative setting, where datasets are not available\nafter training, MOE mixing, model merging, and LoRA-based orthogonal subspace\nsequential learning, using a small weight on the orthogonality term, perform\nwell. In the fully-cooperative setting where datasets remain available, joint\ntraining and sequential training with replay are both effective approaches with\nLoRA training generally preferable to full fine-tuning. The codes needed to\nreproduce the results are provided in an open source repository."
                },
                "authors": [
                    {
                        "name": "Rhui Dih Lee"
                    },
                    {
                        "name": "Laura Wynter"
                    }
                ],
                "author_detail": {
                    "name": "Laura Wynter"
                },
                "author": "Laura Wynter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14753v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14753v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04411v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04411v2",
                "updated": "2024-10-29T07:02:51Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    7,
                    2,
                    51,
                    1,
                    303,
                    0
                ],
                "published": "2024-07-05T10:51:33Z",
                "published_parsed": [
                    2024,
                    7,
                    5,
                    10,
                    51,
                    33,
                    4,
                    187,
                    0
                ],
                "title": "Waterfall: Framework for Robust and Scalable Text Watermarking and\n  Provenance for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Waterfall: Framework for Robust and Scalable Text Watermarking and\n  Provenance for LLMs"
                },
                "summary": "Protecting intellectual property (IP) of text such as articles and code is\nincreasingly important, especially as sophisticated attacks become possible,\nsuch as paraphrasing by large language models (LLMs) or even unauthorized\ntraining of LLMs on copyrighted text to infringe such IP. However, existing\ntext watermarking methods are not robust enough against such attacks nor\nscalable to millions of users for practical implementation. In this paper, we\npropose Waterfall, the first training-free framework for robust and scalable\ntext watermarking applicable across multiple text types (e.g., articles, code)\nand languages supportable by LLMs, for general text and LLM data provenance.\nWaterfall comprises several key innovations, such as being the first to use LLM\nas paraphrasers for watermarking along with a novel combination of techniques\nthat are surprisingly effective in achieving robust verifiability and\nscalability. We empirically demonstrate that Waterfall achieves significantly\nbetter scalability, robust verifiability, and computational efficiency compared\nto SOTA article-text watermarking methods, and showed how it could be directly\napplied to the watermarking of code. We also demonstrated that Waterfall can be\nused for LLM data provenance, where the watermarks of LLM training data can be\ndetected in LLM output, allowing for detection of unauthorized use of data for\nLLM training and potentially enabling model-centric watermarking of\nopen-sourced LLMs which has been a limitation of existing LLM watermarking\nworks. Our code is available at https://github.com/aoi3142/Waterfall.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Protecting intellectual property (IP) of text such as articles and code is\nincreasingly important, especially as sophisticated attacks become possible,\nsuch as paraphrasing by large language models (LLMs) or even unauthorized\ntraining of LLMs on copyrighted text to infringe such IP. However, existing\ntext watermarking methods are not robust enough against such attacks nor\nscalable to millions of users for practical implementation. In this paper, we\npropose Waterfall, the first training-free framework for robust and scalable\ntext watermarking applicable across multiple text types (e.g., articles, code)\nand languages supportable by LLMs, for general text and LLM data provenance.\nWaterfall comprises several key innovations, such as being the first to use LLM\nas paraphrasers for watermarking along with a novel combination of techniques\nthat are surprisingly effective in achieving robust verifiability and\nscalability. We empirically demonstrate that Waterfall achieves significantly\nbetter scalability, robust verifiability, and computational efficiency compared\nto SOTA article-text watermarking methods, and showed how it could be directly\napplied to the watermarking of code. We also demonstrated that Waterfall can be\nused for LLM data provenance, where the watermarks of LLM training data can be\ndetected in LLM output, allowing for detection of unauthorized use of data for\nLLM training and potentially enabling model-centric watermarking of\nopen-sourced LLMs which has been a limitation of existing LLM watermarking\nworks. Our code is available at https://github.com/aoi3142/Waterfall."
                },
                "authors": [
                    {
                        "name": "Gregory Kang Ruey Lau"
                    },
                    {
                        "name": "Xinyuan Niu"
                    },
                    {
                        "name": "Hieu Dao"
                    },
                    {
                        "name": "Jiangwei Chen"
                    },
                    {
                        "name": "Chuan-Sheng Foo"
                    },
                    {
                        "name": "Bryan Kian Hsiang Low"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Kian Hsiang Low"
                },
                "author": "Bryan Kian Hsiang Low",
                "arxiv_comment": "Accepted to EMNLP 2024 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.04411v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04411v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21791v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21791v1",
                "updated": "2024-10-29T06:54:00Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    6,
                    54,
                    0,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T06:54:00Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    6,
                    54,
                    0,
                    1,
                    303,
                    0
                ],
                "title": "Enhancing Adversarial Attacks through Chain of Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Adversarial Attacks through Chain of Thought"
                },
                "summary": "Large language models (LLMs) have demonstrated impressive performance across\nvarious domains but remain susceptible to safety concerns. Prior research\nindicates that gradient-based adversarial attacks are particularly effective\nagainst aligned LLMs and the chain of thought (CoT) prompting can elicit\ndesired answers through step-by-step reasoning. This paper proposes enhancing\nthe robustness of adversarial attacks on aligned LLMs by integrating CoT\nprompts with the greedy coordinate gradient (GCG) technique. Using CoT triggers\ninstead of affirmative targets stimulates the reasoning abilities of backend\nLLMs, thereby improving the transferability and universality of adversarial\nattacks. We conducted an ablation study comparing our CoT-GCG approach with\nAmazon Web Services auto-cot. Results revealed our approach outperformed both\nthe baseline GCG attack and CoT prompting. Additionally, we used Llama Guard to\nevaluate potentially harmful interactions, providing a more objective risk\nassessment of entire conversations compared to matching outputs to rejection\nphrases. The code of this paper is available at\nhttps://github.com/sujingbo0217/CS222W24-LLM-Attack.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated impressive performance across\nvarious domains but remain susceptible to safety concerns. Prior research\nindicates that gradient-based adversarial attacks are particularly effective\nagainst aligned LLMs and the chain of thought (CoT) prompting can elicit\ndesired answers through step-by-step reasoning. This paper proposes enhancing\nthe robustness of adversarial attacks on aligned LLMs by integrating CoT\nprompts with the greedy coordinate gradient (GCG) technique. Using CoT triggers\ninstead of affirmative targets stimulates the reasoning abilities of backend\nLLMs, thereby improving the transferability and universality of adversarial\nattacks. We conducted an ablation study comparing our CoT-GCG approach with\nAmazon Web Services auto-cot. Results revealed our approach outperformed both\nthe baseline GCG attack and CoT prompting. Additionally, we used Llama Guard to\nevaluate potentially harmful interactions, providing a more objective risk\nassessment of entire conversations compared to matching outputs to rejection\nphrases. The code of this paper is available at\nhttps://github.com/sujingbo0217/CS222W24-LLM-Attack."
                },
                "authors": [
                    {
                        "name": "Jingbo Su"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Su"
                },
                "author": "Jingbo Su",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21791v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21791v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11832v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11832v2",
                "updated": "2024-10-29T06:44:36Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    6,
                    44,
                    36,
                    1,
                    303,
                    0
                ],
                "published": "2024-06-17T17:59:44Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    17,
                    59,
                    44,
                    0,
                    169,
                    0
                ],
                "title": "Unveiling Encoder-Free Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling Encoder-Free Vision-Language Models"
                },
                "summary": "Existing vision-language models (VLMs) mostly rely on vision encoders to\nextract visual features followed by large language models (LLMs) for\nvisual-language tasks. However, the vision encoders set a strong inductive bias\nin abstracting visual representation, e.g., resolution, aspect ratio, and\nsemantic priors, which could impede the flexibility and efficiency of the VLMs.\nTraining pure VLMs that accept the seamless vision and language inputs, i.e.,\nwithout vision encoders, remains challenging and rarely explored. Empirical\nobservations reveal that direct training without encoders results in slow\nconvergence and large performance gaps. In this work, we bridge the gap between\nencoder-based and encoder-free models, and present a simple yet effective\ntraining recipe towards pure VLMs. Specifically, we unveil the key aspects of\ntraining encoder-free VLMs efficiently via thorough experiments: (1) Bridging\nvision-language representation inside one unified decoder; (2) Enhancing visual\nrecognition capability via extra supervision. With these strategies, we launch\nEVE, an encoder-free vision-language model that can be trained and forwarded\nefficiently. Notably, solely utilizing 35M publicly accessible data, EVE can\nimpressively rival the encoder-based VLMs of similar capacities across multiple\nvision-language benchmarks. It significantly outperforms the counterpart\nFuyu-8B with mysterious training procedures and undisclosed training data. We\nbelieve that EVE provides a transparent and efficient route for developing a\npure decoder-only architecture across modalities. Our code and models are\npublicly available at: https://github.com/baaivision/EVE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing vision-language models (VLMs) mostly rely on vision encoders to\nextract visual features followed by large language models (LLMs) for\nvisual-language tasks. However, the vision encoders set a strong inductive bias\nin abstracting visual representation, e.g., resolution, aspect ratio, and\nsemantic priors, which could impede the flexibility and efficiency of the VLMs.\nTraining pure VLMs that accept the seamless vision and language inputs, i.e.,\nwithout vision encoders, remains challenging and rarely explored. Empirical\nobservations reveal that direct training without encoders results in slow\nconvergence and large performance gaps. In this work, we bridge the gap between\nencoder-based and encoder-free models, and present a simple yet effective\ntraining recipe towards pure VLMs. Specifically, we unveil the key aspects of\ntraining encoder-free VLMs efficiently via thorough experiments: (1) Bridging\nvision-language representation inside one unified decoder; (2) Enhancing visual\nrecognition capability via extra supervision. With these strategies, we launch\nEVE, an encoder-free vision-language model that can be trained and forwarded\nefficiently. Notably, solely utilizing 35M publicly accessible data, EVE can\nimpressively rival the encoder-based VLMs of similar capacities across multiple\nvision-language benchmarks. It significantly outperforms the counterpart\nFuyu-8B with mysterious training procedures and undisclosed training data. We\nbelieve that EVE provides a transparent and efficient route for developing a\npure decoder-only architecture across modalities. Our code and models are\npublicly available at: https://github.com/baaivision/EVE."
                },
                "authors": [
                    {
                        "name": "Haiwen Diao"
                    },
                    {
                        "name": "Yufeng Cui"
                    },
                    {
                        "name": "Xiaotong Li"
                    },
                    {
                        "name": "Yueze Wang"
                    },
                    {
                        "name": "Huchuan Lu"
                    },
                    {
                        "name": "Xinlong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinlong Wang"
                },
                "author": "Xinlong Wang",
                "arxiv_comment": "17 pages, 8 figures, Accepted by NeurIPS2024 (spotlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11832v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11832v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21784v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21784v1",
                "updated": "2024-10-29T06:42:27Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    6,
                    42,
                    27,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T06:42:27Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    6,
                    42,
                    27,
                    1,
                    303,
                    0
                ],
                "title": "MARCO: Multi-Agent Real-time Chat Orchestration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARCO: Multi-Agent Real-time Chat Orchestration"
                },
                "summary": "Large language model advancements have enabled the development of multi-agent\nframeworks to tackle complex, real-world problems such as to automate tasks\nthat require interactions with diverse tools, reasoning, and human\ncollaboration. We present MARCO, a Multi-Agent Real-time Chat Orchestration\nframework for automating tasks using LLMs. MARCO addresses key challenges in\nutilizing LLMs for complex, multi-step task execution. It incorporates robust\nguardrails to steer LLM behavior, validate outputs, and recover from errors\nthat stem from inconsistent output formatting, function and parameter\nhallucination, and lack of domain knowledge. Through extensive experiments we\ndemonstrate MARCO's superior performance with 94.48% and 92.74% accuracy on\ntask execution for Digital Restaurant Service Platform conversations and Retail\nconversations datasets respectively along with 44.91% improved latency and\n33.71% cost reduction. We also report effects of guardrails in performance gain\nalong with comparisons of various LLM models, both open-source and proprietary.\nThe modular and generic design of MARCO allows it to be adapted for automating\ntasks across domains and to execute complex usecases through multi-turn\ninteractions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model advancements have enabled the development of multi-agent\nframeworks to tackle complex, real-world problems such as to automate tasks\nthat require interactions with diverse tools, reasoning, and human\ncollaboration. We present MARCO, a Multi-Agent Real-time Chat Orchestration\nframework for automating tasks using LLMs. MARCO addresses key challenges in\nutilizing LLMs for complex, multi-step task execution. It incorporates robust\nguardrails to steer LLM behavior, validate outputs, and recover from errors\nthat stem from inconsistent output formatting, function and parameter\nhallucination, and lack of domain knowledge. Through extensive experiments we\ndemonstrate MARCO's superior performance with 94.48% and 92.74% accuracy on\ntask execution for Digital Restaurant Service Platform conversations and Retail\nconversations datasets respectively along with 44.91% improved latency and\n33.71% cost reduction. We also report effects of guardrails in performance gain\nalong with comparisons of various LLM models, both open-source and proprietary.\nThe modular and generic design of MARCO allows it to be adapted for automating\ntasks across domains and to execute complex usecases through multi-turn\ninteractions."
                },
                "authors": [
                    {
                        "name": "Anubhav Shrimal"
                    },
                    {
                        "name": "Stanley Kanagaraj"
                    },
                    {
                        "name": "Kriti Biswas"
                    },
                    {
                        "name": "Swarnalatha Raghuraman"
                    },
                    {
                        "name": "Anish Nediyanchath"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Promod Yenigalla"
                    }
                ],
                "author_detail": {
                    "name": "Promod Yenigalla"
                },
                "author": "Promod Yenigalla",
                "arxiv_comment": "EMNLP 2024 Industry Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21784v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21784v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21779v1",
                "updated": "2024-10-29T06:38:46Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    6,
                    38,
                    46,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T06:38:46Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    6,
                    38,
                    46,
                    1,
                    303,
                    0
                ],
                "title": "Leveraging LLMs for Hypothetical Deduction in Logical Inference: A\n  Neuro-Symbolic Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging LLMs for Hypothetical Deduction in Logical Inference: A\n  Neuro-Symbolic Approach"
                },
                "summary": "Large Language Models (LLMs) have exhibited remarkable potential across a\nwide array of reasoning tasks, including logical reasoning. Although massive\nefforts have been made to empower the logical reasoning ability of LLMs via\nexternal logical symbolic solvers, crucial challenges of the poor\ngeneralization ability to questions with different features and inevitable\nquestion information loss of symbolic solver-driven approaches remain\nunresolved. To mitigate these issues, we introduce LINA, a LLM-driven\nneuro-symbolic approach for faithful logical reasoning. By enabling an LLM to\nautonomously perform the transition from propositional logic extraction to\nsophisticated logical reasoning, LINA not only bolsters the resilience of the\nreasoning process but also eliminates the dependency on external solvers.\nAdditionally, through its adoption of a hypothetical-deductive reasoning\nparadigm, LINA effectively circumvents the expansive search space challenge\nthat plagues traditional forward reasoning methods. Empirical evaluations\ndemonstrate that LINA substantially outperforms both established propositional\nlogic frameworks and conventional prompting techniques across a spectrum of\nfive logical reasoning tasks. Specifically, LINA achieves an improvement of\n24.34% over LINC on the FOLIO dataset, while also surpassing prompting\nstrategies like CoT and CoT-SC by up to 24.02%. Our code is available at\nhttps://github.com/wufeiwuwoshihua/nshy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited remarkable potential across a\nwide array of reasoning tasks, including logical reasoning. Although massive\nefforts have been made to empower the logical reasoning ability of LLMs via\nexternal logical symbolic solvers, crucial challenges of the poor\ngeneralization ability to questions with different features and inevitable\nquestion information loss of symbolic solver-driven approaches remain\nunresolved. To mitigate these issues, we introduce LINA, a LLM-driven\nneuro-symbolic approach for faithful logical reasoning. By enabling an LLM to\nautonomously perform the transition from propositional logic extraction to\nsophisticated logical reasoning, LINA not only bolsters the resilience of the\nreasoning process but also eliminates the dependency on external solvers.\nAdditionally, through its adoption of a hypothetical-deductive reasoning\nparadigm, LINA effectively circumvents the expansive search space challenge\nthat plagues traditional forward reasoning methods. Empirical evaluations\ndemonstrate that LINA substantially outperforms both established propositional\nlogic frameworks and conventional prompting techniques across a spectrum of\nfive logical reasoning tasks. Specifically, LINA achieves an improvement of\n24.34% over LINC on the FOLIO dataset, while also surpassing prompting\nstrategies like CoT and CoT-SC by up to 24.02%. Our code is available at\nhttps://github.com/wufeiwuwoshihua/nshy."
                },
                "authors": [
                    {
                        "name": "Qingchuan Li"
                    },
                    {
                        "name": "Jiatong Li"
                    },
                    {
                        "name": "Tongxuan Liu"
                    },
                    {
                        "name": "Yuting Zeng"
                    },
                    {
                        "name": "Mingyue Cheng"
                    },
                    {
                        "name": "Weizhe Huang"
                    },
                    {
                        "name": "Qi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Qi Liu"
                },
                "author": "Qi Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15145v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15145v2",
                "updated": "2024-10-29T06:14:47Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    6,
                    14,
                    47,
                    1,
                    303,
                    0
                ],
                "published": "2024-05-24T01:49:02Z",
                "published_parsed": [
                    2024,
                    5,
                    24,
                    1,
                    49,
                    2,
                    4,
                    145,
                    0
                ],
                "title": "CulturePark: Boosting Cross-cultural Understanding in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CulturePark: Boosting Cross-cultural Understanding in Large Language\n  Models"
                },
                "summary": "Cultural bias is pervasive in many large language models (LLMs), largely due\nto the deficiency of data representative of different cultures. Typically,\ncultural datasets and benchmarks are constructed either by extracting subsets\nof existing datasets or by aggregating from platforms such as Wikipedia and\nsocial media. However, these approaches are highly dependent on real-world data\nand human annotations, making them costly and difficult to scale. Inspired by\ncognitive theories on social communication, this paper introduces CulturePark,\nan LLM-powered multi-agent communication framework for cultural data\ncollection. CulturePark simulates cross-cultural human communication with\nLLM-based agents playing roles in different cultures. It generates high-quality\ncross-cultural dialogues encapsulating human beliefs, norms, and customs. Using\nCulturePark, we generated 41,000 cultural samples to fine-tune eight\nculture-specific LLMs. We evaluated these models across three downstream tasks:\ncontent moderation, cultural alignment, and cultural education. Results show\nthat for content moderation, our GPT-3.5-based models either match or\noutperform GPT-4 on datasets. Regarding cultural alignment, our models surpass\nGPT-4 on Hofstede's VSM 13 framework. Furthermore, for cultural education of\nhuman participants, our models demonstrate superior outcomes in both learning\nefficacy and user experience compared to GPT-4. CulturePark proves an important\nstep in addressing cultural bias and advancing the democratization of AI,\nhighlighting the critical role of culturally inclusive data in model training.\nCode is released at https://github.com/Scarelette/CulturePark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cultural bias is pervasive in many large language models (LLMs), largely due\nto the deficiency of data representative of different cultures. Typically,\ncultural datasets and benchmarks are constructed either by extracting subsets\nof existing datasets or by aggregating from platforms such as Wikipedia and\nsocial media. However, these approaches are highly dependent on real-world data\nand human annotations, making them costly and difficult to scale. Inspired by\ncognitive theories on social communication, this paper introduces CulturePark,\nan LLM-powered multi-agent communication framework for cultural data\ncollection. CulturePark simulates cross-cultural human communication with\nLLM-based agents playing roles in different cultures. It generates high-quality\ncross-cultural dialogues encapsulating human beliefs, norms, and customs. Using\nCulturePark, we generated 41,000 cultural samples to fine-tune eight\nculture-specific LLMs. We evaluated these models across three downstream tasks:\ncontent moderation, cultural alignment, and cultural education. Results show\nthat for content moderation, our GPT-3.5-based models either match or\noutperform GPT-4 on datasets. Regarding cultural alignment, our models surpass\nGPT-4 on Hofstede's VSM 13 framework. Furthermore, for cultural education of\nhuman participants, our models demonstrate superior outcomes in both learning\nefficacy and user experience compared to GPT-4. CulturePark proves an important\nstep in addressing cultural bias and advancing the democratization of AI,\nhighlighting the critical role of culturally inclusive data in model training.\nCode is released at https://github.com/Scarelette/CulturePark."
                },
                "authors": [
                    {
                        "name": "Cheng Li"
                    },
                    {
                        "name": "Damien Teney"
                    },
                    {
                        "name": "Linyi Yang"
                    },
                    {
                        "name": "Qingsong Wen"
                    },
                    {
                        "name": "Xing Xie"
                    },
                    {
                        "name": "Jindong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jindong Wang"
                },
                "author": "Jindong Wang",
                "arxiv_comment": "NeurIPS 2024; Code is released at\n  https://github.com/Scarelette/CulturePark. arXiv admin note: substantial text\n  overlap with arXiv:2402.10946",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15145v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15145v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21771v1",
                "updated": "2024-10-29T06:13:38Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    6,
                    13,
                    38,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T06:13:38Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    6,
                    13,
                    38,
                    1,
                    303,
                    0
                ],
                "title": "Why is it so hard to find a job now? Enter Ghost Jobs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why is it so hard to find a job now? Enter Ghost Jobs"
                },
                "summary": "This study investigates the emerging phenomenon of \"ghost hiring\" or \"ghost\njobs\", where employers advertise job openings without intending to fill them.\nUsing a novel dataset from Glassdoor and employing a LLM-BERT technique, I find\nthat up to 21% of job ads may be ghost jobs, and this is particularly prevalent\nin specialized industries and in larger firms. The trend could be due to the\nlow marginal cost of posting additional job ads and to maintain a pipeline of\ntalents. After adjusting for yearly trends, I find that ghost jobs can explain\nthe recent disconnect in the Beveridge Curve in the past fifteen years. The\nresults show that policy-makers should be aware of such a practice as it causes\nsignificant job fatigue and distorts market signals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates the emerging phenomenon of \"ghost hiring\" or \"ghost\njobs\", where employers advertise job openings without intending to fill them.\nUsing a novel dataset from Glassdoor and employing a LLM-BERT technique, I find\nthat up to 21% of job ads may be ghost jobs, and this is particularly prevalent\nin specialized industries and in larger firms. The trend could be due to the\nlow marginal cost of posting additional job ads and to maintain a pipeline of\ntalents. After adjusting for yearly trends, I find that ghost jobs can explain\nthe recent disconnect in the Beveridge Curve in the past fifteen years. The\nresults show that policy-makers should be aware of such a practice as it causes\nsignificant job fatigue and distorts market signals."
                },
                "authors": [
                    {
                        "name": "Hunter Ng"
                    }
                ],
                "author_detail": {
                    "name": "Hunter Ng"
                },
                "author": "Hunter Ng",
                "arxiv_comment": "17 pages main text",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "91B82, 91B84, 68T50, 62P25",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.4; H.3.3; I.2.7; I.5.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.10946v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.10946v2",
                "updated": "2024-10-29T06:11:44Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    6,
                    11,
                    44,
                    1,
                    303,
                    0
                ],
                "published": "2024-02-09T04:02:43Z",
                "published_parsed": [
                    2024,
                    2,
                    9,
                    4,
                    2,
                    43,
                    4,
                    40,
                    0
                ],
                "title": "CultureLLM: Incorporating Cultural Differences into Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CultureLLM: Incorporating Cultural Differences into Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) are reported to be partial to certain cultures\nowing to the training data dominance from the English corpora. Since\nmultilingual cultural data are often expensive to collect, existing efforts\nhandle this by prompt engineering or culture-specific pre-training. However,\nthey might overlook the knowledge deficiency of low-resource culture and\nrequire extensive computing resources. In this paper, we propose CultureLLM, a\ncost-effective solution to incorporate cultural differences into LLMs.\nCultureLLM adopts World Value Survey (WVS) as seed data and generates\nsemantically equivalent training data via the proposed semantic data\naugmentation. Using only 50 seed samples from WVS with augmented data, we\nfine-tune culture-specific LLMs and one unified model (CultureLLM-One) for 9\ncultures covering rich and low-resource languages. Extensive experiments on 60\nculture-related datasets demonstrate that CultureLLM significantly outperforms\nvarious counterparts such as GPT-3.5 (by 8.1%) and Gemini Pro (by 9.5%) with\ncomparable performance to GPT-4 or even better. Our human study shows that the\ngenerated samples are semantically equivalent to the original samples,\nproviding an effective solution for LLMs augmentation. Code is released at\nhttps://github.com/Scarelette/CultureLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are reported to be partial to certain cultures\nowing to the training data dominance from the English corpora. Since\nmultilingual cultural data are often expensive to collect, existing efforts\nhandle this by prompt engineering or culture-specific pre-training. However,\nthey might overlook the knowledge deficiency of low-resource culture and\nrequire extensive computing resources. In this paper, we propose CultureLLM, a\ncost-effective solution to incorporate cultural differences into LLMs.\nCultureLLM adopts World Value Survey (WVS) as seed data and generates\nsemantically equivalent training data via the proposed semantic data\naugmentation. Using only 50 seed samples from WVS with augmented data, we\nfine-tune culture-specific LLMs and one unified model (CultureLLM-One) for 9\ncultures covering rich and low-resource languages. Extensive experiments on 60\nculture-related datasets demonstrate that CultureLLM significantly outperforms\nvarious counterparts such as GPT-3.5 (by 8.1%) and Gemini Pro (by 9.5%) with\ncomparable performance to GPT-4 or even better. Our human study shows that the\ngenerated samples are semantically equivalent to the original samples,\nproviding an effective solution for LLMs augmentation. Code is released at\nhttps://github.com/Scarelette/CultureLLM."
                },
                "authors": [
                    {
                        "name": "Cheng Li"
                    },
                    {
                        "name": "Mengzhou Chen"
                    },
                    {
                        "name": "Jindong Wang"
                    },
                    {
                        "name": "Sunayana Sitaram"
                    },
                    {
                        "name": "Xing Xie"
                    }
                ],
                "author_detail": {
                    "name": "Xing Xie"
                },
                "author": "Xing Xie",
                "arxiv_comment": "NeurIPS 2024; Code is at https://github.com/Scarelette/CultureLLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.10946v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.10946v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.01109v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.01109v5",
                "updated": "2024-10-29T06:06:30Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    6,
                    6,
                    30,
                    1,
                    303,
                    0
                ],
                "published": "2024-02-02T02:56:50Z",
                "published_parsed": [
                    2024,
                    2,
                    2,
                    2,
                    56,
                    50,
                    4,
                    33,
                    0
                ],
                "title": "Vaccine: Perturbation-aware Alignment for Large Language Models against\n  Harmful Fine-tuning Attack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vaccine: Perturbation-aware Alignment for Large Language Models against\n  Harmful Fine-tuning Attack"
                },
                "summary": "The new paradigm of finetuning-as-a-service introduces a new attack surface\nfor Large Language Models (LLMs): a few harmful data uploaded by users can\neasily trick the finetuning to produce an alignment-broken model. We conduct an\nempirical analysis and uncover a \\textit{harmful embedding drift} phenomenon,\nshowing a probable cause of the alignment-broken effect. Inspired by our\nfindings, we propose Vaccine, a perturbation-aware alignment technique to\nmitigate the security risk of users finetuning. The core idea of Vaccine is to\nproduce invariant hidden embeddings by progressively adding crafted\nperturbation to them in the alignment phase. This enables the embeddings to\nwithstand harmful perturbation from un-sanitized user data in the finetuning\nphase. Our results on open source mainstream LLMs (e.g., Llama2, Opt, Vicuna)\ndemonstrate that Vaccine can boost the robustness of alignment against harmful\nprompts induced embedding drift while reserving reasoning ability towards\nbenign prompts. Our code is available at\n\\url{https://github.com/git-disl/Vaccine}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The new paradigm of finetuning-as-a-service introduces a new attack surface\nfor Large Language Models (LLMs): a few harmful data uploaded by users can\neasily trick the finetuning to produce an alignment-broken model. We conduct an\nempirical analysis and uncover a \\textit{harmful embedding drift} phenomenon,\nshowing a probable cause of the alignment-broken effect. Inspired by our\nfindings, we propose Vaccine, a perturbation-aware alignment technique to\nmitigate the security risk of users finetuning. The core idea of Vaccine is to\nproduce invariant hidden embeddings by progressively adding crafted\nperturbation to them in the alignment phase. This enables the embeddings to\nwithstand harmful perturbation from un-sanitized user data in the finetuning\nphase. Our results on open source mainstream LLMs (e.g., Llama2, Opt, Vicuna)\ndemonstrate that Vaccine can boost the robustness of alignment against harmful\nprompts induced embedding drift while reserving reasoning ability towards\nbenign prompts. Our code is available at\n\\url{https://github.com/git-disl/Vaccine}."
                },
                "authors": [
                    {
                        "name": "Tiansheng Huang"
                    },
                    {
                        "name": "Sihao Hu"
                    },
                    {
                        "name": "Ling Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ling Liu"
                },
                "author": "Ling Liu",
                "arxiv_comment": "Rejected by ICML2024. Accepted by NeurIPS2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.01109v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.01109v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14546v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14546v2",
                "updated": "2024-10-29T05:58:08Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    5,
                    58,
                    8,
                    1,
                    303,
                    0
                ],
                "published": "2024-06-20T17:55:04Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    17,
                    55,
                    4,
                    3,
                    172,
                    0
                ],
                "title": "Connecting the Dots: LLMs can Infer and Verbalize Latent Structure from\n  Disparate Training Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Connecting the Dots: LLMs can Infer and Verbalize Latent Structure from\n  Disparate Training Data"
                },
                "summary": "One way to address safety risks from large language models (LLMs) is to\ncensor dangerous knowledge from their training data. While this removes the\nexplicit information, implicit information can remain scattered across various\ntraining documents. Could an LLM infer the censored knowledge by piecing\ntogether these implicit hints? As a step towards answering this question, we\nstudy inductive out-of-context reasoning (OOCR), a type of generalization in\nwhich LLMs infer latent information from evidence distributed across training\ndocuments and apply it to downstream tasks without in-context learning. Using a\nsuite of five tasks, we demonstrate that frontier LLMs can perform inductive\nOOCR. In one experiment we finetune an LLM on a corpus consisting only of\ndistances between an unknown city and other known cities. Remarkably, without\nin-context examples or Chain of Thought, the LLM can verbalize that the unknown\ncity is Paris and use this fact to answer downstream questions. Further\nexperiments show that LLMs trained only on individual coin flip outcomes can\nverbalize whether the coin is biased, and those trained only on pairs (x, f\n(x)) can articulate a definition of f and compute inverses. While OOCR succeeds\nin a range of cases, we also show that it is unreliable, particularly for\nsmaller LLMs learning complex structures. Overall, the ability of LLMs to\n\"connect the dots\" without explicit in-context learning poses a potential\nobstacle to monitoring and controlling the knowledge acquired by LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One way to address safety risks from large language models (LLMs) is to\ncensor dangerous knowledge from their training data. While this removes the\nexplicit information, implicit information can remain scattered across various\ntraining documents. Could an LLM infer the censored knowledge by piecing\ntogether these implicit hints? As a step towards answering this question, we\nstudy inductive out-of-context reasoning (OOCR), a type of generalization in\nwhich LLMs infer latent information from evidence distributed across training\ndocuments and apply it to downstream tasks without in-context learning. Using a\nsuite of five tasks, we demonstrate that frontier LLMs can perform inductive\nOOCR. In one experiment we finetune an LLM on a corpus consisting only of\ndistances between an unknown city and other known cities. Remarkably, without\nin-context examples or Chain of Thought, the LLM can verbalize that the unknown\ncity is Paris and use this fact to answer downstream questions. Further\nexperiments show that LLMs trained only on individual coin flip outcomes can\nverbalize whether the coin is biased, and those trained only on pairs (x, f\n(x)) can articulate a definition of f and compute inverses. While OOCR succeeds\nin a range of cases, we also show that it is unreliable, particularly for\nsmaller LLMs learning complex structures. Overall, the ability of LLMs to\n\"connect the dots\" without explicit in-context learning poses a potential\nobstacle to monitoring and controlling the knowledge acquired by LLMs."
                },
                "authors": [
                    {
                        "name": "Johannes Treutlein"
                    },
                    {
                        "name": "Dami Choi"
                    },
                    {
                        "name": "Jan Betley"
                    },
                    {
                        "name": "Samuel Marks"
                    },
                    {
                        "name": "Cem Anil"
                    },
                    {
                        "name": "Roger Grosse"
                    },
                    {
                        "name": "Owain Evans"
                    }
                ],
                "author_detail": {
                    "name": "Owain Evans"
                },
                "author": "Owain Evans",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14546v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14546v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18641v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18641v5",
                "updated": "2024-10-29T05:46:55Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    5,
                    46,
                    55,
                    1,
                    303,
                    0
                ],
                "published": "2024-05-28T22:53:43Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    22,
                    53,
                    43,
                    1,
                    149,
                    0
                ],
                "title": "Lisa: Lazy Safety Alignment for Large Language Models against Harmful\n  Fine-tuning Attack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lisa: Lazy Safety Alignment for Large Language Models against Harmful\n  Fine-tuning Attack"
                },
                "summary": "Recent studies show that Large Language Models (LLMs) with safety alignment\ncan be jail-broken by fine-tuning on a dataset mixed with harmful data. First\ntime in the literature, we show that the jail-broken effect can be mitigated by\nseparating states in the finetuning stage to optimize the alignment and user\ndatasets. Unfortunately, our subsequent study shows that this simple Bi-State\nOptimization (BSO) solution experiences convergence instability when steps\ninvested in its alignment state is too small, leading to downgraded alignment\nperformance. By statistical analysis, we show that the \\textit{excess drift}\ntowards consensus could be a probable reason for the instability. To remedy\nthis issue, we propose \\textbf{L}azy(\\textbf{i}) \\textbf{s}afety\n\\textbf{a}lignment (\\textbf{Lisa}), which introduces a proximal term to\nconstraint the drift of each state. Theoretically, the benefit of the proximal\nterm is supported by the convergence analysis, wherein we show that a\nsufficient large proximal factor is necessary to guarantee Lisa's convergence.\nEmpirically, our results on four downstream finetuning tasks show that Lisa\nwith a proximal term can significantly increase alignment performance while\nmaintaining the LLM's accuracy on the user tasks. Code is available at\n\\url{https://github.com/git-disl/Lisa}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies show that Large Language Models (LLMs) with safety alignment\ncan be jail-broken by fine-tuning on a dataset mixed with harmful data. First\ntime in the literature, we show that the jail-broken effect can be mitigated by\nseparating states in the finetuning stage to optimize the alignment and user\ndatasets. Unfortunately, our subsequent study shows that this simple Bi-State\nOptimization (BSO) solution experiences convergence instability when steps\ninvested in its alignment state is too small, leading to downgraded alignment\nperformance. By statistical analysis, we show that the \\textit{excess drift}\ntowards consensus could be a probable reason for the instability. To remedy\nthis issue, we propose \\textbf{L}azy(\\textbf{i}) \\textbf{s}afety\n\\textbf{a}lignment (\\textbf{Lisa}), which introduces a proximal term to\nconstraint the drift of each state. Theoretically, the benefit of the proximal\nterm is supported by the convergence analysis, wherein we show that a\nsufficient large proximal factor is necessary to guarantee Lisa's convergence.\nEmpirically, our results on four downstream finetuning tasks show that Lisa\nwith a proximal term can significantly increase alignment performance while\nmaintaining the LLM's accuracy on the user tasks. Code is available at\n\\url{https://github.com/git-disl/Lisa}."
                },
                "authors": [
                    {
                        "name": "Tiansheng Huang"
                    },
                    {
                        "name": "Sihao Hu"
                    },
                    {
                        "name": "Fatih Ilhan"
                    },
                    {
                        "name": "Selim Furkan Tekin"
                    },
                    {
                        "name": "Ling Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ling Liu"
                },
                "author": "Ling Liu",
                "arxiv_comment": "Accepted by NeurIPS2024. arXiv admin note: substantial text overlap\n  with arXiv:2402.01109",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18641v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18641v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21747v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21747v1",
                "updated": "2024-10-29T05:25:34Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    5,
                    25,
                    34,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T05:25:34Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    5,
                    25,
                    34,
                    1,
                    303,
                    0
                ],
                "title": "MotionGPT-2: A General-Purpose Motion-Language Model for Motion\n  Generation and Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MotionGPT-2: A General-Purpose Motion-Language Model for Motion\n  Generation and Understanding"
                },
                "summary": "Generating lifelike human motions from descriptive texts has experienced\nremarkable research focus in the recent years, propelled by the emerging\nrequirements of digital humans.Despite impressive advances, existing approaches\nare often constrained by limited control modalities, task specificity, and\nfocus solely on body motion representations.In this paper, we present\nMotionGPT-2, a unified Large Motion-Language Model (LMLM) that addresses these\nlimitations. MotionGPT-2 accommodates multiple motion-relevant tasks and\nsupporting multimodal control conditions through pre-trained Large Language\nModels (LLMs). It quantizes multimodal inputs-such as text and single-frame\nposes-into discrete, LLM-interpretable tokens, seamlessly integrating them into\nthe LLM's vocabulary. These tokens are then organized into unified prompts,\nguiding the LLM to generate motion outputs through a\npretraining-then-finetuning paradigm. We also show that the proposed\nMotionGPT-2 is highly adaptable to the challenging 3D holistic motion\ngeneration task, enabled by the innovative motion discretization framework,\nPart-Aware VQVAE, which ensures fine-grained representations of body and hand\nmovements. Extensive experiments and visualizations validate the effectiveness\nof our method, demonstrating the adaptability of MotionGPT-2 across motion\ngeneration, motion captioning, and generalized motion completion tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating lifelike human motions from descriptive texts has experienced\nremarkable research focus in the recent years, propelled by the emerging\nrequirements of digital humans.Despite impressive advances, existing approaches\nare often constrained by limited control modalities, task specificity, and\nfocus solely on body motion representations.In this paper, we present\nMotionGPT-2, a unified Large Motion-Language Model (LMLM) that addresses these\nlimitations. MotionGPT-2 accommodates multiple motion-relevant tasks and\nsupporting multimodal control conditions through pre-trained Large Language\nModels (LLMs). It quantizes multimodal inputs-such as text and single-frame\nposes-into discrete, LLM-interpretable tokens, seamlessly integrating them into\nthe LLM's vocabulary. These tokens are then organized into unified prompts,\nguiding the LLM to generate motion outputs through a\npretraining-then-finetuning paradigm. We also show that the proposed\nMotionGPT-2 is highly adaptable to the challenging 3D holistic motion\ngeneration task, enabled by the innovative motion discretization framework,\nPart-Aware VQVAE, which ensures fine-grained representations of body and hand\nmovements. Extensive experiments and visualizations validate the effectiveness\nof our method, demonstrating the adaptability of MotionGPT-2 across motion\ngeneration, motion captioning, and generalized motion completion tasks."
                },
                "authors": [
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "Di Huang"
                    },
                    {
                        "name": "Yaqi Zhang"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Jile Jiao"
                    },
                    {
                        "name": "Xuetao Feng"
                    },
                    {
                        "name": "Yan Zhou"
                    },
                    {
                        "name": "Pengfei Wan"
                    },
                    {
                        "name": "Shixiang Tang"
                    },
                    {
                        "name": "Dan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Dan Xu"
                },
                "author": "Dan Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21747v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21747v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08970v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08970v2",
                "updated": "2024-10-29T05:23:53Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    5,
                    23,
                    53,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-11T16:40:03Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    16,
                    40,
                    3,
                    4,
                    285,
                    0
                ],
                "title": "NoVo: Norm Voting off Hallucinations with Attention Heads in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NoVo: Norm Voting off Hallucinations with Attention Heads in Large\n  Language Models"
                },
                "summary": "Hallucinations in Large Language Models (LLMs) remain a major obstacle,\nparticularly in high-stakes applications where factual accuracy is critical.\nWhile representation editing and reading methods have made strides in reducing\nhallucinations, their heavy reliance on specialised tools and training on\nin-domain samples, makes them difficult to scale and prone to overfitting. This\nlimits their accuracy gains and generalizability to diverse datasets. This\npaper presents a lightweight method, Norm Voting (NoVo), which harnesses the\nuntapped potential of attention head norms to dramatically enhance factual\naccuracy in zero-shot multiple-choice questions (MCQs). NoVo begins by\nautomatically selecting truth-correlated head norms with an efficient,\ninference-only algorithm using only 30 random samples, allowing NoVo to\neffortlessly scale to diverse datasets. Afterwards, selected head norms are\nemployed in a simple voting algorithm, which yields significant gains in\nprediction accuracy. On TruthfulQA MC1, NoVo surpasses the current\nstate-of-the-art and all previous methods by an astounding margin -- at least\n19 accuracy points. NoVo demonstrates exceptional generalization to 20 diverse\ndatasets, with significant gains in over 90\\% of them, far exceeding all\ncurrent representation editing and reading methods. NoVo also reveals promising\ngains to finetuning strategies and building textual adversarial defence. NoVo's\neffectiveness with head norms opens new frontiers in LLM interpretability,\nrobustness and reliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucinations in Large Language Models (LLMs) remain a major obstacle,\nparticularly in high-stakes applications where factual accuracy is critical.\nWhile representation editing and reading methods have made strides in reducing\nhallucinations, their heavy reliance on specialised tools and training on\nin-domain samples, makes them difficult to scale and prone to overfitting. This\nlimits their accuracy gains and generalizability to diverse datasets. This\npaper presents a lightweight method, Norm Voting (NoVo), which harnesses the\nuntapped potential of attention head norms to dramatically enhance factual\naccuracy in zero-shot multiple-choice questions (MCQs). NoVo begins by\nautomatically selecting truth-correlated head norms with an efficient,\ninference-only algorithm using only 30 random samples, allowing NoVo to\neffortlessly scale to diverse datasets. Afterwards, selected head norms are\nemployed in a simple voting algorithm, which yields significant gains in\nprediction accuracy. On TruthfulQA MC1, NoVo surpasses the current\nstate-of-the-art and all previous methods by an astounding margin -- at least\n19 accuracy points. NoVo demonstrates exceptional generalization to 20 diverse\ndatasets, with significant gains in over 90\\% of them, far exceeding all\ncurrent representation editing and reading methods. NoVo also reveals promising\ngains to finetuning strategies and building textual adversarial defence. NoVo's\neffectiveness with head norms opens new frontiers in LLM interpretability,\nrobustness and reliability."
                },
                "authors": [
                    {
                        "name": "Zheng Yi Ho"
                    },
                    {
                        "name": "Siyuan Liang"
                    },
                    {
                        "name": "Sen Zhang"
                    },
                    {
                        "name": "Yibing Zhan"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08970v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08970v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15741v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15741v3",
                "updated": "2024-10-29T05:15:09Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    5,
                    15,
                    9,
                    1,
                    303,
                    0
                ],
                "published": "2024-06-22T05:33:35Z",
                "published_parsed": [
                    2024,
                    6,
                    22,
                    5,
                    33,
                    35,
                    5,
                    174,
                    0
                ],
                "title": "Ladder: A Model-Agnostic Framework Boosting LLM-based Machine\n  Translation to the Next Level",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ladder: A Model-Agnostic Framework Boosting LLM-based Machine\n  Translation to the Next Level"
                },
                "summary": "General-purpose Large Language Models (LLMs) like GPT-4 have achieved\nremarkable advancements in machine translation (MT) by leveraging extensive web\ncontent. On the other hand, translation-specific LLMs are built by pre-training\non domain-specific monolingual corpora and fine-tuning with human-annotated\ntranslation data. Despite the superior performance, these methods either demand\nan unprecedented scale of computing and data or substantial human editing and\nannotation efforts. In this paper, we develop MT-Ladder, a novel model-agnostic\nand cost-effective tool to refine the performance of general LLMs for MT.\nMT-Ladder is trained on pseudo-refinement triplets which can be easily obtained\nfrom existing LLMs without additional human cost. During training, we propose a\nhierarchical fine-tuning strategy with an easy-to-hard schema, improving\nMT-Ladder's refining performance progressively. The trained MT-Ladder can be\nseamlessly integrated with any general-purpose LLMs to boost their translation\nperformance. By utilizing Gemma-2B/7B as the backbone, MT-Ladder-2B can elevate\nraw translations to the level of top-tier open-source models (e.g., refining\nBigTranslate-13B with +6.91 BLEU and +3.52 COMET for XX-En), and MT-Ladder-7B\ncan further enhance model performance to be on par with the state-of-the-art\nGPT-4. Extensive ablation and analysis corroborate the effectiveness of\nMT-Ladder in diverse settings. Our code is available at\nhttps://github.com/fzp0424/MT-Ladder",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General-purpose Large Language Models (LLMs) like GPT-4 have achieved\nremarkable advancements in machine translation (MT) by leveraging extensive web\ncontent. On the other hand, translation-specific LLMs are built by pre-training\non domain-specific monolingual corpora and fine-tuning with human-annotated\ntranslation data. Despite the superior performance, these methods either demand\nan unprecedented scale of computing and data or substantial human editing and\nannotation efforts. In this paper, we develop MT-Ladder, a novel model-agnostic\nand cost-effective tool to refine the performance of general LLMs for MT.\nMT-Ladder is trained on pseudo-refinement triplets which can be easily obtained\nfrom existing LLMs without additional human cost. During training, we propose a\nhierarchical fine-tuning strategy with an easy-to-hard schema, improving\nMT-Ladder's refining performance progressively. The trained MT-Ladder can be\nseamlessly integrated with any general-purpose LLMs to boost their translation\nperformance. By utilizing Gemma-2B/7B as the backbone, MT-Ladder-2B can elevate\nraw translations to the level of top-tier open-source models (e.g., refining\nBigTranslate-13B with +6.91 BLEU and +3.52 COMET for XX-En), and MT-Ladder-7B\ncan further enhance model performance to be on par with the state-of-the-art\nGPT-4. Extensive ablation and analysis corroborate the effectiveness of\nMT-Ladder in diverse settings. Our code is available at\nhttps://github.com/fzp0424/MT-Ladder"
                },
                "authors": [
                    {
                        "name": "Zhaopeng Feng"
                    },
                    {
                        "name": "Ruizhe Chen"
                    },
                    {
                        "name": "Yan Zhang"
                    },
                    {
                        "name": "Zijie Meng"
                    },
                    {
                        "name": "Zuozhu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zuozhu Liu"
                },
                "author": "Zuozhu Liu",
                "arxiv_comment": "EMNLP 2024 Main. Data and code are available at\n  https://github.com/fzp0424/MT-Ladder",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15741v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15741v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21741v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21741v1",
                "updated": "2024-10-29T04:58:07Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    4,
                    58,
                    7,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T04:58:07Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    4,
                    58,
                    7,
                    1,
                    303,
                    0
                ],
                "title": "Enhancing Financial Question Answering with a Multi-Agent Reflection\n  Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Financial Question Answering with a Multi-Agent Reflection\n  Framework"
                },
                "summary": "While Large Language Models (LLMs) have shown impressive capabilities in\nnumerous Natural Language Processing (NLP) tasks, they still struggle with\nfinancial question answering (QA), particularly when numerical reasoning is\nrequired. Recently, LLM-based multi-agent frameworks have demonstrated\nremarkable effectiveness in multi-step reasoning, which is crucial for\nfinancial QA tasks as it involves extracting relevant information from tables\nand text and then performing numerical reasoning on the extracted data to infer\nanswers. In this study, we propose a multi-agent framework incorporating a\ncritic agent that reflects on the reasoning steps and final answers for each\nquestion. Additionally, we enhance our system by adding multiple critic agents,\neach focusing on a specific aspect of the answer. Our results indicate that\nthis framework significantly improves performance compared to single-agent\nreasoning, with an average performance increase of 15% for the LLaMA3-8B model\nand 5% for the LLaMA3-70B model. Furthermore, our framework performs on par\nwith, and in some cases surpasses, larger single-agent LLMs such as\nLLaMA3.1-405B and GPT-4o-mini, though it falls slightly short compared to\nClaude-3.5 Sonnet. Overall, our framework presents an effective solution to\nenhance open-source LLMs for financial QA tasks, offering a cost-effective\nalternative to larger models like Claude-3.5 Sonnet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Large Language Models (LLMs) have shown impressive capabilities in\nnumerous Natural Language Processing (NLP) tasks, they still struggle with\nfinancial question answering (QA), particularly when numerical reasoning is\nrequired. Recently, LLM-based multi-agent frameworks have demonstrated\nremarkable effectiveness in multi-step reasoning, which is crucial for\nfinancial QA tasks as it involves extracting relevant information from tables\nand text and then performing numerical reasoning on the extracted data to infer\nanswers. In this study, we propose a multi-agent framework incorporating a\ncritic agent that reflects on the reasoning steps and final answers for each\nquestion. Additionally, we enhance our system by adding multiple critic agents,\neach focusing on a specific aspect of the answer. Our results indicate that\nthis framework significantly improves performance compared to single-agent\nreasoning, with an average performance increase of 15% for the LLaMA3-8B model\nand 5% for the LLaMA3-70B model. Furthermore, our framework performs on par\nwith, and in some cases surpasses, larger single-agent LLMs such as\nLLaMA3.1-405B and GPT-4o-mini, though it falls slightly short compared to\nClaude-3.5 Sonnet. Overall, our framework presents an effective solution to\nenhance open-source LLMs for financial QA tasks, offering a cost-effective\nalternative to larger models like Claude-3.5 Sonnet."
                },
                "authors": [
                    {
                        "name": "Sorouralsadat Fatemi"
                    },
                    {
                        "name": "Yuheng Hu"
                    }
                ],
                "author_detail": {
                    "name": "Yuheng Hu"
                },
                "author": "Yuheng Hu",
                "arxiv_doi": "10.1145/3677052.3698686",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3677052.3698686",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.21741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21741v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by ICAIF 24",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21728v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21728v1",
                "updated": "2024-10-29T04:28:49Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    4,
                    28,
                    49,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T04:28:49Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    4,
                    28,
                    49,
                    1,
                    303,
                    0
                ],
                "title": "Let's Be Self-generated via Step by Step: A Curriculum Learning Approach\n  to Automated Reasoning with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let's Be Self-generated via Step by Step: A Curriculum Learning Approach\n  to Automated Reasoning with Large Language Models"
                },
                "summary": "While Chain of Thought (CoT) prompting approaches have significantly\nconsolidated the reasoning capabilities of large language models (LLMs), they\nstill face limitations that require extensive human effort or have performance\nneeds to be improved. Existing endeavors have focused on bridging these gaps;\nhowever, these approaches either hinge on external data and cannot completely\neliminate manual effort, or they fall short in effectively directing LLMs to\ngenerate high-quality exemplary prompts. To address the said pitfalls, we\npropose a novel prompt approach for automatic reasoning named \\textbf{LBS3},\ninspired by curriculum learning which better reflects human learning habits.\nSpecifically, LBS3 initially steers LLMs to recall easy-to-hard proxy queries\nthat are pertinent to the target query. Following this, it invokes a\nprogressive strategy that utilizes exemplary prompts stemmed from easy-proxy\nqueries to direct LLMs in solving hard-proxy queries, enabling the high-quality\nof the proxy solutions. Finally, our extensive experiments in various\nreasoning-intensive tasks with varying open- and closed-source LLMs show that\nLBS3 achieves strongly competitive performance compared to the SOTA baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Chain of Thought (CoT) prompting approaches have significantly\nconsolidated the reasoning capabilities of large language models (LLMs), they\nstill face limitations that require extensive human effort or have performance\nneeds to be improved. Existing endeavors have focused on bridging these gaps;\nhowever, these approaches either hinge on external data and cannot completely\neliminate manual effort, or they fall short in effectively directing LLMs to\ngenerate high-quality exemplary prompts. To address the said pitfalls, we\npropose a novel prompt approach for automatic reasoning named \\textbf{LBS3},\ninspired by curriculum learning which better reflects human learning habits.\nSpecifically, LBS3 initially steers LLMs to recall easy-to-hard proxy queries\nthat are pertinent to the target query. Following this, it invokes a\nprogressive strategy that utilizes exemplary prompts stemmed from easy-proxy\nqueries to direct LLMs in solving hard-proxy queries, enabling the high-quality\nof the proxy solutions. Finally, our extensive experiments in various\nreasoning-intensive tasks with varying open- and closed-source LLMs show that\nLBS3 achieves strongly competitive performance compared to the SOTA baselines."
                },
                "authors": [
                    {
                        "name": "Kangyang Luo"
                    },
                    {
                        "name": "Zichen Ding"
                    },
                    {
                        "name": "Zhenmin Weng"
                    },
                    {
                        "name": "Lingfeng Qiao"
                    },
                    {
                        "name": "Meng Zhao"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Di Yin"
                    },
                    {
                        "name": "Jinlong Shu"
                    }
                ],
                "author_detail": {
                    "name": "Jinlong Shu"
                },
                "author": "Jinlong Shu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21728v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21728v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21723v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21723v1",
                "updated": "2024-10-29T04:22:28Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    4,
                    22,
                    28,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T04:22:28Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    4,
                    22,
                    28,
                    1,
                    303,
                    0
                ],
                "title": "Fine-tuning Large Language Models for DGA and DNS Exfiltration Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning Large Language Models for DGA and DNS Exfiltration Detection"
                },
                "summary": "Domain Generation Algorithms (DGAs) are malicious techniques used by malware\nto dynamically generate seemingly random domain names for communication with\nCommand & Control (C&C) servers. Due to the fast and simple generation of DGA\ndomains, detection methods must be highly efficient and precise to be\neffective. Large Language Models (LLMs) have demonstrated their proficiency in\nreal-time detection tasks, making them ideal candidates for detecting DGAs. Our\nwork validates the effectiveness of fine-tuned LLMs for detecting DGAs and DNS\nexfiltration attacks. We developed LLM models and conducted comprehensive\nevaluation using a diverse dataset comprising 59 distinct real-world DGA\nmalware families and normal domain data. Our LLM model significantly\noutperformed traditional natural language processing techniques, especially in\ndetecting unknown DGAs. We also evaluated its performance on DNS exfiltration\ndatasets, demonstrating its effectiveness in enhancing cybersecurity measures.\nTo the best of our knowledge, this is the first work that empirically applies\nLLMs for DGA and DNS exfiltration detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain Generation Algorithms (DGAs) are malicious techniques used by malware\nto dynamically generate seemingly random domain names for communication with\nCommand & Control (C&C) servers. Due to the fast and simple generation of DGA\ndomains, detection methods must be highly efficient and precise to be\neffective. Large Language Models (LLMs) have demonstrated their proficiency in\nreal-time detection tasks, making them ideal candidates for detecting DGAs. Our\nwork validates the effectiveness of fine-tuned LLMs for detecting DGAs and DNS\nexfiltration attacks. We developed LLM models and conducted comprehensive\nevaluation using a diverse dataset comprising 59 distinct real-world DGA\nmalware families and normal domain data. Our LLM model significantly\noutperformed traditional natural language processing techniques, especially in\ndetecting unknown DGAs. We also evaluated its performance on DNS exfiltration\ndatasets, demonstrating its effectiveness in enhancing cybersecurity measures.\nTo the best of our knowledge, this is the first work that empirically applies\nLLMs for DGA and DNS exfiltration detection."
                },
                "authors": [
                    {
                        "name": "Md Abu Sayed"
                    },
                    {
                        "name": "Asif Rahman"
                    },
                    {
                        "name": "Christopher Kiekintveld"
                    },
                    {
                        "name": "Sebastian Garcia"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Garcia"
                },
                "author": "Sebastian Garcia",
                "arxiv_comment": "Accepted in Proceedings of the Workshop at AI for Cyber Threat\n  Intelligence (WAITI), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21723v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21723v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21717v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21717v1",
                "updated": "2024-10-29T04:14:32Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    4,
                    14,
                    32,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T04:14:32Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    4,
                    14,
                    32,
                    1,
                    303,
                    0
                ],
                "title": "Generating Realistic Tabular Data with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating Realistic Tabular Data with Large Language Models"
                },
                "summary": "While most generative models show achievements in image data generation, few\nare developed for tabular data generation. Recently, due to success of large\nlanguage models (LLM) in diverse tasks, they have also been used for tabular\ndata generation. However, these methods do not capture the correct correlation\nbetween the features and the target variable, hindering their applications in\ndownstream predictive tasks. To address this problem, we propose a LLM-based\nmethod with three important improvements to correctly capture the ground-truth\nfeature-class correlation in the real data. First, we propose a novel\npermutation strategy for the input data in the fine-tuning phase. Second, we\npropose a feature-conditional sampling approach to generate synthetic samples.\nFinally, we generate the labels by constructing prompts based on the generated\nsamples to query our fine-tuned LLM. Our extensive experiments show that our\nmethod significantly outperforms 10 SOTA baselines on 20 datasets in downstream\ntasks. It also produces highly realistic synthetic samples in terms of quality\nand diversity. More importantly, classifiers trained with our synthetic data\ncan even compete with classifiers trained with the original data on half of the\nbenchmark datasets, which is a significant achievement in tabular data\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While most generative models show achievements in image data generation, few\nare developed for tabular data generation. Recently, due to success of large\nlanguage models (LLM) in diverse tasks, they have also been used for tabular\ndata generation. However, these methods do not capture the correct correlation\nbetween the features and the target variable, hindering their applications in\ndownstream predictive tasks. To address this problem, we propose a LLM-based\nmethod with three important improvements to correctly capture the ground-truth\nfeature-class correlation in the real data. First, we propose a novel\npermutation strategy for the input data in the fine-tuning phase. Second, we\npropose a feature-conditional sampling approach to generate synthetic samples.\nFinally, we generate the labels by constructing prompts based on the generated\nsamples to query our fine-tuned LLM. Our extensive experiments show that our\nmethod significantly outperforms 10 SOTA baselines on 20 datasets in downstream\ntasks. It also produces highly realistic synthetic samples in terms of quality\nand diversity. More importantly, classifiers trained with our synthetic data\ncan even compete with classifiers trained with the original data on half of the\nbenchmark datasets, which is a significant achievement in tabular data\ngeneration."
                },
                "authors": [
                    {
                        "name": "Dang Nguyen"
                    },
                    {
                        "name": "Sunil Gupta"
                    },
                    {
                        "name": "Kien Do"
                    },
                    {
                        "name": "Thin Nguyen"
                    },
                    {
                        "name": "Svetha Venkatesh"
                    }
                ],
                "author_detail": {
                    "name": "Svetha Venkatesh"
                },
                "author": "Svetha Venkatesh",
                "arxiv_comment": "To appear at ICDM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21717v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21717v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21716v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21716v1",
                "updated": "2024-10-29T04:14:23Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    4,
                    14,
                    23,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-29T04:14:23Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    4,
                    14,
                    23,
                    1,
                    303,
                    0
                ],
                "title": "A Bayesian Approach to Harnessing the Power of LLMs in Authorship\n  Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Bayesian Approach to Harnessing the Power of LLMs in Authorship\n  Attribution"
                },
                "summary": "Authorship attribution aims to identify the origin or author of a document.\nTraditional approaches have heavily relied on manual features and fail to\ncapture long-range correlations, limiting their effectiveness. Recent\nadvancements leverage text embeddings from pre-trained language models, which\nrequire significant fine-tuning on labeled data, posing challenges in data\ndependency and limited interpretability. Large Language Models (LLMs), with\ntheir deep reasoning capabilities and ability to maintain long-range textual\nassociations, offer a promising alternative. This study explores the potential\nof pre-trained LLMs in one-shot authorship attribution, specifically utilizing\nBayesian approaches and probability outputs of LLMs. Our methodology calculates\nthe probability that a text entails previous writings of an author, reflecting\na more nuanced understanding of authorship. By utilizing only pre-trained\nmodels such as Llama-3-70B, our results on the IMDb and blog datasets show an\nimpressive 85\\% accuracy in one-shot authorship classification across ten\nauthors. Our findings set new baselines for one-shot authorship analysis using\nLLMs and expand the application scope of these models in forensic linguistics.\nThis work also includes extensive ablation studies to validate our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Authorship attribution aims to identify the origin or author of a document.\nTraditional approaches have heavily relied on manual features and fail to\ncapture long-range correlations, limiting their effectiveness. Recent\nadvancements leverage text embeddings from pre-trained language models, which\nrequire significant fine-tuning on labeled data, posing challenges in data\ndependency and limited interpretability. Large Language Models (LLMs), with\ntheir deep reasoning capabilities and ability to maintain long-range textual\nassociations, offer a promising alternative. This study explores the potential\nof pre-trained LLMs in one-shot authorship attribution, specifically utilizing\nBayesian approaches and probability outputs of LLMs. Our methodology calculates\nthe probability that a text entails previous writings of an author, reflecting\na more nuanced understanding of authorship. By utilizing only pre-trained\nmodels such as Llama-3-70B, our results on the IMDb and blog datasets show an\nimpressive 85\\% accuracy in one-shot authorship classification across ten\nauthors. Our findings set new baselines for one-shot authorship analysis using\nLLMs and expand the application scope of these models in forensic linguistics.\nThis work also includes extensive ablation studies to validate our approach."
                },
                "authors": [
                    {
                        "name": "Zhengmian Hu"
                    },
                    {
                        "name": "Tong Zheng"
                    },
                    {
                        "name": "Heng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Heng Huang"
                },
                "author": "Heng Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21716v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21716v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01946v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01946v3",
                "updated": "2024-10-29T04:12:47Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    4,
                    12,
                    47,
                    1,
                    303,
                    0
                ],
                "published": "2024-06-04T03:58:14Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    3,
                    58,
                    14,
                    1,
                    156,
                    0
                ],
                "title": "Bileve: Securing Text Provenance in Large Language Models Against\n  Spoofing with Bi-level Signature",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bileve: Securing Text Provenance in Large Language Models Against\n  Spoofing with Bi-level Signature"
                },
                "summary": "Text watermarks for large language models (LLMs) have been commonly used to\nidentify the origins of machine-generated content, which is promising for\nassessing liability when combating deepfake or harmful content. While existing\nwatermarking techniques typically prioritize robustness against removal\nattacks, unfortunately, they are vulnerable to spoofing attacks: malicious\nactors can subtly alter the meanings of LLM-generated responses or even forge\nharmful content, potentially misattributing blame to the LLM developer. To\novercome this, we introduce a bi-level signature scheme, Bileve, which embeds\nfine-grained signature bits for integrity checks (mitigating spoofing attacks)\nas well as a coarse-grained signal to trace text sources when the signature is\ninvalid (enhancing detectability) via a novel rank-based sampling strategy.\nCompared to conventional watermark detectors that only output binary results,\nBileve can differentiate 5 scenarios during detection, reliably tracing text\nprovenance and regulating LLMs. The experiments conducted on OPT-1.3B and\nLLaMA-7B demonstrate the effectiveness of Bileve in defeating spoofing attacks\nwith enhanced detectability. Code is available at\nhttps://github.com/Tongzhou0101/Bileve-official.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text watermarks for large language models (LLMs) have been commonly used to\nidentify the origins of machine-generated content, which is promising for\nassessing liability when combating deepfake or harmful content. While existing\nwatermarking techniques typically prioritize robustness against removal\nattacks, unfortunately, they are vulnerable to spoofing attacks: malicious\nactors can subtly alter the meanings of LLM-generated responses or even forge\nharmful content, potentially misattributing blame to the LLM developer. To\novercome this, we introduce a bi-level signature scheme, Bileve, which embeds\nfine-grained signature bits for integrity checks (mitigating spoofing attacks)\nas well as a coarse-grained signal to trace text sources when the signature is\ninvalid (enhancing detectability) via a novel rank-based sampling strategy.\nCompared to conventional watermark detectors that only output binary results,\nBileve can differentiate 5 scenarios during detection, reliably tracing text\nprovenance and regulating LLMs. The experiments conducted on OPT-1.3B and\nLLaMA-7B demonstrate the effectiveness of Bileve in defeating spoofing attacks\nwith enhanced detectability. Code is available at\nhttps://github.com/Tongzhou0101/Bileve-official."
                },
                "authors": [
                    {
                        "name": "Tong Zhou"
                    },
                    {
                        "name": "Xuandong Zhao"
                    },
                    {
                        "name": "Xiaolin Xu"
                    },
                    {
                        "name": "Shaolei Ren"
                    }
                ],
                "author_detail": {
                    "name": "Shaolei Ren"
                },
                "author": "Shaolei Ren",
                "arxiv_comment": "NeurIPS 2024 camera-ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01946v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01946v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05723v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05723v3",
                "updated": "2024-10-29T04:06:31Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    4,
                    6,
                    31,
                    1,
                    303,
                    0
                ],
                "published": "2024-06-09T10:30:25Z",
                "published_parsed": [
                    2024,
                    6,
                    9,
                    10,
                    30,
                    25,
                    6,
                    161,
                    0
                ],
                "title": "Binarized Diffusion Model for Image Super-Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Binarized Diffusion Model for Image Super-Resolution"
                },
                "summary": "Advanced diffusion models (DMs) perform impressively in image\nsuper-resolution (SR), but the high memory and computational costs hinder their\ndeployment. Binarization, an ultra-compression algorithm, offers the potential\nfor effectively accelerating DMs. Nonetheless, due to the model structure and\nthe multi-step iterative attribute of DMs, existing binarization methods result\nin significant performance degradation. In this paper, we introduce a novel\nbinarized diffusion model, BI-DiffSR, for image SR. First, for the model\nstructure, we design a UNet architecture optimized for binarization. We propose\nthe consistent-pixel-downsample (CP-Down) and consistent-pixel-upsample (CP-Up)\nto maintain dimension consistent and facilitate the full-precision information\ntransfer. Meanwhile, we design the channel-shuffle-fusion (CS-Fusion) to\nenhance feature fusion in skip connection. Second, for the activation\ndifference across timestep, we design the timestep-aware redistribution (TaR)\nand activation function (TaA). The TaR and TaA dynamically adjust the\ndistribution of activations based on different timesteps, improving the\nflexibility and representation alability of the binarized module. Comprehensive\nexperiments demonstrate that our BI-DiffSR outperforms existing binarization\nmethods. Code is released at: https://github.com/zhengchen1999/BI-DiffSR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced diffusion models (DMs) perform impressively in image\nsuper-resolution (SR), but the high memory and computational costs hinder their\ndeployment. Binarization, an ultra-compression algorithm, offers the potential\nfor effectively accelerating DMs. Nonetheless, due to the model structure and\nthe multi-step iterative attribute of DMs, existing binarization methods result\nin significant performance degradation. In this paper, we introduce a novel\nbinarized diffusion model, BI-DiffSR, for image SR. First, for the model\nstructure, we design a UNet architecture optimized for binarization. We propose\nthe consistent-pixel-downsample (CP-Down) and consistent-pixel-upsample (CP-Up)\nto maintain dimension consistent and facilitate the full-precision information\ntransfer. Meanwhile, we design the channel-shuffle-fusion (CS-Fusion) to\nenhance feature fusion in skip connection. Second, for the activation\ndifference across timestep, we design the timestep-aware redistribution (TaR)\nand activation function (TaA). The TaR and TaA dynamically adjust the\ndistribution of activations based on different timesteps, improving the\nflexibility and representation alability of the binarized module. Comprehensive\nexperiments demonstrate that our BI-DiffSR outperforms existing binarization\nmethods. Code is released at: https://github.com/zhengchen1999/BI-DiffSR."
                },
                "authors": [
                    {
                        "name": "Zheng Chen"
                    },
                    {
                        "name": "Haotong Qin"
                    },
                    {
                        "name": "Yong Guo"
                    },
                    {
                        "name": "Xiongfei Su"
                    },
                    {
                        "name": "Xin Yuan"
                    },
                    {
                        "name": "Linghe Kong"
                    },
                    {
                        "name": "Yulun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yulun Zhang"
                },
                "author": "Yulun Zhang",
                "arxiv_comment": "Accepted to NeurIPS 2024. Code is available at\n  https://github.com/zhengchen1999/BI-DiffSR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05723v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05723v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19730v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19730v2",
                "updated": "2024-10-29T03:48:33Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    3,
                    48,
                    33,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-25T17:56:24Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    17,
                    56,
                    24,
                    4,
                    299,
                    0
                ],
                "title": "Counting Ability of Large Language Models and Impact of Tokenization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Counting Ability of Large Language Models and Impact of Tokenization"
                },
                "summary": "Transformers, the backbone of modern large language models (LLMs), face\ninherent architectural limitations that impede their reasoning capabilities.\nUnlike recurrent networks, Transformers lack recurrent connections, confining\nthem to constant-depth computation. This restriction places them in the\ncomplexity class TC$^0$, making them theoretically incapable of solving tasks\nthat demand increasingly deep reasoning as input length grows. Counting, a\nfundamental component of many reasoning tasks, also requires reasoning depth to\ngrow linearly to be performed inductively. While previous studies have\nestablished the upper limits of counting ability in Transformer-based expert\nmodels (i.e., models specifically trained for counting tasks), these findings\ndo not directly extend to general-purpose LLMs due to differences in reasoning\nmechanisms. Recent work has highlighted how Chain of Thought (CoT) reasoning\ncan help alleviate some of the architectural limitations of Transformers in\ncounting tasks. However, little attention has been paid to the role of\ntokenization in these models. Unlike expert models that often use\ncharacter-level tokenization, LLMs typically rely on byte-level (BPE)\ntokenizers, which fundamentally alters the way reasoning is processed. Our work\ninvestigates the impact of tokenization on the counting abilities of LLMs,\nuncovering substantial performance variations based on input tokenization\ndifferences. We provide both theoretical and experimental analyses, offering\ninsights into how tokenization choices can undermine models' theoretical\ncomputability, thereby inspiring the design of new tokenization methods to\nenhance reasoning in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers, the backbone of modern large language models (LLMs), face\ninherent architectural limitations that impede their reasoning capabilities.\nUnlike recurrent networks, Transformers lack recurrent connections, confining\nthem to constant-depth computation. This restriction places them in the\ncomplexity class TC$^0$, making them theoretically incapable of solving tasks\nthat demand increasingly deep reasoning as input length grows. Counting, a\nfundamental component of many reasoning tasks, also requires reasoning depth to\ngrow linearly to be performed inductively. While previous studies have\nestablished the upper limits of counting ability in Transformer-based expert\nmodels (i.e., models specifically trained for counting tasks), these findings\ndo not directly extend to general-purpose LLMs due to differences in reasoning\nmechanisms. Recent work has highlighted how Chain of Thought (CoT) reasoning\ncan help alleviate some of the architectural limitations of Transformers in\ncounting tasks. However, little attention has been paid to the role of\ntokenization in these models. Unlike expert models that often use\ncharacter-level tokenization, LLMs typically rely on byte-level (BPE)\ntokenizers, which fundamentally alters the way reasoning is processed. Our work\ninvestigates the impact of tokenization on the counting abilities of LLMs,\nuncovering substantial performance variations based on input tokenization\ndifferences. We provide both theoretical and experimental analyses, offering\ninsights into how tokenization choices can undermine models' theoretical\ncomputability, thereby inspiring the design of new tokenization methods to\nenhance reasoning in LLMs."
                },
                "authors": [
                    {
                        "name": "Xiang Zhang"
                    },
                    {
                        "name": "Juntai Cao"
                    },
                    {
                        "name": "Chenyu You"
                    }
                ],
                "author_detail": {
                    "name": "Chenyu You"
                },
                "author": "Chenyu You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19730v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19730v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15522v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15522v2",
                "updated": "2024-10-29T03:28:42Z",
                "updated_parsed": [
                    2024,
                    10,
                    29,
                    3,
                    28,
                    42,
                    1,
                    303,
                    0
                ],
                "published": "2024-10-20T22:09:44Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    22,
                    9,
                    44,
                    6,
                    294,
                    0
                ],
                "title": "M-RewardBench: Evaluating Reward Models in Multilingual Settings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M-RewardBench: Evaluating Reward Models in Multilingual Settings"
                },
                "summary": "Reward models (RMs) have driven the state-of-the-art performance of LLMs\ntoday by enabling the integration of human feedback into the language modeling\nprocess. However, RMs are primarily trained and evaluated in English, and their\ncapabilities in multilingual settings remain largely understudied. In this\nwork, we conduct a systematic evaluation of several reward models in\nmultilingual settings. We first construct the first-of-its-kind multilingual RM\nevaluation benchmark, M-RewardBench, consisting of 2.87k preference instances\nfor 23 typologically diverse languages, that tests the chat, safety, reasoning,\nand translation capabilities of RMs. We then rigorously evaluate a wide range\nof reward models on M-RewardBench, offering fresh insights into their\nperformance across diverse languages. We identify a significant gap in RMs'\nperformances between English and non-English languages and show that RM\npreferences can change substantially from one language to another. We also\npresent several findings on how different multilingual aspects impact RM\nperformance. Specifically, we show that the performance of RMs is improved with\nimproved translation quality. Similarly, we demonstrate that the models exhibit\nbetter performance for high-resource languages. We release M-RewardBench\ndataset and the codebase in this study to facilitate a better understanding of\nRM evaluation in multilingual settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward models (RMs) have driven the state-of-the-art performance of LLMs\ntoday by enabling the integration of human feedback into the language modeling\nprocess. However, RMs are primarily trained and evaluated in English, and their\ncapabilities in multilingual settings remain largely understudied. In this\nwork, we conduct a systematic evaluation of several reward models in\nmultilingual settings. We first construct the first-of-its-kind multilingual RM\nevaluation benchmark, M-RewardBench, consisting of 2.87k preference instances\nfor 23 typologically diverse languages, that tests the chat, safety, reasoning,\nand translation capabilities of RMs. We then rigorously evaluate a wide range\nof reward models on M-RewardBench, offering fresh insights into their\nperformance across diverse languages. We identify a significant gap in RMs'\nperformances between English and non-English languages and show that RM\npreferences can change substantially from one language to another. We also\npresent several findings on how different multilingual aspects impact RM\nperformance. Specifically, we show that the performance of RMs is improved with\nimproved translation quality. Similarly, we demonstrate that the models exhibit\nbetter performance for high-resource languages. We release M-RewardBench\ndataset and the codebase in this study to facilitate a better understanding of\nRM evaluation in multilingual settings."
                },
                "authors": [
                    {
                        "name": "Srishti Gureja"
                    },
                    {
                        "name": "Lester James V. Miranda"
                    },
                    {
                        "name": "Shayekh Bin Islam"
                    },
                    {
                        "name": "Rishabh Maheshwary"
                    },
                    {
                        "name": "Drishti Sharma"
                    },
                    {
                        "name": "Gusti Winata"
                    },
                    {
                        "name": "Nathan Lambert"
                    },
                    {
                        "name": "Sebastian Ruder"
                    },
                    {
                        "name": "Sara Hooker"
                    },
                    {
                        "name": "Marzieh Fadaee"
                    }
                ],
                "author_detail": {
                    "name": "Marzieh Fadaee"
                },
                "author": "Marzieh Fadaee",
                "arxiv_comment": "16 pages, 6 figures, 10 tables. Website:\n  https://m-rewardbench.github.io/ , Updated results with latest models. Added\n  more author information",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15522v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15522v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]