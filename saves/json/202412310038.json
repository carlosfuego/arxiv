[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.19442v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19442v1",
                "updated": "2024-12-27T04:17:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    4,
                    17,
                    57,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T04:17:57Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    4,
                    17,
                    57,
                    4,
                    362,
                    0
                ],
                "title": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management"
                },
                "summary": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}."
                },
                "authors": [
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Anxin Tian"
                    },
                    {
                        "name": "Tianhao Tang"
                    },
                    {
                        "name": "Zhanchao Xu"
                    },
                    {
                        "name": "Xuejia Chen"
                    },
                    {
                        "name": "Nicole Hu"
                    },
                    {
                        "name": "Wei Dong"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19442v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19442v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19255v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19255v1",
                "updated": "2024-12-26T15:45:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    15,
                    45,
                    45,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T15:45:45Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    15,
                    45,
                    45,
                    3,
                    361,
                    0
                ],
                "title": "Multi-matrix Factorization Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-matrix Factorization Attention"
                },
                "summary": "We propose novel attention architectures, Multi-matrix Factorization\nAttention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard\nMulti-Head Attention (MHA), including SOTA methods like MLA, fail to maintain\nas strong performance under stringent Key-Value cache (KV cache) constraints.\nMFA enhances model capacity by efficiently scaling up both the number and\ndimension of attention heads through low-rank matrix factorization in the\nQuery-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory\nrequirements by repurposing the key cache as value through value projection\nre-parameterization. MFA's design enables strong model capacity when working\nunder tight KV cache budget, while MFA-KR is suitable for even harsher KV cache\nlimits with minor performance trade-off. Notably, in our extensive and\nlarge-scale experiments, the proposed architecture outperforms MLA and performs\ncomparably to MHA, while reducing KV cache usage by up to 56% and 93.7%,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose novel attention architectures, Multi-matrix Factorization\nAttention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard\nMulti-Head Attention (MHA), including SOTA methods like MLA, fail to maintain\nas strong performance under stringent Key-Value cache (KV cache) constraints.\nMFA enhances model capacity by efficiently scaling up both the number and\ndimension of attention heads through low-rank matrix factorization in the\nQuery-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory\nrequirements by repurposing the key cache as value through value projection\nre-parameterization. MFA's design enables strong model capacity when working\nunder tight KV cache budget, while MFA-KR is suitable for even harsher KV cache\nlimits with minor performance trade-off. Notably, in our extensive and\nlarge-scale experiments, the proposed architecture outperforms MLA and performs\ncomparably to MHA, while reducing KV cache usage by up to 56% and 93.7%,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Jingcheng Hu"
                    },
                    {
                        "name": "Houyi Li"
                    },
                    {
                        "name": "Yinmin Zhang"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Shuigeng Zhou"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Heung-Yeung Shum"
                    }
                ],
                "author_detail": {
                    "name": "Heung-Yeung Shum"
                },
                "author": "Heung-Yeung Shum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19255v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19255v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19051v1",
                "updated": "2024-12-26T04:13:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    4,
                    13,
                    52,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T04:13:52Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    4,
                    13,
                    52,
                    3,
                    361,
                    0
                ],
                "title": "Performance Characterization and Optimizations of Traditional ML\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Characterization and Optimizations of Traditional ML\n  Applications"
                },
                "summary": "Even in the era of Deep Learning based methods, traditional machine learning\nmethods with large data sets continue to attract significant attention.\nHowever, we find an apparent lack of a detailed performance characterization of\nthese methods in the context of large training datasets. In this work, we study\nthe system's behavior of a number of traditional ML methods as implemented in\npopular free software libraries/modules to identify critical performance\nbottlenecks experienced by these applications. The performance characterization\nstudy reveals several interesting insights on the performance of these\napplications. Then we evaluate the performance benefits of applying some\nwell-known optimizations at the levels of caches and the main memory. More\nspecifically, we test the usefulness of optimizations such as (i) software\nprefetching to improve cache performance and (ii) data layout and computation\nreordering optimizations to improve locality in DRAM accesses. These\noptimizations are implemented as modifications to the well-known scikit-learn\nlibrary, and hence can be easily leveraged by application programmers. We\nevaluate the impact of the proposed optimizations using a combination of\nsimulation and execution on a real system. The software prefetching\noptimization results in performance benefits varying from 5.2%-27.1% on\ndifferent ML applications while the data layout and computation reordering\napproaches yield 6.16%-28.0% performance improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Even in the era of Deep Learning based methods, traditional machine learning\nmethods with large data sets continue to attract significant attention.\nHowever, we find an apparent lack of a detailed performance characterization of\nthese methods in the context of large training datasets. In this work, we study\nthe system's behavior of a number of traditional ML methods as implemented in\npopular free software libraries/modules to identify critical performance\nbottlenecks experienced by these applications. The performance characterization\nstudy reveals several interesting insights on the performance of these\napplications. Then we evaluate the performance benefits of applying some\nwell-known optimizations at the levels of caches and the main memory. More\nspecifically, we test the usefulness of optimizations such as (i) software\nprefetching to improve cache performance and (ii) data layout and computation\nreordering optimizations to improve locality in DRAM accesses. These\noptimizations are implemented as modifications to the well-known scikit-learn\nlibrary, and hence can be easily leveraged by application programmers. We\nevaluate the impact of the proposed optimizations using a combination of\nsimulation and execution on a real system. The software prefetching\noptimization results in performance benefits varying from 5.2%-27.1% on\ndifferent ML applications while the data layout and computation reordering\napproaches yield 6.16%-28.0% performance improvement."
                },
                "authors": [
                    {
                        "name": "Harsh Kumar"
                    },
                    {
                        "name": "R. Govindarajan"
                    }
                ],
                "author_detail": {
                    "name": "R. Govindarajan"
                },
                "author": "R. Govindarajan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18960v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18960v1",
                "updated": "2024-12-25T18:36:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    18,
                    36,
                    21,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T18:36:21Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    18,
                    36,
                    21,
                    2,
                    360,
                    0
                ],
                "title": "XRFlux: Virtual Reality Benchmark for Edge Caching Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XRFlux: Virtual Reality Benchmark for Edge Caching Systems"
                },
                "summary": "We introduce a Unity based benchmark XRFlux for evaluating Virtual Reality\n(VR) delivery systems using edge-cloud caching. As VR applications and systems\nprogress, the need to meet strict latency and Quality of Experience (QoE)\nrequirements is increasingly evident. In the context of VR, traditional cloud\narchitectures (e.g., remote AWS S3 for content delivery) often struggle to meet\nthese demands, especially for users of the same application in different\nlocations. With edge computing, resources are brought closer to users in\nefforts to reduce latency and improve QoEs. However, VR's dynamic nature, with\nchanging fields of view (FoVs) and user synchronization requirements, creates\nvarious challenges for edge caching. We address the lack of suitable benchmarks\nand propose a framework that simulates multiuser VR scenarios while logging\nusers' interaction with objects within their actual and predicted FoVs. The\nbenchmark's activity log can then be played back through an edge cache to\nassess the resulting QoEs. This tool fills a gap by supporting research in the\noptimization of edge caching (and other edge-cloud functions) for VR streaming.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a Unity based benchmark XRFlux for evaluating Virtual Reality\n(VR) delivery systems using edge-cloud caching. As VR applications and systems\nprogress, the need to meet strict latency and Quality of Experience (QoE)\nrequirements is increasingly evident. In the context of VR, traditional cloud\narchitectures (e.g., remote AWS S3 for content delivery) often struggle to meet\nthese demands, especially for users of the same application in different\nlocations. With edge computing, resources are brought closer to users in\nefforts to reduce latency and improve QoEs. However, VR's dynamic nature, with\nchanging fields of view (FoVs) and user synchronization requirements, creates\nvarious challenges for edge caching. We address the lack of suitable benchmarks\nand propose a framework that simulates multiuser VR scenarios while logging\nusers' interaction with objects within their actual and predicted FoVs. The\nbenchmark's activity log can then be played back through an edge cache to\nassess the resulting QoEs. This tool fills a gap by supporting research in the\noptimization of edge caching (and other edge-cloud functions) for VR streaming."
                },
                "authors": [
                    {
                        "name": "Nader Alfares"
                    },
                    {
                        "name": "George Kesidis"
                    }
                ],
                "author_detail": {
                    "name": "George Kesidis"
                },
                "author": "George Kesidis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18960v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18960v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18914v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18914v1",
                "updated": "2024-12-25T14:14:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    14,
                    31,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T14:14:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    14,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "Long-Range Tasks Using Short-Context LLMs: Incremental Reasoning With\n  Structured Memories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Range Tasks Using Short-Context LLMs: Incremental Reasoning With\n  Structured Memories"
                },
                "summary": "Long-range tasks require reasoning over long inputs. Existing solutions\neither need large compute budgets, training data, access to model weights, or\nuse complex, task-specific approaches. We present PRISM, which alleviates these\nconcerns by processing information as a stream of chunks, maintaining a\nstructured in-context memory specified by a typed hierarchy schema. This\napproach demonstrates superior performance to baselines on diverse tasks while\nusing at least 4x smaller contexts than long-context models. Moreover, PRISM is\ntoken-efficient. By producing short outputs and efficiently leveraging\nkey-value (KV) caches, it achieves up to 54% cost reduction when compared to\nalternative short-context approaches. The method also scales down to tiny\ninformation chunks (e.g., 500 tokens) without increasing the number of tokens\nencoded or sacrificing quality. Furthermore, we show that it is possible to\ngenerate schemas to generalize our approach to new tasks with minimal effort.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-range tasks require reasoning over long inputs. Existing solutions\neither need large compute budgets, training data, access to model weights, or\nuse complex, task-specific approaches. We present PRISM, which alleviates these\nconcerns by processing information as a stream of chunks, maintaining a\nstructured in-context memory specified by a typed hierarchy schema. This\napproach demonstrates superior performance to baselines on diverse tasks while\nusing at least 4x smaller contexts than long-context models. Moreover, PRISM is\ntoken-efficient. By producing short outputs and efficiently leveraging\nkey-value (KV) caches, it achieves up to 54% cost reduction when compared to\nalternative short-context approaches. The method also scales down to tiny\ninformation chunks (e.g., 500 tokens) without increasing the number of tokens\nencoded or sacrificing quality. Furthermore, we show that it is possible to\ngenerate schemas to generalize our approach to new tasks with minimal effort."
                },
                "authors": [
                    {
                        "name": "Dulhan Jayalath"
                    },
                    {
                        "name": "James Bradley Wendt"
                    },
                    {
                        "name": "Nicholas Monath"
                    },
                    {
                        "name": "Sandeep Tata"
                    },
                    {
                        "name": "Beliz Gunel"
                    }
                ],
                "author_detail": {
                    "name": "Beliz Gunel"
                },
                "author": "Beliz Gunel",
                "arxiv_comment": "23 pages, 7 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18914v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18914v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18911v1",
                "updated": "2024-12-25T14:00:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    0,
                    14,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T14:00:14Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    0,
                    14,
                    2,
                    360,
                    0
                ],
                "title": "Accelerating Diffusion Transformers with Dual Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformers with Dual Feature Caching"
                },
                "summary": "Diffusion Transformers (DiT) have become the dominant methods in image and\nvideo generation yet still suffer substantial computational costs. As an\neffective approach for DiT acceleration, feature caching methods are designed\nto cache the features of DiT in previous timesteps and reuse them in the next\ntimesteps, allowing us to skip the computation in the next timesteps. However,\non the one hand, aggressively reusing all the features cached in previous\ntimesteps leads to a severe drop in generation quality. On the other hand,\nconservatively caching only the features in the redundant layers or tokens but\nstill computing the important ones successfully preserves the generation\nquality but results in reductions in acceleration ratios. Observing such a\ntradeoff between generation quality and acceleration performance, this paper\nbegins by quantitatively studying the accumulated error from cached features.\nSurprisingly, we find that aggressive caching does not introduce significantly\nmore caching errors in the caching step, and the conservative feature caching\ncan fix the error introduced by aggressive caching. Thereby, we propose a dual\ncaching strategy that adopts aggressive and conservative caching iteratively,\nleading to significant acceleration and high generation quality at the same\ntime. Besides, we further introduce a V-caching strategy for token-wise\nconservative caching, which is compatible with flash attention and requires no\ntraining and calibration data.\n  Our codes have been released in Github: \\textbf{Code:\n\\href{https://github.com/Shenyi-Z/DuCa}{\\texttt{\\textcolor{cyan}{https://github.com/Shenyi-Z/DuCa}}}}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have become the dominant methods in image and\nvideo generation yet still suffer substantial computational costs. As an\neffective approach for DiT acceleration, feature caching methods are designed\nto cache the features of DiT in previous timesteps and reuse them in the next\ntimesteps, allowing us to skip the computation in the next timesteps. However,\non the one hand, aggressively reusing all the features cached in previous\ntimesteps leads to a severe drop in generation quality. On the other hand,\nconservatively caching only the features in the redundant layers or tokens but\nstill computing the important ones successfully preserves the generation\nquality but results in reductions in acceleration ratios. Observing such a\ntradeoff between generation quality and acceleration performance, this paper\nbegins by quantitatively studying the accumulated error from cached features.\nSurprisingly, we find that aggressive caching does not introduce significantly\nmore caching errors in the caching step, and the conservative feature caching\ncan fix the error introduced by aggressive caching. Thereby, we propose a dual\ncaching strategy that adopts aggressive and conservative caching iteratively,\nleading to significant acceleration and high generation quality at the same\ntime. Besides, we further introduce a V-caching strategy for token-wise\nconservative caching, which is compatible with flash attention and requires no\ntraining and calibration data.\n  Our codes have been released in Github: \\textbf{Code:\n\\href{https://github.com/Shenyi-Z/DuCa}{\\texttt{\\textcolor{cyan}{https://github.com/Shenyi-Z/DuCa}}}}"
                },
                "authors": [
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Evelyn Zhang"
                    },
                    {
                        "name": "Runlin Guo"
                    },
                    {
                        "name": "Haohang Xu"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18885v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18885v1",
                "updated": "2024-12-25T11:59:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    11,
                    59,
                    17,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T11:59:17Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    11,
                    59,
                    17,
                    2,
                    360,
                    0
                ],
                "title": "Aspect-oriented Programming with Julia",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aspect-oriented Programming with Julia"
                },
                "summary": "This paper proposes integrating Aspect-oriented Programming (AOP) into Julia,\na language widely used in scientific and High-Performance Computing (HPC). AOP\nenhances software modularity by encapsulating cross-cutting concerns, such as\nlogging, caching, and parallelizing, into separate, reusable aspects.\nLeveraging Julia's powerful metaprogramming and abstract syntax tree (AST)\nmanipulation capabilities, we introduce AspectJulia, an AOP framework designed\nto operate within Julia's runtime environment as a package. AspectJulia enables\ndevelopers to define and apply aspects seamlessly, leading to more modular,\nmaintainable, and adaptable code. We detail the implementation of AspectJulia\nand present diverse use cases, ranging from HPC and scientific computing to\nbusiness applications, demonstrating its effectiveness in managing\ncross-cutting concerns. This integration simplifies application development and\nimproves the adaptability of existing Julia modules and packages, paving the\nway for more efficient and maintainable software systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes integrating Aspect-oriented Programming (AOP) into Julia,\na language widely used in scientific and High-Performance Computing (HPC). AOP\nenhances software modularity by encapsulating cross-cutting concerns, such as\nlogging, caching, and parallelizing, into separate, reusable aspects.\nLeveraging Julia's powerful metaprogramming and abstract syntax tree (AST)\nmanipulation capabilities, we introduce AspectJulia, an AOP framework designed\nto operate within Julia's runtime environment as a package. AspectJulia enables\ndevelopers to define and apply aspects seamlessly, leading to more modular,\nmaintainable, and adaptable code. We detail the implementation of AspectJulia\nand present diverse use cases, ranging from HPC and scientific computing to\nbusiness applications, demonstrating its effectiveness in managing\ncross-cutting concerns. This integration simplifies application development and\nimproves the adaptability of existing Julia modules and packages, paving the\nway for more efficient and maintainable software systems."
                },
                "authors": [
                    {
                        "name": "Osamu Ishimura"
                    },
                    {
                        "name": "Yoshihide Yoshimoto"
                    }
                ],
                "author_detail": {
                    "name": "Yoshihide Yoshimoto"
                },
                "author": "Yoshihide Yoshimoto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18885v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18885v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16187v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16187v2",
                "updated": "2024-12-24T13:04:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    13,
                    4,
                    45,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-13T06:00:27Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    6,
                    0,
                    27,
                    4,
                    348,
                    0
                ],
                "title": "HashEvict: A Pre-Attention KV Cache Eviction Strategy using\n  Locality-Sensitive Hashing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HashEvict: A Pre-Attention KV Cache Eviction Strategy using\n  Locality-Sensitive Hashing"
                },
                "summary": "Transformer-based large language models (LLMs) use the key-value (KV) cache\nto significantly accelerate inference by storing the key and value embeddings\nof past tokens. However, this cache consumes significant GPU memory. In this\nwork, we introduce HashEvict, an algorithm that uses locality-sensitive hashing\n(LSH) to compress the KV cache. HashEvict quickly locates tokens in the cache\nthat are cosine dissimilar to the current query token. This is achieved by\ncomputing the Hamming distance between binarized Gaussian projections of the\ncurrent token query and cached token keys, with a projection length much\nsmaller than the embedding dimension. We maintain a lightweight binary\nstructure in GPU memory to facilitate these calculations. Unlike existing\ncompression strategies that compute attention to determine token retention,\nHashEvict makes these decisions pre-attention, thereby reducing computational\ncosts. Additionally, HashEvict is dynamic - at every decoding step, the key and\nvalue of the current token replace the embeddings of a token expected to\nproduce the lowest attention score. We demonstrate that HashEvict can compress\nthe KV cache by 30%-70% while maintaining high performance across reasoning,\nmultiple-choice, long-context retrieval and summarization tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) use the key-value (KV) cache\nto significantly accelerate inference by storing the key and value embeddings\nof past tokens. However, this cache consumes significant GPU memory. In this\nwork, we introduce HashEvict, an algorithm that uses locality-sensitive hashing\n(LSH) to compress the KV cache. HashEvict quickly locates tokens in the cache\nthat are cosine dissimilar to the current query token. This is achieved by\ncomputing the Hamming distance between binarized Gaussian projections of the\ncurrent token query and cached token keys, with a projection length much\nsmaller than the embedding dimension. We maintain a lightweight binary\nstructure in GPU memory to facilitate these calculations. Unlike existing\ncompression strategies that compute attention to determine token retention,\nHashEvict makes these decisions pre-attention, thereby reducing computational\ncosts. Additionally, HashEvict is dynamic - at every decoding step, the key and\nvalue of the current token replace the embeddings of a token expected to\nproduce the lowest attention score. We demonstrate that HashEvict can compress\nthe KV cache by 30%-70% while maintaining high performance across reasoning,\nmultiple-choice, long-context retrieval and summarization tasks."
                },
                "authors": [
                    {
                        "name": "Minghui Liu"
                    },
                    {
                        "name": "Tahseen Rabbani"
                    },
                    {
                        "name": "Tony O'Halloran"
                    },
                    {
                        "name": "Ananth Sankaralingam"
                    },
                    {
                        "name": "Mary-Anne Hartley"
                    },
                    {
                        "name": "Brian Gravelle"
                    },
                    {
                        "name": "Furong Huang"
                    },
                    {
                        "name": "Cornelia Fermüller"
                    },
                    {
                        "name": "Yiannis Aloimonos"
                    }
                ],
                "author_detail": {
                    "name": "Yiannis Aloimonos"
                },
                "author": "Yiannis Aloimonos",
                "arxiv_comment": "10 pages, 6 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16187v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16187v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01959v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01959v2",
                "updated": "2024-12-24T00:46:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    0,
                    46,
                    0,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-02T20:39:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    20,
                    39,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Development and Application of a Decentralized Domain Name Service",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development and Application of a Decentralized Domain Name Service"
                },
                "summary": "The current Domain Name System (DNS), as a core infrastructure of the\ninternet, exhibits several shortcomings: its centralized architecture leads to\ncensorship risks and single points of failure, making domain name resolution\nvulnerable to attacks. The lack of encryption in the resolution process exposes\nit to DNS hijacking and cache poisoning attacks. Additionally, the high\noperational costs limit participation and innovation among small to\nmedium-sized users. To address these issues, this paper proposes a\nDecentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and\ndistributed storage (IPFS). By leveraging the immutability of blockchain and\nthe content verification of IPFS, the system achieves decentralized storage and\ndistribution of domain name records, eliminating the centralized dependencies\nof traditional DNS. With a block time of 15 seconds, the system supports rapid\nbroadcasting of domain name updates, significantly improving resolution\nefficiency. The DDNS aims to serve as a complement or backup to the existing\nDNS system, providing a pollution-resistant, censorship-resistant,\nhigh-performance, and low-cost domain name resolution solution, offering a new\ntechnical path for the security and stability of the internet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current Domain Name System (DNS), as a core infrastructure of the\ninternet, exhibits several shortcomings: its centralized architecture leads to\ncensorship risks and single points of failure, making domain name resolution\nvulnerable to attacks. The lack of encryption in the resolution process exposes\nit to DNS hijacking and cache poisoning attacks. Additionally, the high\noperational costs limit participation and innovation among small to\nmedium-sized users. To address these issues, this paper proposes a\nDecentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and\ndistributed storage (IPFS). By leveraging the immutability of blockchain and\nthe content verification of IPFS, the system achieves decentralized storage and\ndistribution of domain name records, eliminating the centralized dependencies\nof traditional DNS. With a block time of 15 seconds, the system supports rapid\nbroadcasting of domain name updates, significantly improving resolution\nefficiency. The DDNS aims to serve as a complement or backup to the existing\nDNS system, providing a pollution-resistant, censorship-resistant,\nhigh-performance, and low-cost domain name resolution solution, offering a new\ntechnical path for the security and stability of the internet."
                },
                "authors": [
                    {
                        "name": "Guang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Guang Yang"
                },
                "author": "Guang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01959v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01959v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17747v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17747v1",
                "updated": "2024-12-23T18:02:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    18,
                    2,
                    25,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T18:02:25Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    18,
                    2,
                    25,
                    0,
                    358,
                    0
                ],
                "title": "Deliberation in Latent Space via Differentiable Cache Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deliberation in Latent Space via Differentiable Cache Augmentation"
                },
                "summary": "Techniques enabling large language models (LLMs) to \"think more\" by\ngenerating and attending to intermediate reasoning steps have shown promise in\nsolving complex problems. However, the standard approaches generate sequences\nof discrete tokens immediately before responding, and so they can incur\nsignificant latency costs and be challenging to optimize. In this work, we\ndemonstrate that a frozen LLM can be augmented with an offline coprocessor that\noperates on the model's key-value (kv) cache. This coprocessor augments the\ncache with a set of latent embeddings designed to improve the fidelity of\nsubsequent decoding. We train this coprocessor using the language modeling loss\nfrom the decoder on standard pretraining data, while keeping the decoder itself\nfrozen. This approach enables the model to learn, in an end-to-end\ndifferentiable fashion, how to distill additional computation into its\nkv-cache. Because the decoder remains unchanged, the coprocessor can operate\noffline and asynchronously, and the language model can function normally if the\ncoprocessor is unavailable or if a given cache is deemed not to require extra\ncomputation. We show experimentally that when a cache is augmented, the decoder\nachieves lower perplexity on numerous subsequent tokens. Furthermore, even\nwithout any task-specific training, our experiments demonstrate that cache\naugmentation consistently reduces perplexity and improves performance across a\nrange of reasoning-intensive tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Techniques enabling large language models (LLMs) to \"think more\" by\ngenerating and attending to intermediate reasoning steps have shown promise in\nsolving complex problems. However, the standard approaches generate sequences\nof discrete tokens immediately before responding, and so they can incur\nsignificant latency costs and be challenging to optimize. In this work, we\ndemonstrate that a frozen LLM can be augmented with an offline coprocessor that\noperates on the model's key-value (kv) cache. This coprocessor augments the\ncache with a set of latent embeddings designed to improve the fidelity of\nsubsequent decoding. We train this coprocessor using the language modeling loss\nfrom the decoder on standard pretraining data, while keeping the decoder itself\nfrozen. This approach enables the model to learn, in an end-to-end\ndifferentiable fashion, how to distill additional computation into its\nkv-cache. Because the decoder remains unchanged, the coprocessor can operate\noffline and asynchronously, and the language model can function normally if the\ncoprocessor is unavailable or if a given cache is deemed not to require extra\ncomputation. We show experimentally that when a cache is augmented, the decoder\nachieves lower perplexity on numerous subsequent tokens. Furthermore, even\nwithout any task-specific training, our experiments demonstrate that cache\naugmentation consistently reduces perplexity and improves performance across a\nrange of reasoning-intensive tasks."
                },
                "authors": [
                    {
                        "name": "Luyang Liu"
                    },
                    {
                        "name": "Jonas Pfeiffer"
                    },
                    {
                        "name": "Jiaxing Wu"
                    },
                    {
                        "name": "Jun Xie"
                    },
                    {
                        "name": "Arthur Szlam"
                    }
                ],
                "author_detail": {
                    "name": "Arthur Szlam"
                },
                "author": "Arthur Szlam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17747v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17747v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17685v1",
                "updated": "2024-12-23T16:11:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    16,
                    11,
                    18,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T16:11:18Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    16,
                    11,
                    18,
                    0,
                    358,
                    0
                ],
                "title": "A Reproducible Method for Mapping Electricity Transmission\n  Infrastructure for Space Weather Risk Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Reproducible Method for Mapping Electricity Transmission\n  Infrastructure for Space Weather Risk Assessment"
                },
                "summary": "Space weather impact assessment is constrained by the lack of available asset\ninformation to undertake modeling of Geomagnetically Induced Currents (GICs) in\nExtra High Voltage electricity infrastructure networks. The U.S. National Space\nWeather Strategy and Action Plan identifies underutilized data as a central\nissue for improving risk assessment, motivating this research. Accurate GIC\nprediction is generally not possible without information on the electrical\ncircuit, therefore we define a reproducible method based on open-source data,\nwhich enables risk analysts to collect their own substation component data.\nThis process converts OpenStreetMap (OSM) substation locations to\nhigh-resolution, component-level mapping of electricity transmission assets by\nutilizing an innovative web-browser platform to facilitate component\nannotation. As a case study example, we convert an initial 1,313 high-voltage\n(>115 kV) substations to 52,273 substation components via Google Earth APIs\nutilizing low-altitude, satellite, and Streetview imagery. We find that a total\nof 41,642 substation components (79.6%) connect to the highest substation\nvoltage levels (>345 kV) and are possibly susceptible to GIC, with a total of\n7,949 transformers identified. Compared to the initial OSM baseline, we provide\nnew detailed insights on voltage levels, line capacities, and substation\nconfigurations. Two validation workshops were undertaken to align the method\nand data with GIC assessment needs. The approach ensures consistency and rapid\nscalability, enabling users to quickly count components via a flexible\nweb-browser application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Space weather impact assessment is constrained by the lack of available asset\ninformation to undertake modeling of Geomagnetically Induced Currents (GICs) in\nExtra High Voltage electricity infrastructure networks. The U.S. National Space\nWeather Strategy and Action Plan identifies underutilized data as a central\nissue for improving risk assessment, motivating this research. Accurate GIC\nprediction is generally not possible without information on the electrical\ncircuit, therefore we define a reproducible method based on open-source data,\nwhich enables risk analysts to collect their own substation component data.\nThis process converts OpenStreetMap (OSM) substation locations to\nhigh-resolution, component-level mapping of electricity transmission assets by\nutilizing an innovative web-browser platform to facilitate component\nannotation. As a case study example, we convert an initial 1,313 high-voltage\n(>115 kV) substations to 52,273 substation components via Google Earth APIs\nutilizing low-altitude, satellite, and Streetview imagery. We find that a total\nof 41,642 substation components (79.6%) connect to the highest substation\nvoltage levels (>345 kV) and are possibly susceptible to GIC, with a total of\n7,949 transformers identified. Compared to the initial OSM baseline, we provide\nnew detailed insights on voltage levels, line capacities, and substation\nconfigurations. Two validation workshops were undertaken to align the method\nand data with GIC assessment needs. The approach ensures consistency and rapid\nscalability, enabling users to quickly count components via a flexible\nweb-browser application."
                },
                "authors": [
                    {
                        "name": "Edward J. Oughton"
                    },
                    {
                        "name": "Evan Alexander Peters"
                    },
                    {
                        "name": "Dennies Bor"
                    },
                    {
                        "name": "Noah Rivera"
                    },
                    {
                        "name": "C. Trevor Gaunt"
                    },
                    {
                        "name": "Robert Weigel"
                    }
                ],
                "author_detail": {
                    "name": "Robert Weigel"
                },
                "author": "Robert Weigel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.geo-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18919v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18919v2",
                "updated": "2024-12-23T14:40:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    14,
                    40,
                    26,
                    0,
                    358,
                    0
                ],
                "published": "2024-05-29T09:22:25Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    9,
                    22,
                    25,
                    2,
                    150,
                    0
                ],
                "title": "Inter-Satellite Link-Enhanced Transmission Scheme Towards Aviation IoT\n  in SAGIN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inter-Satellite Link-Enhanced Transmission Scheme Towards Aviation IoT\n  in SAGIN"
                },
                "summary": "The rapid development of the aviation Internet of Things (IoT) has positioned\nin-flight connectivity (IFC) as one of its critical applications.\nSpace-air-ground integrated networks (SAGIN) are essential for ensuring the\nperformance of IFC by enabling seamless and reliable connectivity. However,\nmost existing research treats satellites merely as transparent forwarding nodes\nand overlooks their potential caching capabilities to enhance IFC data rates.\nIn this article, we explore an IFC-oriented SAGIN where satellites and ground\nstations (GSs) work together to transmit content to airborne passengers,\nthereby facilitating airborne communication. By categorizing files into cached\n(instantly accessible via satellites) and non-cached files (available only\nthrough GSs), this article pioneers the integration of multiple inter-satellite\nlinks (ISLs) into the IFC framework, thus innovating the content delivery\nprocess for both types of files. To minimize the average delay of content\ndelivery, we formulate the corresponding optimization problems: 1) For cached\nfiles, we propose an exact penalty-based method to determine the satellite\nassociation scheme. 2) For non-cached files, we present an efficient algorithm\nbased on alternating optimization to jointly optimize satellite association and\nGS bandwidth allocation. Our proposed framework is low in complexity, paving\nthe way for high-speed Internet connectivity for aviation passengers. Finally,\nsimulation results are provided to demonstrate the effectiveness of our\nproposed IFC framework for SAGIN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of the aviation Internet of Things (IoT) has positioned\nin-flight connectivity (IFC) as one of its critical applications.\nSpace-air-ground integrated networks (SAGIN) are essential for ensuring the\nperformance of IFC by enabling seamless and reliable connectivity. However,\nmost existing research treats satellites merely as transparent forwarding nodes\nand overlooks their potential caching capabilities to enhance IFC data rates.\nIn this article, we explore an IFC-oriented SAGIN where satellites and ground\nstations (GSs) work together to transmit content to airborne passengers,\nthereby facilitating airborne communication. By categorizing files into cached\n(instantly accessible via satellites) and non-cached files (available only\nthrough GSs), this article pioneers the integration of multiple inter-satellite\nlinks (ISLs) into the IFC framework, thus innovating the content delivery\nprocess for both types of files. To minimize the average delay of content\ndelivery, we formulate the corresponding optimization problems: 1) For cached\nfiles, we propose an exact penalty-based method to determine the satellite\nassociation scheme. 2) For non-cached files, we present an efficient algorithm\nbased on alternating optimization to jointly optimize satellite association and\nGS bandwidth allocation. Our proposed framework is low in complexity, paving\nthe way for high-speed Internet connectivity for aviation passengers. Finally,\nsimulation results are provided to demonstrate the effectiveness of our\nproposed IFC framework for SAGIN."
                },
                "authors": [
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Chenyu Wu"
                    },
                    {
                        "name": "Shuai Han"
                    },
                    {
                        "name": "Weixiao Meng"
                    },
                    {
                        "name": "Tony Q. S. Quek"
                    }
                ],
                "author_detail": {
                    "name": "Tony Q. S. Quek"
                },
                "author": "Tony Q. S. Quek",
                "arxiv_comment": "14 pages, 13 figures. This work has been accepted by IEEE Internet of\n  Things Journal. It is expanded on our previous research presented at the IEEE\n  Globecom 2024: Q. Chen, C. Wu, S. Han, W. Meng, and T. Q. Quek, \"Exploiting\n  Inter-Satellite Links for In-Flight Connectivity Scheme in Space-Air-Ground\n  Integrated Networks,\" in Proc. GLOBECOM 2024, Cape Town, South Africa, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18919v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18919v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.03408v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.03408v3",
                "updated": "2024-12-23T12:55:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    12,
                    55,
                    21,
                    0,
                    358,
                    0
                ],
                "published": "2024-02-05T15:10:42Z",
                "published_parsed": [
                    2024,
                    2,
                    5,
                    15,
                    10,
                    42,
                    0,
                    36,
                    0
                ],
                "title": "A Framework for Effective Invocation Methods of Various LLM Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Framework for Effective Invocation Methods of Various LLM Services"
                },
                "summary": "Large Language Models (LLMs) have shown impressive abilities in solving\nvarious natural language processing tasks and are now widely offered as\nservices. LLM services enable users to accomplish tasks without requiring\nspecialized knowledge, simply by paying service providers. However, numerous\nproviders offer various LLM services with variations in pricing, latency, and\nperformance. These factors are also affected by different invocation methods,\nsuch as the choice of context and the use of cache, which lead to unpredictable\nand uncontrollable service cost and quality. Consequently, utilizing various\nLLM services invocation methods to construct an effective (cost-saving,\nlow-latency and high-performance) invocation strategy that best meets task\ndemands becomes a pressing challenge. This paper provides a comprehensive\noverview of methods help LLM services to be invoked efficiently. Technically,\nwe define the problem of constructing an effective LLM services invocation\nstrategy, and based on this, propose a unified LLM service invocation\nframework. The framework classifies existing methods into four categories:\ninput abstraction, semantic cache, solution design, and output enhancement,\nwhich can be used separately or jointly during the invocation life cycle. We\ndiscuss the methods in each category and compare them to provide valuable\nguidance for researchers. Finally, we emphasize the open challenges in this\ndomain and shed light on future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive abilities in solving\nvarious natural language processing tasks and are now widely offered as\nservices. LLM services enable users to accomplish tasks without requiring\nspecialized knowledge, simply by paying service providers. However, numerous\nproviders offer various LLM services with variations in pricing, latency, and\nperformance. These factors are also affected by different invocation methods,\nsuch as the choice of context and the use of cache, which lead to unpredictable\nand uncontrollable service cost and quality. Consequently, utilizing various\nLLM services invocation methods to construct an effective (cost-saving,\nlow-latency and high-performance) invocation strategy that best meets task\ndemands becomes a pressing challenge. This paper provides a comprehensive\noverview of methods help LLM services to be invoked efficiently. Technically,\nwe define the problem of constructing an effective LLM services invocation\nstrategy, and based on this, propose a unified LLM service invocation\nframework. The framework classifies existing methods into four categories:\ninput abstraction, semantic cache, solution design, and output enhancement,\nwhich can be used separately or jointly during the invocation life cycle. We\ndiscuss the methods in each category and compare them to provide valuable\nguidance for researchers. Finally, we emphasize the open challenges in this\ndomain and shed light on future research."
                },
                "authors": [
                    {
                        "name": "Can Wang"
                    },
                    {
                        "name": "Dianbo Sui"
                    },
                    {
                        "name": "Bolin Zhang"
                    },
                    {
                        "name": "Xiaoyu Liu"
                    },
                    {
                        "name": "Jiabao Kang"
                    },
                    {
                        "name": "Zhidong Qiao"
                    },
                    {
                        "name": "Zhiying Tu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiying Tu"
                },
                "author": "Zhiying Tu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.03408v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.03408v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17464v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17464v1",
                "updated": "2024-12-23T10:41:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    10,
                    41,
                    18,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T10:41:18Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    10,
                    41,
                    18,
                    0,
                    358,
                    0
                ],
                "title": "CALLIC: Content Adaptive Learning for Lossless Image Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CALLIC: Content Adaptive Learning for Lossless Image Compression"
                },
                "summary": "Learned lossless image compression has achieved significant advancements in\nrecent years. However, existing methods often rely on training amortized\ngenerative models on massive datasets, resulting in sub-optimal probability\ndistribution estimation for specific testing images during encoding process. To\naddress this challenge, we explore the connection between the Minimum\nDescription Length (MDL) principle and Parameter-Efficient Transfer Learning\n(PETL), leading to the development of a novel content-adaptive approach for\nlearned lossless image compression, dubbed CALLIC. Specifically, we first\npropose a content-aware autoregressive self-attention mechanism by leveraging\nconvolutional gating operations, termed Masked Gated ConvFormer (MGCF), and\npretrain MGCF on training dataset. Cache then Crop Inference (CCI) is proposed\nto accelerate the coding process. During encoding, we decompose pre-trained\nlayers, including depth-wise convolutions, using low-rank matrices and then\nadapt the incremental weights on testing image by Rate-guided Progressive\nFine-Tuning (RPFT). RPFT fine-tunes with gradually increasing patches that are\nsorted in descending order by estimated entropy, optimizing learning process\nand reducing adaptation time. Extensive experiments across diverse datasets\ndemonstrate that CALLIC sets a new state-of-the-art (SOTA) for learned lossless\nimage compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learned lossless image compression has achieved significant advancements in\nrecent years. However, existing methods often rely on training amortized\ngenerative models on massive datasets, resulting in sub-optimal probability\ndistribution estimation for specific testing images during encoding process. To\naddress this challenge, we explore the connection between the Minimum\nDescription Length (MDL) principle and Parameter-Efficient Transfer Learning\n(PETL), leading to the development of a novel content-adaptive approach for\nlearned lossless image compression, dubbed CALLIC. Specifically, we first\npropose a content-aware autoregressive self-attention mechanism by leveraging\nconvolutional gating operations, termed Masked Gated ConvFormer (MGCF), and\npretrain MGCF on training dataset. Cache then Crop Inference (CCI) is proposed\nto accelerate the coding process. During encoding, we decompose pre-trained\nlayers, including depth-wise convolutions, using low-rank matrices and then\nadapt the incremental weights on testing image by Rate-guided Progressive\nFine-Tuning (RPFT). RPFT fine-tunes with gradually increasing patches that are\nsorted in descending order by estimated entropy, optimizing learning process\nand reducing adaptation time. Extensive experiments across diverse datasets\ndemonstrate that CALLIC sets a new state-of-the-art (SOTA) for learned lossless\nimage compression."
                },
                "authors": [
                    {
                        "name": "Daxin Li"
                    },
                    {
                        "name": "Yuanchao Bai"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Junjun Jiang"
                    },
                    {
                        "name": "Xianming Liu"
                    },
                    {
                        "name": "Wen Gao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Gao"
                },
                "author": "Wen Gao",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17464v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17464v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17246v1",
                "updated": "2024-12-23T03:38:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    3,
                    38,
                    46,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T03:38:46Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    3,
                    38,
                    46,
                    0,
                    358,
                    0
                ],
                "title": "Fast and Live Model Auto Scaling with O(1) Host Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and Live Model Auto Scaling with O(1) Host Caching"
                },
                "summary": "Model autoscaling is the key mechanism to achieve serverless\nmodel-as-a-service, but it faces a fundamental trade-off between scaling speed\nand storage/memory usage to cache parameters, and cannot meet frequent scaling\nrequirements across multiple hosts. The key problem is that data plane\nperformance is slow, and scaled instances remain stopped while parameters are\nloading. We first show that data plane can be made fast with no/O(1) caching by\nloading parameters through the compute network between GPUs because: (1) its\nspeed is comparable host cache and is underutilized; (2) scaling multiple\ninstances requires no or O(1) caching with network-optimized multicast. Second,\nautoscaling can be made live by breaking the scaling abstraction from a\ncoarse-grained instance-level to a fine-grained layer-level. This allows us to\noffload the layer computation from the overloaded serving instances to the\nscaled instance with cooperative execution, thus handles cases even when the\ncompute network is not sufficiently fast. Our system BLITZSCALE reduces the\nserving tail latencies by up to 86% without caching, and we achieve comparable\nperformance (or even better) to an optimal setup where all the parameters are\ncached at all the host for autoscaling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model autoscaling is the key mechanism to achieve serverless\nmodel-as-a-service, but it faces a fundamental trade-off between scaling speed\nand storage/memory usage to cache parameters, and cannot meet frequent scaling\nrequirements across multiple hosts. The key problem is that data plane\nperformance is slow, and scaled instances remain stopped while parameters are\nloading. We first show that data plane can be made fast with no/O(1) caching by\nloading parameters through the compute network between GPUs because: (1) its\nspeed is comparable host cache and is underutilized; (2) scaling multiple\ninstances requires no or O(1) caching with network-optimized multicast. Second,\nautoscaling can be made live by breaking the scaling abstraction from a\ncoarse-grained instance-level to a fine-grained layer-level. This allows us to\noffload the layer computation from the overloaded serving instances to the\nscaled instance with cooperative execution, thus handles cases even when the\ncompute network is not sufficiently fast. Our system BLITZSCALE reduces the\nserving tail latencies by up to 86% without caching, and we achieve comparable\nperformance (or even better) to an optimal setup where all the parameters are\ncached at all the host for autoscaling."
                },
                "authors": [
                    {
                        "name": "Dingyan Zhang"
                    },
                    {
                        "name": "Haotian Wang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05831v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05831v2",
                "updated": "2024-12-23T02:52:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    2,
                    52,
                    36,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-08T06:37:27Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    6,
                    37,
                    27,
                    6,
                    343,
                    0
                ],
                "title": "Semi-Supervised Contrastive Learning for Controllable Video-to-Music\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semi-Supervised Contrastive Learning for Controllable Video-to-Music\n  Retrieval"
                },
                "summary": "Content creators often use music to enhance their videos, from soundtracks in\nmovies to background music in video blogs and social media content. However,\nidentifying the best music for a video can be a difficult and time-consuming\ntask. To address this challenge, we propose a novel framework for automatically\nretrieving a matching music clip for a given video, and vice versa. Our\napproach leverages annotated music labels, as well as the inherent artistic\ncorrespondence between visual and music elements. Distinct from previous\ncross-modal music retrieval works, our method combines both self-supervised and\nsupervised training objectives. We use self-supervised and label-supervised\ncontrastive learning to train a joint embedding space between music and video.\nWe show the effectiveness of our approach by using music genre labels for the\nsupervised training component, and our framework can be generalized to other\nmusic annotations (e.g., emotion, instrument, etc.). Furthermore, our method\nenables fine-grained control over how much the retrieval process focuses on\nself-supervised vs. label information at inference time. We evaluate the\nlearned embeddings through a variety of video-to-music and music-to-video\nretrieval tasks. Our experiments show that the proposed approach successfully\ncombines self-supervised and supervised objectives and is effective for\ncontrollable music-video retrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content creators often use music to enhance their videos, from soundtracks in\nmovies to background music in video blogs and social media content. However,\nidentifying the best music for a video can be a difficult and time-consuming\ntask. To address this challenge, we propose a novel framework for automatically\nretrieving a matching music clip for a given video, and vice versa. Our\napproach leverages annotated music labels, as well as the inherent artistic\ncorrespondence between visual and music elements. Distinct from previous\ncross-modal music retrieval works, our method combines both self-supervised and\nsupervised training objectives. We use self-supervised and label-supervised\ncontrastive learning to train a joint embedding space between music and video.\nWe show the effectiveness of our approach by using music genre labels for the\nsupervised training component, and our framework can be generalized to other\nmusic annotations (e.g., emotion, instrument, etc.). Furthermore, our method\nenables fine-grained control over how much the retrieval process focuses on\nself-supervised vs. label information at inference time. We evaluate the\nlearned embeddings through a variety of video-to-music and music-to-video\nretrieval tasks. Our experiments show that the proposed approach successfully\ncombines self-supervised and supervised objectives and is effective for\ncontrollable music-video retrieval."
                },
                "authors": [
                    {
                        "name": "Shanti Stewart"
                    },
                    {
                        "name": "Gouthaman KV"
                    },
                    {
                        "name": "Lie Lu"
                    },
                    {
                        "name": "Andrea Fanelli"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Fanelli"
                },
                "author": "Andrea Fanelli",
                "arxiv_comment": "Accepted at ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05831v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05831v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17203v1",
                "updated": "2024-12-23T00:46:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    0,
                    46,
                    53,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T00:46:53Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    0,
                    46,
                    53,
                    0,
                    358,
                    0
                ],
                "title": "Agile TLB Prefetching and Prediction Replacement Policy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agile TLB Prefetching and Prediction Replacement Policy"
                },
                "summary": "Virtual-to-physical address translation is a critical performance bottleneck\nin paging-based virtual memory systems. The Translation Lookaside Buffer (TLB)\naccelerates address translation by caching frequently accessed mappings, but\nTLB misses lead to costly page walks. Hardware and software techniques address\nthis challenge. Hardware approaches enhance TLB reach through system-level\nsupport, while software optimizations include TLB prefetching, replacement\npolicies, superpages, and page size adjustments. Prefetching Page Table Entries\n(PTEs) for future accesses reduces bottlenecks but may incur overhead from\nincorrect predictions. Integrating an Agile TLB Prefetcher (ATP) with SBFP\noptimizes performance by leveraging page table locality and dynamically\nidentifying essential free PTEs during page walks. Predictive replacement\npolicies further improve TLB performance. Traditional LRU replacement is\nlimited to near-instant references, while advanced policies like SRRIP, GHRP,\nSHiP, SDBP, and CHiRP enhance performance by targeting specific inefficiencies.\nCHiRP, tailored for L2 TLBs, surpasses other policies by leveraging control\nflow history to detect dead blocks, utilizing L2 TLB entries for learning\ninstead of sampling. These integrated techniques collectively address key\nchallenges in virtual memory management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virtual-to-physical address translation is a critical performance bottleneck\nin paging-based virtual memory systems. The Translation Lookaside Buffer (TLB)\naccelerates address translation by caching frequently accessed mappings, but\nTLB misses lead to costly page walks. Hardware and software techniques address\nthis challenge. Hardware approaches enhance TLB reach through system-level\nsupport, while software optimizations include TLB prefetching, replacement\npolicies, superpages, and page size adjustments. Prefetching Page Table Entries\n(PTEs) for future accesses reduces bottlenecks but may incur overhead from\nincorrect predictions. Integrating an Agile TLB Prefetcher (ATP) with SBFP\noptimizes performance by leveraging page table locality and dynamically\nidentifying essential free PTEs during page walks. Predictive replacement\npolicies further improve TLB performance. Traditional LRU replacement is\nlimited to near-instant references, while advanced policies like SRRIP, GHRP,\nSHiP, SDBP, and CHiRP enhance performance by targeting specific inefficiencies.\nCHiRP, tailored for L2 TLBs, surpasses other policies by leveraging control\nflow history to detect dead blocks, utilizing L2 TLB entries for learning\ninstead of sampling. These integrated techniques collectively address key\nchallenges in virtual memory management."
                },
                "authors": [
                    {
                        "name": "Melkamu Mersha"
                    },
                    {
                        "name": "Tsion Abay"
                    },
                    {
                        "name": "Mingziem Bitewa"
                    },
                    {
                        "name": "Gedare Bloom"
                    }
                ],
                "author_detail": {
                    "name": "Gedare Bloom"
                },
                "author": "Gedare Bloom",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16897v1",
                "updated": "2024-12-22T07:14:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    22,
                    7,
                    14,
                    45,
                    6,
                    357,
                    0
                ],
                "published": "2024-12-22T07:14:45Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    7,
                    14,
                    45,
                    6,
                    357,
                    0
                ],
                "title": "MVREC: A General Few-shot Defect Classification Model Using Multi-View\n  Region-Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MVREC: A General Few-shot Defect Classification Model Using Multi-View\n  Region-Context"
                },
                "summary": "Few-shot defect multi-classification (FSDMC) is an emerging trend in quality\ncontrol within industrial manufacturing. However, current FSDMC research often\nlacks generalizability due to its focus on specific datasets. Additionally,\ndefect classification heavily relies on contextual information within images,\nand existing methods fall short of effectively extracting this information. To\naddress these challenges, we propose a general FSDMC framework called MVREC,\nwhich offers two primary advantages: (1) MVREC extracts general features for\ndefect instances by incorporating the pre-trained AlphaCLIP model. (2) It\nutilizes a region-context framework to enhance defect features by leveraging\nmask region input and multi-view context augmentation. Furthermore, Few-shot\nZip-Adapter(-F) classifiers within the model are introduced to cache the visual\nfeatures of the support set and perform few-shot classification. We also\nintroduce MVTec-FS, a new FSDMC benchmark based on MVTec AD, which includes\n1228 defect images with instance-level mask annotations and 46 defect types.\nExtensive experiments conducted on MVTec-FS and four additional datasets\ndemonstrate its effectiveness in general defect classification and its ability\nto incorporate contextual information to improve classification performance.\nCode: https://github.com/ShuaiLYU/MVREC",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-shot defect multi-classification (FSDMC) is an emerging trend in quality\ncontrol within industrial manufacturing. However, current FSDMC research often\nlacks generalizability due to its focus on specific datasets. Additionally,\ndefect classification heavily relies on contextual information within images,\nand existing methods fall short of effectively extracting this information. To\naddress these challenges, we propose a general FSDMC framework called MVREC,\nwhich offers two primary advantages: (1) MVREC extracts general features for\ndefect instances by incorporating the pre-trained AlphaCLIP model. (2) It\nutilizes a region-context framework to enhance defect features by leveraging\nmask region input and multi-view context augmentation. Furthermore, Few-shot\nZip-Adapter(-F) classifiers within the model are introduced to cache the visual\nfeatures of the support set and perform few-shot classification. We also\nintroduce MVTec-FS, a new FSDMC benchmark based on MVTec AD, which includes\n1228 defect images with instance-level mask annotations and 46 defect types.\nExtensive experiments conducted on MVTec-FS and four additional datasets\ndemonstrate its effectiveness in general defect classification and its ability\nto incorporate contextual information to improve classification performance.\nCode: https://github.com/ShuaiLYU/MVREC"
                },
                "authors": [
                    {
                        "name": "Shuai Lyu"
                    },
                    {
                        "name": "Fangjian Liao"
                    },
                    {
                        "name": "Zeqi Ma"
                    },
                    {
                        "name": "Rongchen Zhang"
                    },
                    {
                        "name": "Dongmei Mo"
                    },
                    {
                        "name": "Waikeung Wong"
                    }
                ],
                "author_detail": {
                    "name": "Waikeung Wong"
                },
                "author": "Waikeung Wong",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17565v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17565v3",
                "updated": "2024-12-21T13:55:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    13,
                    55,
                    49,
                    5,
                    356,
                    0
                ],
                "published": "2024-06-25T14:02:08Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    14,
                    2,
                    8,
                    1,
                    177,
                    0
                ],
                "title": "MemServe: Context Caching for Disaggregated LLM Serving with Elastic\n  Memory Pool",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemServe: Context Caching for Disaggregated LLM Serving with Elastic\n  Memory Pool"
                },
                "summary": "Large language model (LLM) serving has transformed from stateless to stateful\nsystems, utilizing techniques like context caching and disaggregated inference.\nThese optimizations extend the lifespan and domain of the KV cache,\nnecessitating a new architectural approach. We present MemServe, a unified\nsystem that integrates both inter-request and intra-request optimizations.\nMemServe introduces MemPool, an elastic memory pool managing distributed memory\nand KV caches across serving instances. Using MemPool APIs, MemServe combines\ncontext caching with disaggregated inference for the first time, supported by a\nglobal scheduler that enhances cache reuse through a global prompt tree-based\nlocality-aware policy. Tests show that MemServe significantly improves job\ncompletion time and time-to-first-time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) serving has transformed from stateless to stateful\nsystems, utilizing techniques like context caching and disaggregated inference.\nThese optimizations extend the lifespan and domain of the KV cache,\nnecessitating a new architectural approach. We present MemServe, a unified\nsystem that integrates both inter-request and intra-request optimizations.\nMemServe introduces MemPool, an elastic memory pool managing distributed memory\nand KV caches across serving instances. Using MemPool APIs, MemServe combines\ncontext caching with disaggregated inference for the first time, supported by a\nglobal scheduler that enhances cache reuse through a global prompt tree-based\nlocality-aware policy. Tests show that MemServe significantly improves job\ncompletion time and time-to-first-time."
                },
                "authors": [
                    {
                        "name": "Cunchen Hu"
                    },
                    {
                        "name": "Heyang Huang"
                    },
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Jiang Xu"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Tao Xie"
                    },
                    {
                        "name": "Chenxi Wang"
                    },
                    {
                        "name": "Sa Wang"
                    },
                    {
                        "name": "Yungang Bao"
                    },
                    {
                        "name": "Ninghui Sun"
                    },
                    {
                        "name": "Yizhou Shan"
                    }
                ],
                "author_detail": {
                    "name": "Yizhou Shan"
                },
                "author": "Yizhou Shan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17565v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17565v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16585v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16585v1",
                "updated": "2024-12-21T11:20:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    11,
                    20,
                    26,
                    5,
                    356,
                    0
                ],
                "published": "2024-12-21T11:20:26Z",
                "published_parsed": [
                    2024,
                    12,
                    21,
                    11,
                    20,
                    26,
                    5,
                    356,
                    0
                ],
                "title": "Parameterized Complexity of Caching in Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameterized Complexity of Caching in Networks"
                },
                "summary": "The fundamental caching problem in networks asks to find an allocation of\ncontents to a network of caches with the aim of maximizing the cache hit rate.\nDespite the problem's importance to a variety of research areas -- including\nnot only content delivery, but also edge intelligence and inference -- and the\nextensive body of work on empirical aspects of caching, very little is known\nabout the exact boundaries of tractability for the problem beyond its general\nNP-hardness. We close this gap by performing a comprehensive\ncomplexity-theoretic analysis of the problem through the lens of the\nparameterized complexity paradigm, which is designed to provide more precise\nstatements regarding algorithmic tractability than classical complexity. Our\nresults include algorithmic lower and upper bounds which together establish the\nconditions under which the caching problem becomes tractable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The fundamental caching problem in networks asks to find an allocation of\ncontents to a network of caches with the aim of maximizing the cache hit rate.\nDespite the problem's importance to a variety of research areas -- including\nnot only content delivery, but also edge intelligence and inference -- and the\nextensive body of work on empirical aspects of caching, very little is known\nabout the exact boundaries of tractability for the problem beyond its general\nNP-hardness. We close this gap by performing a comprehensive\ncomplexity-theoretic analysis of the problem through the lens of the\nparameterized complexity paradigm, which is designed to provide more precise\nstatements regarding algorithmic tractability than classical complexity. Our\nresults include algorithmic lower and upper bounds which together establish the\nconditions under which the caching problem becomes tractable."
                },
                "authors": [
                    {
                        "name": "Robert Ganian"
                    },
                    {
                        "name": "Fionn Mc Inerney"
                    },
                    {
                        "name": "Dimitra Tsigkari"
                    }
                ],
                "author_detail": {
                    "name": "Dimitra Tsigkari"
                },
                "author": "Dimitra Tsigkari",
                "arxiv_comment": "A shorter version of this paper will appear in the proceedings of\n  AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16585v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16585v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01253v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01253v4",
                "updated": "2024-12-21T02:36:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    2,
                    36,
                    3,
                    5,
                    356,
                    0
                ],
                "published": "2024-12-02T08:22:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    22,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Yi-Lightning Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yi-Lightning Technical Report"
                },
                "summary": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com."
                },
                "authors": [
                    {
                        "name": "Alan Wake"
                    },
                    {
                        "name": "Bei Chen"
                    },
                    {
                        "name": "C. X. Lv"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Chengen Huang"
                    },
                    {
                        "name": "Chenglin Cai"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Daniel Cooper"
                    },
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Feng Hu"
                    },
                    {
                        "name": "Guoyin Wang"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Howard Qiu"
                    },
                    {
                        "name": "Jiangcheng Zhu"
                    },
                    {
                        "name": "Jun Tian"
                    },
                    {
                        "name": "Katherine Su"
                    },
                    {
                        "name": "Lihuan Zhang"
                    },
                    {
                        "name": "Liying Li"
                    },
                    {
                        "name": "Ming Song"
                    },
                    {
                        "name": "Mou Li"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Qicheng Hu"
                    },
                    {
                        "name": "Shawn Wang"
                    },
                    {
                        "name": "Shijun Zhou"
                    },
                    {
                        "name": "Shiming Yang"
                    },
                    {
                        "name": "Shiyong Li"
                    },
                    {
                        "name": "Tianhang Zhu"
                    },
                    {
                        "name": "Wen Xie"
                    },
                    {
                        "name": "Xiang He"
                    },
                    {
                        "name": "Xiaobo Chen"
                    },
                    {
                        "name": "Xiaohui Hu"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Xinyao Niu"
                    },
                    {
                        "name": "Yanpeng Li"
                    },
                    {
                        "name": "Yongke Zhao"
                    },
                    {
                        "name": "Yongzhen Luo"
                    },
                    {
                        "name": "Yuchi Xu"
                    },
                    {
                        "name": "Yuxuan Sha"
                    },
                    {
                        "name": "Zhaodong Yan"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Zirui Zhang"
                    },
                    {
                        "name": "Zonghong Dai"
                    }
                ],
                "author_detail": {
                    "name": "Zonghong Dai"
                },
                "author": "Zonghong Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01253v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01253v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16434v1",
                "updated": "2024-12-21T01:48:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    1,
                    48,
                    52,
                    5,
                    356,
                    0
                ],
                "published": "2024-12-21T01:48:52Z",
                "published_parsed": [
                    2024,
                    12,
                    21,
                    1,
                    48,
                    52,
                    5,
                    356,
                    0
                ],
                "title": "SYMPHONY: Improving Memory Management for LLM Inference Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SYMPHONY: Improving Memory Management for LLM Inference Workloads"
                },
                "summary": "Large Language Models (LLMs) are increasingly being deployed in applications\nsuch as chatbots, code editors, and conversational agents. A key feature of\nLLMs is their ability to engage in multi-turn interactions with humans or\nexternal tools, enabling a wide range of tasks. Each new request in a\nmulti-turn interaction depends on the intermediate state, specifically the\nkey-value (K,V) caches, from previous requests in the ongoing interaction.\nExisting serving engines either recompute the K,V caches or offload them to\nmain memory. Profiling reveals that recomputation can result in over 99% of\nprocessed tokens being redundant. On the other hand, offloading K,V caches from\nGPU memory makes inference serving stateful, leading to load imbalances across\nthe cluster. To address these challenges, we developed SYMPHONY. SYMPHONY\nleverages the observation that multi-turn work loads provide additional hints\nthat allow K,V caches to be migrated off the critical serving path. By\nutilizing these hints, SYMPHONY dynamically migrates K,V caches to enable\nfinegrained scheduling of inference requests. Our experiments demonstrate that\nSYMPHONY can handle over 8x the number of requests compared to state-of-the-art\nbaselines, with a similar latency profile.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly being deployed in applications\nsuch as chatbots, code editors, and conversational agents. A key feature of\nLLMs is their ability to engage in multi-turn interactions with humans or\nexternal tools, enabling a wide range of tasks. Each new request in a\nmulti-turn interaction depends on the intermediate state, specifically the\nkey-value (K,V) caches, from previous requests in the ongoing interaction.\nExisting serving engines either recompute the K,V caches or offload them to\nmain memory. Profiling reveals that recomputation can result in over 99% of\nprocessed tokens being redundant. On the other hand, offloading K,V caches from\nGPU memory makes inference serving stateful, leading to load imbalances across\nthe cluster. To address these challenges, we developed SYMPHONY. SYMPHONY\nleverages the observation that multi-turn work loads provide additional hints\nthat allow K,V caches to be migrated off the critical serving path. By\nutilizing these hints, SYMPHONY dynamically migrates K,V caches to enable\nfinegrained scheduling of inference requests. Our experiments demonstrate that\nSYMPHONY can handle over 8x the number of requests compared to state-of-the-art\nbaselines, with a similar latency profile."
                },
                "authors": [
                    {
                        "name": "Saurabh Agarwal"
                    },
                    {
                        "name": "Anyong Mao"
                    },
                    {
                        "name": "Aditya Akella"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16001v1",
                "updated": "2024-12-20T15:51:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    51,
                    42,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T15:51:42Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    51,
                    42,
                    4,
                    355,
                    0
                ],
                "title": "Multi-Strided Access Patterns to Boost Hardware Prefetching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Strided Access Patterns to Boost Hardware Prefetching"
                },
                "summary": "Important memory-bound kernels, such as linear algebra, convolutions, and\nstencils, rely on SIMD instructions as well as optimizations targeting improved\nvectorized data traversal and data re-use to attain satisfactory performance.\nOn on temporary CPU architectures, the hardware prefetcher is of key importance\nfor efficient utilization of the memory hierarchy. In this paper, we\ndemonstrate that transforming a memory access pattern consisting of a single\nstride to one that concurrently accesses multiple strides, can boost the\nutilization of the hardware prefetcher, and in turn improves the performance of\nmemory-bound kernels significantly. Using a set of micro-benchmarks, we\nestablish that accessing memory in a multi-strided manner enables more cache\nlines to be concurrently brought into the cache, resulting in improved cache\nhit ratios and higher effective memory bandwidth without the introduction of\ncostly software prefetch instructions. Subsequently, we show that multi-strided\nvariants of a collection of six memory-bound dense compute kernels outperform\nstate-of-the-art counterparts on three different micro-architectures. More\nspecifically, for kernels among which Matrix Vector Multiplication, Convolution\nStencil and kernels from PolyBench, we achieve significant speedups of up to\n12.55x over Polly, 2.99x over MKL, 1.98x over OpenBLAS, 1.08x over Halide and\n1.87x over OpenCV. The code transformation to take advantage of multi-strided\nmemory access is a natural extension of the loop unroll and loop interchange\ntechniques, allowing this method to be incorporated into compiler pipelines in\nthe future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Important memory-bound kernels, such as linear algebra, convolutions, and\nstencils, rely on SIMD instructions as well as optimizations targeting improved\nvectorized data traversal and data re-use to attain satisfactory performance.\nOn on temporary CPU architectures, the hardware prefetcher is of key importance\nfor efficient utilization of the memory hierarchy. In this paper, we\ndemonstrate that transforming a memory access pattern consisting of a single\nstride to one that concurrently accesses multiple strides, can boost the\nutilization of the hardware prefetcher, and in turn improves the performance of\nmemory-bound kernels significantly. Using a set of micro-benchmarks, we\nestablish that accessing memory in a multi-strided manner enables more cache\nlines to be concurrently brought into the cache, resulting in improved cache\nhit ratios and higher effective memory bandwidth without the introduction of\ncostly software prefetch instructions. Subsequently, we show that multi-strided\nvariants of a collection of six memory-bound dense compute kernels outperform\nstate-of-the-art counterparts on three different micro-architectures. More\nspecifically, for kernels among which Matrix Vector Multiplication, Convolution\nStencil and kernels from PolyBench, we achieve significant speedups of up to\n12.55x over Polly, 2.99x over MKL, 1.98x over OpenBLAS, 1.08x over Halide and\n1.87x over OpenCV. The code transformation to take advantage of multi-strided\nmemory access is a natural extension of the loop unroll and loop interchange\ntechniques, allowing this method to be incorporated into compiler pipelines in\nthe future."
                },
                "authors": [
                    {
                        "name": "Miguel O. Blom"
                    },
                    {
                        "name": "Kristian F. D. Rietveld"
                    },
                    {
                        "name": "Rob V. van Nieuwpoort"
                    }
                ],
                "author_detail": {
                    "name": "Rob V. van Nieuwpoort"
                },
                "author": "Rob V. van Nieuwpoort",
                "arxiv_comment": "12 pages, 6 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14485v2",
                "updated": "2024-12-20T15:18:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    18,
                    44,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-19T03:11:33Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    3,
                    11,
                    33,
                    3,
                    354,
                    0
                ],
                "title": "Towards Projected and Incremental Pseudo-Boolean Model Counting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Projected and Incremental Pseudo-Boolean Model Counting"
                },
                "summary": "Model counting is a fundamental task that involves determining the number of\nsatisfying assignments to a logical formula, typically in conjunctive normal\nform (CNF). While CNF model counting has received extensive attention over\nrecent decades, interest in Pseudo-Boolean (PB) model counting is just emerging\npartly due to the greater flexibility of PB formulas. As such, we observed\nfeature gaps in existing PB counters such as a lack of support for projected\nand incremental settings, which could hinder adoption. In this work, our main\ncontribution is the introduction of the PB model counter PBCount2, the first\nexact PB model counter with support for projected and incremental model\ncounting. Our counter, PBCount2, uses our Least Occurrence Weighted Min Degree\n(LOW-MD) computation ordering heuristic to support projected model counting and\na cache mechanism to enable incremental model counting. In our evaluations,\nPBCount2 completed at least 1.40x the number of benchmarks of competing methods\nfor projected model counting and at least 1.18x of competing methods in\nincremental model counting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model counting is a fundamental task that involves determining the number of\nsatisfying assignments to a logical formula, typically in conjunctive normal\nform (CNF). While CNF model counting has received extensive attention over\nrecent decades, interest in Pseudo-Boolean (PB) model counting is just emerging\npartly due to the greater flexibility of PB formulas. As such, we observed\nfeature gaps in existing PB counters such as a lack of support for projected\nand incremental settings, which could hinder adoption. In this work, our main\ncontribution is the introduction of the PB model counter PBCount2, the first\nexact PB model counter with support for projected and incremental model\ncounting. Our counter, PBCount2, uses our Least Occurrence Weighted Min Degree\n(LOW-MD) computation ordering heuristic to support projected model counting and\na cache mechanism to enable incremental model counting. In our evaluations,\nPBCount2 completed at least 1.40x the number of benchmarks of competing methods\nfor projected model counting and at least 1.18x of competing methods in\nincremental model counting."
                },
                "authors": [
                    {
                        "name": "Suwei Yang"
                    },
                    {
                        "name": "Kuldeep S. Meel"
                    }
                ],
                "author_detail": {
                    "name": "Kuldeep S. Meel"
                },
                "author": "Kuldeep S. Meel",
                "arxiv_comment": "To appear in AAAI25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15605v1",
                "updated": "2024-12-20T06:58:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    6,
                    58,
                    32,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T06:58:32Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    6,
                    58,
                    32,
                    4,
                    355,
                    0
                ],
                "title": "Don't Do RAG: When Cache-Augmented Generation is All You Need for\n  Knowledge Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't Do RAG: When Cache-Augmented Generation is All You Need for\n  Knowledge Tasks"
                },
                "summary": "Retrieval-augmented generation (RAG) has gained traction as a powerful\napproach for enhancing language models by integrating external knowledge\nsources. However, RAG introduces challenges such as retrieval latency,\npotential errors in document selection, and increased system complexity. With\nthe advent of large language models (LLMs) featuring significantly extended\ncontext windows, this paper proposes an alternative paradigm, cache-augmented\ngeneration (CAG) that bypasses real-time retrieval. Our method involves\npreloading all relevant resources, especially when the documents or knowledge\nfor retrieval are of a limited and manageable size, into the LLM's extended\ncontext and caching its runtime parameters. During inference, the model\nutilizes these preloaded parameters to answer queries without additional\nretrieval steps. Comparative analyses reveal that CAG eliminates retrieval\nlatency and minimizes retrieval errors while maintaining context relevance.\nPerformance evaluations across multiple benchmarks highlight scenarios where\nlong-context LLMs either outperform or complement traditional RAG pipelines.\nThese findings suggest that, for certain applications, particularly those with\na constrained knowledge base, CAG provide a streamlined and efficient\nalternative to RAG, achieving comparable or superior results with reduced\ncomplexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has gained traction as a powerful\napproach for enhancing language models by integrating external knowledge\nsources. However, RAG introduces challenges such as retrieval latency,\npotential errors in document selection, and increased system complexity. With\nthe advent of large language models (LLMs) featuring significantly extended\ncontext windows, this paper proposes an alternative paradigm, cache-augmented\ngeneration (CAG) that bypasses real-time retrieval. Our method involves\npreloading all relevant resources, especially when the documents or knowledge\nfor retrieval are of a limited and manageable size, into the LLM's extended\ncontext and caching its runtime parameters. During inference, the model\nutilizes these preloaded parameters to answer queries without additional\nretrieval steps. Comparative analyses reveal that CAG eliminates retrieval\nlatency and minimizes retrieval errors while maintaining context relevance.\nPerformance evaluations across multiple benchmarks highlight scenarios where\nlong-context LLMs either outperform or complement traditional RAG pipelines.\nThese findings suggest that, for certain applications, particularly those with\na constrained knowledge base, CAG provide a streamlined and efficient\nalternative to RAG, achieving comparable or superior results with reduced\ncomplexity."
                },
                "authors": [
                    {
                        "name": "Brian J Chan"
                    },
                    {
                        "name": "Chao-Ting Chen"
                    },
                    {
                        "name": "Jui-Hung Cheng"
                    },
                    {
                        "name": "Hen-Hsen Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hen-Hsen Huang"
                },
                "author": "Hen-Hsen Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02820v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02820v3",
                "updated": "2024-12-19T23:52:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    23,
                    52,
                    16,
                    3,
                    354,
                    0
                ],
                "published": "2024-11-05T05:41:41Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "title": "DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM\n  Serving"
                },
                "summary": "Large Language Models (LLMs) are increasingly employed in complex workflows,\nwhere different LLMs and fine-tuned variants collaboratively address complex\ntasks. However, these systems face significant inefficiencies due to redundant\ncontext processing of the shared context. We propose DroidSpeak, a framework\nthat optimizes context sharing between fine-tuned LLMs derived from the same\nfoundational model. DroidSpeak identifies critical layers in the KV cache and\nselectively recomputes them, enabling effective reuse of intermediate data\nwhile maintaining high accuracy.\n  Our approach balances computational efficiency and task fidelity,\nsignificantly reducing inference latency and throughput bottlenecks.\nExperiments on diverse datasets and model pairs demonstrate that DroidSpeak\nachieves up to 3x higher throughputs and 2.6x faster prefill times with\nnegligible accuracy loss compared to full recomputation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly employed in complex workflows,\nwhere different LLMs and fine-tuned variants collaboratively address complex\ntasks. However, these systems face significant inefficiencies due to redundant\ncontext processing of the shared context. We propose DroidSpeak, a framework\nthat optimizes context sharing between fine-tuned LLMs derived from the same\nfoundational model. DroidSpeak identifies critical layers in the KV cache and\nselectively recomputes them, enabling effective reuse of intermediate data\nwhile maintaining high accuracy.\n  Our approach balances computational efficiency and task fidelity,\nsignificantly reducing inference latency and throughput bottlenecks.\nExperiments on diverse datasets and model pairs demonstrate that DroidSpeak\nachieves up to 3x higher throughputs and 2.6x faster prefill times with\nnegligible accuracy loss compared to full recomputation."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Zhuohan Gu"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Junchen Jiang"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Madan Musuvathi"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02820v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02820v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12592v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12592v2",
                "updated": "2024-12-19T22:34:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    22,
                    34,
                    37,
                    3,
                    354,
                    0
                ],
                "published": "2024-08-22T17:56:29Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    56,
                    29,
                    3,
                    235,
                    0
                ],
                "title": "Exposing Shadow Branches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exposing Shadow Branches"
                },
                "summary": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation."
                },
                "authors": [
                    {
                        "name": "Chrysanthos Pepi"
                    },
                    {
                        "name": "Bhargav Reddy Godala"
                    },
                    {
                        "name": "Krishnam Tibrewala"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Paul V. Gratz"
                    },
                    {
                        "name": "Daniel A. Jiménez"
                    },
                    {
                        "name": "Gilles A. Pokam"
                    },
                    {
                        "name": "David I. August"
                    }
                ],
                "author_detail": {
                    "name": "David I. August"
                },
                "author": "David I. August",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12592v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12592v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14838v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14838v1",
                "updated": "2024-12-19T13:28:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    28,
                    42,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T13:28:42Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    28,
                    42,
                    3,
                    354,
                    0
                ],
                "title": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs"
                },
                "summary": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased."
                },
                "authors": [
                    {
                        "name": "Xiabin Zhou"
                    },
                    {
                        "name": "Wenbin Wang"
                    },
                    {
                        "name": "Minyan Zeng"
                    },
                    {
                        "name": "Jiaxian Guo"
                    },
                    {
                        "name": "Xuebo Liu"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Liang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Liang Ding"
                },
                "author": "Liang Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14838v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14838v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05317v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05317v3",
                "updated": "2024-12-19T12:38:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    12,
                    38,
                    23,
                    3,
                    354,
                    0
                ],
                "published": "2024-10-05T03:47:06Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    3,
                    47,
                    6,
                    5,
                    279,
                    0
                ],
                "title": "Accelerating Diffusion Transformers with Token-wise Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformers with Token-wise Feature Caching"
                },
                "summary": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality."
                },
                "authors": [
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Siteng Huang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "In this version, we achieved a nearly lossless acceleration of 1.51\n  times for ToCa on FLUX in the appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05317v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05317v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14392v1",
                "updated": "2024-12-18T22:52:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    52,
                    12,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T22:52:12Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    52,
                    12,
                    2,
                    353,
                    0
                ],
                "title": "Nemesis: Noise-randomized Encryption with Modular Efficiency and Secure\n  Integration in Machine Learning Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nemesis: Noise-randomized Encryption with Modular Efficiency and Secure\n  Integration in Machine Learning Systems"
                },
                "summary": "Machine learning (ML) systems that guarantee security and privacy often rely\non Fully Homomorphic Encryption (FHE) as a cornerstone technique, enabling\ncomputations on encrypted data without exposing sensitive information. However,\na critical limitation of FHE is its computational inefficiency, making it\nimpractical for large-scale applications. In this work, we propose\n\\textit{Nemesis}, a framework that accelerates FHE-based systems without\ncompromising accuracy or security. The design of Nemesis is inspired by Rache\n(SIGMOD'23), which introduced a caching mechanism for encrypted integers and\nscalars. Nemesis extends this idea with more advanced caching techniques and\nmathematical tools, enabling efficient operations over multi-slot FHE schemes\nand overcoming Rache's limitations to support general plaintext structures. We\nformally prove the security of Nemesis under standard cryptographic assumptions\nand evaluate its performance extensively on widely used datasets, including\nMNIST, FashionMNIST, and CIFAR-10. Experimental results show that Nemesis\nsignificantly reduces the computational overhead of FHE-based ML systems,\npaving the way for broader adoption of privacy-preserving technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning (ML) systems that guarantee security and privacy often rely\non Fully Homomorphic Encryption (FHE) as a cornerstone technique, enabling\ncomputations on encrypted data without exposing sensitive information. However,\na critical limitation of FHE is its computational inefficiency, making it\nimpractical for large-scale applications. In this work, we propose\n\\textit{Nemesis}, a framework that accelerates FHE-based systems without\ncompromising accuracy or security. The design of Nemesis is inspired by Rache\n(SIGMOD'23), which introduced a caching mechanism for encrypted integers and\nscalars. Nemesis extends this idea with more advanced caching techniques and\nmathematical tools, enabling efficient operations over multi-slot FHE schemes\nand overcoming Rache's limitations to support general plaintext structures. We\nformally prove the security of Nemesis under standard cryptographic assumptions\nand evaluate its performance extensively on widely used datasets, including\nMNIST, FashionMNIST, and CIFAR-10. Experimental results show that Nemesis\nsignificantly reduces the computational overhead of FHE-based ML systems,\npaving the way for broader adoption of privacy-preserving technologies."
                },
                "authors": [
                    {
                        "name": "Dongfang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Dongfang Zhao"
                },
                "author": "Dongfang Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14363v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14363v1",
                "updated": "2024-12-18T22:01:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    1,
                    55,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T22:01:55Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    1,
                    55,
                    2,
                    353,
                    0
                ],
                "title": "ResQ: Mixed-Precision Quantization of Large Language Models with\n  Low-Rank Residuals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ResQ: Mixed-Precision Quantization of Large Language Models with\n  Low-Rank Residuals"
                },
                "summary": "Post-training quantization (PTQ) of large language models (LLMs) holds the\npromise in reducing the prohibitive computational cost at inference time.\nQuantization of all weight, activation and key-value (KV) cache tensors to\n4-bit without significantly degrading generalizability is challenging, due to\nthe high quantization error caused by extreme outliers in activations. To\ntackle this problem, we propose ResQ, a PTQ method that pushes further the\nstate-of-the-art. By means of principal component analysis (PCA), it identifies\na low-rank subspace (in practice 1/8 of the hidden dimension) in which\nactivation variances are highest, and keep the coefficients within this\nsubspace in high precision, e.g. 8-bit, while quantizing the rest to 4-bit.\nWithin each subspace, invariant random rotation is applied to further suppress\noutliers. We show that this is a provably optimal mixed precision quantization\nscheme that minimizes error. With the Llama families of models, we demonstrate\nthat ResQ outperforms recent uniform and mixed precision PTQ methods on a\nvariety of benchmarks, achieving up to 33% lower perplexity on Wikitext than\nthe next best method SpinQuant, and a 2.4x speedup over 16-bit baseline. Code\nis available at https://github.com/utkarsh-dmx/project-resq.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) of large language models (LLMs) holds the\npromise in reducing the prohibitive computational cost at inference time.\nQuantization of all weight, activation and key-value (KV) cache tensors to\n4-bit without significantly degrading generalizability is challenging, due to\nthe high quantization error caused by extreme outliers in activations. To\ntackle this problem, we propose ResQ, a PTQ method that pushes further the\nstate-of-the-art. By means of principal component analysis (PCA), it identifies\na low-rank subspace (in practice 1/8 of the hidden dimension) in which\nactivation variances are highest, and keep the coefficients within this\nsubspace in high precision, e.g. 8-bit, while quantizing the rest to 4-bit.\nWithin each subspace, invariant random rotation is applied to further suppress\noutliers. We show that this is a provably optimal mixed precision quantization\nscheme that minimizes error. With the Llama families of models, we demonstrate\nthat ResQ outperforms recent uniform and mixed precision PTQ methods on a\nvariety of benchmarks, achieving up to 33% lower perplexity on Wikitext than\nthe next best method SpinQuant, and a 2.4x speedup over 16-bit baseline. Code\nis available at https://github.com/utkarsh-dmx/project-resq."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Sayeh Sharify"
                    },
                    {
                        "name": "Kaushik Roy"
                    },
                    {
                        "name": "Xin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Wang"
                },
                "author": "Xin Wang",
                "arxiv_comment": "14 pages, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14363v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14363v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14335v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14335v1",
                "updated": "2024-12-18T21:09:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    21,
                    9,
                    8,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T21:09:08Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    21,
                    9,
                    8,
                    2,
                    353,
                    0
                ],
                "title": "Optimizing ML Concurrent Computation and Communication with GPU DMA\n  Engines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing ML Concurrent Computation and Communication with GPU DMA\n  Engines"
                },
                "summary": "Concurrent computation and communication (C3) is a pervasive paradigm in ML\nand other domains, making its performance optimization crucial. In this paper,\nwe carefully characterize C3 in ML on GPUs, which are most widely deployed for\nML training and inference. We observe that while C3 leads to performance\nuplifts, the uplifts are far lower than ideal speedups (serial computation and\ncommunication versus maximum of computation or communication; all times from\nisolated executions). C3 on average achieves only 21% of ideal speedup, this is\ndue to known challenges of compute and memory interference between concurrent\nGPU kernels (that is, sharing of GPU's compute units, caches and HBM).\n  To attain better performance for C3, first, we evaluate dual strategies of\nschedule prioritization and careful resource partitioning of compute units on\nGPUs to push performance attained with C3 (on average 42% of ideal speedup). We\nalso provide heuristics that can guide a runtime while employing these\nstrategies. To further enhance C3 performance, we propose to mitigate C3\ninterference by offloading communication tasks to the GPU's DMA engines. To\nthis end, we build Concurrent Communication CoLlectives (ConCCL)\nproof-of-concepts that harness DMA engines for communication. We show how\nConCCL considerably closes the gap between realized and ideal speedup for C3\n(on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall,\nour work makes a strong case for GPU DMA engine advancements to better support\nC3 on GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concurrent computation and communication (C3) is a pervasive paradigm in ML\nand other domains, making its performance optimization crucial. In this paper,\nwe carefully characterize C3 in ML on GPUs, which are most widely deployed for\nML training and inference. We observe that while C3 leads to performance\nuplifts, the uplifts are far lower than ideal speedups (serial computation and\ncommunication versus maximum of computation or communication; all times from\nisolated executions). C3 on average achieves only 21% of ideal speedup, this is\ndue to known challenges of compute and memory interference between concurrent\nGPU kernels (that is, sharing of GPU's compute units, caches and HBM).\n  To attain better performance for C3, first, we evaluate dual strategies of\nschedule prioritization and careful resource partitioning of compute units on\nGPUs to push performance attained with C3 (on average 42% of ideal speedup). We\nalso provide heuristics that can guide a runtime while employing these\nstrategies. To further enhance C3 performance, we propose to mitigate C3\ninterference by offloading communication tasks to the GPU's DMA engines. To\nthis end, we build Concurrent Communication CoLlectives (ConCCL)\nproof-of-concepts that harness DMA engines for communication. We show how\nConCCL considerably closes the gap between realized and ideal speedup for C3\n(on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall,\nour work makes a strong case for GPU DMA engine advancements to better support\nC3 on GPUs."
                },
                "authors": [
                    {
                        "name": "Anirudha Agrawal"
                    },
                    {
                        "name": "Shaizeen Aga"
                    },
                    {
                        "name": "Suchita Pati"
                    },
                    {
                        "name": "Mahzabeen Islam"
                    }
                ],
                "author_detail": {
                    "name": "Mahzabeen Islam"
                },
                "author": "Mahzabeen Islam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14335v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14335v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16179v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16179v4",
                "updated": "2024-12-18T17:36:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    36,
                    36,
                    2,
                    353,
                    0
                ],
                "published": "2024-10-21T16:44:51Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    16,
                    44,
                    51,
                    0,
                    295,
                    0
                ],
                "title": "MagicPIG: LSH Sampling for Efficient LLM Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicPIG: LSH Sampling for Efficient LLM Generation"
                },
                "summary": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by up to $5\\times$ across various GPU hardware and achieve 54ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\nhttps://github.com/Infini-AI-Lab/MagicPIG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by up to $5\\times$ across various GPU hardware and achieve 54ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\nhttps://github.com/Infini-AI-Lab/MagicPIG."
                },
                "authors": [
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Jianyu Zhang"
                    },
                    {
                        "name": "Niklas Nolte"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Matthijs Douze"
                    },
                    {
                        "name": "Leon Bottou"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16179v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16179v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13779v1",
                "updated": "2024-12-18T12:16:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    16,
                    41,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T12:16:41Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    16,
                    41,
                    2,
                    353,
                    0
                ],
                "title": "Rehearsal-Free Continual Federated Learning with Synergistic\n  Regularization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rehearsal-Free Continual Federated Learning with Synergistic\n  Regularization"
                },
                "summary": "Continual Federated Learning (CFL) allows distributed devices to\ncollaboratively learn novel concepts from continuously shifting training data\nwhile avoiding knowledge forgetting of previously seen tasks. To tackle this\nchallenge, most current CFL approaches rely on extensive rehearsal of previous\ndata. Despite effectiveness, rehearsal comes at a cost to memory, and it may\nalso violate data privacy. Considering these, we seek to apply regularization\ntechniques to CFL by considering their cost-efficient properties that do not\nrequire sample caching or rehearsal. Specifically, we first apply traditional\nregularization techniques to CFL and observe that existing regularization\ntechniques, especially synaptic intelligence, can achieve promising results\nunder homogeneous data distribution but fail when the data is heterogeneous.\nBased on this observation, we propose a simple yet effective regularization\nalgorithm for CFL named FedSSI, which tailors the synaptic intelligence for the\nCFL with heterogeneous data settings. FedSSI can not only reduce computational\noverhead without rehearsal but also address the data heterogeneity issue.\nExtensive experiments show that FedSSI achieves superior performance compared\nto state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Federated Learning (CFL) allows distributed devices to\ncollaboratively learn novel concepts from continuously shifting training data\nwhile avoiding knowledge forgetting of previously seen tasks. To tackle this\nchallenge, most current CFL approaches rely on extensive rehearsal of previous\ndata. Despite effectiveness, rehearsal comes at a cost to memory, and it may\nalso violate data privacy. Considering these, we seek to apply regularization\ntechniques to CFL by considering their cost-efficient properties that do not\nrequire sample caching or rehearsal. Specifically, we first apply traditional\nregularization techniques to CFL and observe that existing regularization\ntechniques, especially synaptic intelligence, can achieve promising results\nunder homogeneous data distribution but fail when the data is heterogeneous.\nBased on this observation, we propose a simple yet effective regularization\nalgorithm for CFL named FedSSI, which tailors the synaptic intelligence for the\nCFL with heterogeneous data settings. FedSSI can not only reduce computational\noverhead without rehearsal but also address the data heterogeneity issue.\nExtensive experiments show that FedSSI achieves superior performance compared\nto state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Yichen Li"
                    },
                    {
                        "name": "Yuying Wang"
                    },
                    {
                        "name": "Tianzhe Xiao"
                    },
                    {
                        "name": "Haozhao Wang"
                    },
                    {
                        "name": "Yining Qi"
                    },
                    {
                        "name": "Ruixuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruixuan Li"
                },
                "author": "Ruixuan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13771v1",
                "updated": "2024-12-18T12:07:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    7,
                    58,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T12:07:58Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    7,
                    58,
                    2,
                    353,
                    0
                ],
                "title": "Semantic Convergence: Harmonizing Recommender Systems via Two-Stage\n  Alignment and Behavioral Semantic Tokenization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Convergence: Harmonizing Recommender Systems via Two-Stage\n  Alignment and Behavioral Semantic Tokenization"
                },
                "summary": "Large language models (LLMs), endowed with exceptional reasoning\ncapabilities, are adept at discerning profound user interests from historical\nbehaviors, thereby presenting a promising avenue for the advancement of\nrecommendation systems. However, a notable discrepancy persists between the\nsparse collaborative semantics typically found in recommendation systems and\nthe dense token representations within LLMs. In our study, we propose a novel\nframework that harmoniously merges traditional recommendation models with the\nprowess of LLMs. We initiate this integration by transforming ItemIDs into\nsequences that align semantically with the LLMs space, through the proposed\nAlignment Tokenization module. Additionally, we design a series of specialized\nsupervised learning tasks aimed at aligning collaborative signals with the\nsubtleties of natural language semantics. To ensure practical applicability, we\noptimize online inference by pre-caching the top-K results for each user,\nreducing latency and improving effciency. Extensive experimental evidence\nindicates that our model markedly improves recall metrics and displays\nremarkable scalability of recommendation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), endowed with exceptional reasoning\ncapabilities, are adept at discerning profound user interests from historical\nbehaviors, thereby presenting a promising avenue for the advancement of\nrecommendation systems. However, a notable discrepancy persists between the\nsparse collaborative semantics typically found in recommendation systems and\nthe dense token representations within LLMs. In our study, we propose a novel\nframework that harmoniously merges traditional recommendation models with the\nprowess of LLMs. We initiate this integration by transforming ItemIDs into\nsequences that align semantically with the LLMs space, through the proposed\nAlignment Tokenization module. Additionally, we design a series of specialized\nsupervised learning tasks aimed at aligning collaborative signals with the\nsubtleties of natural language semantics. To ensure practical applicability, we\noptimize online inference by pre-caching the top-K results for each user,\nreducing latency and improving effciency. Extensive experimental evidence\nindicates that our model markedly improves recall metrics and displays\nremarkable scalability of recommendation systems."
                },
                "authors": [
                    {
                        "name": "Guanghan Li"
                    },
                    {
                        "name": "Xun Zhang"
                    },
                    {
                        "name": "Yufei Zhang"
                    },
                    {
                        "name": "Yifan Yin"
                    },
                    {
                        "name": "Guojun Yin"
                    },
                    {
                        "name": "Wei Lin"
                    }
                ],
                "author_detail": {
                    "name": "Wei Lin"
                },
                "author": "Wei Lin",
                "arxiv_comment": "7 pages, 3 figures, AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15024v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15024v2",
                "updated": "2024-12-18T09:47:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    47,
                    25,
                    2,
                    353,
                    0
                ],
                "published": "2024-11-22T15:55:19Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    55,
                    19,
                    4,
                    327,
                    0
                ],
                "title": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models"
                },
                "summary": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "12 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15024v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15024v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13649v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13649v1",
                "updated": "2024-12-18T09:27:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    27,
                    33,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T09:27:33Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    27,
                    33,
                    2,
                    353,
                    0
                ],
                "title": "SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation"
                },
                "summary": "Key-Value (KV) cache has become a bottleneck of LLMs for long-context\ngeneration. Despite the numerous efforts in this area, the optimization for the\ndecoding phase is generally ignored. However, we believe such optimization is\ncrucial, especially for long-output generation tasks based on the following two\nobservations: (i) Excessive compression during the prefill phase, which\nrequires specific full context impairs the comprehension of the reasoning task;\n(ii) Deviation of heavy hitters occurs in the reasoning tasks with long\noutputs. Therefore, SCOPE, a simple yet efficient framework that separately\nperforms KV cache optimization during the prefill and decoding phases, is\nintroduced. Specifically, the KV cache during the prefill phase is preserved to\nmaintain the essential information, while a novel strategy based on sliding is\nproposed to select essential heavy hitters for the decoding phase. Memory usage\nand memory transfer are further optimized using adaptive and discontinuous\nstrategies. Extensive experiments on LongGenBench show the effectiveness and\ngeneralization of SCOPE and its compatibility as a plug-in to other\nprefill-only KV compression methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) cache has become a bottleneck of LLMs for long-context\ngeneration. Despite the numerous efforts in this area, the optimization for the\ndecoding phase is generally ignored. However, we believe such optimization is\ncrucial, especially for long-output generation tasks based on the following two\nobservations: (i) Excessive compression during the prefill phase, which\nrequires specific full context impairs the comprehension of the reasoning task;\n(ii) Deviation of heavy hitters occurs in the reasoning tasks with long\noutputs. Therefore, SCOPE, a simple yet efficient framework that separately\nperforms KV cache optimization during the prefill and decoding phases, is\nintroduced. Specifically, the KV cache during the prefill phase is preserved to\nmaintain the essential information, while a novel strategy based on sliding is\nproposed to select essential heavy hitters for the decoding phase. Memory usage\nand memory transfer are further optimized using adaptive and discontinuous\nstrategies. Extensive experiments on LongGenBench show the effectiveness and\ngeneralization of SCOPE and its compatibility as a plug-in to other\nprefill-only KV compression methods."
                },
                "authors": [
                    {
                        "name": "Jialong Wu"
                    },
                    {
                        "name": "Zhenglin Wang"
                    },
                    {
                        "name": "Linhai Zhang"
                    },
                    {
                        "name": "Yilong Lai"
                    },
                    {
                        "name": "Yulan He"
                    },
                    {
                        "name": "Deyu Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Deyu Zhou"
                },
                "author": "Deyu Zhou",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13649v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13649v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08584v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08584v2",
                "updated": "2024-12-18T07:45:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    7,
                    45,
                    11,
                    2,
                    353,
                    0
                ],
                "published": "2024-10-11T07:24:21Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    7,
                    24,
                    21,
                    4,
                    285,
                    0
                ],
                "title": "ZipVL: Efficient Large Vision-Language Models with Dynamic Token\n  Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipVL: Efficient Large Vision-Language Models with Dynamic Token\n  Sparsification"
                },
                "summary": "The efficiency of large vision-language models (LVLMs) is constrained by the\ncomputational bottleneck of the attention mechanism during the prefill phase\nand the memory bottleneck of fetching the key-value (KV) cache in the decoding\nphase, particularly in scenarios involving high-resolution images or videos.\nVisual content often exhibits substantial redundancy, resulting in highly\nsparse attention maps within LVLMs. This sparsity can be leveraged to\naccelerate attention computation or compress the KV cache through various\napproaches. However, most studies focus on addressing only one of these\nbottlenecks and do not adequately support dynamic adjustment of sparsity\nconcerning distinct layers or tasks. In this paper, we present ZipVL, an\nefficient inference framework designed for LVLMs through a dynamic ratio\nallocation strategy of important tokens. This ratio is adaptively determined\nbased on the layer-specific distribution of attention scores, rather than fixed\nhyper-parameters, thereby improving efficiency for less complex tasks while\nmaintaining high performance for more challenging ones. Then we select\nimportant tokens based on their normalized attention scores and perform sparse\nattention mechanism solely on those important tokens, reducing the latency in\nthe prefill phase. Tokens deemed less important will be discarded to reduce KV\ncache size, alleviating the memory bottleneck in the decoding phase. Our\nexperiments demonstrate that ZipVL can accelerate the prefill phase by\n2.3$\\times$ and improve decoding throughput by 2.8$\\times$, with a minimal\naccuracy reduction of only 0.5\\% on VQAv2 benchmark over LLaVA-Next-13B model,\neffectively enhancing the generation efficiency of LVLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of large vision-language models (LVLMs) is constrained by the\ncomputational bottleneck of the attention mechanism during the prefill phase\nand the memory bottleneck of fetching the key-value (KV) cache in the decoding\nphase, particularly in scenarios involving high-resolution images or videos.\nVisual content often exhibits substantial redundancy, resulting in highly\nsparse attention maps within LVLMs. This sparsity can be leveraged to\naccelerate attention computation or compress the KV cache through various\napproaches. However, most studies focus on addressing only one of these\nbottlenecks and do not adequately support dynamic adjustment of sparsity\nconcerning distinct layers or tasks. In this paper, we present ZipVL, an\nefficient inference framework designed for LVLMs through a dynamic ratio\nallocation strategy of important tokens. This ratio is adaptively determined\nbased on the layer-specific distribution of attention scores, rather than fixed\nhyper-parameters, thereby improving efficiency for less complex tasks while\nmaintaining high performance for more challenging ones. Then we select\nimportant tokens based on their normalized attention scores and perform sparse\nattention mechanism solely on those important tokens, reducing the latency in\nthe prefill phase. Tokens deemed less important will be discarded to reduce KV\ncache size, alleviating the memory bottleneck in the decoding phase. Our\nexperiments demonstrate that ZipVL can accelerate the prefill phase by\n2.3$\\times$ and improve decoding throughput by 2.8$\\times$, with a minimal\naccuracy reduction of only 0.5\\% on VQAv2 benchmark over LLaVA-Next-13B model,\neffectively enhancing the generation efficiency of LVLMs."
                },
                "authors": [
                    {
                        "name": "Yefei He"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Hong Zhou"
                    },
                    {
                        "name": "Kaipeng Zhang"
                    },
                    {
                        "name": "Bohan Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Bohan Zhuang"
                },
                "author": "Bohan Zhuang",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08584v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08584v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13509v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13509v1",
                "updated": "2024-12-18T05:16:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    5,
                    16,
                    11,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T05:16:11Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    5,
                    16,
                    11,
                    2,
                    353,
                    0
                ],
                "title": "Vivar: A Generative AR System for Intuitive Multi-Modal Sensor Data\n  Presentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vivar: A Generative AR System for Intuitive Multi-Modal Sensor Data\n  Presentation"
                },
                "summary": "Understanding sensor data can be challenging for non-experts because of the\ncomplexity and unique semantic meanings of sensor modalities. This calls for\nintuitive and effective methods to present sensor information. However,\ncreating intuitive sensor data visualizations presents three key challenges:\nthe variability of sensor readings, gaps in domain comprehension, and the\ndynamic nature of sensor data. To address these issues, we develop Vivar, a\nnovel AR system that integrates multi-modal sensor data and presents 3D\nvolumetric content for visualization. In particular, we introduce a cross-modal\nembedding approach that maps sensor data into a pre-trained visual embedding\nspace through barycentric interpolation. This allows for accurate and\ncontinuous integration of multi-modal sensor information. Vivar also\nincorporates sensor-aware AR scene generation using foundation models and 3D\nGaussian Splatting (3DGS) without requiring domain expertise. In addition,\nVivar leverages latent reuse and caching strategies to accelerate 2D and AR\ncontent generation. Our extensive experiments demonstrate that our system\nachieves 11$\\times$ latency reduction without compromising quality. A user\nstudy involving over 485 participants, including domain experts, demonstrates\nVivar's effectiveness in accuracy, consistency, and real-world applicability,\npaving the way for more intuitive sensor data visualization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding sensor data can be challenging for non-experts because of the\ncomplexity and unique semantic meanings of sensor modalities. This calls for\nintuitive and effective methods to present sensor information. However,\ncreating intuitive sensor data visualizations presents three key challenges:\nthe variability of sensor readings, gaps in domain comprehension, and the\ndynamic nature of sensor data. To address these issues, we develop Vivar, a\nnovel AR system that integrates multi-modal sensor data and presents 3D\nvolumetric content for visualization. In particular, we introduce a cross-modal\nembedding approach that maps sensor data into a pre-trained visual embedding\nspace through barycentric interpolation. This allows for accurate and\ncontinuous integration of multi-modal sensor information. Vivar also\nincorporates sensor-aware AR scene generation using foundation models and 3D\nGaussian Splatting (3DGS) without requiring domain expertise. In addition,\nVivar leverages latent reuse and caching strategies to accelerate 2D and AR\ncontent generation. Our extensive experiments demonstrate that our system\nachieves 11$\\times$ latency reduction without compromising quality. A user\nstudy involving over 485 participants, including domain experts, demonstrates\nVivar's effectiveness in accuracy, consistency, and real-world applicability,\npaving the way for more intuitive sensor data visualization."
                },
                "authors": [
                    {
                        "name": "Yunqi Guo"
                    },
                    {
                        "name": "Kaiyuan Hou"
                    },
                    {
                        "name": "Heming Fu"
                    },
                    {
                        "name": "Hongkai Chen"
                    },
                    {
                        "name": "Zhenyu Yan"
                    },
                    {
                        "name": "Guoliang Xing"
                    },
                    {
                        "name": "Xiaofan Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofan Jiang"
                },
                "author": "Xiaofan Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13509v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13509v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12486v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12486v2",
                "updated": "2024-12-18T05:08:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    5,
                    8,
                    39,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-17T02:43:54Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    2,
                    43,
                    54,
                    1,
                    352,
                    0
                ],
                "title": "Boosting Long-Context Management via Query-Guided Activation Refilling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Long-Context Management via Query-Guided Activation Refilling"
                },
                "summary": "Processing long contexts poses a significant challenge for large language\nmodels (LLMs) due to their inherent context-window limitations and the\ncomputational burden of extensive key-value (KV) activations, which severely\nimpact efficiency. For information-seeking tasks, full context perception is\noften unnecessary, as a query's information needs can dynamically range from\nlocalized details to a global perspective, depending on its complexity.\nHowever, existing methods struggle to adapt effectively to these dynamic\ninformation needs.\n  In the paper, we propose a method for processing long-context\ninformation-seeking tasks via query-guided Activation Refilling (ACRE). ACRE\nconstructs a Bi-layer KV Cache for long contexts, where the layer-1 (L1) cache\ncompactly captures global information, and the layer-2 (L2) cache provides\ndetailed and localized information. ACRE establishes a proxying relationship\nbetween the two caches, allowing the input query to attend to the L1 cache and\ndynamically refill it with relevant entries from the L2 cache. This mechanism\nintegrates global understanding with query-specific local details, thus\nimproving answer decoding. Experiments on a variety of long-context\ninformation-seeking datasets demonstrate ACRE's effectiveness, achieving\nimprovements in both performance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long contexts poses a significant challenge for large language\nmodels (LLMs) due to their inherent context-window limitations and the\ncomputational burden of extensive key-value (KV) activations, which severely\nimpact efficiency. For information-seeking tasks, full context perception is\noften unnecessary, as a query's information needs can dynamically range from\nlocalized details to a global perspective, depending on its complexity.\nHowever, existing methods struggle to adapt effectively to these dynamic\ninformation needs.\n  In the paper, we propose a method for processing long-context\ninformation-seeking tasks via query-guided Activation Refilling (ACRE). ACRE\nconstructs a Bi-layer KV Cache for long contexts, where the layer-1 (L1) cache\ncompactly captures global information, and the layer-2 (L2) cache provides\ndetailed and localized information. ACRE establishes a proxying relationship\nbetween the two caches, allowing the input query to attend to the L1 cache and\ndynamically refill it with relevant entries from the L2 cache. This mechanism\nintegrates global understanding with query-specific local details, thus\nimproving answer decoding. Experiments on a variety of long-context\ninformation-seeking datasets demonstrate ACRE's effectiveness, achieving\nimprovements in both performance and efficiency."
                },
                "authors": [
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Zhicheng Dou"
                    },
                    {
                        "name": "Defu Lian"
                    }
                ],
                "author_detail": {
                    "name": "Defu Lian"
                },
                "author": "Defu Lian",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12486v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12486v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12094v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12094v2",
                "updated": "2024-12-17T20:41:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    20,
                    41,
                    59,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-16T18:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator"
                },
                "summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities."
                },
                "authors": [
                    {
                        "name": "Guoxuan Chen"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Yihang Gao"
                    },
                    {
                        "name": "Xiaozhe Ren"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "We have made our code publicly available at sepllm.github.io. Our\n  codebase supports efficient multi-node distributed training with accelerated\n  attention module Sep-Attention and also supports numerous existing Fusion\n  Operators to accelerate the training process, such as fused rope, etc. If you\n  find our code helpful, please kindly consider giving us a **star** on\n  GitHub^_^. Thank you very much!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12094v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12094v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00876v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00876v3",
                "updated": "2024-12-17T14:45:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    45,
                    12,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-01T16:32:31Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    16,
                    32,
                    31,
                    6,
                    336,
                    0
                ],
                "title": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava ."
                },
                "authors": [
                    {
                        "name": "Wenxuan Huang"
                    },
                    {
                        "name": "Zijie Zhai"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Shaosheng Cao"
                    },
                    {
                        "name": "Fei Zhao"
                    },
                    {
                        "name": "Xiangfeng Xu"
                    },
                    {
                        "name": "Zheyu Ye"
                    },
                    {
                        "name": "Shaohui Lin"
                    }
                ],
                "author_detail": {
                    "name": "Shaohui Lin"
                },
                "author": "Shaohui Lin",
                "arxiv_comment": "Code is available at https://github.com/Osilly/dynamic_llava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00876v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00876v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12953v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12953v1",
                "updated": "2024-12-17T14:34:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    34,
                    51,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T14:34:51Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    34,
                    51,
                    1,
                    352,
                    0
                ],
                "title": "Efficient Diffusion Transformer Policies with Mixture of Expert\n  Denoisers for Multitask Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Diffusion Transformer Policies with Mixture of Expert\n  Denoisers for Multitask Learning"
                },
                "summary": "Diffusion Policies have become widely used in Imitation Learning, offering\nseveral appealing properties, such as generating multimodal and discontinuous\nbehavior. As models are becoming larger to capture more complex capabilities,\ntheir computational demands increase, as shown by recent scaling laws.\nTherefore, continuing with the current architectures will present a\ncomputational roadblock. To address this gap, we propose Mixture-of-Denoising\nExperts (MoDE) as a novel policy for Imitation Learning. MoDE surpasses current\nstate-of-the-art Transformer-based Diffusion Policies while enabling\nparameter-efficient scaling through sparse experts and noise-conditioned\nrouting, reducing both active parameters by 40% and inference costs by 90% via\nexpert caching. Our architecture combines this efficient scaling with\nnoise-conditioned self-attention mechanism, enabling more effective denoising\nacross different noise levels. MoDE achieves state-of-the-art performance on\n134 tasks in four established imitation learning benchmarks (CALVIN and\nLIBERO). Notably, by pretraining MoDE on diverse robotics data, we achieve 4.01\non CALVIN ABC and 0.95 on LIBERO-90. It surpasses both CNN-based and\nTransformer Diffusion Policies by an average of 57% across 4 benchmarks, while\nusing 90% fewer FLOPs and fewer active parameters compared to default Diffusion\nTransformer architectures. Furthermore, we conduct comprehensive ablations on\nMoDE's components, providing insights for designing efficient and scalable\nTransformer architectures for Diffusion Policies. Code and demonstrations are\navailable at https://mbreuss.github.io/MoDE_Diffusion_Policy/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Policies have become widely used in Imitation Learning, offering\nseveral appealing properties, such as generating multimodal and discontinuous\nbehavior. As models are becoming larger to capture more complex capabilities,\ntheir computational demands increase, as shown by recent scaling laws.\nTherefore, continuing with the current architectures will present a\ncomputational roadblock. To address this gap, we propose Mixture-of-Denoising\nExperts (MoDE) as a novel policy for Imitation Learning. MoDE surpasses current\nstate-of-the-art Transformer-based Diffusion Policies while enabling\nparameter-efficient scaling through sparse experts and noise-conditioned\nrouting, reducing both active parameters by 40% and inference costs by 90% via\nexpert caching. Our architecture combines this efficient scaling with\nnoise-conditioned self-attention mechanism, enabling more effective denoising\nacross different noise levels. MoDE achieves state-of-the-art performance on\n134 tasks in four established imitation learning benchmarks (CALVIN and\nLIBERO). Notably, by pretraining MoDE on diverse robotics data, we achieve 4.01\non CALVIN ABC and 0.95 on LIBERO-90. It surpasses both CNN-based and\nTransformer Diffusion Policies by an average of 57% across 4 benchmarks, while\nusing 90% fewer FLOPs and fewer active parameters compared to default Diffusion\nTransformer architectures. Furthermore, we conduct comprehensive ablations on\nMoDE's components, providing insights for designing efficient and scalable\nTransformer architectures for Diffusion Policies. Code and demonstrations are\navailable at https://mbreuss.github.io/MoDE_Diffusion_Policy/."
                },
                "authors": [
                    {
                        "name": "Moritz Reuss"
                    },
                    {
                        "name": "Jyothish Pari"
                    },
                    {
                        "name": "Pulkit Agrawal"
                    },
                    {
                        "name": "Rudolf Lioutikov"
                    }
                ],
                "author_detail": {
                    "name": "Rudolf Lioutikov"
                },
                "author": "Rudolf Lioutikov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12953v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12953v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12798v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12798v1",
                "updated": "2024-12-17T11:00:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    11,
                    0,
                    56,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T11:00:56Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    11,
                    0,
                    56,
                    1,
                    352,
                    0
                ],
                "title": "ZoRI: Towards Discriminative Zero-Shot Remote Sensing Instance\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZoRI: Towards Discriminative Zero-Shot Remote Sensing Instance\n  Segmentation"
                },
                "summary": "Instance segmentation algorithms in remote sensing are typically based on\nconventional methods, limiting their application to seen scenarios and\nclosed-set predictions. In this work, we propose a novel task called zero-shot\nremote sensing instance segmentation, aimed at identifying aerial objects that\nare absent from training data. Challenges arise when classifying aerial\ncategories with high inter-class similarity and intra-class variance. Besides,\nthe domain gap between vision-language models' pretraining datasets and remote\nsensing datasets hinders the zero-shot capabilities of the pretrained model\nwhen it is directly applied to remote sensing images. To address these\nchallenges, we propose a $\\textbf{Z}$ero-Sh$\\textbf{o}$t $\\textbf{R}$emote\nSensing $\\textbf{I}$nstance Segmentation framework, dubbed $\\textbf{ZoRI}$. Our\napproach features a discrimination-enhanced classifier that uses refined\ntextual embeddings to increase the awareness of class disparities. Instead of\ndirect fine-tuning, we propose a knowledge-maintained adaptation strategy that\ndecouples semantic-related information to preserve the pretrained\nvision-language alignment while adjusting features to capture remote sensing\ndomain-specific visual cues. Additionally, we introduce a prior-injected\nprediction with cache bank of aerial visual prototypes to supplement the\nsemantic richness of text embeddings and seamlessly integrate aerial\nrepresentations, adapting to the remote sensing domain. We establish new\nexperimental protocols and benchmarks, and extensive experiments convincingly\ndemonstrate that ZoRI achieves the state-of-art performance on the zero-shot\nremote sensing instance segmentation task. Our code is available at\nhttps://github.com/HuangShiqi128/ZoRI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instance segmentation algorithms in remote sensing are typically based on\nconventional methods, limiting their application to seen scenarios and\nclosed-set predictions. In this work, we propose a novel task called zero-shot\nremote sensing instance segmentation, aimed at identifying aerial objects that\nare absent from training data. Challenges arise when classifying aerial\ncategories with high inter-class similarity and intra-class variance. Besides,\nthe domain gap between vision-language models' pretraining datasets and remote\nsensing datasets hinders the zero-shot capabilities of the pretrained model\nwhen it is directly applied to remote sensing images. To address these\nchallenges, we propose a $\\textbf{Z}$ero-Sh$\\textbf{o}$t $\\textbf{R}$emote\nSensing $\\textbf{I}$nstance Segmentation framework, dubbed $\\textbf{ZoRI}$. Our\napproach features a discrimination-enhanced classifier that uses refined\ntextual embeddings to increase the awareness of class disparities. Instead of\ndirect fine-tuning, we propose a knowledge-maintained adaptation strategy that\ndecouples semantic-related information to preserve the pretrained\nvision-language alignment while adjusting features to capture remote sensing\ndomain-specific visual cues. Additionally, we introduce a prior-injected\nprediction with cache bank of aerial visual prototypes to supplement the\nsemantic richness of text embeddings and seamlessly integrate aerial\nrepresentations, adapting to the remote sensing domain. We establish new\nexperimental protocols and benchmarks, and extensive experiments convincingly\ndemonstrate that ZoRI achieves the state-of-art performance on the zero-shot\nremote sensing instance segmentation task. Our code is available at\nhttps://github.com/HuangShiqi128/ZoRI."
                },
                "authors": [
                    {
                        "name": "Shiqi Huang"
                    },
                    {
                        "name": "Shuting He"
                    },
                    {
                        "name": "Bihan Wen"
                    }
                ],
                "author_detail": {
                    "name": "Bihan Wen"
                },
                "author": "Bihan Wen",
                "arxiv_comment": "AAAI 2025, code see https://github.com/HuangShiqi128/ZoRI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12798v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12798v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12706v1",
                "updated": "2024-12-17T09:20:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    20,
                    31,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T09:20:31Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    20,
                    31,
                    1,
                    352,
                    0
                ],
                "title": "More Tokens, Lower Precision: Towards the Optimal Token-Precision\n  Trade-off in KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More Tokens, Lower Precision: Towards the Optimal Token-Precision\n  Trade-off in KV Cache Compression"
                },
                "summary": "As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension and seldom explore the\nefficiency of their combination. In this paper, we comprehensively investigate\nthe token-precision trade-off in KV cache compression. Experiments demonstrate\nthat storing more tokens in the KV cache with lower precision, i.e., quantized\npruning, can significantly enhance the long-context performance of LLMs.\nFurthermore, in-depth analysis regarding token-precision trade-off from a\nseries of key aspects exhibit that, quantized pruning achieves substantial\nimprovements in retrieval-related tasks and consistently performs well across\nvarying input lengths. Moreover, quantized pruning demonstrates notable\nstability across different KV pruning methods, quantization strategies, and\nmodel scales. These findings provide valuable insights into the token-precision\ntrade-off in KV cache compression. We plan to release our code in the near\nfuture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension and seldom explore the\nefficiency of their combination. In this paper, we comprehensively investigate\nthe token-precision trade-off in KV cache compression. Experiments demonstrate\nthat storing more tokens in the KV cache with lower precision, i.e., quantized\npruning, can significantly enhance the long-context performance of LLMs.\nFurthermore, in-depth analysis regarding token-precision trade-off from a\nseries of key aspects exhibit that, quantized pruning achieves substantial\nimprovements in retrieval-related tasks and consistently performs well across\nvarying input lengths. Moreover, quantized pruning demonstrates notable\nstability across different KV pruning methods, quantization strategies, and\nmodel scales. These findings provide valuable insights into the token-precision\ntrade-off in KV cache compression. We plan to release our code in the near\nfuture."
                },
                "authors": [
                    {
                        "name": "Jiebin Zhang"
                    },
                    {
                        "name": "Dawei Zhu"
                    },
                    {
                        "name": "Yifan Song"
                    },
                    {
                        "name": "Wenhao Wu"
                    },
                    {
                        "name": "Chuqiao Kuang"
                    },
                    {
                        "name": "Xiaoguang Li"
                    },
                    {
                        "name": "Lifeng Shang"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Sujian Li"
                    }
                ],
                "author_detail": {
                    "name": "Sujian Li"
                },
                "author": "Sujian Li",
                "arxiv_comment": "13pages,7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12513v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12513v2",
                "updated": "2024-12-17T09:11:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    11,
                    47,
                    1,
                    352,
                    0
                ],
                "published": "2024-10-16T12:45:35Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "title": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction"
                },
                "summary": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments."
                },
                "authors": [
                    {
                        "name": "Akriti Jain"
                    },
                    {
                        "name": "Saransh Sharma"
                    },
                    {
                        "name": "Koyel Mukherjee"
                    },
                    {
                        "name": "Soumyabrata Pal"
                    }
                ],
                "author_detail": {
                    "name": "Soumyabrata Pal"
                },
                "author": "Soumyabrata Pal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12513v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12513v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08585v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08585v3",
                "updated": "2024-12-17T05:40:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    5,
                    40,
                    9,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-11T18:03:05Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    3,
                    5,
                    2,
                    346,
                    0
                ],
                "title": "TurboAttention: Efficient Attention Approximation For High Throughputs\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TurboAttention: Efficient Attention Approximation For High Throughputs\n  LLMs"
                },
                "summary": "Large language model (LLM) inference demands significant amount of\ncomputation and memory, especially in the key attention mechanism. While\ntechniques, such as quantization and acceleration algorithms, like\nFlashAttention, have improved efficiency of the overall inference, they address\ndifferent aspects of the problem: quantization focuses on weight-activation\noperations, while FlashAttention improves execution but requires high-precision\nformats. Recent Key-value (KV) cache quantization reduces memory bandwidth but\nstill needs floating-point dequantization for attention operation.\n  We present TurboAttention, a comprehensive approach to enable quantized\nexecution of attention that simultaneously addresses both memory and\ncomputational efficiency. Our solution introduces two key innovations: FlashQ,\na headwise attention quantization technique that enables both compression of KV\ncache and quantized execution of activation-activation multiplication, and\nSparsity-based Softmax Approximation (SAS), which eliminates the need for\ndequantization to FP32 during exponentiation operation in attention.\nExperimental results demonstrate that TurboAttention achieves 1.2-1.8x speedup\nin attention, reduces the KV cache size by over 4.4x, and enables up to 2.37x\nmaximum throughput over the FP16 baseline while outperforming state-of-the-art\nquantization and compression techniques across various datasets and models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference demands significant amount of\ncomputation and memory, especially in the key attention mechanism. While\ntechniques, such as quantization and acceleration algorithms, like\nFlashAttention, have improved efficiency of the overall inference, they address\ndifferent aspects of the problem: quantization focuses on weight-activation\noperations, while FlashAttention improves execution but requires high-precision\nformats. Recent Key-value (KV) cache quantization reduces memory bandwidth but\nstill needs floating-point dequantization for attention operation.\n  We present TurboAttention, a comprehensive approach to enable quantized\nexecution of attention that simultaneously addresses both memory and\ncomputational efficiency. Our solution introduces two key innovations: FlashQ,\na headwise attention quantization technique that enables both compression of KV\ncache and quantized execution of activation-activation multiplication, and\nSparsity-based Softmax Approximation (SAS), which eliminates the need for\ndequantization to FP32 during exponentiation operation in attention.\nExperimental results demonstrate that TurboAttention achieves 1.2-1.8x speedup\nin attention, reduces the KV cache size by over 4.4x, and enables up to 2.37x\nmaximum throughput over the FP16 baseline while outperforming state-of-the-art\nquantization and compression techniques across various datasets and models."
                },
                "authors": [
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Srikant Bharadwaj"
                    },
                    {
                        "name": "James Hensman"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Victor Ruhle"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    }
                ],
                "author_detail": {
                    "name": "Saravan Rajmohan"
                },
                "author": "Saravan Rajmohan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08585v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08585v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12543v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12543v1",
                "updated": "2024-12-17T05:09:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    5,
                    9,
                    45,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T05:09:45Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    5,
                    9,
                    45,
                    1,
                    352,
                    0
                ],
                "title": "Personalized Federated Deep Reinforcement Learning for Heterogeneous\n  Edge Content Caching Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized Federated Deep Reinforcement Learning for Heterogeneous\n  Edge Content Caching Networks"
                },
                "summary": "Proactive caching is essential for minimizing latency and improving Quality\nof Experience (QoE) in multi-server edge networks. Federated Deep Reinforcement\nLearning (FDRL) is a promising approach for developing cache policies tailored\nto dynamic content requests. However, FDRL faces challenges such as an\nexpanding caching action space due to increased content numbers and difficulty\nin adapting global information to heterogeneous edge environments. In this\npaper, we propose a Personalized Federated Deep Reinforcement Learning\nframework for Caching, called PF-DRL-Ca, with the aim to maximize system\nutility while satisfying caching capability constraints. To manage the\nexpanding action space, we employ a new DRL algorithm, Multi-head Deep\nQ-Network (MH-DQN), which reshapes the action output layers of DQN into a\nmulti-head structure where each head generates a sub-dimensional action. We\nnext integrate the proposed MH-DQN into a personalized federated training\nframework, employing a layer-wise approach for training to derive a\npersonalized model that can adapt to heterogeneous environments while\nexploiting the global information to accelerate learning convergence. Our\nextensive experimental results demonstrate the superiority of MH-DQN over\ntraditional DRL algorithms on a single server, as well as the advantages of the\npersonal federated training architecture compared to other frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proactive caching is essential for minimizing latency and improving Quality\nof Experience (QoE) in multi-server edge networks. Federated Deep Reinforcement\nLearning (FDRL) is a promising approach for developing cache policies tailored\nto dynamic content requests. However, FDRL faces challenges such as an\nexpanding caching action space due to increased content numbers and difficulty\nin adapting global information to heterogeneous edge environments. In this\npaper, we propose a Personalized Federated Deep Reinforcement Learning\nframework for Caching, called PF-DRL-Ca, with the aim to maximize system\nutility while satisfying caching capability constraints. To manage the\nexpanding action space, we employ a new DRL algorithm, Multi-head Deep\nQ-Network (MH-DQN), which reshapes the action output layers of DQN into a\nmulti-head structure where each head generates a sub-dimensional action. We\nnext integrate the proposed MH-DQN into a personalized federated training\nframework, employing a layer-wise approach for training to derive a\npersonalized model that can adapt to heterogeneous environments while\nexploiting the global information to accelerate learning convergence. Our\nextensive experimental results demonstrate the superiority of MH-DQN over\ntraditional DRL algorithms on a single server, as well as the advantages of the\npersonal federated training architecture compared to other frameworks."
                },
                "authors": [
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Tan Li"
                    },
                    {
                        "name": "Hai Liu"
                    },
                    {
                        "name": "Tse-Tin Chan"
                    }
                ],
                "author_detail": {
                    "name": "Tse-Tin Chan"
                },
                "author": "Tse-Tin Chan",
                "arxiv_comment": "8 pages, 8 figures, WiOpt 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12543v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12543v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12488v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12488v1",
                "updated": "2024-12-17T02:44:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    2,
                    44,
                    43,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T02:44:43Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    2,
                    44,
                    43,
                    1,
                    352,
                    0
                ],
                "title": "A System for Microserving of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A System for Microserving of LLMs"
                },
                "summary": "The recent advances in LLMs bring a strong demand for efficient system\nsupport to improve overall serving efficiency. As LLM inference scales towards\nmultiple GPUs and even multiple compute nodes, various coordination patterns,\nsuch as prefill-decode disaggregation and context migration, arise in serving\nsystems. Most inference services today expose a coarse-grained request-level\nAPI with a pre-configured coordination strategy, limiting the ability to\ncustomize and dynamically reconfigure the coordination. In this paper, we\npropose LLM microserving, a multi-level architecture for structuring and\nprogramming LLM inference services. We introduces simple yet effective\nmicroserving APIs to support fine-grained sub-request level actions. A\nprogrammable router transforms user requests into sub-request calls, enabling\nthe dynamic reconfiguration of serving patterns. To support diverse execution\npatterns, we develop a unified KV cache interface that handles various KV\ncompute, transfer, and reuse scenarios. Our evaluation shows that LLM\nmicroserving can be reconfigured to support multiple disaggregation\norchestration strategies in a few lines of Python code while maintaining\nstate-of-the-art performance for LLM inference tasks. Additionally, it allows\nus to explore new strategy variants that reduce up to 47% of job completion\ntime compared to the existing strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent advances in LLMs bring a strong demand for efficient system\nsupport to improve overall serving efficiency. As LLM inference scales towards\nmultiple GPUs and even multiple compute nodes, various coordination patterns,\nsuch as prefill-decode disaggregation and context migration, arise in serving\nsystems. Most inference services today expose a coarse-grained request-level\nAPI with a pre-configured coordination strategy, limiting the ability to\ncustomize and dynamically reconfigure the coordination. In this paper, we\npropose LLM microserving, a multi-level architecture for structuring and\nprogramming LLM inference services. We introduces simple yet effective\nmicroserving APIs to support fine-grained sub-request level actions. A\nprogrammable router transforms user requests into sub-request calls, enabling\nthe dynamic reconfiguration of serving patterns. To support diverse execution\npatterns, we develop a unified KV cache interface that handles various KV\ncompute, transfer, and reuse scenarios. Our evaluation shows that LLM\nmicroserving can be reconfigured to support multiple disaggregation\norchestration strategies in a few lines of Python code while maintaining\nstate-of-the-art performance for LLM inference tasks. Additionally, it allows\nus to explore new strategy variants that reduce up to 47% of job completion\ntime compared to the existing strategies."
                },
                "authors": [
                    {
                        "name": "Hongyi Jin"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Charlie F. Ruan"
                    },
                    {
                        "name": "Yingcheng Wang"
                    },
                    {
                        "name": "Todd C. Mowry"
                    },
                    {
                        "name": "Xupeng Miao"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Tianqi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianqi Chen"
                },
                "author": "Tianqi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12488v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12444v1",
                "updated": "2024-12-17T01:12:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    1,
                    12,
                    35,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T01:12:35Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    1,
                    12,
                    35,
                    1,
                    352,
                    0
                ],
                "title": "LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers"
                },
                "summary": "Diffusion Transformers have emerged as the preeminent models for a wide array\nof generative tasks, demonstrating superior performance and efficacy across\nvarious applications. The promising results come at the cost of slow inference,\nas each denoising step requires running the whole transformer model with a\nlarge amount of parameters. In this paper, we show that performing the full\ncomputation of the model at each diffusion step is unnecessary, as some\ncomputations can be skipped by lazily reusing the results of previous steps.\nFurthermore, we show that the lower bound of similarity between outputs at\nconsecutive steps is notably high, and this similarity can be linearly\napproximated using the inputs. To verify our demonstrations, we propose the\n\\textbf{LazyDiT}, a lazy learning framework that efficiently leverages cached\nresults from earlier steps to skip redundant computations. Specifically, we\nincorporate lazy learning layers into the model, effectively trained to\nmaximize laziness, enabling dynamic skipping of redundant computations.\nExperimental results show that LazyDiT outperforms the DDIM sampler across\nmultiple diffusion transformer models at various resolutions. Furthermore, we\nimplement our method on mobile devices, achieving better performance than DDIM\nwith similar latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers have emerged as the preeminent models for a wide array\nof generative tasks, demonstrating superior performance and efficacy across\nvarious applications. The promising results come at the cost of slow inference,\nas each denoising step requires running the whole transformer model with a\nlarge amount of parameters. In this paper, we show that performing the full\ncomputation of the model at each diffusion step is unnecessary, as some\ncomputations can be skipped by lazily reusing the results of previous steps.\nFurthermore, we show that the lower bound of similarity between outputs at\nconsecutive steps is notably high, and this similarity can be linearly\napproximated using the inputs. To verify our demonstrations, we propose the\n\\textbf{LazyDiT}, a lazy learning framework that efficiently leverages cached\nresults from earlier steps to skip redundant computations. Specifically, we\nincorporate lazy learning layers into the model, effectively trained to\nmaximize laziness, enabling dynamic skipping of redundant computations.\nExperimental results show that LazyDiT outperforms the DDIM sampler across\nmultiple diffusion transformer models at various resolutions. Furthermore, we\nimplement our method on mobile devices, achieving better performance than DDIM\nwith similar latency."
                },
                "authors": [
                    {
                        "name": "Xuan Shen"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Yufa Zhou"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Yanyu Li"
                    },
                    {
                        "name": "Yifan Gong"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Hao Tan"
                    },
                    {
                        "name": "Jason Kuen"
                    },
                    {
                        "name": "Henghui Ding"
                    },
                    {
                        "name": "Zhihao Shu"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Pu Zhao"
                    },
                    {
                        "name": "Yanzhi Wang"
                    },
                    {
                        "name": "Jiuxiang Gu"
                    }
                ],
                "author_detail": {
                    "name": "Jiuxiang Gu"
                },
                "author": "Jiuxiang Gu",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11828v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11828v1",
                "updated": "2024-12-16T14:49:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    49,
                    32,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T14:49:32Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    49,
                    32,
                    0,
                    351,
                    0
                ],
                "title": "The Selection Problem in Multi-Query Optimization: a Comprehensive\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Selection Problem in Multi-Query Optimization: a Comprehensive\n  Survey"
                },
                "summary": "View materialization, index selection, and plan caching are well-known\ntechniques for optimization of query processing in database systems. The\nessence of these tasks is to select and save a subset of the most useful\ncandidates (views/indexes/plans) for reuse within given space/time budget\nconstraints. In this paper, based on the View Selection Problem, we propose a\nunified view on these problems. We identify the root causes of the complexity\nof these selection problems and provide a detailed analysis of techniques to\ncope with them. Our survey provides a modern classification of selection\nalgorithms known in the literature, including the latest ones based on Machine\nLearning. We provide a ground for the reuse of the selection techniques between\ndifferent optimization scenarios and highlight challenges and promising\ndirections in the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "View materialization, index selection, and plan caching are well-known\ntechniques for optimization of query processing in database systems. The\nessence of these tasks is to select and save a subset of the most useful\ncandidates (views/indexes/plans) for reuse within given space/time budget\nconstraints. In this paper, based on the View Selection Problem, we propose a\nunified view on these problems. We identify the root causes of the complexity\nof these selection problems and provide a detailed analysis of techniques to\ncope with them. Our survey provides a modern classification of selection\nalgorithms known in the literature, including the latest ones based on Machine\nLearning. We provide a ground for the reuse of the selection techniques between\ndifferent optimization scenarios and highlight challenges and promising\ndirections in the field."
                },
                "authors": [
                    {
                        "name": "Sergey Zinchenko"
                    },
                    {
                        "name": "Denis Ponomaryov"
                    }
                ],
                "author_detail": {
                    "name": "Denis Ponomaryov"
                },
                "author": "Denis Ponomaryov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11828v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11828v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11741v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11741v1",
                "updated": "2024-12-16T13:01:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    1,
                    53,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T13:01:53Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    1,
                    53,
                    0,
                    351,
                    0
                ],
                "title": "CSR:Achieving 1 Bit Key-Value Cache via Sparse Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSR:Achieving 1 Bit Key-Value Cache via Sparse Representation"
                },
                "summary": "The emergence of long-context text applications utilizing large language\nmodels (LLMs) has presented significant scalability challenges, particularly in\nmemory footprint. The linear growth of the Key-Value (KV) cache responsible for\nstoring attention keys and values to minimize redundant computations can lead\nto substantial increases in memory consumption, potentially causing models to\nfail to serve with limited memory resources. To address this issue, we propose\na novel approach called Cache Sparse Representation (CSR), which converts the\nKV cache by transforming the dense Key-Value cache tensor into sparse indexes\nand weights, offering a more memory-efficient representation during LLM\ninference. Furthermore, we introduce NeuralDict, a novel neural network-based\nmethod for automatically generating the dictionary used in our sparse\nrepresentation. Our extensive experiments demonstrate that CSR achieves\nperformance comparable to state-of-the-art KV cache quantization algorithms\nwhile maintaining robust functionality in memory-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of long-context text applications utilizing large language\nmodels (LLMs) has presented significant scalability challenges, particularly in\nmemory footprint. The linear growth of the Key-Value (KV) cache responsible for\nstoring attention keys and values to minimize redundant computations can lead\nto substantial increases in memory consumption, potentially causing models to\nfail to serve with limited memory resources. To address this issue, we propose\na novel approach called Cache Sparse Representation (CSR), which converts the\nKV cache by transforming the dense Key-Value cache tensor into sparse indexes\nand weights, offering a more memory-efficient representation during LLM\ninference. Furthermore, we introduce NeuralDict, a novel neural network-based\nmethod for automatically generating the dictionary used in our sparse\nrepresentation. Our extensive experiments demonstrate that CSR achieves\nperformance comparable to state-of-the-art KV cache quantization algorithms\nwhile maintaining robust functionality in memory-constrained environments."
                },
                "authors": [
                    {
                        "name": "Hongxuan Zhang"
                    },
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Jiaqi Zheng"
                    },
                    {
                        "name": "Chenyi Zhuang"
                    },
                    {
                        "name": "Jinjie Gu"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11741v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11706v1",
                "updated": "2024-12-16T12:28:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    28,
                    22,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T12:28:22Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    28,
                    22,
                    0,
                    351,
                    0
                ],
                "title": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration"
                },
                "summary": "Video Diffusion Transformers (DiTs) have demonstrated significant potential\nfor generating high-fidelity videos but are computationally intensive. Existing\nacceleration methods include distillation, which requires costly retraining,\nand feature caching, which is highly sensitive to network architecture. Recent\ntoken reduction methods are training-free and architecture-agnostic, offering\ngreater flexibility and wider applicability. However, they enforce the same\nsequence length across different components, constraining their acceleration\npotential. We observe that intra-sequence redundancy in video DiTs varies\nacross features, blocks, and denoising timesteps. Building on this observation,\nwe propose Asymmetric Reduction and Restoration (AsymRnR), a training-free\napproach to accelerate video DiTs. It offers a flexible and adaptive strategy\nthat reduces the number of tokens based on their redundancy to enhance both\nacceleration and generation quality. We further propose matching cache to\nfacilitate faster processing. Integrated into state-of-the-art video DiTs,\nAsymRnR achieves a superior speedup without compromising the quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Diffusion Transformers (DiTs) have demonstrated significant potential\nfor generating high-fidelity videos but are computationally intensive. Existing\nacceleration methods include distillation, which requires costly retraining,\nand feature caching, which is highly sensitive to network architecture. Recent\ntoken reduction methods are training-free and architecture-agnostic, offering\ngreater flexibility and wider applicability. However, they enforce the same\nsequence length across different components, constraining their acceleration\npotential. We observe that intra-sequence redundancy in video DiTs varies\nacross features, blocks, and denoising timesteps. Building on this observation,\nwe propose Asymmetric Reduction and Restoration (AsymRnR), a training-free\napproach to accelerate video DiTs. It offers a flexible and adaptive strategy\nthat reduces the number of tokens based on their redundancy to enhance both\nacceleration and generation quality. We further propose matching cache to\nfacilitate faster processing. Integrated into state-of-the-art video DiTs,\nAsymRnR achieves a superior speedup without compromising the quality."
                },
                "authors": [
                    {
                        "name": "Wenhao Sun"
                    },
                    {
                        "name": "Rong-Cheng Tu"
                    },
                    {
                        "name": "Jingyi Liao"
                    },
                    {
                        "name": "Zhao Jin"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11685v1",
                "updated": "2024-12-16T11:55:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    11,
                    55,
                    26,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T11:55:26Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    11,
                    55,
                    26,
                    0,
                    351,
                    0
                ],
                "title": "Ultra-High-Definition Dynamic Multi-Exposure Image Fusion via Infinite\n  Pixel Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultra-High-Definition Dynamic Multi-Exposure Image Fusion via Infinite\n  Pixel Learning"
                },
                "summary": "With the continuous improvement of device imaging resolution, the popularity\nof Ultra-High-Definition (UHD) images is increasing. Unfortunately, existing\nmethods for fusing multi-exposure images in dynamic scenes are designed for\nlow-resolution images, which makes them inefficient for generating high-quality\nUHD images on a resource-constrained device. To alleviate the limitations of\nextremely long-sequence inputs, inspired by the Large Language Model (LLM) for\nprocessing infinitely long texts, we propose a novel learning paradigm to\nachieve UHD multi-exposure dynamic scene image fusion on a single\nconsumer-grade GPU, named Infinite Pixel Learning (IPL). The design of our\napproach comes from three key components: The first step is to slice the input\nsequences to relieve the pressure generated by the model processing the data\nstream; Second, we develop an attention cache technique, which is similar to KV\ncache for infinite data stream processing; Finally, we design a method for\nattention cache compression to alleviate the storage burden of the cache on the\ndevice. In addition, we provide a new UHD benchmark to evaluate the\neffectiveness of our method. Extensive experimental results show that our\nmethod maintains high-quality visual performance while fusing UHD dynamic\nmulti-exposure images in real-time (>40fps) on a single consumer-grade GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the continuous improvement of device imaging resolution, the popularity\nof Ultra-High-Definition (UHD) images is increasing. Unfortunately, existing\nmethods for fusing multi-exposure images in dynamic scenes are designed for\nlow-resolution images, which makes them inefficient for generating high-quality\nUHD images on a resource-constrained device. To alleviate the limitations of\nextremely long-sequence inputs, inspired by the Large Language Model (LLM) for\nprocessing infinitely long texts, we propose a novel learning paradigm to\nachieve UHD multi-exposure dynamic scene image fusion on a single\nconsumer-grade GPU, named Infinite Pixel Learning (IPL). The design of our\napproach comes from three key components: The first step is to slice the input\nsequences to relieve the pressure generated by the model processing the data\nstream; Second, we develop an attention cache technique, which is similar to KV\ncache for infinite data stream processing; Finally, we design a method for\nattention cache compression to alleviate the storage burden of the cache on the\ndevice. In addition, we provide a new UHD benchmark to evaluate the\neffectiveness of our method. Extensive experimental results show that our\nmethod maintains high-quality visual performance while fusing UHD dynamic\nmulti-exposure images in real-time (>40fps) on a single consumer-grade GPU."
                },
                "authors": [
                    {
                        "name": "Xingchi Chen"
                    },
                    {
                        "name": "Zhuoran Zheng"
                    },
                    {
                        "name": "Xuerui Li"
                    },
                    {
                        "name": "Yuying Chen"
                    },
                    {
                        "name": "Shu Wang"
                    },
                    {
                        "name": "Wenqi Ren"
                    }
                ],
                "author_detail": {
                    "name": "Wenqi Ren"
                },
                "author": "Wenqi Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14201v1",
                "updated": "2024-12-15T21:02:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    15,
                    21,
                    2,
                    16,
                    6,
                    350,
                    0
                ],
                "published": "2024-12-15T21:02:16Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    21,
                    2,
                    16,
                    6,
                    350,
                    0
                ],
                "title": "The \"Huh?\" Button: Improving Understanding in Educational Videos with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The \"Huh?\" Button: Improving Understanding in Educational Videos with\n  Large Language Models"
                },
                "summary": "We propose a simple way to use large language models (LLMs) in education.\nSpecifically, our method aims to improve individual comprehension by adding a\nnovel feature to online videos. We combine the low threshold for interactivity\nin digital experiences with the benefits of rephrased and elaborated\nexplanations typical of face-to-face interactions, thereby supporting to close\nknowledge gaps at scale. To demonstrate the technical feasibility of our\napproach, we conducted a proof-of-concept experiment and implemented a\nprototype which is available for testing online. Through the use case, we also\nshow how caching can be applied in LLM-powered applications to reduce their\ncarbon footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a simple way to use large language models (LLMs) in education.\nSpecifically, our method aims to improve individual comprehension by adding a\nnovel feature to online videos. We combine the low threshold for interactivity\nin digital experiences with the benefits of rephrased and elaborated\nexplanations typical of face-to-face interactions, thereby supporting to close\nknowledge gaps at scale. To demonstrate the technical feasibility of our\napproach, we conducted a proof-of-concept experiment and implemented a\nprototype which is available for testing online. Through the use case, we also\nshow how caching can be applied in LLM-powered applications to reduce their\ncarbon footprint."
                },
                "authors": [
                    {
                        "name": "Boris Ruf"
                    },
                    {
                        "name": "Marcin Detyniecki"
                    }
                ],
                "author_detail": {
                    "name": "Marcin Detyniecki"
                },
                "author": "Marcin Detyniecki",
                "arxiv_comment": "Presented at the 18th IEEE International Workshop on Multimedia\n  Technologies for E-Learning (MTEL), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.02388v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.02388v3",
                "updated": "2024-12-15T03:29:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    15,
                    3,
                    29,
                    54,
                    6,
                    350,
                    0
                ],
                "published": "2023-05-03T19:07:06Z",
                "published_parsed": [
                    2023,
                    5,
                    3,
                    19,
                    7,
                    6,
                    2,
                    123,
                    0
                ],
                "title": "PULSE: Accelerating Distributed Pointer-Traversals on Disaggregated\n  Memory (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PULSE: Accelerating Distributed Pointer-Traversals on Disaggregated\n  Memory (Extended Version)"
                },
                "summary": "Caches at CPU nodes in disaggregated memory architectures amortize the high\ndata access latency over the network. However, such caches are fundamentally\nunable to improve performance for workloads requiring pointer traversals across\nlinked data structures. We argue for accelerating these pointer traversals\ncloser to disaggregated memory in a manner that preserves expressiveness for\nsupporting various linked structures, ensures energy efficiency and\nperformance, and supports distributed execution. We design PULSE, a distributed\npointer-traversal framework for rack-scale disaggregated memory to meet all the\nabove requirements. Our evaluation of PULSE shows that it enables low-latency,\nhigh-throughput, and energy-efficient execution for a wide range of pointer\ntraversal workloads on disaggregated memory that fare poorly with caching\nalone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caches at CPU nodes in disaggregated memory architectures amortize the high\ndata access latency over the network. However, such caches are fundamentally\nunable to improve performance for workloads requiring pointer traversals across\nlinked data structures. We argue for accelerating these pointer traversals\ncloser to disaggregated memory in a manner that preserves expressiveness for\nsupporting various linked structures, ensures energy efficiency and\nperformance, and supports distributed execution. We design PULSE, a distributed\npointer-traversal framework for rack-scale disaggregated memory to meet all the\nabove requirements. Our evaluation of PULSE shows that it enables low-latency,\nhigh-throughput, and energy-efficient execution for a wide range of pointer\ntraversal workloads on disaggregated memory that fare poorly with caching\nalone."
                },
                "authors": [
                    {
                        "name": "Yupeng Tang"
                    },
                    {
                        "name": "Seung-seob Lee"
                    },
                    {
                        "name": "Abhishek Bhattacharjee"
                    },
                    {
                        "name": "Anurag Khandelwal"
                    }
                ],
                "author_detail": {
                    "name": "Anurag Khandelwal"
                },
                "author": "Anurag Khandelwal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.02388v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.02388v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11021v1",
                "updated": "2024-12-15T02:30:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    15,
                    2,
                    30,
                    9,
                    6,
                    350,
                    0
                ],
                "published": "2024-12-15T02:30:09Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    2,
                    30,
                    9,
                    6,
                    350,
                    0
                ],
                "title": "SparseMap: Loop Mapping for Sparse CNNs on Streaming Coarse-grained\n  Reconfigurable Array",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseMap: Loop Mapping for Sparse CNNs on Streaming Coarse-grained\n  Reconfigurable Array"
                },
                "summary": "Streaming coarse-grained reconfgurable array (CGRA) is a promising\narchitecture for data/computing-intensive applications because of its\nfexibility, high throughput and efcient memory system. However,when\naccelerating sparse CNNs, the irregular input data demands inside sparse CNNs\nwould cause excessive caching operations (COPs) and multi-cycle internal\ndependencies (MCIDs) between operations, declining the throughput of the\nstreaming CGRA. We propose a mapping method for sparse CNNs onto streaming\nCGRA, SparseMap, which incorporates an efcient I/O data management along with\noperation scheduling and binding, to reduce the COPs and MCIDs, thereby\nensuring the optimal throughput of streaming CGRA.The experimental results show\nSparseMap reduces 92.5% COPs and 46.0 % MCIDs while achieves the same or even\nsmaller initiation interval (II) compared to previous works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming coarse-grained reconfgurable array (CGRA) is a promising\narchitecture for data/computing-intensive applications because of its\nfexibility, high throughput and efcient memory system. However,when\naccelerating sparse CNNs, the irregular input data demands inside sparse CNNs\nwould cause excessive caching operations (COPs) and multi-cycle internal\ndependencies (MCIDs) between operations, declining the throughput of the\nstreaming CGRA. We propose a mapping method for sparse CNNs onto streaming\nCGRA, SparseMap, which incorporates an efcient I/O data management along with\noperation scheduling and binding, to reduce the COPs and MCIDs, thereby\nensuring the optimal throughput of streaming CGRA.The experimental results show\nSparseMap reduces 92.5% COPs and 46.0 % MCIDs while achieves the same or even\nsmaller initiation interval (II) compared to previous works."
                },
                "authors": [
                    {
                        "name": "Xiaobing Ni"
                    },
                    {
                        "name": "Mengke Ge"
                    },
                    {
                        "name": "Jiaheng Ruan"
                    },
                    {
                        "name": "Song Chen"
                    },
                    {
                        "name": "Yi Kang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Kang"
                },
                "author": "Yi Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15246v1",
                "updated": "2024-12-14T06:47:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    14,
                    6,
                    47,
                    56,
                    5,
                    349,
                    0
                ],
                "published": "2024-12-14T06:47:56Z",
                "published_parsed": [
                    2024,
                    12,
                    14,
                    6,
                    47,
                    56,
                    5,
                    349,
                    0
                ],
                "title": "Accelerating Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Retrieval-Augmented Generation"
                },
                "summary": "An evolving solution to address hallucination and enhance accuracy in large\nlanguage models (LLMs) is Retrieval-Augmented Generation (RAG), which involves\naugmenting LLMs with information retrieved from an external knowledge source,\nsuch as the web. This paper profiles several RAG execution pipelines and\ndemystifies the complex interplay between their retrieval and generation\nphases. We demonstrate that while exact retrieval schemes are expensive, they\ncan reduce inference time compared to approximate retrieval variants because an\nexact retrieval model can send a smaller but more accurate list of documents to\nthe generative model while maintaining the same end-to-end accuracy. This\nobservation motivates the acceleration of the exact nearest neighbor search for\nRAG.\n  In this work, we design Intelligent Knowledge Store (IKS), a type-2 CXL\ndevice that implements a scale-out near-memory acceleration architecture with a\nnovel cache-coherent interface between the host CPU and near-memory\naccelerators. IKS offers 13.4-27.9x faster exact nearest neighbor search over a\n512GB vector database compared with executing the search on Intel Sapphire\nRapids CPUs. This higher search performance translates to 1.7-26.3x lower\nend-to-end inference time for representative RAG applications. IKS is\ninherently a memory expander; its internal DRAM can be disaggregated and used\nfor other applications running on the server to prevent DRAM, which is the most\nexpensive component in today's servers, from being stranded.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An evolving solution to address hallucination and enhance accuracy in large\nlanguage models (LLMs) is Retrieval-Augmented Generation (RAG), which involves\naugmenting LLMs with information retrieved from an external knowledge source,\nsuch as the web. This paper profiles several RAG execution pipelines and\ndemystifies the complex interplay between their retrieval and generation\nphases. We demonstrate that while exact retrieval schemes are expensive, they\ncan reduce inference time compared to approximate retrieval variants because an\nexact retrieval model can send a smaller but more accurate list of documents to\nthe generative model while maintaining the same end-to-end accuracy. This\nobservation motivates the acceleration of the exact nearest neighbor search for\nRAG.\n  In this work, we design Intelligent Knowledge Store (IKS), a type-2 CXL\ndevice that implements a scale-out near-memory acceleration architecture with a\nnovel cache-coherent interface between the host CPU and near-memory\naccelerators. IKS offers 13.4-27.9x faster exact nearest neighbor search over a\n512GB vector database compared with executing the search on Intel Sapphire\nRapids CPUs. This higher search performance translates to 1.7-26.3x lower\nend-to-end inference time for representative RAG applications. IKS is\ninherently a memory expander; its internal DRAM can be disaggregated and used\nfor other applications running on the server to prevent DRAM, which is the most\nexpensive component in today's servers, from being stranded."
                },
                "authors": [
                    {
                        "name": "Derrick Quinn"
                    },
                    {
                        "name": "Mohammad Nouri"
                    },
                    {
                        "name": "Neel Patel"
                    },
                    {
                        "name": "John Salihu"
                    },
                    {
                        "name": "Alireza Salemi"
                    },
                    {
                        "name": "Sukhan Lee"
                    },
                    {
                        "name": "Hamed Zamani"
                    },
                    {
                        "name": "Mohammad Alian"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Alian"
                },
                "author": "Mohammad Alian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10685v1",
                "updated": "2024-12-14T05:20:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    14,
                    5,
                    20,
                    50,
                    5,
                    349,
                    0
                ],
                "published": "2024-12-14T05:20:50Z",
                "published_parsed": [
                    2024,
                    12,
                    14,
                    5,
                    20,
                    50,
                    5,
                    349,
                    0
                ],
                "title": "RMCSA Algorithm for Congestion-Aware and Service Latency Aware Dynamic\n  Service Provisioning in Software-Defined SDM-EONs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RMCSA Algorithm for Congestion-Aware and Service Latency Aware Dynamic\n  Service Provisioning in Software-Defined SDM-EONs"
                },
                "summary": "The implementation of 5G and the future deployment of 6G necessitate the\nutilization of optical networks that possess substantial capacity and exhibit\nminimal latency. The dynamic arrival and departure of connection requests in\noptical networks result in particular central links experiencing more traffic\nand congestion than non-central links. The occurrence of congested links leads\nto service blocking despite the availability of resources within the network,\nrestricting the efficient utilization of network resources. The available\nalgorithms in the literature that aim to balance load among network links offer\na trade-off between blocking performance and algorithmic complexity, thus\nincreasing service provisioning time. This work proposes a dynamic\nrouting-based congestion-aware routing, modulation, core, and spectrum\nassignment (RMCSA) algorithm for space division multiplexing elastic optical\nnetworks (SDM-EONs). The algorithm finds alternative candidate paths based on\nreal-time link occupancy metrics to minimize blocking due to link congestion\nunder dynamic traffic scenarios. As a result, the algorithm reduces the\nformation of congestion hotspots in the network owing to link-betweenness\ncentrality. We have performed extensive simulations using two realistic network\ntopologies to compare the performance of the proposed algorithm with relevant\nRMCSA algorithms available in the literature. The simulation results verify the\nsuperior performance of our proposed algorithm compared to the benchmark Yen's\nK-shortest paths and K-Disjoint shortest paths RMCSA algorithms in connection\nblocking ratio and spectrum utilization efficiency. To expedite the\nroute-finding process, we present a novel caching strategy that allows the\nproposed algorithm to demonstrate a much-reduced service delay time compared to\nthe recently developed adaptive link weight-based load-balancing RMCSA\nalgorithm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The implementation of 5G and the future deployment of 6G necessitate the\nutilization of optical networks that possess substantial capacity and exhibit\nminimal latency. The dynamic arrival and departure of connection requests in\noptical networks result in particular central links experiencing more traffic\nand congestion than non-central links. The occurrence of congested links leads\nto service blocking despite the availability of resources within the network,\nrestricting the efficient utilization of network resources. The available\nalgorithms in the literature that aim to balance load among network links offer\na trade-off between blocking performance and algorithmic complexity, thus\nincreasing service provisioning time. This work proposes a dynamic\nrouting-based congestion-aware routing, modulation, core, and spectrum\nassignment (RMCSA) algorithm for space division multiplexing elastic optical\nnetworks (SDM-EONs). The algorithm finds alternative candidate paths based on\nreal-time link occupancy metrics to minimize blocking due to link congestion\nunder dynamic traffic scenarios. As a result, the algorithm reduces the\nformation of congestion hotspots in the network owing to link-betweenness\ncentrality. We have performed extensive simulations using two realistic network\ntopologies to compare the performance of the proposed algorithm with relevant\nRMCSA algorithms available in the literature. The simulation results verify the\nsuperior performance of our proposed algorithm compared to the benchmark Yen's\nK-shortest paths and K-Disjoint shortest paths RMCSA algorithms in connection\nblocking ratio and spectrum utilization efficiency. To expedite the\nroute-finding process, we present a novel caching strategy that allows the\nproposed algorithm to demonstrate a much-reduced service delay time compared to\nthe recently developed adaptive link weight-based load-balancing RMCSA\nalgorithm."
                },
                "authors": [
                    {
                        "name": "Baljinder Singh Heera"
                    },
                    {
                        "name": "Shrinivas Petale"
                    },
                    {
                        "name": "Yatindra Nath Singh"
                    },
                    {
                        "name": "Suresh Subramaniam"
                    }
                ],
                "author_detail": {
                    "name": "Suresh Subramaniam"
                },
                "author": "Suresh Subramaniam",
                "arxiv_comment": "The preliminary work was presented at ONDM 2023 conference.\n  https://doi.org/10.23919/ONDM57372.2023.10144866",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10319v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10319v1",
                "updated": "2024-12-13T17:59:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    59,
                    52,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T17:59:52Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    59,
                    52,
                    4,
                    348,
                    0
                ],
                "title": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods"
                },
                "summary": "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench."
                },
                "authors": [
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qianhui Wu"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Surin Ahn"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Amir H. Abdi"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10319v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10319v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10302v1",
                "updated": "2024-12-13T17:37:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    37,
                    48,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T17:37:48Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    37,
                    48,
                    4,
                    348,
                    0
                ],
                "title": "DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced\n  Multimodal Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced\n  Multimodal Understanding"
                },
                "summary": "We present DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE)\nVision-Language Models that significantly improves upon its predecessor,\nDeepSeek-VL, through two key major upgrades. For the vision component, we\nincorporate a dynamic tiling vision encoding strategy designed for processing\nhigh-resolution images with different aspect ratios. For the language\ncomponent, we leverage DeepSeekMoE models with the Multi-head Latent Attention\nmechanism, which compresses Key-Value cache into latent vectors, to enable\nefficient inference and high throughput. Trained on an improved vision-language\ndataset, DeepSeek-VL2 demonstrates superior capabilities across various tasks,\nincluding but not limited to visual question answering, optical character\nrecognition, document/table/chart understanding, and visual grounding. Our\nmodel series is composed of three variants: DeepSeek-VL2-Tiny,\nDeepSeek-VL2-Small and DeepSeek-VL2, with 1.0B, 2.8B and 4.5B activated\nparameters respectively. DeepSeek-VL2 achieves competitive or state-of-the-art\nperformance with similar or fewer activated parameters compared to existing\nopen-source dense and MoE-based models. Codes and pre-trained models are\npublicly accessible at https://github.com/deepseek-ai/DeepSeek-VL2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE)\nVision-Language Models that significantly improves upon its predecessor,\nDeepSeek-VL, through two key major upgrades. For the vision component, we\nincorporate a dynamic tiling vision encoding strategy designed for processing\nhigh-resolution images with different aspect ratios. For the language\ncomponent, we leverage DeepSeekMoE models with the Multi-head Latent Attention\nmechanism, which compresses Key-Value cache into latent vectors, to enable\nefficient inference and high throughput. Trained on an improved vision-language\ndataset, DeepSeek-VL2 demonstrates superior capabilities across various tasks,\nincluding but not limited to visual question answering, optical character\nrecognition, document/table/chart understanding, and visual grounding. Our\nmodel series is composed of three variants: DeepSeek-VL2-Tiny,\nDeepSeek-VL2-Small and DeepSeek-VL2, with 1.0B, 2.8B and 4.5B activated\nparameters respectively. DeepSeek-VL2 achieves competitive or state-of-the-art\nperformance with similar or fewer activated parameters compared to existing\nopen-source dense and MoE-based models. Codes and pre-trained models are\npublicly accessible at https://github.com/deepseek-ai/DeepSeek-VL2."
                },
                "authors": [
                    {
                        "name": "Zhiyu Wu"
                    },
                    {
                        "name": "Xiaokang Chen"
                    },
                    {
                        "name": "Zizheng Pan"
                    },
                    {
                        "name": "Xingchao Liu"
                    },
                    {
                        "name": "Wen Liu"
                    },
                    {
                        "name": "Damai Dai"
                    },
                    {
                        "name": "Huazuo Gao"
                    },
                    {
                        "name": "Yiyang Ma"
                    },
                    {
                        "name": "Chengyue Wu"
                    },
                    {
                        "name": "Bingxuan Wang"
                    },
                    {
                        "name": "Zhenda Xie"
                    },
                    {
                        "name": "Yu Wu"
                    },
                    {
                        "name": "Kai Hu"
                    },
                    {
                        "name": "Jiawei Wang"
                    },
                    {
                        "name": "Yaofeng Sun"
                    },
                    {
                        "name": "Yukun Li"
                    },
                    {
                        "name": "Yishi Piao"
                    },
                    {
                        "name": "Kang Guan"
                    },
                    {
                        "name": "Aixin Liu"
                    },
                    {
                        "name": "Xin Xie"
                    },
                    {
                        "name": "Yuxiang You"
                    },
                    {
                        "name": "Kai Dong"
                    },
                    {
                        "name": "Xingkai Yu"
                    },
                    {
                        "name": "Haowei Zhang"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Yisong Wang"
                    },
                    {
                        "name": "Chong Ruan"
                    }
                ],
                "author_detail": {
                    "name": "Chong Ruan"
                },
                "author": "Chong Ruan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18566v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18566v2",
                "updated": "2024-12-13T16:13:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    16,
                    13,
                    39,
                    4,
                    348,
                    0
                ],
                "published": "2024-11-27T18:09:29Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    9,
                    29,
                    2,
                    332,
                    0
                ],
                "title": "OASIS-UROS: Open Acquisition System for IEPE Sensors -- Upgraded,\n  Refined, and Overhauled Software",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS-UROS: Open Acquisition System for IEPE Sensors -- Upgraded,\n  Refined, and Overhauled Software"
                },
                "summary": "OASIS-UROS continues the previously published Open Acquisition System for\nIEPE Sensors (OASIS). While still building on the ESP32 microcontroller, this\nversion improves the overall performance by switching to an SD card caching\nsystem and upgrading the analog-digital converter to an AD7606C-18, which has a\nhigher resolution, provides eight channels, oversampling, and\nsoftware-adjustable voltage ranges. Also improved is the IEPE front-end and\npower supply, as well as the firmware of the acquisition system, which can now\nachieve a sample rate of up to 36 kHz while sampling all eight channels. This\npaper documents the hardware and software of OASIS-UROS and provides all\nmaterials required to reproduce the open acquisition system. Lastly, the system\nwas validated against commercial hardware and software in an experimental modal\nanalysis context. This showed that the system performs close to the commercial\none in some aspects with respect to the utilized test case. While OASIS-UROS\ncannot match the full performance of the commercial system, the developed\nsystem can be a viable alternative for students, people in academia, or smaller\ncompanies that have a constrained budget or require complete insight as well as\nadaptability of the hardware and software.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS-UROS continues the previously published Open Acquisition System for\nIEPE Sensors (OASIS). While still building on the ESP32 microcontroller, this\nversion improves the overall performance by switching to an SD card caching\nsystem and upgrading the analog-digital converter to an AD7606C-18, which has a\nhigher resolution, provides eight channels, oversampling, and\nsoftware-adjustable voltage ranges. Also improved is the IEPE front-end and\npower supply, as well as the firmware of the acquisition system, which can now\nachieve a sample rate of up to 36 kHz while sampling all eight channels. This\npaper documents the hardware and software of OASIS-UROS and provides all\nmaterials required to reproduce the open acquisition system. Lastly, the system\nwas validated against commercial hardware and software in an experimental modal\nanalysis context. This showed that the system performs close to the commercial\none in some aspects with respect to the utilized test case. While OASIS-UROS\ncannot match the full performance of the commercial system, the developed\nsystem can be a viable alternative for students, people in academia, or smaller\ncompanies that have a constrained budget or require complete insight as well as\nadaptability of the hardware and software."
                },
                "authors": [
                    {
                        "name": "Oliver Maximilian Zobel"
                    },
                    {
                        "name": "Johannes Maierhofer"
                    },
                    {
                        "name": "Andreas Köstler"
                    },
                    {
                        "name": "Daniel J. Rixen"
                    }
                ],
                "author_detail": {
                    "name": "Daniel J. Rixen"
                },
                "author": "Daniel J. Rixen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18566v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18566v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10153v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10153v1",
                "updated": "2024-12-13T14:11:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    11,
                    42,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T14:11:42Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    11,
                    42,
                    4,
                    348,
                    0
                ],
                "title": "EVOS: Efficient Implicit Neural Training via EVOlutionary Selector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EVOS: Efficient Implicit Neural Training via EVOlutionary Selector"
                },
                "summary": "We propose EVOlutionary Selector (EVOS), an efficient training paradigm for\naccelerating Implicit Neural Representation (INR). Unlike conventional INR\ntraining that feeds all samples through the neural network in each iteration,\nour approach restricts training to strategically selected points, reducing\ncomputational overhead by eliminating redundant forward passes. Specifically,\nwe treat each sample as an individual in an evolutionary process, where only\nthose fittest ones survive and merit inclusion in training, adaptively evolving\nwith the neural network dynamics. While this is conceptually similar to\nEvolutionary Algorithms, their distinct objectives (selection for acceleration\nvs. iterative solution optimization) require a fundamental redefinition of\nevolutionary mechanisms for our context. In response, we design sparse fitness\nevaluation, frequency-guided crossover, and augmented unbiased mutation to\ncomprise EVOS. These components respectively guide sample selection with\nreduced computational cost, enhance performance through frequency-domain\nbalance, and mitigate selection bias from cached evaluation. Extensive\nexperiments demonstrate that our method achieves approximately 48%-66%\nreduction in training time while ensuring superior convergence without\nadditional cost, establishing state-of-the-art acceleration among recent\nsampling-based strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose EVOlutionary Selector (EVOS), an efficient training paradigm for\naccelerating Implicit Neural Representation (INR). Unlike conventional INR\ntraining that feeds all samples through the neural network in each iteration,\nour approach restricts training to strategically selected points, reducing\ncomputational overhead by eliminating redundant forward passes. Specifically,\nwe treat each sample as an individual in an evolutionary process, where only\nthose fittest ones survive and merit inclusion in training, adaptively evolving\nwith the neural network dynamics. While this is conceptually similar to\nEvolutionary Algorithms, their distinct objectives (selection for acceleration\nvs. iterative solution optimization) require a fundamental redefinition of\nevolutionary mechanisms for our context. In response, we design sparse fitness\nevaluation, frequency-guided crossover, and augmented unbiased mutation to\ncomprise EVOS. These components respectively guide sample selection with\nreduced computational cost, enhance performance through frequency-domain\nbalance, and mitigate selection bias from cached evaluation. Extensive\nexperiments demonstrate that our method achieves approximately 48%-66%\nreduction in training time while ensuring superior convergence without\nadditional cost, establishing state-of-the-art acceleration among recent\nsampling-based strategies."
                },
                "authors": [
                    {
                        "name": "Weixiang Zhang"
                    },
                    {
                        "name": "Shuzhao Xie"
                    },
                    {
                        "name": "Chengwei Ren"
                    },
                    {
                        "name": "Siyi Xie"
                    },
                    {
                        "name": "Chen Tang"
                    },
                    {
                        "name": "Shijia Ge"
                    },
                    {
                        "name": "Mingzi Wang"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10153v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12021v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12021v2",
                "updated": "2024-12-13T14:08:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    8,
                    55,
                    4,
                    348,
                    0
                ],
                "published": "2024-09-18T14:31:33Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    14,
                    31,
                    33,
                    2,
                    262,
                    0
                ],
                "title": "Optimal Offline ORAM with Perfect Security via Simple Oblivious Priority\n  Queues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Offline ORAM with Perfect Security via Simple Oblivious Priority\n  Queues"
                },
                "summary": "Oblivious RAM (ORAM) is a well-researched primitive to hide the memory access\npattern of a RAM computation; it has a variety of applications in trusted\ncomputing, outsourced storage, and multiparty computation. In this paper, we\nstudy the so-called offline ORAM in which the sequence of memory access\nlocations to be hidden is known in advance. Apart from their theoretical\nsignificance, offline ORAMs can be used to construct efficient oblivious\nalgorithms.\n  We obtain the first optimal offline ORAM with perfect security from oblivious\npriority queues via time-forward processing. For this, we present a simple\nconstruction of an oblivious priority queue with perfect security. Our\nconstruction achieves an asymptotically optimal (amortized) runtime of\n$\\Theta(\\log N)$ per operation for a capacity of $N$ elements and is of\nindependent interest.\n  Building on our construction, we additionally present efficient\nexternal-memory instantiations of our oblivious, perfectly-secure construction:\nFor the cache-aware setting, we match the optimal I/O complexity of\n$\\Theta(\\frac{1}{B} \\log \\frac{N}{M})$ per operation (amortized), and for the\ncache-oblivious setting we achieve a near-optimal I/O complexity of\n$O(\\frac{1}{B} \\log \\frac{N}{M} \\log\\log_M N)$ per operation (amortized).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oblivious RAM (ORAM) is a well-researched primitive to hide the memory access\npattern of a RAM computation; it has a variety of applications in trusted\ncomputing, outsourced storage, and multiparty computation. In this paper, we\nstudy the so-called offline ORAM in which the sequence of memory access\nlocations to be hidden is known in advance. Apart from their theoretical\nsignificance, offline ORAMs can be used to construct efficient oblivious\nalgorithms.\n  We obtain the first optimal offline ORAM with perfect security from oblivious\npriority queues via time-forward processing. For this, we present a simple\nconstruction of an oblivious priority queue with perfect security. Our\nconstruction achieves an asymptotically optimal (amortized) runtime of\n$\\Theta(\\log N)$ per operation for a capacity of $N$ elements and is of\nindependent interest.\n  Building on our construction, we additionally present efficient\nexternal-memory instantiations of our oblivious, perfectly-secure construction:\nFor the cache-aware setting, we match the optimal I/O complexity of\n$\\Theta(\\frac{1}{B} \\log \\frac{N}{M})$ per operation (amortized), and for the\ncache-oblivious setting we achieve a near-optimal I/O complexity of\n$O(\\frac{1}{B} \\log \\frac{N}{M} \\log\\log_M N)$ per operation (amortized)."
                },
                "authors": [
                    {
                        "name": "Thore Thießen"
                    },
                    {
                        "name": "Jan Vahrenhold"
                    }
                ],
                "author_detail": {
                    "name": "Jan Vahrenhold"
                },
                "author": "Jan Vahrenhold",
                "arxiv_doi": "10.4230/LIPIcs.ISAAC.2024.55",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.4230/LIPIcs.ISAAC.2024.55",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.12021v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12021v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "23 pages, full version of the paper in ISAAC 2024; minor changes",
                "arxiv_journal_ref": "Thore Thie{\\ss}en and Jan Vahrenhold. Optimal offline ORAM with\n  perfect security via simple oblivious priority queues. In 35th International\n  Symposium on Algorithms and Computation (ISAAC 2024), 18 pages. 2024",
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12178v1",
                "updated": "2024-12-13T02:26:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    2,
                    26,
                    54,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T02:26:54Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    2,
                    26,
                    54,
                    4,
                    348,
                    0
                ],
                "title": "Activation Sparsity Opportunities for Compressing General Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation Sparsity Opportunities for Compressing General Large Language\n  Models"
                },
                "summary": "Deploying local AI models, such as Large Language Models (LLMs), to edge\ndevices can substantially enhance devices' independent capabilities, alleviate\nthe server's burden, and lower the response time. Owing to these tremendous\npotentials, many big tech companies have released several lightweight Small\nLanguage Models (SLMs) to bridge this gap. However, we still have huge\nmotivations to deploy more powerful (LLMs) AI models on edge devices and\nenhance their smartness level. Unlike the conventional approaches for AI model\ncompression, we investigate activation sparsity. The activation sparsity method\nis orthogonal and combinable with existing techniques to maximize compression\nrate while maintaining great accuracy. LLMs' Feed-Forward Network (FFN)\ncomponents, which typically comprise a large proportion of parameters (around\n3/2), ensure that our FFN optimizations would have a better chance of achieving\neffective compression. Moreover, our findings are beneficial to general LLMs\nand are not restricted to ReLU-based models. This work systematically\ninvestigates the tradeoff between enforcing activation sparsity and perplexity\n(accuracy) on state-of-the-art LLMs. Our empirical analysis demonstrates that\nwe can obtain around 50% of main memory and computing reductions for critical\nFFN components with negligible accuracy degradation. This extra 50% sparsity\ndoes not naturally exist in the current LLMs, which require tuning LLMs'\nactivation outputs by injecting zero-enforcing thresholds. To obtain the\nbenefits of activation sparsity, we provide a guideline for the system\narchitect for LLM prediction and prefetching. The success prediction allows the\nsystem to prefetch the necessary weights while omitting the inactive ones and\ntheir successors, therefore lowering cache and memory pollution and reducing\nLLM execution time on resource-constrained edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying local AI models, such as Large Language Models (LLMs), to edge\ndevices can substantially enhance devices' independent capabilities, alleviate\nthe server's burden, and lower the response time. Owing to these tremendous\npotentials, many big tech companies have released several lightweight Small\nLanguage Models (SLMs) to bridge this gap. However, we still have huge\nmotivations to deploy more powerful (LLMs) AI models on edge devices and\nenhance their smartness level. Unlike the conventional approaches for AI model\ncompression, we investigate activation sparsity. The activation sparsity method\nis orthogonal and combinable with existing techniques to maximize compression\nrate while maintaining great accuracy. LLMs' Feed-Forward Network (FFN)\ncomponents, which typically comprise a large proportion of parameters (around\n3/2), ensure that our FFN optimizations would have a better chance of achieving\neffective compression. Moreover, our findings are beneficial to general LLMs\nand are not restricted to ReLU-based models. This work systematically\ninvestigates the tradeoff between enforcing activation sparsity and perplexity\n(accuracy) on state-of-the-art LLMs. Our empirical analysis demonstrates that\nwe can obtain around 50% of main memory and computing reductions for critical\nFFN components with negligible accuracy degradation. This extra 50% sparsity\ndoes not naturally exist in the current LLMs, which require tuning LLMs'\nactivation outputs by injecting zero-enforcing thresholds. To obtain the\nbenefits of activation sparsity, we provide a guideline for the system\narchitect for LLM prediction and prefetching. The success prediction allows the\nsystem to prefetch the necessary weights while omitting the inactive ones and\ntheir successors, therefore lowering cache and memory pollution and reducing\nLLM execution time on resource-constrained edge devices."
                },
                "authors": [
                    {
                        "name": "Nobel Dhar"
                    },
                    {
                        "name": "Bobin Deng"
                    },
                    {
                        "name": "Md Romyull Islam"
                    },
                    {
                        "name": "Kazi Fahim Ahmad Nasif"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Kun Suo"
                    }
                ],
                "author_detail": {
                    "name": "Kun Suo"
                },
                "author": "Kun Suo",
                "arxiv_comment": "Conference submission for IPCCC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09474v1",
                "updated": "2024-12-12T17:20:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    17,
                    20,
                    26,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T17:20:26Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    17,
                    20,
                    26,
                    3,
                    347,
                    0
                ],
                "title": "Optimizing CDN Architectures: Multi-Metric Algorithmic Breakthroughs for\n  Edge and Distributed Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing CDN Architectures: Multi-Metric Algorithmic Breakthroughs for\n  Edge and Distributed Performance"
                },
                "summary": "A Content Delivery Network (CDN) is a powerful system of distributed caching\nservers that aims to accelerate content delivery, like high-definition video,\nIoT applications, and ultra-low-latency services, efficiently and with fast\nvelocity. This has become of paramount importance in the post-pandemic era.\nChallenges arise when exponential content volume growth and scalability across\ndifferent geographic locations are required. This paper investigates\ndata-driven evaluations of CDN algorithms in dynamic server selection for\nlatency reduction, bandwidth throttling for efficient resource management,\nreal-time Round Trip Time analysis for adaptive routing, and programmatic\nnetwork delay simulation to emulate various conditions. Key performance\nmetrics, such as round-trip time (RTT) and CPU usage, are carefully analyzed to\nevaluate scalability and algorithmic efficiency through two experimental\nsetups: a constrained edge-like local system and a scalable FABRIC testbed. The\nstatistical validation of RTT trends, alongside CPU utilization, is presented\nin the results. The optimization process reveals significant trade-offs between\nscalability and resource consumption, providing actionable insights for\neffectively deploying and enhancing CDN algorithms in edge and distributed\ncomputing environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Content Delivery Network (CDN) is a powerful system of distributed caching\nservers that aims to accelerate content delivery, like high-definition video,\nIoT applications, and ultra-low-latency services, efficiently and with fast\nvelocity. This has become of paramount importance in the post-pandemic era.\nChallenges arise when exponential content volume growth and scalability across\ndifferent geographic locations are required. This paper investigates\ndata-driven evaluations of CDN algorithms in dynamic server selection for\nlatency reduction, bandwidth throttling for efficient resource management,\nreal-time Round Trip Time analysis for adaptive routing, and programmatic\nnetwork delay simulation to emulate various conditions. Key performance\nmetrics, such as round-trip time (RTT) and CPU usage, are carefully analyzed to\nevaluate scalability and algorithmic efficiency through two experimental\nsetups: a constrained edge-like local system and a scalable FABRIC testbed. The\nstatistical validation of RTT trends, alongside CPU utilization, is presented\nin the results. The optimization process reveals significant trade-offs between\nscalability and resource consumption, providing actionable insights for\neffectively deploying and enhancing CDN algorithms in edge and distributed\ncomputing environments."
                },
                "authors": [
                    {
                        "name": "Md Nurul Absur"
                    },
                    {
                        "name": "Sourya Saha"
                    },
                    {
                        "name": "Sifat Nawrin Nova"
                    },
                    {
                        "name": "Kazi Fahim Ahmad Nasif"
                    },
                    {
                        "name": "Md Rahat Ul Nasib"
                    }
                ],
                "author_detail": {
                    "name": "Md Rahat Ul Nasib"
                },
                "author": "Md Rahat Ul Nasib",
                "arxiv_comment": "6 Pages, 10 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09416v1",
                "updated": "2024-12-12T16:24:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    24,
                    35,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T16:24:35Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    24,
                    35,
                    3,
                    347,
                    0
                ],
                "title": "Unifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical\n  Ability Assessment of LLM-Powered AI Tutors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical\n  Ability Assessment of LLM-Powered AI Tutors"
                },
                "summary": "In this paper, we investigate whether current state-of-the-art large language\nmodels (LLMs) are effective as AI tutors and whether they demonstrate\npedagogical abilities necessary for good AI tutoring in educational dialogues.\nPrevious efforts towards evaluation have been limited to subjective protocols\nand benchmarks. To bridge this gap, we propose a unified evaluation taxonomy\nwith eight pedagogical dimensions based on key learning sciences principles,\nwhich is designed to assess the pedagogical value of LLM-powered AI tutor\nresponses grounded in student mistakes or confusion in the mathematical domain.\nWe release MRBench -- a new evaluation benchmark containing 192 conversations\nand 1,596 responses from seven state-of-the-art LLM-based and human tutors,\nproviding gold annotations for eight pedagogical dimensions. We assess\nreliability of the popular Prometheus2 LLM as an evaluator and analyze each\ntutor's pedagogical abilities, highlighting which LLMs are good tutors and\nwhich ones are more suitable as question-answering systems. We believe that the\npresented taxonomy, benchmark, and human-annotated labels will streamline the\nevaluation process and help track the progress in AI tutors' development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate whether current state-of-the-art large language\nmodels (LLMs) are effective as AI tutors and whether they demonstrate\npedagogical abilities necessary for good AI tutoring in educational dialogues.\nPrevious efforts towards evaluation have been limited to subjective protocols\nand benchmarks. To bridge this gap, we propose a unified evaluation taxonomy\nwith eight pedagogical dimensions based on key learning sciences principles,\nwhich is designed to assess the pedagogical value of LLM-powered AI tutor\nresponses grounded in student mistakes or confusion in the mathematical domain.\nWe release MRBench -- a new evaluation benchmark containing 192 conversations\nand 1,596 responses from seven state-of-the-art LLM-based and human tutors,\nproviding gold annotations for eight pedagogical dimensions. We assess\nreliability of the popular Prometheus2 LLM as an evaluator and analyze each\ntutor's pedagogical abilities, highlighting which LLMs are good tutors and\nwhich ones are more suitable as question-answering systems. We believe that the\npresented taxonomy, benchmark, and human-annotated labels will streamline the\nevaluation process and help track the progress in AI tutors' development."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Kseniia Petukhova"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03174v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03174v3",
                "updated": "2024-12-12T15:39:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    15,
                    39,
                    48,
                    3,
                    347,
                    0
                ],
                "published": "2024-11-05T15:22:11Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    22,
                    11,
                    1,
                    310,
                    0
                ],
                "title": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression"
                },
                "summary": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Alex Zhong"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03174v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03174v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08760v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08760v2",
                "updated": "2024-12-12T14:43:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    14,
                    43,
                    48,
                    3,
                    347,
                    0
                ],
                "published": "2024-10-11T12:19:18Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    12,
                    19,
                    18,
                    4,
                    285,
                    0
                ],
                "title": "Unlocking FedNL: Self-Contained Compute-Optimized Implementation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking FedNL: Self-Contained Compute-Optimized Implementation"
                },
                "summary": "Federated Learning (FL) is an emerging paradigm that enables intelligent\nagents to collaboratively train Machine Learning (ML) models in a distributed\nmanner, eliminating the need for sharing their local data. The recent work\n(arXiv:2106.02969) introduces a family of Federated Newton Learn (FedNL)\nalgorithms, marking a significant step towards applying second-order methods to\nFL and large-scale optimization. However, the reference FedNL prototype\nexhibits three serious practical drawbacks: (i) It requires 4.8 hours to launch\na single experiment in a sever-grade workstation; (ii) The prototype only\nsimulates multi-node setting; (iii) Prototype integration into\nresource-constrained applications is challenging. To bridge the gap between\ntheory and practice, we present a self-contained implementation of FedNL,\nFedNL-LS, FedNL-PP for single-node and multi-node settings. Our work resolves\nthe aforementioned issues and reduces the wall clock time by x1000. With this\nFedNL outperforms alternatives for training logistic regression in a\nsingle-node -- CVXPY (arXiv:1603.00943), and in a multi-node -- Apache Spark\n(arXiv:1505.06807), Ray/Scikit-Learn (arXiv:1712.05889). Finally, we propose\ntwo practical-orientated compressors for FedNL - adaptive TopLEK and\ncache-aware RandSeqK, which fulfill the theory of FedNL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) is an emerging paradigm that enables intelligent\nagents to collaboratively train Machine Learning (ML) models in a distributed\nmanner, eliminating the need for sharing their local data. The recent work\n(arXiv:2106.02969) introduces a family of Federated Newton Learn (FedNL)\nalgorithms, marking a significant step towards applying second-order methods to\nFL and large-scale optimization. However, the reference FedNL prototype\nexhibits three serious practical drawbacks: (i) It requires 4.8 hours to launch\na single experiment in a sever-grade workstation; (ii) The prototype only\nsimulates multi-node setting; (iii) Prototype integration into\nresource-constrained applications is challenging. To bridge the gap between\ntheory and practice, we present a self-contained implementation of FedNL,\nFedNL-LS, FedNL-PP for single-node and multi-node settings. Our work resolves\nthe aforementioned issues and reduces the wall clock time by x1000. With this\nFedNL outperforms alternatives for training logistic regression in a\nsingle-node -- CVXPY (arXiv:1603.00943), and in a multi-node -- Apache Spark\n(arXiv:1505.06807), Ray/Scikit-Learn (arXiv:1712.05889). Finally, we propose\ntwo practical-orientated compressors for FedNL - adaptive TopLEK and\ncache-aware RandSeqK, which fulfill the theory of FedNL."
                },
                "authors": [
                    {
                        "name": "Konstantin Burlachenko"
                    },
                    {
                        "name": "Peter Richtárik"
                    }
                ],
                "author_detail": {
                    "name": "Peter Richtárik"
                },
                "author": "Peter Richtárik",
                "arxiv_comment": "55 pages, 12 figures, 12 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08760v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08760v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.4; C.3; I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06282v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06282v3",
                "updated": "2024-12-12T12:24:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    12,
                    24,
                    18,
                    3,
                    347,
                    0
                ],
                "published": "2024-06-10T14:01:21Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    14,
                    1,
                    21,
                    0,
                    162,
                    0
                ],
                "title": "PowerInfer-2: Fast Large Language Model Inference on a Smartphone",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PowerInfer-2: Fast Large Language Model Inference on a Smartphone"
                },
                "summary": "Large language models (LLMs) on smartphones enable real-time AI assistance\nand privacy-preserving, offline operation. However, resource constraints of\nsmartphones limit current deployments to small language models (SLMs),\nsignificantly compromising their capabilities. This paper introduces\nPowerInfer-2, a smartphone-based framework that enables fast inference for LLMs\nexceeding the memory capacity. The key insight is decomposing matrix operations\ninto neuron clusters as the basic processing unit, which enables flexible\nscheduling and efficient I/O-computation pipelining. PowerInfer-2 leverages\nthis neuron-cluster-based design in both computation and storage. For\ncomputation, neuron clusters with dense activations are processed on NPU, while\nsparse clusters use CPU. The storage engine provides a fine-grained pipeline\nmechanism that coordinates cluster-level computation and I/O operations,\nenhanced by a segmented neuron cache to reduce I/O activities. PowerInfer-2\nachieves up to a 27.8x speed increase compared to state-of-the-art frameworks.\nPowerInfer-2 is the first system to serve a 47B LLM on a smartphone, achieving\n11.68 tokens/s. Notably, these performance improvements preserve model quality\nwith negligible accuracy degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) on smartphones enable real-time AI assistance\nand privacy-preserving, offline operation. However, resource constraints of\nsmartphones limit current deployments to small language models (SLMs),\nsignificantly compromising their capabilities. This paper introduces\nPowerInfer-2, a smartphone-based framework that enables fast inference for LLMs\nexceeding the memory capacity. The key insight is decomposing matrix operations\ninto neuron clusters as the basic processing unit, which enables flexible\nscheduling and efficient I/O-computation pipelining. PowerInfer-2 leverages\nthis neuron-cluster-based design in both computation and storage. For\ncomputation, neuron clusters with dense activations are processed on NPU, while\nsparse clusters use CPU. The storage engine provides a fine-grained pipeline\nmechanism that coordinates cluster-level computation and I/O operations,\nenhanced by a segmented neuron cache to reduce I/O activities. PowerInfer-2\nachieves up to a 27.8x speed increase compared to state-of-the-art frameworks.\nPowerInfer-2 is the first system to serve a 47B LLM on a smartphone, achieving\n11.68 tokens/s. Notably, these performance improvements preserve model quality\nwith negligible accuracy degradation."
                },
                "authors": [
                    {
                        "name": "Zhenliang Xue"
                    },
                    {
                        "name": "Yixin Song"
                    },
                    {
                        "name": "Zeyu Mi"
                    },
                    {
                        "name": "Xinrui Zheng"
                    },
                    {
                        "name": "Yubin Xia"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06282v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06282v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01288v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01288v3",
                "updated": "2024-12-12T12:03:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    12,
                    3,
                    19,
                    3,
                    347,
                    0
                ],
                "published": "2024-11-02T15:45:54Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    15,
                    45,
                    54,
                    5,
                    307,
                    0
                ],
                "title": "HEXA-MoE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO\n  Computation Redundancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HEXA-MoE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO\n  Computation Redundancy"
                },
                "summary": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE."
                },
                "authors": [
                    {
                        "name": "Shuqing Luo"
                    },
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Pingzhi Li"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01288v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01288v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01415v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01415v2",
                "updated": "2024-12-12T10:07:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    10,
                    7,
                    17,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-02T11:57:03Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    57,
                    3,
                    0,
                    337,
                    0
                ],
                "title": "Excitation of quasi-monochromotic waves by a high-voltage pulse in a\n  ferrite coaxial line with the periodic structure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Excitation of quasi-monochromotic waves by a high-voltage pulse in a\n  ferrite coaxial line with the periodic structure"
                },
                "summary": "Experimental data and results of numerical simulations are presented,\nconcerning excitation of narrowband gigahertz-range wave trains in coaxial\nguiding structures that are partially filled with ferromagnetic material and\nmay involve periodically arranged metal inserts. The experiments performed\nconfirm the possibility of exciting weakly damped electromagnetic waves by\nfeeding high voltage, unilateral electromagnetic pulses of short duration into\nthe line. The coax line was of outer diameter 50.5 mm, filled with an isotropic\ndielectric (relative dielectric constant {\\epsilon} = 2.25) and a set of\nferrite rings with {\\epsilon}=16 and saturated-state {\\mu} about 4 to 5. With a\npeak voltage of the primary pulse close to 160 kV and a magnetizing field of\n17.5 kA/m, the parameters of the waves excited reached magnitudes as: frequency\n1.89 GHz to 2.1 GHz; bandwidth 16%; VHF power at the output about 20 MW.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experimental data and results of numerical simulations are presented,\nconcerning excitation of narrowband gigahertz-range wave trains in coaxial\nguiding structures that are partially filled with ferromagnetic material and\nmay involve periodically arranged metal inserts. The experiments performed\nconfirm the possibility of exciting weakly damped electromagnetic waves by\nfeeding high voltage, unilateral electromagnetic pulses of short duration into\nthe line. The coax line was of outer diameter 50.5 mm, filled with an isotropic\ndielectric (relative dielectric constant {\\epsilon} = 2.25) and a set of\nferrite rings with {\\epsilon}=16 and saturated-state {\\mu} about 4 to 5. With a\npeak voltage of the primary pulse close to 160 kV and a magnetizing field of\n17.5 kA/m, the parameters of the waves excited reached magnitudes as: frequency\n1.89 GHz to 2.1 GHz; bandwidth 16%; VHF power at the output about 20 MW."
                },
                "authors": [
                    {
                        "name": "A. B. Batrakov"
                    },
                    {
                        "name": "S. Yu. Karelin"
                    },
                    {
                        "name": "O. M. Lebedenko"
                    },
                    {
                        "name": "V. S. Mukhin"
                    },
                    {
                        "name": "I. N. Onishchenko"
                    },
                    {
                        "name": "O. L. Rak"
                    },
                    {
                        "name": "V. G. Sinitsin"
                    },
                    {
                        "name": "M. V. Volovenko"
                    }
                ],
                "author_detail": {
                    "name": "M. V. Volovenko"
                },
                "author": "M. V. Volovenko",
                "arxiv_comment": "4 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01415v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01415v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.acc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.acc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09057v1",
                "updated": "2024-12-12T08:33:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    8,
                    33,
                    39,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T08:33:39Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    8,
                    33,
                    39,
                    3,
                    347,
                    0
                ],
                "title": "PhishIntel: Toward Practical Deployment of Reference-based Phishing\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhishIntel: Toward Practical Deployment of Reference-based Phishing\n  Detection"
                },
                "summary": "Phishing is a critical cyber threat, exploiting deceptive tactics to\ncompromise victims and cause significant financial losses. While\nreference-based phishing detectors (RBPDs) achieve high precision by analyzing\nbrand-domain consistency, their real-world deployment is hindered by challenges\nsuch as high latency and inefficiency in URL analysis. To address these\nlimitations, we present PhishIntel, an end-to-end phishing detection system for\nreal-world deployment. PhishIntel intelligently determines whether a URL can be\nprocessed immediately or not, segmenting the detection process into two\ndistinct tasks: a fast task that checks against local blacklists and result\ncache, and a slow task that conducts online blacklist verification, URL\ncrawling, and webpage analysis using an RBPD. This fast-slow task system\narchitecture ensures low response latency while retaining the robust detection\ncapabilities of RBPDs for zero-day phishing threats. Furthermore, we develop\ntwo downstream applications based on PhishIntel: a phishing intelligence\nplatform and a phishing email detection plugin for Microsoft Outlook,\ndemonstrating its practical efficacy and utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phishing is a critical cyber threat, exploiting deceptive tactics to\ncompromise victims and cause significant financial losses. While\nreference-based phishing detectors (RBPDs) achieve high precision by analyzing\nbrand-domain consistency, their real-world deployment is hindered by challenges\nsuch as high latency and inefficiency in URL analysis. To address these\nlimitations, we present PhishIntel, an end-to-end phishing detection system for\nreal-world deployment. PhishIntel intelligently determines whether a URL can be\nprocessed immediately or not, segmenting the detection process into two\ndistinct tasks: a fast task that checks against local blacklists and result\ncache, and a slow task that conducts online blacklist verification, URL\ncrawling, and webpage analysis using an RBPD. This fast-slow task system\narchitecture ensures low response latency while retaining the robust detection\ncapabilities of RBPDs for zero-day phishing threats. Furthermore, we develop\ntwo downstream applications based on PhishIntel: a phishing intelligence\nplatform and a phishing email detection plugin for Microsoft Outlook,\ndemonstrating its practical efficacy and utility."
                },
                "authors": [
                    {
                        "name": "Yuexin Li"
                    },
                    {
                        "name": "Hiok Kuek Tan"
                    },
                    {
                        "name": "Qiaoran Meng"
                    },
                    {
                        "name": "Mei Lin Lock"
                    },
                    {
                        "name": "Tri Cao"
                    },
                    {
                        "name": "Shumin Deng"
                    },
                    {
                        "name": "Nay Oo"
                    },
                    {
                        "name": "Hoon Wei Lim"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09036v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09036v1",
                "updated": "2024-12-12T07:52:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    7,
                    52,
                    56,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T07:52:56Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    7,
                    52,
                    56,
                    3,
                    347,
                    0
                ],
                "title": "ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based\n  on Layer Uncertainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based\n  on Layer Uncertainty"
                },
                "summary": "Large Language models (LLMs) have become a research hotspot. To accelerate\nthe inference of LLMs, storing computed caches in memory has become the\nstandard technique. However, as the inference length increases, growing KV\ncaches might lead to out-of-memory issues. Many existing methods address this\nissue through KV cache compression, primarily by preserving key tokens\nthroughout all layers to reduce information loss. Most of them allocate a\nuniform budget size for each layer to retain. However, we observe that the\nminimum budget sizes needed to retain essential information vary across layers\nand models based on the perspectives of attention and hidden state output.\nBuilding on this observation, this paper proposes a simple yet effective KV\ncache compression method that leverages layer uncertainty to allocate budget\nsize for each layer. Experimental results show that the proposed method can\nreduce memory usage of the KV caches to only $\\sim$20\\% when compared to Full\nKV inference while achieving nearly lossless performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language models (LLMs) have become a research hotspot. To accelerate\nthe inference of LLMs, storing computed caches in memory has become the\nstandard technique. However, as the inference length increases, growing KV\ncaches might lead to out-of-memory issues. Many existing methods address this\nissue through KV cache compression, primarily by preserving key tokens\nthroughout all layers to reduce information loss. Most of them allocate a\nuniform budget size for each layer to retain. However, we observe that the\nminimum budget sizes needed to retain essential information vary across layers\nand models based on the perspectives of attention and hidden state output.\nBuilding on this observation, this paper proposes a simple yet effective KV\ncache compression method that leverages layer uncertainty to allocate budget\nsize for each layer. Experimental results show that the proposed method can\nreduce memory usage of the KV caches to only $\\sim$20\\% when compared to Full\nKV inference while achieving nearly lossless performance."
                },
                "authors": [
                    {
                        "name": "Meizhi Zhong"
                    },
                    {
                        "name": "Xikai Liu"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Yikun Lei"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09036v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09036v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13853v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13853v3",
                "updated": "2024-12-12T03:21:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    3,
                    21,
                    13,
                    3,
                    347,
                    0
                ],
                "published": "2024-07-18T18:47:52Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    18,
                    47,
                    52,
                    3,
                    200,
                    0
                ],
                "title": "Forecasting GPU Performance for Deep Learning Training and Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting GPU Performance for Deep Learning Training and Inference"
                },
                "summary": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 121.4% and 30.8% to 2.3% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior work, where both GPT3 and H100 were not used\nto train the framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 121.4% and 30.8% to 2.3% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior work, where both GPT3 and H100 were not used\nto train the framework."
                },
                "authors": [
                    {
                        "name": "Seonho Lee"
                    },
                    {
                        "name": "Amar Phanishayee"
                    },
                    {
                        "name": "Divya Mahajan"
                    }
                ],
                "author_detail": {
                    "name": "Divya Mahajan"
                },
                "author": "Divya Mahajan",
                "arxiv_doi": "10.1145/3669940.3707265",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3669940.3707265",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.13853v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13853v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at the 30th ACM International Conference on Architectural\n  Support for Programming Languages and Operating Systems (ASPLOS), 2025",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08890v1",
                "updated": "2024-12-12T03:00:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    3,
                    0,
                    29,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T03:00:29Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    3,
                    0,
                    29,
                    3,
                    347,
                    0
                ],
                "title": "Lexico: Extreme KV Cache Compression via Sparse Coding over Universal\n  Dictionaries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lexico: Extreme KV Cache Compression via Sparse Coding over Universal\n  Dictionaries"
                },
                "summary": "We introduce Lexico, a novel KV cache compression method that leverages\nsparse coding with a universal dictionary. Our key finding is that key-value\ncache in modern LLMs can be accurately approximated using sparse linear\ncombination from a small, input-agnostic dictionary of ~4k atoms, enabling\nefficient compression across different input prompts, tasks and models. Using\northogonal matching pursuit for sparse approximation, Lexico achieves flexible\ncompression ratios through direct sparsity control. On GSM8K, across multiple\nmodel families (Mistral, Llama 3, Qwen2.5), Lexico maintains 90-95% of the\noriginal performance while using only 15-25% of the full KV-cache memory,\noutperforming both quantization and token eviction methods. Notably, Lexico\nremains effective in low memory regimes where 2-bit quantization fails,\nachieving up to 1.7x better compression on LongBench and GSM8K while\nmaintaining high accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Lexico, a novel KV cache compression method that leverages\nsparse coding with a universal dictionary. Our key finding is that key-value\ncache in modern LLMs can be accurately approximated using sparse linear\ncombination from a small, input-agnostic dictionary of ~4k atoms, enabling\nefficient compression across different input prompts, tasks and models. Using\northogonal matching pursuit for sparse approximation, Lexico achieves flexible\ncompression ratios through direct sparsity control. On GSM8K, across multiple\nmodel families (Mistral, Llama 3, Qwen2.5), Lexico maintains 90-95% of the\noriginal performance while using only 15-25% of the full KV-cache memory,\noutperforming both quantization and token eviction methods. Notably, Lexico\nremains effective in low memory regimes where 2-bit quantization fails,\nachieving up to 1.7x better compression on LongBench and GSM8K while\nmaintaining high accuracy."
                },
                "authors": [
                    {
                        "name": "Junhyuck Kim"
                    },
                    {
                        "name": "Jongho Park"
                    },
                    {
                        "name": "Jaewoong Cho"
                    },
                    {
                        "name": "Dimitris Papailiopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Dimitris Papailiopoulos"
                },
                "author": "Dimitris Papailiopoulos",
                "arxiv_comment": "18 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08521v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08521v1",
                "updated": "2024-12-11T16:35:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    35,
                    13,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T16:35:13Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    35,
                    13,
                    2,
                    346,
                    0
                ],
                "title": "EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache\n  Compression Based on Global-Local Importance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache\n  Compression Based on Global-Local Importance"
                },
                "summary": "As large language models (LLMs) continue to advance, the demand for higher\nquality and faster processing of long contexts across various applications is\ngrowing. KV cache is widely adopted as it stores previously generated key and\nvalue tokens, effectively reducing redundant computations during inference.\nHowever, as memory overhead becomes a significant concern, efficient\ncompression of KV cache has gained increasing attention. Most existing methods\nperform compression from two perspectives: identifying important tokens and\ndesigning compression strategies. However, these approaches often produce\nbiased distributions of important tokens due to the influence of accumulated\nattention scores or positional encoding. Furthermore, they overlook the\nsparsity and redundancy across different heads, which leads to difficulties in\npreserving the most effective information at the head level. To this end, we\npropose EMS to overcome these limitations, while achieving better KV cache\ncompression under extreme compression ratios. Specifically, we introduce a\nGlobal-Local score that combines accumulated attention scores from both global\nand local KV tokens to better identify the token importance. For the\ncompression strategy, we design an adaptive and unified Evict-then-Merge\nframework that accounts for the sparsity and redundancy of KV tokens across\ndifferent heads. Additionally, we implement the head-wise parallel compression\nthrough a zero-class mechanism to enhance efficiency. Extensive experiments\ndemonstrate our SOTA performance even under extreme compression ratios. EMS\nconsistently achieves the lowest perplexity, improves scores by over 1.28\npoints across four LLMs on LongBench under a 256 cache budget, and preserves\n95% retrieval accuracy with a cache budget less than 2% of the context length\nin the Needle-in-a-Haystack task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to advance, the demand for higher\nquality and faster processing of long contexts across various applications is\ngrowing. KV cache is widely adopted as it stores previously generated key and\nvalue tokens, effectively reducing redundant computations during inference.\nHowever, as memory overhead becomes a significant concern, efficient\ncompression of KV cache has gained increasing attention. Most existing methods\nperform compression from two perspectives: identifying important tokens and\ndesigning compression strategies. However, these approaches often produce\nbiased distributions of important tokens due to the influence of accumulated\nattention scores or positional encoding. Furthermore, they overlook the\nsparsity and redundancy across different heads, which leads to difficulties in\npreserving the most effective information at the head level. To this end, we\npropose EMS to overcome these limitations, while achieving better KV cache\ncompression under extreme compression ratios. Specifically, we introduce a\nGlobal-Local score that combines accumulated attention scores from both global\nand local KV tokens to better identify the token importance. For the\ncompression strategy, we design an adaptive and unified Evict-then-Merge\nframework that accounts for the sparsity and redundancy of KV tokens across\ndifferent heads. Additionally, we implement the head-wise parallel compression\nthrough a zero-class mechanism to enhance efficiency. Extensive experiments\ndemonstrate our SOTA performance even under extreme compression ratios. EMS\nconsistently achieves the lowest perplexity, improves scores by over 1.28\npoints across four LLMs on LongBench under a 256 cache budget, and preserves\n95% retrieval accuracy with a cache budget less than 2% of the context length\nin the Needle-in-a-Haystack task."
                },
                "authors": [
                    {
                        "name": "Yingxin Li"
                    },
                    {
                        "name": "Ye Li"
                    },
                    {
                        "name": "Yuan Meng"
                    },
                    {
                        "name": "Xinzhu Ma"
                    },
                    {
                        "name": "Zihan Geng"
                    },
                    {
                        "name": "Shutao Xia"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08521v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08521v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21324v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21324v3",
                "updated": "2024-12-11T12:03:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    12,
                    3,
                    40,
                    2,
                    346,
                    0
                ],
                "published": "2024-07-31T04:16:20Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    4,
                    16,
                    20,
                    2,
                    213,
                    0
                ],
                "title": "Pushing the Limits of In-Network Caching for Key-Value Stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pushing the Limits of In-Network Caching for Key-Value Stores"
                },
                "summary": "We present OrbitCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, OrbitCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement an OrbitCache prototype on an Intel Tofino\nswitch. Our experimental results show that OrbitCache can balance highly skewed\nworkloads and is robust to various system conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present OrbitCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, OrbitCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement an OrbitCache prototype on an Intel Tofino\nswitch. Our experimental results show that OrbitCache can balance highly skewed\nworkloads and is robust to various system conditions."
                },
                "authors": [
                    {
                        "name": "Gyuyeong Kim"
                    }
                ],
                "author_detail": {
                    "name": "Gyuyeong Kim"
                },
                "author": "Gyuyeong Kim",
                "arxiv_comment": "To be appeared in USENIX NSDI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21324v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21324v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08176v1",
                "updated": "2024-12-11T08:07:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    8,
                    7,
                    12,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T08:07:12Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    8,
                    7,
                    12,
                    2,
                    346,
                    0
                ],
                "title": "TextRefiner: Internal Visual Feature as Efficient Refiner for\n  Vision-Language Models Prompt Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TextRefiner: Internal Visual Feature as Efficient Refiner for\n  Vision-Language Models Prompt Tuning"
                },
                "summary": "Despite the efficiency of prompt learning in transferring vision-language\nmodels (VLMs) to downstream tasks, existing methods mainly learn the prompts in\na coarse-grained manner where the learned prompt vectors are shared across all\ncategories. Consequently, the tailored prompts often fail to discern\nclass-specific visual concepts, thereby hindering the transferred performance\nfor classes that share similar or complex visual attributes. Recent advances\nmitigate this challenge by leveraging external knowledge from Large Language\nModels (LLMs) to furnish class descriptions, yet incurring notable inference\ncosts. In this paper, we introduce TextRefiner, a plug-and-play method to\nrefine the text prompts of existing methods by leveraging the internal\nknowledge of VLMs. Particularly, TextRefiner builds a novel local cache module\nto encapsulate fine-grained visual concepts derivedfrom local tokens within the\nimage branch. By aggregating and aligning the cached visual descriptions with\nthe original output of the text branch, TextRefiner can efficiently refine and\nenrich the learned prompts from existing methods without relying on any\nexternal expertise. For example, it improves the performance of CoOp from 71.66\n% to 76.94 % on 11 benchmarks, surpassing CoCoOp which introduces instance-wise\nfeatures for text prompts. Equipped with TextRefiner, PromptKD achieves\nstate-of-the-art performance and is efficient in inference. Our code is relesed\nat https://github.com/xjjxmu/TextRefiner",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the efficiency of prompt learning in transferring vision-language\nmodels (VLMs) to downstream tasks, existing methods mainly learn the prompts in\na coarse-grained manner where the learned prompt vectors are shared across all\ncategories. Consequently, the tailored prompts often fail to discern\nclass-specific visual concepts, thereby hindering the transferred performance\nfor classes that share similar or complex visual attributes. Recent advances\nmitigate this challenge by leveraging external knowledge from Large Language\nModels (LLMs) to furnish class descriptions, yet incurring notable inference\ncosts. In this paper, we introduce TextRefiner, a plug-and-play method to\nrefine the text prompts of existing methods by leveraging the internal\nknowledge of VLMs. Particularly, TextRefiner builds a novel local cache module\nto encapsulate fine-grained visual concepts derivedfrom local tokens within the\nimage branch. By aggregating and aligning the cached visual descriptions with\nthe original output of the text branch, TextRefiner can efficiently refine and\nenrich the learned prompts from existing methods without relying on any\nexternal expertise. For example, it improves the performance of CoOp from 71.66\n% to 76.94 % on 11 benchmarks, surpassing CoCoOp which introduces instance-wise\nfeatures for text prompts. Equipped with TextRefiner, PromptKD achieves\nstate-of-the-art performance and is efficient in inference. Our code is relesed\nat https://github.com/xjjxmu/TextRefiner"
                },
                "authors": [
                    {
                        "name": "Jingjing Xie"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Jun Peng"
                    },
                    {
                        "name": "Zhaohong Huang"
                    },
                    {
                        "name": "Liujuan Cao"
                    }
                ],
                "author_detail": {
                    "name": "Liujuan Cao"
                },
                "author": "Liujuan Cao",
                "arxiv_comment": "Accepted by AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08063v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08063v1",
                "updated": "2024-12-11T03:15:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    3,
                    15,
                    49,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T03:15:49Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    3,
                    15,
                    49,
                    2,
                    346,
                    0
                ],
                "title": "ContextModule: Improving Code Completion via Repository-level Contextual\n  Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ContextModule: Improving Code Completion via Repository-level Contextual\n  Information"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\ncode completion tasks, where they assist developers by predicting and\ngenerating new code in real-time. However, existing LLM-based code completion\nsystems primarily rely on the immediate context of the file being edited, often\nmissing valuable repository-level information, user behaviour and edit history\nthat could improve suggestion accuracy. Additionally, challenges such as\nefficiently retrieving relevant code snippets from large repositories,\nincorporating user behavior, and balancing accuracy with low-latency\nrequirements in production environments remain unresolved. In this paper, we\npropose ContextModule, a framework designed to enhance LLM-based code\ncompletion by retrieving and integrating three types of contextual information\nfrom the repository: user behavior-based code, similar code snippets, and\ncritical symbol definitions. By capturing user interactions across files and\nleveraging repository-wide static analysis, ContextModule improves the\nrelevance and precision of generated code. We implement performance\noptimizations, such as index caching, to ensure the system meets the latency\nconstraints of real-world coding environments. Experimental results and\nindustrial practise demonstrate that ContextModule significantly improves code\ncompletion accuracy and user acceptance rates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities in\ncode completion tasks, where they assist developers by predicting and\ngenerating new code in real-time. However, existing LLM-based code completion\nsystems primarily rely on the immediate context of the file being edited, often\nmissing valuable repository-level information, user behaviour and edit history\nthat could improve suggestion accuracy. Additionally, challenges such as\nefficiently retrieving relevant code snippets from large repositories,\nincorporating user behavior, and balancing accuracy with low-latency\nrequirements in production environments remain unresolved. In this paper, we\npropose ContextModule, a framework designed to enhance LLM-based code\ncompletion by retrieving and integrating three types of contextual information\nfrom the repository: user behavior-based code, similar code snippets, and\ncritical symbol definitions. By capturing user interactions across files and\nleveraging repository-wide static analysis, ContextModule improves the\nrelevance and precision of generated code. We implement performance\noptimizations, such as index caching, to ensure the system meets the latency\nconstraints of real-world coding environments. Experimental results and\nindustrial practise demonstrate that ContextModule significantly improves code\ncompletion accuracy and user acceptance rates."
                },
                "authors": [
                    {
                        "name": "Zhanming Guan"
                    },
                    {
                        "name": "Junlin Liu"
                    },
                    {
                        "name": "Jierui Liu"
                    },
                    {
                        "name": "Chao Peng"
                    },
                    {
                        "name": "Dexin Liu"
                    },
                    {
                        "name": "Ningyuan Sun"
                    },
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Wenchao Li"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Hang Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Hang Zhu"
                },
                "author": "Hang Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08063v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08063v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.12952v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.12952v2",
                "updated": "2024-12-10T22:53:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    22,
                    53,
                    16,
                    1,
                    345,
                    0
                ],
                "published": "2024-03-19T17:54:34Z",
                "published_parsed": [
                    2024,
                    3,
                    19,
                    17,
                    54,
                    34,
                    1,
                    79,
                    0
                ],
                "title": "Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization\n  with Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization\n  with Vision-Language Models"
                },
                "summary": "Advancements in vision-language models (VLMs) have propelled the field of\ncomputer vision, particularly in the zero-shot learning setting. Despite their\npromise, the effectiveness of these models often diminishes due to domain\nshifts in test environments. To address this, we introduce the Test-Time\nPrototype Shifting (TPS) framework, a pioneering approach designed to adapt\nVLMs to test datasets using unlabeled test inputs. Our method is based on the\nnotion of modulating per-class prototypes in the shared embedding space. By\npre-computing and caching prototypes generated with the pre-trained text\nencoder, TPS not only facilitates optimization-free prototype reuse for\nsubsequent predictions but also enables seamless integration with current\nadvancements in prompt engineering. At test-time, TPS dynamically learns shift\nvectors for each prototype based solely on the given test sample, effectively\nbridging the domain gap and enhancing classification accuracy. A notable aspect\nof our framework is its significantly reduced memory and computational demands\nwhen compared to conventional text-prompt tuning methods. Extensive evaluations\nacross 15 image classification datasets involving natural distribution shifts\nand cross-dataset generalization, as well as in context-dependent visual\nreasoning, demonstrate TPS's superior performance, achieving state-of-the-art\nresults while reducing resource requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in vision-language models (VLMs) have propelled the field of\ncomputer vision, particularly in the zero-shot learning setting. Despite their\npromise, the effectiveness of these models often diminishes due to domain\nshifts in test environments. To address this, we introduce the Test-Time\nPrototype Shifting (TPS) framework, a pioneering approach designed to adapt\nVLMs to test datasets using unlabeled test inputs. Our method is based on the\nnotion of modulating per-class prototypes in the shared embedding space. By\npre-computing and caching prototypes generated with the pre-trained text\nencoder, TPS not only facilitates optimization-free prototype reuse for\nsubsequent predictions but also enables seamless integration with current\nadvancements in prompt engineering. At test-time, TPS dynamically learns shift\nvectors for each prototype based solely on the given test sample, effectively\nbridging the domain gap and enhancing classification accuracy. A notable aspect\nof our framework is its significantly reduced memory and computational demands\nwhen compared to conventional text-prompt tuning methods. Extensive evaluations\nacross 15 image classification datasets involving natural distribution shifts\nand cross-dataset generalization, as well as in context-dependent visual\nreasoning, demonstrate TPS's superior performance, achieving state-of-the-art\nresults while reducing resource requirements."
                },
                "authors": [
                    {
                        "name": "Elaine Sui"
                    },
                    {
                        "name": "Xiaohan Wang"
                    },
                    {
                        "name": "Serena Yeung-Levy"
                    }
                ],
                "author_detail": {
                    "name": "Serena Yeung-Levy"
                },
                "author": "Serena Yeung-Levy",
                "arxiv_comment": "Accepted at WACV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.12952v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.12952v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07772v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07772v1",
                "updated": "2024-12-10T18:59:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    50,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T18:59:50Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    50,
                    1,
                    345,
                    0
                ],
                "title": "From Slow Bidirectional to Fast Causal Video Generators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Slow Bidirectional to Fast Causal Video Generators"
                },
                "summary": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to a causal\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nsupports fast streaming generation of high quality videos at 9.4 FPS on a\nsingle GPU thanks to KV caching. Our approach also enables streaming\nvideo-to-video translation, image-to-video, and dynamic prompting in a\nzero-shot manner. We will release the code based on an open-source model in the\nfuture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to a causal\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nsupports fast streaming generation of high quality videos at 9.4 FPS on a\nsingle GPU thanks to KV caching. Our approach also enables streaming\nvideo-to-video translation, image-to-video, and dynamic prompting in a\nzero-shot manner. We will release the code based on an open-source model in the\nfuture."
                },
                "authors": [
                    {
                        "name": "Tianwei Yin"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Richard Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Fredo Durand"
                    },
                    {
                        "name": "Eli Shechtman"
                    },
                    {
                        "name": "Xun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xun Huang"
                },
                "author": "Xun Huang",
                "arxiv_comment": "Project Page: https://causvid.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07772v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07772v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07752v1",
                "updated": "2024-12-10T18:50:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    50,
                    37,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T18:50:37Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    50,
                    37,
                    1,
                    345,
                    0
                ],
                "title": "FlashRNN: Optimizing Traditional RNNs on Modern Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashRNN: Optimizing Traditional RNNs on Modern Hardware"
                },
                "summary": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: \\url{https://github.com/NX-AI/flashrnn}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: \\url{https://github.com/NX-AI/flashrnn}"
                },
                "authors": [
                    {
                        "name": "Korbinian Pöppel"
                    },
                    {
                        "name": "Maximilian Beck"
                    },
                    {
                        "name": "Sepp Hochreiter"
                    }
                ],
                "author_detail": {
                    "name": "Sepp Hochreiter"
                },
                "author": "Sepp Hochreiter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07720v1",
                "updated": "2024-12-10T18:13:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    13,
                    20,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T18:13:20Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    13,
                    20,
                    1,
                    345,
                    0
                ],
                "title": "ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion\n  Transformer"
                },
                "summary": "The recent surge of interest in comprehensive multimodal models has\nnecessitated the unification of diverse modalities. However, the unification\nsuffers from disparate methodologies. Continuous visual generation necessitates\nthe full-sequence diffusion-based approach, despite its divergence from the\nautoregressive modeling in the text domain. We posit that autoregressive\nmodeling, i.e., predicting the future based on past deterministic experience,\nremains crucial in developing both a visual generation model and a potential\nunified multimodal model. In this paper, we explore an interpolation between\nthe autoregressive modeling and full-parameters diffusion to model visual\ninformation. At its core, we present ACDiT, an Autoregressive blockwise\nConditional Diffusion Transformer, where the block size of diffusion, i.e., the\nsize of autoregressive units, can be flexibly adjusted to interpolate between\ntoken-wise autoregression and full-sequence diffusion. ACDiT is easy to\nimplement, as simple as creating a Skip-Causal Attention Mask (SCAM) during\ntraining. During inference, the process iterates between diffusion denoising\nand autoregressive decoding that can make full use of KV-Cache. We verify the\neffectiveness of ACDiT on image and video generation tasks. We also demonstrate\nthat benefitted from autoregressive modeling, ACDiT can be seamlessly used in\nvisual understanding tasks despite being trained on the diffusion objective.\nThe analysis of the trade-off between autoregressive modeling and diffusion\ndemonstrates the potential of ACDiT to be used in long-horizon visual\ngeneration tasks. These strengths make it promising as the backbone of future\nunified models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent surge of interest in comprehensive multimodal models has\nnecessitated the unification of diverse modalities. However, the unification\nsuffers from disparate methodologies. Continuous visual generation necessitates\nthe full-sequence diffusion-based approach, despite its divergence from the\nautoregressive modeling in the text domain. We posit that autoregressive\nmodeling, i.e., predicting the future based on past deterministic experience,\nremains crucial in developing both a visual generation model and a potential\nunified multimodal model. In this paper, we explore an interpolation between\nthe autoregressive modeling and full-parameters diffusion to model visual\ninformation. At its core, we present ACDiT, an Autoregressive blockwise\nConditional Diffusion Transformer, where the block size of diffusion, i.e., the\nsize of autoregressive units, can be flexibly adjusted to interpolate between\ntoken-wise autoregression and full-sequence diffusion. ACDiT is easy to\nimplement, as simple as creating a Skip-Causal Attention Mask (SCAM) during\ntraining. During inference, the process iterates between diffusion denoising\nand autoregressive decoding that can make full use of KV-Cache. We verify the\neffectiveness of ACDiT on image and video generation tasks. We also demonstrate\nthat benefitted from autoregressive modeling, ACDiT can be seamlessly used in\nvisual understanding tasks despite being trained on the diffusion objective.\nThe analysis of the trade-off between autoregressive modeling and diffusion\ndemonstrates the potential of ACDiT to be used in long-horizon visual\ngeneration tasks. These strengths make it promising as the backbone of future\nunified models."
                },
                "authors": [
                    {
                        "name": "Jinyi Hu"
                    },
                    {
                        "name": "Shengding Hu"
                    },
                    {
                        "name": "Yuxuan Song"
                    },
                    {
                        "name": "Yufei Huang"
                    },
                    {
                        "name": "Mingxuan Wang"
                    },
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Wei-Ying Ma"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14485v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14485v4",
                "updated": "2024-12-10T12:45:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    12,
                    45,
                    31,
                    1,
                    345,
                    0
                ],
                "published": "2024-09-22T15:13:31Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    15,
                    13,
                    31,
                    6,
                    266,
                    0
                ],
                "title": "Video-XL: Extra-Long Vision Language Model for Hour-Scale Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-XL: Extra-Long Vision Language Model for Hour-Scale Video\n  Understanding"
                },
                "summary": "Long video understanding poses a significant challenge for current\nMulti-modal Large Language Models (MLLMs). Notably, the MLLMs are constrained\nby their limited context lengths and the substantial costs while processing\nlong videos. Although several existing methods attempt to reduce visual tokens,\ntheir strategies encounter severe bottleneck, restricting MLLMs' ability to\nperceive fine-grained visual details. In this work, we propose Video-XL, a\nnovel approach that leverages MLLMs' inherent key-value (KV) sparsification\ncapacity to condense the visual input. Specifically, we introduce a new special\ntoken, the Visual Summarization Token (VST), for each interval of the video,\nwhich summarizes the visual information within the interval as its associated\nKV. The VST module is trained by instruction fine-tuning, where two optimizing\nstrategies are offered. 1.Curriculum learning, where VST learns to make small\n(easy) and large compression (hard) progressively. 2. Composite data curation,\nwhich integrates single-image, multi-image, and synthetic data to overcome the\nscarcity of long-video instruction data. The compression quality is further\nimproved by dynamic compression, which customizes compression granularity based\non the information density of different video intervals. Video-XL's\neffectiveness is verified from three aspects. First, it achieves a superior\nlong-video understanding capability, outperforming state-of-the-art models of\ncomparable sizes across multiple popular benchmarks. Second, it effectively\npreserves video information, with minimal compression loss even at 16x\ncompression ratio. Third, it realizes outstanding cost-effectiveness, enabling\nhigh-quality processing of thousands of frames on a single A100 GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long video understanding poses a significant challenge for current\nMulti-modal Large Language Models (MLLMs). Notably, the MLLMs are constrained\nby their limited context lengths and the substantial costs while processing\nlong videos. Although several existing methods attempt to reduce visual tokens,\ntheir strategies encounter severe bottleneck, restricting MLLMs' ability to\nperceive fine-grained visual details. In this work, we propose Video-XL, a\nnovel approach that leverages MLLMs' inherent key-value (KV) sparsification\ncapacity to condense the visual input. Specifically, we introduce a new special\ntoken, the Visual Summarization Token (VST), for each interval of the video,\nwhich summarizes the visual information within the interval as its associated\nKV. The VST module is trained by instruction fine-tuning, where two optimizing\nstrategies are offered. 1.Curriculum learning, where VST learns to make small\n(easy) and large compression (hard) progressively. 2. Composite data curation,\nwhich integrates single-image, multi-image, and synthetic data to overcome the\nscarcity of long-video instruction data. The compression quality is further\nimproved by dynamic compression, which customizes compression granularity based\non the information density of different video intervals. Video-XL's\neffectiveness is verified from three aspects. First, it achieves a superior\nlong-video understanding capability, outperforming state-of-the-art models of\ncomparable sizes across multiple popular benchmarks. Second, it effectively\npreserves video information, with minimal compression loss even at 16x\ncompression ratio. Third, it realizes outstanding cost-effectiveness, enabling\nhigh-quality processing of thousands of frames on a single A100 GPU."
                },
                "authors": [
                    {
                        "name": "Yan Shu"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Minghao Qin"
                    },
                    {
                        "name": "Junjie Zhou"
                    },
                    {
                        "name": "Zhengyang Liang"
                    },
                    {
                        "name": "Tiejun Huang"
                    },
                    {
                        "name": "Bo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zhao"
                },
                "author": "Bo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14485v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14485v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05276v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05276v3",
                "updated": "2024-12-09T01:44:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    1,
                    44,
                    10,
                    0,
                    344,
                    0
                ],
                "published": "2024-11-08T02:21:19Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    2,
                    21,
                    19,
                    4,
                    313,
                    0
                ],
                "title": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching"
                },
                "summary": "Large Language Models (LLMs), such as GPT, have revolutionized artificial\nintelligence by enabling nuanced understanding and generation of human-like\ntext across a wide range of applications. However, the high computational and\nfinancial costs associated with frequent API calls to these models present a\nsubstantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique achieves a notable reduction in operational costs while\nsignificantly enhancing response times, making it a robust solution for\noptimizing LLM-powered applications. Our experiments demonstrate that GPT\nSemantic Cache reduces API calls by up to 68.8% across various query\ncategories, with cache hit rates ranging from 61.6% to 68.8%. Additionally, the\nsystem achieves high accuracy, with positive hit rates exceeding 97%,\nconfirming the reliability of cached responses. This technique not only reduces\noperational costs, but also improves response times, enhancing the efficiency\nof LLM-powered applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), such as GPT, have revolutionized artificial\nintelligence by enabling nuanced understanding and generation of human-like\ntext across a wide range of applications. However, the high computational and\nfinancial costs associated with frequent API calls to these models present a\nsubstantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique achieves a notable reduction in operational costs while\nsignificantly enhancing response times, making it a robust solution for\noptimizing LLM-powered applications. Our experiments demonstrate that GPT\nSemantic Cache reduces API calls by up to 68.8% across various query\ncategories, with cache hit rates ranging from 61.6% to 68.8%. Additionally, the\nsystem achieves high accuracy, with positive hit rates exceeding 97%,\nconfirming the reliability of cached responses. This technique not only reduces\noperational costs, but also improves response times, enhancing the efficiency\nof LLM-powered applications."
                },
                "authors": [
                    {
                        "name": "Sajal Regmi"
                    },
                    {
                        "name": "Chetan Phakami Pun"
                    }
                ],
                "author_detail": {
                    "name": "Chetan Phakami Pun"
                },
                "author": "Chetan Phakami Pun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05276v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05276v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.01844v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.01844v3",
                "updated": "2024-12-09T01:39:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    1,
                    39,
                    15,
                    0,
                    344,
                    0
                ],
                "published": "2024-05-03T04:27:32Z",
                "published_parsed": [
                    2024,
                    5,
                    3,
                    4,
                    27,
                    32,
                    4,
                    124,
                    0
                ],
                "title": "A Survey on Privacy-Preserving Caching at Network Edge: Classification,\n  Solutions, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Privacy-Preserving Caching at Network Edge: Classification,\n  Solutions, and Challenges"
                },
                "summary": "Caching content at the edge network is a popular and effective technique\nwidely deployed to alleviate the burden of network backhaul, shorten service\ndelay and improve service quality. However, there has been some controversy\nover privacy violations in caching content at the edge network. On the one\nhand, the multi-access open edge network provides an ideal entrance or\ninterface for external attackers to obtain private data from edge caches by\nextracting sensitive information. On the other hand, privacy can be infringed\non by curious edge caching providers through caching trace analysis targeting\nthe achievement of better caching performance or higher profits. Therefore, an\nin-depth understanding of privacy issues in edge caching networks is vital and\nindispensable for creating a privacy-preserving caching service at the edge\nnetwork. In this article, we are among the first to fill this gap by examining\nprivacy-preserving techniques for caching content at the edge network. Firstly,\nwe provide an introduction to the background of privacy-preserving edge caching\n(PPEC). Next, we summarize the key privacy issues and present a taxonomy for\ncaching at the edge network from the perspective of private information.\nAdditionally, we conduct a retrospective review of the state-of-the-art\ncountermeasures against privacy leakage from content caching at the edge\nnetwork. Finally, we conclude the survey and envision challenges for future\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching content at the edge network is a popular and effective technique\nwidely deployed to alleviate the burden of network backhaul, shorten service\ndelay and improve service quality. However, there has been some controversy\nover privacy violations in caching content at the edge network. On the one\nhand, the multi-access open edge network provides an ideal entrance or\ninterface for external attackers to obtain private data from edge caches by\nextracting sensitive information. On the other hand, privacy can be infringed\non by curious edge caching providers through caching trace analysis targeting\nthe achievement of better caching performance or higher profits. Therefore, an\nin-depth understanding of privacy issues in edge caching networks is vital and\nindispensable for creating a privacy-preserving caching service at the edge\nnetwork. In this article, we are among the first to fill this gap by examining\nprivacy-preserving techniques for caching content at the edge network. Firstly,\nwe provide an introduction to the background of privacy-preserving edge caching\n(PPEC). Next, we summarize the key privacy issues and present a taxonomy for\ncaching at the edge network from the perspective of private information.\nAdditionally, we conduct a retrospective review of the state-of-the-art\ncountermeasures against privacy leakage from content caching at the edge\nnetwork. Finally, we conclude the survey and envision challenges for future\nresearch."
                },
                "authors": [
                    {
                        "name": "Xianzhi Zhang"
                    },
                    {
                        "name": "Yipeng Zhou"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Quan Z. Sheng"
                    },
                    {
                        "name": "Shazia Riaz"
                    },
                    {
                        "name": "Miao Hu"
                    },
                    {
                        "name": "Linchang Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Linchang Xiao"
                },
                "author": "Linchang Xiao",
                "arxiv_doi": "10.1145/3706630",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706630",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.01844v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.01844v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05896v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05896v1",
                "updated": "2024-12-08T11:32:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    8,
                    11,
                    32,
                    8,
                    6,
                    343,
                    0
                ],
                "published": "2024-12-08T11:32:08Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    11,
                    32,
                    8,
                    6,
                    343,
                    0
                ],
                "title": "XKV: Personalized KV Cache Memory Reduction for Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XKV: Personalized KV Cache Memory Reduction for Long-Context LLM\n  Inference"
                },
                "summary": "Recently the generative Large Language Model (LLM) has achieved remarkable\nsuccess in numerous applications. Notably its inference generates output tokens\none-by-one, leading to many redundant computations. The widely-used KV-Cache\nframework makes a compromise between time and space complexities. However,\ncaching data generates the increasingly growing memory demand, that can quickly\nexhaust the limited memory capacity of the modern accelerator like GPUs,\nparticularly in long-context inference tasks. Existing studies reduce memory\nconsumption by evicting some of cached data that have less important impact on\ninference accuracy. But the benefit in practice is far from ideal due to the\nstatic cache allocation across different LLM network layers. This paper\nobserves that the layer-specific cached data have very different impacts on\naccuracy. We quantify this difference, and give experimental and theoretical\nvalidation. We accordingly make a formal analysis and shows that customizing\nthe cache size for each layer in a personalized manner can yield a significant\nmemory reduction, while still providing comparable accuracy. We simulate the\ncache allocation as a combinatorial optimization problem and give a global\noptimal solution. In particular, we devise a mini- and sampling-based inference\nover a lightweight variant of the LLM model, so as to quickly capture the\ndifference and then feed it into the personalized algorithms. Extensive\nexperiments on real-world datasets demonstrate that our proposals can reduce KV\ncache memory consumption by 61.6% on average, improve computational efficiency\nby 2.1x and then increase the throughput by up to 5.5x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently the generative Large Language Model (LLM) has achieved remarkable\nsuccess in numerous applications. Notably its inference generates output tokens\none-by-one, leading to many redundant computations. The widely-used KV-Cache\nframework makes a compromise between time and space complexities. However,\ncaching data generates the increasingly growing memory demand, that can quickly\nexhaust the limited memory capacity of the modern accelerator like GPUs,\nparticularly in long-context inference tasks. Existing studies reduce memory\nconsumption by evicting some of cached data that have less important impact on\ninference accuracy. But the benefit in practice is far from ideal due to the\nstatic cache allocation across different LLM network layers. This paper\nobserves that the layer-specific cached data have very different impacts on\naccuracy. We quantify this difference, and give experimental and theoretical\nvalidation. We accordingly make a formal analysis and shows that customizing\nthe cache size for each layer in a personalized manner can yield a significant\nmemory reduction, while still providing comparable accuracy. We simulate the\ncache allocation as a combinatorial optimization problem and give a global\noptimal solution. In particular, we devise a mini- and sampling-based inference\nover a lightweight variant of the LLM model, so as to quickly capture the\ndifference and then feed it into the personalized algorithms. Extensive\nexperiments on real-world datasets demonstrate that our proposals can reduce KV\ncache memory consumption by 61.6% on average, improve computational efficiency\nby 2.1x and then increase the throughput by up to 5.5x."
                },
                "authors": [
                    {
                        "name": "Weizhuo Li"
                    },
                    {
                        "name": "Zhigang Wang"
                    },
                    {
                        "name": "Yu Gu"
                    },
                    {
                        "name": "Ge Yu"
                    }
                ],
                "author_detail": {
                    "name": "Ge Yu"
                },
                "author": "Ge Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05896v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05896v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05704v1",
                "updated": "2024-12-07T17:22:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    17,
                    22,
                    14,
                    5,
                    342,
                    0
                ],
                "published": "2024-12-07T17:22:14Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    17,
                    22,
                    14,
                    5,
                    342,
                    0
                ],
                "title": "Ultrafast lattice and electron dynamics induced in a PbSe crystal by an\n  intense terahertz pulse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultrafast lattice and electron dynamics induced in a PbSe crystal by an\n  intense terahertz pulse"
                },
                "summary": "We have studied the ultrafast optical response of a PbSe crystal to an\nintense picosecond terahertz pulse with a peak electric field strength of up to\n$\\sim$ 500 kV/cm. The reflectivity anisotropy signal contains oscillations at\nthe fundamental frequency of the resonant infrared-active phonon mode as well\nas its second, third, and fourth harmonics. The effect is ascribed to coherent\nanharmonic phonons resonantly excited by the strong terahertz field. Pump\nterahertz pulses also induce an almost instantaneous Kerr effect and a\nlong-lived optical anisotropy of the crystal with a characteristic decay time\nof $\\gtrsim$ 100 ps. We consider lattice distortion and phonon-assisted side\nvalley population as possible origins of this metastable state.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We have studied the ultrafast optical response of a PbSe crystal to an\nintense picosecond terahertz pulse with a peak electric field strength of up to\n$\\sim$ 500 kV/cm. The reflectivity anisotropy signal contains oscillations at\nthe fundamental frequency of the resonant infrared-active phonon mode as well\nas its second, third, and fourth harmonics. The effect is ascribed to coherent\nanharmonic phonons resonantly excited by the strong terahertz field. Pump\nterahertz pulses also induce an almost instantaneous Kerr effect and a\nlong-lived optical anisotropy of the crystal with a characteristic decay time\nof $\\gtrsim$ 100 ps. We consider lattice distortion and phonon-assisted side\nvalley population as possible origins of this metastable state."
                },
                "authors": [
                    {
                        "name": "A. A. Melnikov"
                    },
                    {
                        "name": "Yu. G. Selivanov"
                    },
                    {
                        "name": "D. G. Poydashev"
                    },
                    {
                        "name": "S. V. Chekalin"
                    }
                ],
                "author_detail": {
                    "name": "S. V. Chekalin"
                },
                "author": "S. V. Chekalin",
                "arxiv_comment": "7 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05693v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05693v1",
                "updated": "2024-12-07T16:41:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    16,
                    41,
                    54,
                    5,
                    342,
                    0
                ],
                "published": "2024-12-07T16:41:54Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    16,
                    41,
                    54,
                    5,
                    342,
                    0
                ],
                "title": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression"
                },
                "summary": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy."
                },
                "authors": [
                    {
                        "name": "Michael R. Metel"
                    },
                    {
                        "name": "Boxing Chen"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    }
                ],
                "author_detail": {
                    "name": "Mehdi Rezagholizadeh"
                },
                "author": "Mehdi Rezagholizadeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05693v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05693v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06567v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06567v2",
                "updated": "2024-12-07T13:23:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    13,
                    23,
                    39,
                    5,
                    342,
                    0
                ],
                "published": "2024-06-03T13:28:43Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    13,
                    28,
                    43,
                    0,
                    155,
                    0
                ],
                "title": "DHA: Learning Decoupled-Head Attention from Transformer Checkpoints via\n  Adaptive Heads Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DHA: Learning Decoupled-Head Attention from Transformer Checkpoints via\n  Adaptive Heads Fusion"
                },
                "summary": "Large language models (LLMs) with billions of parameters demonstrate\nimpressive performance. However, the widely used Multi-Head Attention (MHA) in\nLLMs incurs substantial computational and memory costs during inference. While\nsome efforts have optimized attention mechanisms by pruning heads or sharing\nparameters among heads, these methods often lead to performance degradation or\nnecessitate substantial continued pre-training costs to restore performance.\nBased on the analysis of attention redundancy, we design a Decoupled-Head\nAttention (DHA) mechanism. DHA adaptively configures group sharing for key\nheads and value heads across various layers, achieving a better balance between\nperformance and efficiency. Inspired by the observation of clustering similar\nheads, we propose to progressively transform the MHA checkpoint into the DHA\nmodel through linear fusion of similar head parameters step by step, retaining\nthe parametric knowledge of the MHA checkpoint. We construct DHA models by\ntransforming various scales of MHA checkpoints given target head budgets. Our\nexperiments show that DHA remarkably requires a mere 0.25\\% of the original\nmodel's pre-training budgets to achieve 97.6\\% of performance while saving 75\\%\nof KV cache. Compared to Group-Query Attention (GQA), DHA achieves a 5$\\times$\ntraining acceleration, a maximum of 13.93\\% performance improvement under\n0.01\\% pre-training budget, and 4\\% relative improvement under 0.05\\%\npre-training budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with billions of parameters demonstrate\nimpressive performance. However, the widely used Multi-Head Attention (MHA) in\nLLMs incurs substantial computational and memory costs during inference. While\nsome efforts have optimized attention mechanisms by pruning heads or sharing\nparameters among heads, these methods often lead to performance degradation or\nnecessitate substantial continued pre-training costs to restore performance.\nBased on the analysis of attention redundancy, we design a Decoupled-Head\nAttention (DHA) mechanism. DHA adaptively configures group sharing for key\nheads and value heads across various layers, achieving a better balance between\nperformance and efficiency. Inspired by the observation of clustering similar\nheads, we propose to progressively transform the MHA checkpoint into the DHA\nmodel through linear fusion of similar head parameters step by step, retaining\nthe parametric knowledge of the MHA checkpoint. We construct DHA models by\ntransforming various scales of MHA checkpoints given target head budgets. Our\nexperiments show that DHA remarkably requires a mere 0.25\\% of the original\nmodel's pre-training budgets to achieve 97.6\\% of performance while saving 75\\%\nof KV cache. Compared to Group-Query Attention (GQA), DHA achieves a 5$\\times$\ntraining acceleration, a maximum of 13.93\\% performance improvement under\n0.01\\% pre-training budget, and 4\\% relative improvement under 0.05\\%\npre-training budget."
                },
                "authors": [
                    {
                        "name": "Yilong Chen"
                    },
                    {
                        "name": "Linhao Zhang"
                    },
                    {
                        "name": "Junyuan Shang"
                    },
                    {
                        "name": "Zhenyu Zhang"
                    },
                    {
                        "name": "Tingwen Liu"
                    },
                    {
                        "name": "Shuohuan Wang"
                    },
                    {
                        "name": "Yu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yu Sun"
                },
                "author": "Yu Sun",
                "arxiv_comment": "Accepted at NeurIPS 2024 10 pages, 9 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06567v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06567v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03409v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03409v2",
                "updated": "2024-12-07T13:23:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    13,
                    23,
                    39,
                    5,
                    342,
                    0
                ],
                "published": "2024-12-04T15:48:59Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    15,
                    48,
                    59,
                    2,
                    339,
                    0
                ],
                "title": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation"
                },
                "summary": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at \\url{https://github.com/THU-MIG/PrefixKV}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at \\url{https://github.com/THU-MIG/PrefixKV}."
                },
                "authors": [
                    {
                        "name": "Ao Wang"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Jianchao Tan"
                    },
                    {
                        "name": "Kefeng Zhang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "12 pages, 5 figures;",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03409v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03409v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v2",
                "updated": "2024-12-07T04:08:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    4,
                    8,
                    56,
                    5,
                    342,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05392v1",
                "updated": "2024-12-06T19:35:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    19,
                    35,
                    52,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T19:35:52Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    19,
                    35,
                    52,
                    4,
                    341,
                    0
                ],
                "title": "Effect of electric field on excitons in wide quantum wells",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effect of electric field on excitons in wide quantum wells"
                },
                "summary": "A microscopic model of a heterostructure with a quantum well (QW) is proposed\nto study the exciton behavior in an external electric field. The effect of an\nelectric field ranging from 0 to 6 kV/cm applied to the GaAs/AlGaAs QW\nstructure in the growth direction is studied for several QWs of various widths\nup to 100 nm. The three-dimensional Schr\\\"odinger equation (SE) of exciton is\nnumerically solved using the finite difference method. Wave functions and\nenergies for several states of the heavy-hole and light-hole excitons are\ncalculated. Dependencies of the exciton state energy, the binding energy, the\nradiative broadening, and the static dipole moment on the applied electric\nfields are determined. The threshold of exciton dissociation for the 100-nm QW\nis also determined. In addition, we found the electric-field-induced shift of\nthe center of mass of the heavy-hole and light-hole exciton in the QWs.\nFinally, we have modeled reflection spectra of heterostructures with the\nGaAs/AlGaAs QWs in the electric field using the calculated energies and\nradiative broadenings of excitons.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A microscopic model of a heterostructure with a quantum well (QW) is proposed\nto study the exciton behavior in an external electric field. The effect of an\nelectric field ranging from 0 to 6 kV/cm applied to the GaAs/AlGaAs QW\nstructure in the growth direction is studied for several QWs of various widths\nup to 100 nm. The three-dimensional Schr\\\"odinger equation (SE) of exciton is\nnumerically solved using the finite difference method. Wave functions and\nenergies for several states of the heavy-hole and light-hole excitons are\ncalculated. Dependencies of the exciton state energy, the binding energy, the\nradiative broadening, and the static dipole moment on the applied electric\nfields are determined. The threshold of exciton dissociation for the 100-nm QW\nis also determined. In addition, we found the electric-field-induced shift of\nthe center of mass of the heavy-hole and light-hole exciton in the QWs.\nFinally, we have modeled reflection spectra of heterostructures with the\nGaAs/AlGaAs QWs in the electric field using the calculated energies and\nradiative broadenings of excitons."
                },
                "authors": [
                    {
                        "name": "Shiming Zheng"
                    },
                    {
                        "name": "E. S. Khramtsov"
                    },
                    {
                        "name": "I. V. Ignatiev"
                    }
                ],
                "author_detail": {
                    "name": "I. V. Ignatiev"
                },
                "author": "I. V. Ignatiev",
                "arxiv_comment": "12 pages, 8 figures, to be published in Physical Review B",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05228v1",
                "updated": "2024-12-06T17:58:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    58,
                    57,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T17:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    58,
                    57,
                    4,
                    341,
                    0
                ],
                "title": "MC3: Memory Contention based Covert Channel Communication on Shared DRAM\n  System-on-Chips",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MC3: Memory Contention based Covert Channel Communication on Shared DRAM\n  System-on-Chips"
                },
                "summary": "Shared-memory system-on-chips (SM-SoC) are ubiquitously employed by a\nwide-range of mobile computing platforms, including edge/IoT devices,\nautonomous systems and smartphones. In SM-SoCs, system-wide shared physical\nmemory enables a convenient and financially-feasible way to make data\naccessible by dozens of processing units (PUs), such as CPU cores and domain\nspecific accelerators. In this study, we investigate vulnerabilities that stem\nfrom the shared use of physical memory in such systems. Due to the diverse\ncomputational characteristics of the PUs they embed, SM-SoCs often do not\nemploy a shared last level cache (LLC). While the literature proposes covert\nchannel attacks for shared memory systems, high-throughput communication is\ncurrently possible by either relying on an LLC or privileged/physical access to\nthe shared memory subsystem.\n  In this study, we introduce a new memory-contention based covert\ncommunication attack, MC3, which specifically targets the shared system memory\nin mobile SoCs. Different from existing attacks, our approach achieves high\nthroughput communication between applications running on CPU and GPU without\nthe need for an LLC or elevated access to the system. We extensively explore\nthe effectiveness of our methodology by demonstrating the trade-off between the\nchannel transmission rate and the robustness of the communication. We\ndemonstrate the utility of MC3 on NVIDIA Orin AGX, Orin NX, and Orin Nano up to\na transmit rate of 6.4 kbps with less than 1% error rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared-memory system-on-chips (SM-SoC) are ubiquitously employed by a\nwide-range of mobile computing platforms, including edge/IoT devices,\nautonomous systems and smartphones. In SM-SoCs, system-wide shared physical\nmemory enables a convenient and financially-feasible way to make data\naccessible by dozens of processing units (PUs), such as CPU cores and domain\nspecific accelerators. In this study, we investigate vulnerabilities that stem\nfrom the shared use of physical memory in such systems. Due to the diverse\ncomputational characteristics of the PUs they embed, SM-SoCs often do not\nemploy a shared last level cache (LLC). While the literature proposes covert\nchannel attacks for shared memory systems, high-throughput communication is\ncurrently possible by either relying on an LLC or privileged/physical access to\nthe shared memory subsystem.\n  In this study, we introduce a new memory-contention based covert\ncommunication attack, MC3, which specifically targets the shared system memory\nin mobile SoCs. Different from existing attacks, our approach achieves high\nthroughput communication between applications running on CPU and GPU without\nthe need for an LLC or elevated access to the system. We extensively explore\nthe effectiveness of our methodology by demonstrating the trade-off between the\nchannel transmission rate and the robustness of the communication. We\ndemonstrate the utility of MC3 on NVIDIA Orin AGX, Orin NX, and Orin Nano up to\na transmit rate of 6.4 kbps with less than 1% error rate."
                },
                "authors": [
                    {
                        "name": "Ismet Dagli"
                    },
                    {
                        "name": "James Crea"
                    },
                    {
                        "name": "Soner Seckiner"
                    },
                    {
                        "name": "Yuanchao Xu"
                    },
                    {
                        "name": "Selçuk Köse"
                    },
                    {
                        "name": "Mehmet E. Belviranli"
                    }
                ],
                "author_detail": {
                    "name": "Mehmet E. Belviranli"
                },
                "author": "Mehmet E. Belviranli",
                "arxiv_comment": "This paper is accepted to 2025 Design, Automation Test in Europe\n  Conference Exhibition (DATE)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02031v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02031v2",
                "updated": "2024-12-06T11:47:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    11,
                    47,
                    6,
                    4,
                    341,
                    0
                ],
                "published": "2024-07-02T07:59:08Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    7,
                    59,
                    8,
                    1,
                    184,
                    0
                ],
                "title": "SwiftDiffusion: Efficient Diffusion Model Serving with Add-on Modules",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiftDiffusion: Efficient Diffusion Model Serving with Add-on Modules"
                },
                "summary": "Text-to-image (T2I) generation using diffusion models has become a\nblockbuster service in today's AI cloud. A production T2I service typically\ninvolves a serving workflow where a base diffusion model is augmented with\nvarious \"add-on\" modules, notably ControlNet and LoRA, to enhance image\ngeneration control. Compared to serving the base model alone, these add-on\nmodules introduce significant loading and computational overhead, resulting in\nincreased latency. In this paper, we present SwiftDiffusion, a system that\nefficiently serves a T2I workflow through a holistic approach. SwiftDiffusion\ndecouples ControNet from the base model and deploys it as a separate,\nindependently scaled service on dedicated GPUs, enabling ControlNet caching,\nparallelization, and sharing. To mitigate the high loading overhead of LoRA\nserving, SwiftDiffusion employs a bounded asynchronous LoRA loading (BAL)\ntechnique, allowing LoRA loading to overlap with the initial base model\nexecution by up to k steps without compromising image quality. Furthermore,\nSwiftDiffusion optimizes base model execution with a novel latent parallelism\ntechnique. Collectively, these designs enable SwiftDiffusion to outperform the\nstate-of-the-art T2I serving systems, achieving up to 7.8x latency reduction\nand 1.6x throughput improvement in serving SDXL models on H800 GPUs, without\nsacrificing image quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image (T2I) generation using diffusion models has become a\nblockbuster service in today's AI cloud. A production T2I service typically\ninvolves a serving workflow where a base diffusion model is augmented with\nvarious \"add-on\" modules, notably ControlNet and LoRA, to enhance image\ngeneration control. Compared to serving the base model alone, these add-on\nmodules introduce significant loading and computational overhead, resulting in\nincreased latency. In this paper, we present SwiftDiffusion, a system that\nefficiently serves a T2I workflow through a holistic approach. SwiftDiffusion\ndecouples ControNet from the base model and deploys it as a separate,\nindependently scaled service on dedicated GPUs, enabling ControlNet caching,\nparallelization, and sharing. To mitigate the high loading overhead of LoRA\nserving, SwiftDiffusion employs a bounded asynchronous LoRA loading (BAL)\ntechnique, allowing LoRA loading to overlap with the initial base model\nexecution by up to k steps without compromising image quality. Furthermore,\nSwiftDiffusion optimizes base model execution with a novel latent parallelism\ntechnique. Collectively, these designs enable SwiftDiffusion to outperform the\nstate-of-the-art T2I serving systems, achieving up to 7.8x latency reduction\nand 1.6x throughput improvement in serving SDXL models on H800 GPUs, without\nsacrificing image quality."
                },
                "authors": [
                    {
                        "name": "Suyi Li"
                    },
                    {
                        "name": "Lingyun Yang"
                    },
                    {
                        "name": "Xiaoxiao Jiang"
                    },
                    {
                        "name": "Hanfeng Lu"
                    },
                    {
                        "name": "Dakai An"
                    },
                    {
                        "name": "Zhipeng Di"
                    },
                    {
                        "name": "Weiyi Lu"
                    },
                    {
                        "name": "Jiawei Chen"
                    },
                    {
                        "name": "Kan Liu"
                    },
                    {
                        "name": "Yinghao Yu"
                    },
                    {
                        "name": "Tao Lan"
                    },
                    {
                        "name": "Guodong Yang"
                    },
                    {
                        "name": "Lin Qu"
                    },
                    {
                        "name": "Liping Zhang"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02031v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02031v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04757v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04757v1",
                "updated": "2024-12-06T03:46:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    3,
                    46,
                    6,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T03:46:06Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    3,
                    46,
                    6,
                    4,
                    341,
                    0
                ],
                "title": "Ltri-LLM: Streaming Long Context Inference for LLMs with Training-Free\n  Dynamic Triangular Attention Pattern",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ltri-LLM: Streaming Long Context Inference for LLMs with Training-Free\n  Dynamic Triangular Attention Pattern"
                },
                "summary": "The quadratic computational complexity of the attention mechanism in current\nLarge Language Models (LLMs) renders inference with long contexts prohibitively\nexpensive. To address this challenge, various approaches aim to retain critical\nportions of the context to optimally approximate Full Attention (FA) through\nKey-Value (KV) compression or Sparse Attention (SA), enabling the processing of\nvirtually unlimited text lengths in a streaming manner. However, these methods\nstruggle to achieve performance levels comparable to FA, particularly in\nretrieval tasks. In this paper, our analysis of attention head patterns reveals\nthat LLMs' attention distributions show strong local correlations, naturally\nreflecting a chunking mechanism for input context. We propose Ltri-LLM\nframework, which divides KVs into spans, stores them in an offline index, and\nretrieves the relevant KVs into memory for various queries. Experimental\nresults on popular long text benchmarks show that Ltri-LLM can achieve\nperformance close to FA while maintaining efficient, streaming-based inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The quadratic computational complexity of the attention mechanism in current\nLarge Language Models (LLMs) renders inference with long contexts prohibitively\nexpensive. To address this challenge, various approaches aim to retain critical\nportions of the context to optimally approximate Full Attention (FA) through\nKey-Value (KV) compression or Sparse Attention (SA), enabling the processing of\nvirtually unlimited text lengths in a streaming manner. However, these methods\nstruggle to achieve performance levels comparable to FA, particularly in\nretrieval tasks. In this paper, our analysis of attention head patterns reveals\nthat LLMs' attention distributions show strong local correlations, naturally\nreflecting a chunking mechanism for input context. We propose Ltri-LLM\nframework, which divides KVs into spans, stores them in an offline index, and\nretrieves the relevant KVs into memory for various queries. Experimental\nresults on popular long text benchmarks show that Ltri-LLM can achieve\nperformance close to FA while maintaining efficient, streaming-based inference."
                },
                "authors": [
                    {
                        "name": "Hongyin Tang"
                    },
                    {
                        "name": "Di Xiu"
                    },
                    {
                        "name": "Lanrui Wang"
                    },
                    {
                        "name": "Xiurui Geng"
                    },
                    {
                        "name": "Jingang Wang"
                    },
                    {
                        "name": "Xunliang Cai"
                    }
                ],
                "author_detail": {
                    "name": "Xunliang Cai"
                },
                "author": "Xunliang Cai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04757v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04757v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04698v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04698v1",
                "updated": "2024-12-06T01:20:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    1,
                    20,
                    47,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T01:20:47Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    1,
                    20,
                    47,
                    4,
                    341,
                    0
                ],
                "title": "One-Hop Sub-Query Result Caches for Graph Database Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One-Hop Sub-Query Result Caches for Graph Database Systems"
                },
                "summary": "This paper introduces a novel one-hop sub-query result cache for processing\ngraph read transactions, gR-Txs, in a graph database system. The one-hop\nnavigation is from a vertex using either its in-coming or out-going edges with\nselection predicates that filter edges and vertices. Its cache entry identifies\na unique one-hop sub-query (key) and its result set consisting of immutable\nvertex ids (value). When processing a gR-Tx, the query processor identifies its\nsequence of individual one-hop sub-queries and looks up their results in the\ncache. A cache hit fetches less data from the storage manager and eliminates\nthe requirement to process the one-hop sub-query. A cache miss populates the\ncache asynchronously and in a transactional manner, maintaining the separation\nof read and write paths of our transactional storage manager. A graph read and\nwrite transaction, gRW-Tx, identifies the impacted cache entries and either\ndeletes or updates them. Our implementation of the cache is inside the graph\nquery processing engine and transparent to a user application. We evaluate the\ncache using our eCommerce production workload and with rules that re-write\ngraph queries to maximize the performance enhancements observed with the cache.\nObtained results show the cache enhances 95th and 99th percentile of query\nresponse times by at least 2x and 1.63x, respectively. When combined with query\nre-writing, the enhancements are at least 2.33x and 4.48x, respectively. An\ninteresting result is the significant performance enhancement observed by the\nindirect beneficiaries of the cache, gRW-Txs and gR-Txs that do not reference\none-hop sub-queries. The cache frees system resources to expedite their\nprocessing significantly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel one-hop sub-query result cache for processing\ngraph read transactions, gR-Txs, in a graph database system. The one-hop\nnavigation is from a vertex using either its in-coming or out-going edges with\nselection predicates that filter edges and vertices. Its cache entry identifies\na unique one-hop sub-query (key) and its result set consisting of immutable\nvertex ids (value). When processing a gR-Tx, the query processor identifies its\nsequence of individual one-hop sub-queries and looks up their results in the\ncache. A cache hit fetches less data from the storage manager and eliminates\nthe requirement to process the one-hop sub-query. A cache miss populates the\ncache asynchronously and in a transactional manner, maintaining the separation\nof read and write paths of our transactional storage manager. A graph read and\nwrite transaction, gRW-Tx, identifies the impacted cache entries and either\ndeletes or updates them. Our implementation of the cache is inside the graph\nquery processing engine and transparent to a user application. We evaluate the\ncache using our eCommerce production workload and with rules that re-write\ngraph queries to maximize the performance enhancements observed with the cache.\nObtained results show the cache enhances 95th and 99th percentile of query\nresponse times by at least 2x and 1.63x, respectively. When combined with query\nre-writing, the enhancements are at least 2.33x and 4.48x, respectively. An\ninteresting result is the significant performance enhancement observed by the\nindirect beneficiaries of the cache, gRW-Txs and gR-Txs that do not reference\none-hop sub-queries. The cache frees system resources to expedite their\nprocessing significantly."
                },
                "authors": [
                    {
                        "name": "Hieu Nguyen"
                    },
                    {
                        "name": "Jun Li"
                    },
                    {
                        "name": "Shahram Ghandeharizadeh"
                    }
                ],
                "author_detail": {
                    "name": "Shahram Ghandeharizadeh"
                },
                "author": "Shahram Ghandeharizadeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04698v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04698v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04652v1",
                "updated": "2024-12-05T22:47:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    22,
                    47,
                    17,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T22:47:17Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    22,
                    47,
                    17,
                    3,
                    340,
                    0
                ],
                "title": "Cross-Self KV Cache Pruning for Efficient Vision-Language Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Self KV Cache Pruning for Efficient Vision-Language Inference"
                },
                "summary": "KV cache pruning has emerged as a promising technique for reducing memory and\ncomputation costs in long-context auto-regressive generation. Existing methods\nfor vision-language models (VLMs) typically rely on self-attention scores from\nlarge language models (LLMs) to identify and prune irrelevant tokens. However,\nthese approaches overlook the inherent distributional discrepancies between\nmodalities, often leading to inaccurate token importance estimation and the\nover-pruning of critical visual tokens. To address this, we propose decomposing\nattention scores into intra-modality attention (within the same modality) and\ninter-modality attention (across modalities), enabling more precise KV cache\npruning by independently managing these distinct attention types. Additionally,\nwe introduce an n-softmax function to counteract distribution shifts caused by\npruning, preserving the original smoothness of attention scores and ensuring\nstable performance. Our final training-free method,\n\\textbf{C}ross-\\textbf{S}elf \\textbf{P}runing (CSP), achieves competitive\nperformance compared to models with full KV caches while significantly\noutperforming previous pruning methods. Extensive evaluations on MileBench, a\nbenchmark encompassing 29 multimodal datasets, demonstrate CSP's effectiveness,\nachieving up to a 41\\% performance improvement on challenging tasks like\nconversational embodied dialogue while reducing the KV cache budget by 13.6\\%.\nThe code is available at https://github.com/TerryPei/CSP",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache pruning has emerged as a promising technique for reducing memory and\ncomputation costs in long-context auto-regressive generation. Existing methods\nfor vision-language models (VLMs) typically rely on self-attention scores from\nlarge language models (LLMs) to identify and prune irrelevant tokens. However,\nthese approaches overlook the inherent distributional discrepancies between\nmodalities, often leading to inaccurate token importance estimation and the\nover-pruning of critical visual tokens. To address this, we propose decomposing\nattention scores into intra-modality attention (within the same modality) and\ninter-modality attention (across modalities), enabling more precise KV cache\npruning by independently managing these distinct attention types. Additionally,\nwe introduce an n-softmax function to counteract distribution shifts caused by\npruning, preserving the original smoothness of attention scores and ensuring\nstable performance. Our final training-free method,\n\\textbf{C}ross-\\textbf{S}elf \\textbf{P}runing (CSP), achieves competitive\nperformance compared to models with full KV caches while significantly\noutperforming previous pruning methods. Extensive evaluations on MileBench, a\nbenchmark encompassing 29 multimodal datasets, demonstrate CSP's effectiveness,\nachieving up to a 41\\% performance improvement on challenging tasks like\nconversational embodied dialogue while reducing the KV cache budget by 13.6\\%.\nThe code is available at https://github.com/TerryPei/CSP"
                },
                "authors": [
                    {
                        "name": "Xiaohuan Pei"
                    },
                    {
                        "name": "Tao Huang"
                    },
                    {
                        "name": "Chang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chang Xu"
                },
                "author": "Chang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04634v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04634v1",
                "updated": "2024-12-05T22:06:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    22,
                    6,
                    23,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T22:06:23Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    22,
                    6,
                    23,
                    3,
                    340,
                    0
                ],
                "title": "Neural Two-Level Monte Carlo Real-Time Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Two-Level Monte Carlo Real-Time Rendering"
                },
                "summary": "We introduce an efficient Two-Level Monte Carlo (subset of Multi-Level Monte\nCarlo, MLMC) estimator for real-time rendering of scenes with global\nillumination. Using MLMC we split the shading integral into two parts: the\nradiance cache integral and the residual error integral that compensates for\nthe bias of the first one. For the first part, we developed the Neural Incident\nRadiance Cache (NIRC) leveraging the power of fully-fused tiny neural networks\nas a building block, which is trained on the fly. The cache is designed to\nprovide a fast and reasonable approximation of the incident radiance: an\nevaluation takes 2-25x less compute time than a path tracing sample. This\nenables us to estimate the radiance cache integral with a high number of\nsamples and by this achieve faster convergence. For the residual error\nintegral, we compute the difference between the NIRC predictions and the\nunbiased path tracing simulation. Our method makes no assumptions about the\ngeometry, materials, or lighting of a scene and has only few intuitive\nhyper-parameters. We provide a comprehensive comparative analysis in different\nexperimental scenarios. Since the algorithm is trained in an on-line fashion,\nit demonstrates significant noise level reduction even for dynamic scenes and\ncan easily be combined with other importance sampling schemes and noise\nreduction techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce an efficient Two-Level Monte Carlo (subset of Multi-Level Monte\nCarlo, MLMC) estimator for real-time rendering of scenes with global\nillumination. Using MLMC we split the shading integral into two parts: the\nradiance cache integral and the residual error integral that compensates for\nthe bias of the first one. For the first part, we developed the Neural Incident\nRadiance Cache (NIRC) leveraging the power of fully-fused tiny neural networks\nas a building block, which is trained on the fly. The cache is designed to\nprovide a fast and reasonable approximation of the incident radiance: an\nevaluation takes 2-25x less compute time than a path tracing sample. This\nenables us to estimate the radiance cache integral with a high number of\nsamples and by this achieve faster convergence. For the residual error\nintegral, we compute the difference between the NIRC predictions and the\nunbiased path tracing simulation. Our method makes no assumptions about the\ngeometry, materials, or lighting of a scene and has only few intuitive\nhyper-parameters. We provide a comprehensive comparative analysis in different\nexperimental scenarios. Since the algorithm is trained in an on-line fashion,\nit demonstrates significant noise level reduction even for dynamic scenes and\ncan easily be combined with other importance sampling schemes and noise\nreduction techniques."
                },
                "authors": [
                    {
                        "name": "Mikhail Dereviannykh"
                    },
                    {
                        "name": "Dmitrii Klepikov"
                    },
                    {
                        "name": "Johannes Hanika"
                    },
                    {
                        "name": "Carsten Dachsbacher"
                    }
                ],
                "author_detail": {
                    "name": "Carsten Dachsbacher"
                },
                "author": "Carsten Dachsbacher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04634v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04634v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.19796v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19796v1",
                "updated": "2024-12-27T18:51:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    18,
                    51,
                    15,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T18:51:15Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    18,
                    51,
                    15,
                    4,
                    362,
                    0
                ],
                "title": "Generalized Grade-of-Membership Estimation for High-dimensional Locally\n  Dependent Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalized Grade-of-Membership Estimation for High-dimensional Locally\n  Dependent Data"
                },
                "summary": "This work focuses on the mixed membership models for multivariate categorical\ndata widely used for analyzing survey responses and population genetics data.\nThese grade of membership (GoM) models offer rich modeling power but present\nsignificant estimation challenges for high-dimensional polytomous data. Popular\nexisting approaches, such as Bayesian MCMC inference, are not scalable and lack\ntheoretical guarantees in high-dimensional settings. To address this, we first\nobserve that data from this model can be reformulated as a three-way\n(quasi-)tensor, with many subjects responding to many items with varying\nnumbers of categories. We introduce a novel and simple approach that flattens\nthe three-way quasi-tensor into a \"fat\" matrix, and then perform a singular\nvalue decomposition of it to estimate parameters by exploiting the singular\nsubspace geometry. Our fast spectral method can accommodate a broad range of\ndata distributions with arbitrarily locally dependent noise, which we formalize\nas the generalized-GoM models. We establish finite-sample entrywise error\nbounds for the generalized-GoM model parameters. This is supported by a new\nsharp two-to-infinity singular subspace perturbation theory for locally\ndependent and flexibly distributed noise, a contribution of independent\ninterest. Simulations and applications to data in political surveys, population\ngenetics, and single-cell sequencing demonstrate our method's superior\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work focuses on the mixed membership models for multivariate categorical\ndata widely used for analyzing survey responses and population genetics data.\nThese grade of membership (GoM) models offer rich modeling power but present\nsignificant estimation challenges for high-dimensional polytomous data. Popular\nexisting approaches, such as Bayesian MCMC inference, are not scalable and lack\ntheoretical guarantees in high-dimensional settings. To address this, we first\nobserve that data from this model can be reformulated as a three-way\n(quasi-)tensor, with many subjects responding to many items with varying\nnumbers of categories. We introduce a novel and simple approach that flattens\nthe three-way quasi-tensor into a \"fat\" matrix, and then perform a singular\nvalue decomposition of it to estimate parameters by exploiting the singular\nsubspace geometry. Our fast spectral method can accommodate a broad range of\ndata distributions with arbitrarily locally dependent noise, which we formalize\nas the generalized-GoM models. We establish finite-sample entrywise error\nbounds for the generalized-GoM model parameters. This is supported by a new\nsharp two-to-infinity singular subspace perturbation theory for locally\ndependent and flexibly distributed noise, a contribution of independent\ninterest. Simulations and applications to data in political surveys, population\ngenetics, and single-cell sequencing demonstrate our method's superior\nperformance."
                },
                "authors": [
                    {
                        "name": "Ling Chen"
                    },
                    {
                        "name": "Chengzhu Huang"
                    },
                    {
                        "name": "Yuqi Gu"
                    }
                ],
                "author_detail": {
                    "name": "Yuqi Gu"
                },
                "author": "Yuqi Gu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19796v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19796v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19792v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19792v1",
                "updated": "2024-12-27T18:45:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    18,
                    45,
                    36,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T18:45:36Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    18,
                    45,
                    36,
                    4,
                    362,
                    0
                ],
                "title": "InfAlign: Inference-aware language model alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfAlign: Inference-aware language model alignment"
                },
                "summary": "Language model alignment has become a critical step in training modern\ngenerative language models. The goal of alignment is to finetune a reference\nmodel such that the win rate of a sample from the aligned model over a sample\nfrom the reference model is high, subject to a KL divergence constraint. Today,\nwe are increasingly using inference-time algorithms (e.g., Best-of-N,\ncontrolled decoding, tree search) to decode from language models rather than\nstandard sampling. However, the alignment objective does not capture such\ninference-time decoding procedures. We show that the existing alignment\nframework is sub-optimal in view of such inference-time methods. We then modify\nthe alignment objective and propose a framework for inference-aware alignment\n(IAPO). We prove that for any inference-time decoding algorithm, the optimal\nsolution that optimizes the inference-time win rate of the aligned policy\nagainst the reference policy is the solution to the typical RLHF problem with a\ntransformation of the reward. This motivates us to provide the KL-regularized\ncalibrate-and-transform RL (CTRL) algorithm to solve this problem, which\ninvolves a reward calibration step and a KL-regularized reward maximization\nstep with a transformation of the calibrated reward. We particularize our study\nto two important inference-time strategies: best-of-N sampling and best-of-N\njailbreaking, where N responses are sampled from the model and the one with the\nhighest or lowest reward is selected. We propose specific transformations for\nthese strategies and demonstrate that our framework offers significant\nimprovements over existing state-of-the-art methods for language model\nalignment. Empirically, we outperform baselines that are designed without\ntaking inference-time decoding into consideration by 8-12% and 4-9% on\ninference-time win rates over the Anthropic helpfulness and harmlessness dialog\nbenchmark datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language model alignment has become a critical step in training modern\ngenerative language models. The goal of alignment is to finetune a reference\nmodel such that the win rate of a sample from the aligned model over a sample\nfrom the reference model is high, subject to a KL divergence constraint. Today,\nwe are increasingly using inference-time algorithms (e.g., Best-of-N,\ncontrolled decoding, tree search) to decode from language models rather than\nstandard sampling. However, the alignment objective does not capture such\ninference-time decoding procedures. We show that the existing alignment\nframework is sub-optimal in view of such inference-time methods. We then modify\nthe alignment objective and propose a framework for inference-aware alignment\n(IAPO). We prove that for any inference-time decoding algorithm, the optimal\nsolution that optimizes the inference-time win rate of the aligned policy\nagainst the reference policy is the solution to the typical RLHF problem with a\ntransformation of the reward. This motivates us to provide the KL-regularized\ncalibrate-and-transform RL (CTRL) algorithm to solve this problem, which\ninvolves a reward calibration step and a KL-regularized reward maximization\nstep with a transformation of the calibrated reward. We particularize our study\nto two important inference-time strategies: best-of-N sampling and best-of-N\njailbreaking, where N responses are sampled from the model and the one with the\nhighest or lowest reward is selected. We propose specific transformations for\nthese strategies and demonstrate that our framework offers significant\nimprovements over existing state-of-the-art methods for language model\nalignment. Empirically, we outperform baselines that are designed without\ntaking inference-time decoding into consideration by 8-12% and 4-9% on\ninference-time win rates over the Anthropic helpfulness and harmlessness dialog\nbenchmark datasets."
                },
                "authors": [
                    {
                        "name": "Ananth Balashankar"
                    },
                    {
                        "name": "Ziteng Sun"
                    },
                    {
                        "name": "Jonathan Berant"
                    },
                    {
                        "name": "Jacob Eisenstein"
                    },
                    {
                        "name": "Michael Collins"
                    },
                    {
                        "name": "Adrian Hutter"
                    },
                    {
                        "name": "Jong Lee"
                    },
                    {
                        "name": "Chirag Nagpal"
                    },
                    {
                        "name": "Flavien Prost"
                    },
                    {
                        "name": "Aradhana Sinha"
                    },
                    {
                        "name": "and Ananda Theertha Suresh"
                    },
                    {
                        "name": "Ahmad Beirami"
                    }
                ],
                "author_detail": {
                    "name": "Ahmad Beirami"
                },
                "author": "Ahmad Beirami",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19792v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19792v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.09614v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.09614v3",
                "updated": "2024-12-27T18:43:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    18,
                    43,
                    59,
                    4,
                    362,
                    0
                ],
                "published": "2024-02-14T23:05:44Z",
                "published_parsed": [
                    2024,
                    2,
                    14,
                    23,
                    5,
                    44,
                    2,
                    45,
                    0
                ],
                "title": "Reasoning over Uncertain Text by Generative Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning over Uncertain Text by Generative Large Language Models"
                },
                "summary": "This paper considers the challenges Large Language Models (LLMs) face when\nreasoning over text that includes information involving uncertainty explicitly\nquantified via probability values. This type of reasoning is relevant to a\nvariety of contexts ranging from everyday conversations to medical\ndecision-making. Despite improvements in the mathematical reasoning\ncapabilities of LLMs, they still exhibit significant difficulties when it comes\nto probabilistic reasoning. To deal with this problem, we introduce the\nBayesian Linguistic Inference Dataset (BLInD), a new dataset specifically\ndesigned to test the probabilistic reasoning capabilities of LLMs. We use BLInD\nto find out the limitations of LLMs for tasks involving probabilistic\nreasoning. In addition, we present several prompting strategies that map the\nproblem to different formal representations, including Python code,\nprobabilistic algorithms, and probabilistic logical programming. We conclude by\nproviding an evaluation of our methods on BLInD and an adaptation of a causal\nreasoning question-answering dataset. Our empirical results highlight the\neffectiveness of our proposed strategies for multiple LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper considers the challenges Large Language Models (LLMs) face when\nreasoning over text that includes information involving uncertainty explicitly\nquantified via probability values. This type of reasoning is relevant to a\nvariety of contexts ranging from everyday conversations to medical\ndecision-making. Despite improvements in the mathematical reasoning\ncapabilities of LLMs, they still exhibit significant difficulties when it comes\nto probabilistic reasoning. To deal with this problem, we introduce the\nBayesian Linguistic Inference Dataset (BLInD), a new dataset specifically\ndesigned to test the probabilistic reasoning capabilities of LLMs. We use BLInD\nto find out the limitations of LLMs for tasks involving probabilistic\nreasoning. In addition, we present several prompting strategies that map the\nproblem to different formal representations, including Python code,\nprobabilistic algorithms, and probabilistic logical programming. We conclude by\nproviding an evaluation of our methods on BLInD and an adaptation of a causal\nreasoning question-answering dataset. Our empirical results highlight the\neffectiveness of our proposed strategies for multiple LLMs."
                },
                "authors": [
                    {
                        "name": "Aliakbar Nafar"
                    },
                    {
                        "name": "Kristen Brent Venable"
                    },
                    {
                        "name": "Parisa Kordjamshidi"
                    }
                ],
                "author_detail": {
                    "name": "Parisa Kordjamshidi"
                },
                "author": "Parisa Kordjamshidi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.09614v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.09614v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19785v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19785v1",
                "updated": "2024-12-27T18:32:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    18,
                    32,
                    24,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T18:32:24Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    18,
                    32,
                    24,
                    4,
                    362,
                    0
                ],
                "title": "Enhancing Whisper's Accuracy and Speed for Indian Languages through\n  Prompt-Tuning and Tokenization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Whisper's Accuracy and Speed for Indian Languages through\n  Prompt-Tuning and Tokenization"
                },
                "summary": "Automatic speech recognition has recently seen a significant advancement with\nlarge foundational models such as Whisper. However, these models often struggle\nto perform well in low-resource languages, such as Indian languages. This paper\nexplores two novel approaches to enhance Whisper's multilingual speech\nrecognition performance in Indian languages. First, we propose prompt-tuning\nwith language family information, which enhances Whisper's accuracy in\nlinguistically similar languages. Second, we introduce a novel tokenizer that\nreduces the number of generated tokens, thereby accelerating Whisper's\ninference speed. Our extensive experiments demonstrate that the tokenizer\nsignificantly reduces inference time, while prompt-tuning enhances accuracy\nacross various Whisper model sizes, including Small, Medium, and Large.\nTogether, these techniques achieve a balance between optimal WER and inference\nspeed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic speech recognition has recently seen a significant advancement with\nlarge foundational models such as Whisper. However, these models often struggle\nto perform well in low-resource languages, such as Indian languages. This paper\nexplores two novel approaches to enhance Whisper's multilingual speech\nrecognition performance in Indian languages. First, we propose prompt-tuning\nwith language family information, which enhances Whisper's accuracy in\nlinguistically similar languages. Second, we introduce a novel tokenizer that\nreduces the number of generated tokens, thereby accelerating Whisper's\ninference speed. Our extensive experiments demonstrate that the tokenizer\nsignificantly reduces inference time, while prompt-tuning enhances accuracy\nacross various Whisper model sizes, including Small, Medium, and Large.\nTogether, these techniques achieve a balance between optimal WER and inference\nspeed."
                },
                "authors": [
                    {
                        "name": "Kumud Tripathi"
                    },
                    {
                        "name": "Raj Gothi"
                    },
                    {
                        "name": "Pankaj Wasnik"
                    }
                ],
                "author_detail": {
                    "name": "Pankaj Wasnik"
                },
                "author": "Pankaj Wasnik",
                "arxiv_comment": "Accepted at ICASSP 2025, 5 pages, 1 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19785v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19785v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19784v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19784v1",
                "updated": "2024-12-27T18:25:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    18,
                    25,
                    27,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T18:25:27Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    18,
                    25,
                    27,
                    4,
                    362,
                    0
                ],
                "title": "Can AI Help with Your Personal Finances?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can AI Help with Your Personal Finances?"
                },
                "summary": "In recent years, Large Language Models (LLMs) have emerged as a\ntransformative development in artificial intelligence (AI), drawing significant\nattention from industry and academia. Trained on vast datasets, these\nsophisticated AI systems exhibit impressive natural language processing and\ncontent generation capabilities. This paper explores the potential of LLMs to\naddress key challenges in personal finance, focusing on the United States. We\nevaluate several leading LLMs, including OpenAI's ChatGPT, Google's Gemini,\nAnthropic's Claude, and Meta's Llama, to assess their effectiveness in\nproviding accurate financial advice on topics such as mortgages, taxes, loans,\nand investments. Our findings show that while these models achieve an average\naccuracy rate of approximately 70%, they also display notable limitations in\ncertain areas. Specifically, LLMs struggle to provide accurate responses for\ncomplex financial queries, with performance varying significantly across\ndifferent topics. Despite these limitations, the analysis reveals notable\nimprovements in newer versions of these models, highlighting their growing\nutility for individuals and financial advisors. As these AI systems continue to\nevolve, their potential for advancing AI-driven applications in personal\nfinance becomes increasingly promising.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Large Language Models (LLMs) have emerged as a\ntransformative development in artificial intelligence (AI), drawing significant\nattention from industry and academia. Trained on vast datasets, these\nsophisticated AI systems exhibit impressive natural language processing and\ncontent generation capabilities. This paper explores the potential of LLMs to\naddress key challenges in personal finance, focusing on the United States. We\nevaluate several leading LLMs, including OpenAI's ChatGPT, Google's Gemini,\nAnthropic's Claude, and Meta's Llama, to assess their effectiveness in\nproviding accurate financial advice on topics such as mortgages, taxes, loans,\nand investments. Our findings show that while these models achieve an average\naccuracy rate of approximately 70%, they also display notable limitations in\ncertain areas. Specifically, LLMs struggle to provide accurate responses for\ncomplex financial queries, with performance varying significantly across\ndifferent topics. Despite these limitations, the analysis reveals notable\nimprovements in newer versions of these models, highlighting their growing\nutility for individuals and financial advisors. As these AI systems continue to\nevolve, their potential for advancing AI-driven applications in personal\nfinance becomes increasingly promising."
                },
                "authors": [
                    {
                        "name": "Oudom Hean"
                    },
                    {
                        "name": "Utsha Saha"
                    },
                    {
                        "name": "Binita Saha"
                    }
                ],
                "author_detail": {
                    "name": "Binita Saha"
                },
                "author": "Binita Saha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19784v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19784v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10044v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10044v2",
                "updated": "2024-12-27T18:16:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    18,
                    16,
                    12,
                    4,
                    362,
                    0
                ],
                "published": "2024-10-13T23:17:58Z",
                "published_parsed": [
                    2024,
                    10,
                    13,
                    23,
                    17,
                    58,
                    6,
                    287,
                    0
                ],
                "title": "DAG-aware Transformer for Causal Effect Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DAG-aware Transformer for Causal Effect Estimation"
                },
                "summary": "Causal inference is a critical task across fields such as healthcare,\neconomics, and the social sciences. While recent advances in machine learning,\nespecially those based on the deep-learning architectures, have shown potential\nin estimating causal effects, existing approaches often fall short in handling\ncomplex causal structures and lack adaptability across various causal\nscenarios. In this paper, we present a novel transformer-based method for\ncausal inference that overcomes these challenges. The core innovation of our\nmodel lies in its integration of causal Directed Acyclic Graphs (DAGs) directly\ninto the attention mechanism, enabling it to accurately model the underlying\ncausal structure. This allows for flexible estimation of both average treatment\neffects (ATE) and conditional average treatment effects (CATE). Extensive\nexperiments on both synthetic and real-world datasets demonstrate that our\napproach surpasses existing methods in estimating causal effects across a wide\nrange of scenarios. The flexibility and robustness of our model make it a\nvaluable tool for researchers and practitioners tackling complex causal\ninference problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal inference is a critical task across fields such as healthcare,\neconomics, and the social sciences. While recent advances in machine learning,\nespecially those based on the deep-learning architectures, have shown potential\nin estimating causal effects, existing approaches often fall short in handling\ncomplex causal structures and lack adaptability across various causal\nscenarios. In this paper, we present a novel transformer-based method for\ncausal inference that overcomes these challenges. The core innovation of our\nmodel lies in its integration of causal Directed Acyclic Graphs (DAGs) directly\ninto the attention mechanism, enabling it to accurately model the underlying\ncausal structure. This allows for flexible estimation of both average treatment\neffects (ATE) and conditional average treatment effects (CATE). Extensive\nexperiments on both synthetic and real-world datasets demonstrate that our\napproach surpasses existing methods in estimating causal effects across a wide\nrange of scenarios. The flexibility and robustness of our model make it a\nvaluable tool for researchers and practitioners tackling complex causal\ninference problems."
                },
                "authors": [
                    {
                        "name": "Manqing Liu"
                    },
                    {
                        "name": "David R. Bellamy"
                    },
                    {
                        "name": "Andrew L. Beam"
                    }
                ],
                "author_detail": {
                    "name": "Andrew L. Beam"
                },
                "author": "Andrew L. Beam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10044v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10044v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19770v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19770v1",
                "updated": "2024-12-27T18:06:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    18,
                    6,
                    25,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T18:06:25Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    18,
                    6,
                    25,
                    4,
                    362,
                    0
                ],
                "title": "Fortran2CPP: Automating Fortran-to-C++ Migration using LLMs via\n  Multi-Turn Dialogue and Dual-Agent Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fortran2CPP: Automating Fortran-to-C++ Migration using LLMs via\n  Multi-Turn Dialogue and Dual-Agent Integration"
                },
                "summary": "Migrating Fortran code to C++ is a common task for many scientific computing\nteams, driven by the need to leverage modern programming paradigms, enhance\ncross-platform compatibility, and improve maintainability. Automating this\ntranslation process using large language models (LLMs) has shown promise, but\nthe lack of high-quality, specialized datasets has hindered their\neffectiveness. In this paper, we address this challenge by introducing a novel\nmulti-turn dialogue dataset, Fortran2CPP, specifically designed for\nFortran-to-C++ code migration. Our dataset, significantly larger than existing\nalternatives, is generated using a unique LLM-driven, dual-agent pipeline\nincorporating iterative compilation, execution, and code repair to ensure high\nquality and functional correctness. To demonstrate the effectiveness of our\ndataset, we fine-tuned several open-weight LLMs on Fortran2CPP and evaluated\ntheir performance on two independent benchmarks. Fine-tuning on our dataset led\nto remarkable gains, with models achieving up to a 3.31x increase in CodeBLEU\nscore and a 92\\% improvement in compilation success rate. This highlights the\ndataset's ability to enhance both the syntactic accuracy and compilability of\nthe translated C++ code. Our dataset and model have been open-sourced and are\navailable on our public GitHub\nrepository\\footnote{\\url{https://github.com/HPC-Fortran2CPP/Fortran2Cpp}}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Migrating Fortran code to C++ is a common task for many scientific computing\nteams, driven by the need to leverage modern programming paradigms, enhance\ncross-platform compatibility, and improve maintainability. Automating this\ntranslation process using large language models (LLMs) has shown promise, but\nthe lack of high-quality, specialized datasets has hindered their\neffectiveness. In this paper, we address this challenge by introducing a novel\nmulti-turn dialogue dataset, Fortran2CPP, specifically designed for\nFortran-to-C++ code migration. Our dataset, significantly larger than existing\nalternatives, is generated using a unique LLM-driven, dual-agent pipeline\nincorporating iterative compilation, execution, and code repair to ensure high\nquality and functional correctness. To demonstrate the effectiveness of our\ndataset, we fine-tuned several open-weight LLMs on Fortran2CPP and evaluated\ntheir performance on two independent benchmarks. Fine-tuning on our dataset led\nto remarkable gains, with models achieving up to a 3.31x increase in CodeBLEU\nscore and a 92\\% improvement in compilation success rate. This highlights the\ndataset's ability to enhance both the syntactic accuracy and compilability of\nthe translated C++ code. Our dataset and model have been open-sourced and are\navailable on our public GitHub\nrepository\\footnote{\\url{https://github.com/HPC-Fortran2CPP/Fortran2Cpp}}."
                },
                "authors": [
                    {
                        "name": "Le Chen"
                    },
                    {
                        "name": "Bin Lei"
                    },
                    {
                        "name": "Dunzhi Zhou"
                    },
                    {
                        "name": "Pei-Hung Lin"
                    },
                    {
                        "name": "Chunhua Liao"
                    },
                    {
                        "name": "Caiwen Ding"
                    },
                    {
                        "name": "Ali Jannesari"
                    }
                ],
                "author_detail": {
                    "name": "Ali Jannesari"
                },
                "author": "Ali Jannesari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19770v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19770v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01366v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01366v2",
                "updated": "2024-12-27T17:49:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    17,
                    49,
                    34,
                    4,
                    362,
                    0
                ],
                "published": "2024-09-02T16:41:44Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    16,
                    41,
                    44,
                    0,
                    246,
                    0
                ],
                "title": "CHESS: Optimizing LLM Inference via Channel-Wise Thresholding and\n  Selective Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CHESS: Optimizing LLM Inference via Channel-Wise Thresholding and\n  Selective Sparsification"
                },
                "summary": "Deploying large language models (LLMs) on edge devices presents significant\nchallenges due to the substantial computational overhead and memory\nrequirements. Activation sparsification can mitigate these resource challenges\nby reducing the number of activated neurons during inference. Existing methods\ntypically employ thresholding-based sparsification based on the statistics of\nactivation tensors. However, they do not model the impact of activation\nsparsification on performance, resulting in suboptimal performance degradation.\nTo address the limitations, this paper reformulates the activation\nsparsification problem to explicitly capture the relationship between\nactivation sparsity and model performance. Then, this paper proposes CHESS, a\ngeneral activation sparsification approach via CHannel-wise thrEsholding and\nSelective Sparsification. First, channel-wise thresholding assigns a unique\nthreshold to each activation channel in the feed-forward network (FFN) layers.\nThen, selective sparsification involves applying thresholding-based activation\nsparsification to specific layers within the attention modules. Finally, we\ndetail the implementation of sparse kernels to accelerate LLM inference.\nExperimental results demonstrate that the proposed CHESS achieves lower\nperformance degradation over eight downstream tasks while activating fewer\nparameters than existing methods, thus speeding up the LLM inference by up to\n1.27x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying large language models (LLMs) on edge devices presents significant\nchallenges due to the substantial computational overhead and memory\nrequirements. Activation sparsification can mitigate these resource challenges\nby reducing the number of activated neurons during inference. Existing methods\ntypically employ thresholding-based sparsification based on the statistics of\nactivation tensors. However, they do not model the impact of activation\nsparsification on performance, resulting in suboptimal performance degradation.\nTo address the limitations, this paper reformulates the activation\nsparsification problem to explicitly capture the relationship between\nactivation sparsity and model performance. Then, this paper proposes CHESS, a\ngeneral activation sparsification approach via CHannel-wise thrEsholding and\nSelective Sparsification. First, channel-wise thresholding assigns a unique\nthreshold to each activation channel in the feed-forward network (FFN) layers.\nThen, selective sparsification involves applying thresholding-based activation\nsparsification to specific layers within the attention modules. Finally, we\ndetail the implementation of sparse kernels to accelerate LLM inference.\nExperimental results demonstrate that the proposed CHESS achieves lower\nperformance degradation over eight downstream tasks while activating fewer\nparameters than existing methods, thus speeding up the LLM inference by up to\n1.27x."
                },
                "authors": [
                    {
                        "name": "Junhui He"
                    },
                    {
                        "name": "Shangyu Wu"
                    },
                    {
                        "name": "Weidong Wen"
                    },
                    {
                        "name": "Chun Jason Xue"
                    },
                    {
                        "name": "Qingan Li"
                    }
                ],
                "author_detail": {
                    "name": "Qingan Li"
                },
                "author": "Qingan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01366v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01366v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19755v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19755v1",
                "updated": "2024-12-27T17:33:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    17,
                    33,
                    39,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T17:33:39Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    17,
                    33,
                    39,
                    4,
                    362,
                    0
                ],
                "title": "\"Did my figure do justice to the answer?\" : Towards Multimodal Short\n  Answer Grading with Feedback (MMSAF)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Did my figure do justice to the answer?\" : Towards Multimodal Short\n  Answer Grading with Feedback (MMSAF)"
                },
                "summary": "Personalized feedback plays a vital role in a student's learning process.\nWhile existing systems are adept at providing feedback over MCQ-based\nevaluation, this work focuses more on subjective and open-ended questions,\nwhich is similar to the problem of Automatic Short Answer Grading (ASAG) with\nfeedback. Additionally, we introduce the Multimodal Short Answer grading with\nFeedback (MMSAF) problem over the traditional ASAG feedback problem to address\nthe scenario where the student answer and reference answer might contain\nimages. Moreover, we introduce the MMSAF dataset with 2197 data points along\nwith an automated framework for generating such data sets. Our evaluations on\nexisting LLMs over this dataset achieved an overall accuracy of 55\\% on Level\nof Correctness labels, 75\\% on Image Relevance labels and a score of 4.27 out\nof 5 in correctness level of LLM generated feedback as rated by experts. As per\nexperts, Pixtral achieved a rating of above 4 out of all metrics, indicating\nthat it is more aligned to human judgement, and that it is the best solution\nfor assisting students.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized feedback plays a vital role in a student's learning process.\nWhile existing systems are adept at providing feedback over MCQ-based\nevaluation, this work focuses more on subjective and open-ended questions,\nwhich is similar to the problem of Automatic Short Answer Grading (ASAG) with\nfeedback. Additionally, we introduce the Multimodal Short Answer grading with\nFeedback (MMSAF) problem over the traditional ASAG feedback problem to address\nthe scenario where the student answer and reference answer might contain\nimages. Moreover, we introduce the MMSAF dataset with 2197 data points along\nwith an automated framework for generating such data sets. Our evaluations on\nexisting LLMs over this dataset achieved an overall accuracy of 55\\% on Level\nof Correctness labels, 75\\% on Image Relevance labels and a score of 4.27 out\nof 5 in correctness level of LLM generated feedback as rated by experts. As per\nexperts, Pixtral achieved a rating of above 4 out of all metrics, indicating\nthat it is more aligned to human judgement, and that it is the best solution\nfor assisting students."
                },
                "authors": [
                    {
                        "name": "Pritam Sil"
                    },
                    {
                        "name": "Bhaskaran Raman"
                    },
                    {
                        "name": "Pushpak Bhattacharyya"
                    }
                ],
                "author_detail": {
                    "name": "Pushpak Bhattacharyya"
                },
                "author": "Pushpak Bhattacharyya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19755v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19755v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19727v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19727v1",
                "updated": "2024-12-27T16:31:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    16,
                    31,
                    9,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T16:31:09Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    16,
                    31,
                    9,
                    4,
                    362,
                    0
                ],
                "title": "Learning to Forget: Bayesian Time Series Forecasting using Recurrent\n  Sparse Spectrum Signature Gaussian Processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Forget: Bayesian Time Series Forecasting using Recurrent\n  Sparse Spectrum Signature Gaussian Processes"
                },
                "summary": "The signature kernel is a kernel between time series of arbitrary length and\ncomes with strong theoretical guarantees from stochastic analysis. It has found\napplications in machine learning such as covariance functions for Gaussian\nprocesses. A strength of the underlying signature features is that they provide\na structured global description of a time series. However, this property can\nquickly become a curse when local information is essential and forgetting is\nrequired; so far this has only been addressed with ad-hoc methods such as\nslicing the time series into subsegments. To overcome this, we propose a\nprincipled, data-driven approach by introducing a novel forgetting mechanism\nfor signatures. This allows the model to dynamically adapt its context length\nto focus on more recent information. To achieve this, we revisit the recently\nintroduced Random Fourier Signature Features, and develop Random Fourier\nDecayed Signature Features (RFDSF) with Gaussian processes (GPs). This results\nin a Bayesian time series forecasting algorithm with variational inference,\nthat offers a scalable probabilistic algorithm that processes and transforms a\ntime series into a joint predictive distribution over time steps in one pass\nusing recurrence. For example, processing a sequence of length $10^4$ steps in\n$\\approx 10^{-2}$ seconds and in $< 1\\text{GB}$ of GPU memory. We demonstrate\nthat it outperforms other GP-based alternatives and competes with\nstate-of-the-art probabilistic time series forecasting algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The signature kernel is a kernel between time series of arbitrary length and\ncomes with strong theoretical guarantees from stochastic analysis. It has found\napplications in machine learning such as covariance functions for Gaussian\nprocesses. A strength of the underlying signature features is that they provide\na structured global description of a time series. However, this property can\nquickly become a curse when local information is essential and forgetting is\nrequired; so far this has only been addressed with ad-hoc methods such as\nslicing the time series into subsegments. To overcome this, we propose a\nprincipled, data-driven approach by introducing a novel forgetting mechanism\nfor signatures. This allows the model to dynamically adapt its context length\nto focus on more recent information. To achieve this, we revisit the recently\nintroduced Random Fourier Signature Features, and develop Random Fourier\nDecayed Signature Features (RFDSF) with Gaussian processes (GPs). This results\nin a Bayesian time series forecasting algorithm with variational inference,\nthat offers a scalable probabilistic algorithm that processes and transforms a\ntime series into a joint predictive distribution over time steps in one pass\nusing recurrence. For example, processing a sequence of length $10^4$ steps in\n$\\approx 10^{-2}$ seconds and in $< 1\\text{GB}$ of GPU memory. We demonstrate\nthat it outperforms other GP-based alternatives and competes with\nstate-of-the-art probabilistic time series forecasting algorithms."
                },
                "authors": [
                    {
                        "name": "Csaba Tóth"
                    },
                    {
                        "name": "Masaki Adachi"
                    },
                    {
                        "name": "Michael A. Osborne"
                    },
                    {
                        "name": "Harald Oberhauser"
                    }
                ],
                "author_detail": {
                    "name": "Harald Oberhauser"
                },
                "author": "Harald Oberhauser",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19727v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19727v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19726v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19726v1",
                "updated": "2024-12-27T16:30:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    16,
                    30,
                    12,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T16:30:12Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    16,
                    30,
                    12,
                    4,
                    362,
                    0
                ],
                "title": "Can Large Language Models Adapt to Other Agents In-Context?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Adapt to Other Agents In-Context?"
                },
                "summary": "As the research community aims to build better AI assistants that are more\ndynamic and personalized to the diversity of humans that they interact with,\nthere is increased interest in evaluating the theory of mind capabilities of\nlarge language models (LLMs). Indeed, several recent studies suggest that LLM\ntheory of mind capabilities are quite impressive, approximating human-level\nperformance. Our paper aims to rebuke this narrative and argues instead that\npast studies were not directly measuring agent performance, potentially leading\nto findings that are illusory in nature as a result. We draw a strong\ndistinction between what we call literal theory of mind i.e. measuring the\nagent's ability to predict the behavior of others and functional theory of mind\ni.e. adapting to agents in-context based on a rational response to predictions\nof their behavior. We find that top performing open source LLMs may display\nstrong capabilities in literal theory of mind, depending on how they are\nprompted, but seem to struggle with functional theory of mind -- even when\npartner policies are exceedingly simple. Our work serves to highlight the\ndouble sided nature of inductive bias in LLMs when adapting to new situations.\nWhile this bias can lead to strong performance over limited horizons, it often\nhinders convergence to optimal long-term behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the research community aims to build better AI assistants that are more\ndynamic and personalized to the diversity of humans that they interact with,\nthere is increased interest in evaluating the theory of mind capabilities of\nlarge language models (LLMs). Indeed, several recent studies suggest that LLM\ntheory of mind capabilities are quite impressive, approximating human-level\nperformance. Our paper aims to rebuke this narrative and argues instead that\npast studies were not directly measuring agent performance, potentially leading\nto findings that are illusory in nature as a result. We draw a strong\ndistinction between what we call literal theory of mind i.e. measuring the\nagent's ability to predict the behavior of others and functional theory of mind\ni.e. adapting to agents in-context based on a rational response to predictions\nof their behavior. We find that top performing open source LLMs may display\nstrong capabilities in literal theory of mind, depending on how they are\nprompted, but seem to struggle with functional theory of mind -- even when\npartner policies are exceedingly simple. Our work serves to highlight the\ndouble sided nature of inductive bias in LLMs when adapting to new situations.\nWhile this bias can lead to strong performance over limited horizons, it often\nhinders convergence to optimal long-term behavior."
                },
                "authors": [
                    {
                        "name": "Matthew Riemer"
                    },
                    {
                        "name": "Zahra Ashktorab"
                    },
                    {
                        "name": "Djallel Bouneffouf"
                    },
                    {
                        "name": "Payel Das"
                    },
                    {
                        "name": "Miao Liu"
                    },
                    {
                        "name": "Justin D. Weisz"
                    },
                    {
                        "name": "Murray Campbell"
                    }
                ],
                "author_detail": {
                    "name": "Murray Campbell"
                },
                "author": "Murray Campbell",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19726v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19726v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07343v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07343v4",
                "updated": "2024-12-27T16:29:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    16,
                    29,
                    25,
                    4,
                    362,
                    0
                ],
                "published": "2024-08-14T07:37:07Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    7,
                    37,
                    7,
                    2,
                    227,
                    0
                ],
                "title": "Gradient Alignment Improves Test-Time Adaptation for Medical Image\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gradient Alignment Improves Test-Time Adaptation for Medical Image\n  Segmentation"
                },
                "summary": "Although recent years have witnessed significant advancements in medical\nimage segmentation, the pervasive issue of domain shift among medical images\nfrom diverse centres hinders the effective deployment of pre-trained models.\nMany Test-time Adaptation (TTA) methods have been proposed to address this\nissue by fine-tuning pre-trained models with test data during inference. These\nmethods, however, often suffer from less-satisfactory optimization due to\nsuboptimal optimization direction (dictated by the gradient) and fixed\nstep-size (predicated on the learning rate). In this paper, we propose the\nGradient alignment-based Test-time adaptation (GraTa) method to improve both\nthe gradient direction and learning rate in the optimization procedure. Unlike\nconventional TTA methods, which primarily optimize the pseudo gradient derived\nfrom a self-supervised objective, our method incorporates an auxiliary gradient\nwith the pseudo one to facilitate gradient alignment. Such gradient alignment\nenables the model to excavate the similarities between different gradients and\ncorrect the gradient direction to approximate the empirical gradient related to\nthe current segmentation task. Additionally, we design a dynamic learning rate\nbased on the cosine similarity between the pseudo and auxiliary gradients,\nthereby empowering the adaptive fine-tuning of pre-trained models on diverse\ntest data. Extensive experiments establish the effectiveness of the proposed\ngradient alignment and dynamic learning rate and substantiate the superiority\nof our GraTa method over other state-of-the-art TTA methods on a benchmark\nmedical image segmentation task. The code and weights of pre-trained source\nmodels are available at https://github.com/Chen-Ziyang/GraTa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although recent years have witnessed significant advancements in medical\nimage segmentation, the pervasive issue of domain shift among medical images\nfrom diverse centres hinders the effective deployment of pre-trained models.\nMany Test-time Adaptation (TTA) methods have been proposed to address this\nissue by fine-tuning pre-trained models with test data during inference. These\nmethods, however, often suffer from less-satisfactory optimization due to\nsuboptimal optimization direction (dictated by the gradient) and fixed\nstep-size (predicated on the learning rate). In this paper, we propose the\nGradient alignment-based Test-time adaptation (GraTa) method to improve both\nthe gradient direction and learning rate in the optimization procedure. Unlike\nconventional TTA methods, which primarily optimize the pseudo gradient derived\nfrom a self-supervised objective, our method incorporates an auxiliary gradient\nwith the pseudo one to facilitate gradient alignment. Such gradient alignment\nenables the model to excavate the similarities between different gradients and\ncorrect the gradient direction to approximate the empirical gradient related to\nthe current segmentation task. Additionally, we design a dynamic learning rate\nbased on the cosine similarity between the pseudo and auxiliary gradients,\nthereby empowering the adaptive fine-tuning of pre-trained models on diverse\ntest data. Extensive experiments establish the effectiveness of the proposed\ngradient alignment and dynamic learning rate and substantiate the superiority\nof our GraTa method over other state-of-the-art TTA methods on a benchmark\nmedical image segmentation task. The code and weights of pre-trained source\nmodels are available at https://github.com/Chen-Ziyang/GraTa."
                },
                "authors": [
                    {
                        "name": "Ziyang Chen"
                    },
                    {
                        "name": "Yiwen Ye"
                    },
                    {
                        "name": "Yongsheng Pan"
                    },
                    {
                        "name": "Yong Xia"
                    }
                ],
                "author_detail": {
                    "name": "Yong Xia"
                },
                "author": "Yong Xia",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07343v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07343v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19707v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19707v1",
                "updated": "2024-12-27T16:02:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    16,
                    2,
                    34,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T16:02:34Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    16,
                    2,
                    34,
                    4,
                    362,
                    0
                ],
                "title": "Toward Adaptive Reasoning in Large Language Models with Thought Rollback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Adaptive Reasoning in Large Language Models with Thought Rollback"
                },
                "summary": "Large language models (LLMs) have been routinely used to solve various tasks\nusing step-by-step reasoning. However, the structure of intermediate reasoning\nsteps, or thoughts, is rigid and unidirectional, such as chains, trees, or\nacyclic-directed graphs. Consequently, the resulting inflexible and\nforward-only reasoning may not address challenging tasks and fail when the LLM\nfrequently gives false responses, i.e., ``hallucinations''. This paper proposes\na new reasoning framework, called Thought Rollback (TR), allowing LLMs to\nadaptively build thought structure while maintaining effective reasoning toward\nproblem-solving under ``hallucinations''. The core mechanism of TR is rolling\nback thoughts, which allows LLMs to perform error analysis on thoughts, and\nthus roll back to any previously mistaken thought for revision. Subsequently,\nby including such trial-and-error in the prompt to guide the LLM, each rollback\nleads to one more reliable reasoning path. Therefore, starting with a simple\nprompt without human annotations, LLM with TR adaptively and gradually explores\nthoughts for a correct solution. Comprehensive experiments on mathematical\nproblems and multi-task reasoning demonstrate the state-of-the-art performance\nof TR in terms of problem-solving rate and interaction cost. For instance, the\nsolving rate of GPT-4 with TR outperforms the current best by $9\\%$ on the MATH\ndataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been routinely used to solve various tasks\nusing step-by-step reasoning. However, the structure of intermediate reasoning\nsteps, or thoughts, is rigid and unidirectional, such as chains, trees, or\nacyclic-directed graphs. Consequently, the resulting inflexible and\nforward-only reasoning may not address challenging tasks and fail when the LLM\nfrequently gives false responses, i.e., ``hallucinations''. This paper proposes\na new reasoning framework, called Thought Rollback (TR), allowing LLMs to\nadaptively build thought structure while maintaining effective reasoning toward\nproblem-solving under ``hallucinations''. The core mechanism of TR is rolling\nback thoughts, which allows LLMs to perform error analysis on thoughts, and\nthus roll back to any previously mistaken thought for revision. Subsequently,\nby including such trial-and-error in the prompt to guide the LLM, each rollback\nleads to one more reliable reasoning path. Therefore, starting with a simple\nprompt without human annotations, LLM with TR adaptively and gradually explores\nthoughts for a correct solution. Comprehensive experiments on mathematical\nproblems and multi-task reasoning demonstrate the state-of-the-art performance\nof TR in terms of problem-solving rate and interaction cost. For instance, the\nsolving rate of GPT-4 with TR outperforms the current best by $9\\%$ on the MATH\ndataset."
                },
                "authors": [
                    {
                        "name": "Sijia Chen"
                    },
                    {
                        "name": "Baochun Li"
                    }
                ],
                "author_detail": {
                    "name": "Baochun Li"
                },
                "author": "Baochun Li",
                "arxiv_comment": "ICML 2024 camera-ready version with 24 pages and 12 figures. Code\n  repo with all prompts:\n  https://github.com/iQua/llmpebase/tree/main/examples/ThoughtRollback",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19707v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19707v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.02304v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.02304v3",
                "updated": "2024-12-27T15:37:01Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    15,
                    37,
                    1,
                    4,
                    362,
                    0
                ],
                "published": "2024-03-04T18:35:55Z",
                "published_parsed": [
                    2024,
                    3,
                    4,
                    18,
                    35,
                    55,
                    0,
                    64,
                    0
                ],
                "title": "RUBIES: JWST/NIRSpec Confirmation of an Infrared-luminous, Broad-line\n  Little Red Dot with an Ionized Outflow",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RUBIES: JWST/NIRSpec Confirmation of an Infrared-luminous, Broad-line\n  Little Red Dot with an Ionized Outflow"
                },
                "summary": "The JWST discovery of ``little red dots'' (LRDs) is reshaping our picture of\nthe early Universe, yet the physical mechanisms driving their compact size and\nUV-optical colors remain elusive. Here we report an unusually bright LRD\n($z=3.1$) observed as part of the RUBIES program. This LRD exhibits broad\nemission lines (FWHM $\\sim4000$km/s), a blue UV continuum, a clear Balmer break\nand a red continuum sampled out to rest 4 $\\mu$m with MIRI. We develop a new\njoint galaxy and AGN model within the Prospector Bayesian inference framework\nand perform spectrophotometric modeling using NIRCam, MIRI, and NIRSpec/Prism\nobservations. Our fiducial model reveals a $M_*\\sim 10^9M_\\odot$ galaxy\nalongside a dust-reddened AGN driving the optical emission. Explaining the\nrest-frame optical color as a reddened AGN requires $A_{\\rm v}\\gtrsim3$,\nsuggesting that a great majority of the accretion disk energy is re-radiated as\ndust emission. Yet despite clear AGN signatures, we find a surprising lack of\nhot torus emission, which implies that either the dust emission in this object\nmust be cold, or the red continuum must instead be driven by a massive, evolved\nstellar population of the host galaxy -- seemingly inconsistent with the high\nEW broad lines (H$\\alpha$ EW $\\sim800$\\AA). The widths and luminosities of\nPa$\\beta$, Pa$\\delta$, Pa$\\gamma$, and H$\\alpha$ imply a modest black hole mass\nof $M_{\\rm BH}\\sim10^8M_\\odot$. Additionally, we identify a narrow blue-shifted\nHeI absorption in G395M spectra, signaling an ionized outflow with kinetic\nenergy up to $\\sim1$\\% the luminosity of the AGN. The low redshift of\nRUBIES-BLAGN-1 combined with the depth and richness of the JWST imaging and\nspectroscopic observations provide a unique opportunity to build a physical\nmodel for these so-far mysterious LRDs, which may prove to be a crucial phase\nin the early formation of massive galaxies and their supermassive black holes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The JWST discovery of ``little red dots'' (LRDs) is reshaping our picture of\nthe early Universe, yet the physical mechanisms driving their compact size and\nUV-optical colors remain elusive. Here we report an unusually bright LRD\n($z=3.1$) observed as part of the RUBIES program. This LRD exhibits broad\nemission lines (FWHM $\\sim4000$km/s), a blue UV continuum, a clear Balmer break\nand a red continuum sampled out to rest 4 $\\mu$m with MIRI. We develop a new\njoint galaxy and AGN model within the Prospector Bayesian inference framework\nand perform spectrophotometric modeling using NIRCam, MIRI, and NIRSpec/Prism\nobservations. Our fiducial model reveals a $M_*\\sim 10^9M_\\odot$ galaxy\nalongside a dust-reddened AGN driving the optical emission. Explaining the\nrest-frame optical color as a reddened AGN requires $A_{\\rm v}\\gtrsim3$,\nsuggesting that a great majority of the accretion disk energy is re-radiated as\ndust emission. Yet despite clear AGN signatures, we find a surprising lack of\nhot torus emission, which implies that either the dust emission in this object\nmust be cold, or the red continuum must instead be driven by a massive, evolved\nstellar population of the host galaxy -- seemingly inconsistent with the high\nEW broad lines (H$\\alpha$ EW $\\sim800$\\AA). The widths and luminosities of\nPa$\\beta$, Pa$\\delta$, Pa$\\gamma$, and H$\\alpha$ imply a modest black hole mass\nof $M_{\\rm BH}\\sim10^8M_\\odot$. Additionally, we identify a narrow blue-shifted\nHeI absorption in G395M spectra, signaling an ionized outflow with kinetic\nenergy up to $\\sim1$\\% the luminosity of the AGN. The low redshift of\nRUBIES-BLAGN-1 combined with the depth and richness of the JWST imaging and\nspectroscopic observations provide a unique opportunity to build a physical\nmodel for these so-far mysterious LRDs, which may prove to be a crucial phase\nin the early formation of massive galaxies and their supermassive black holes."
                },
                "authors": [
                    {
                        "name": "Bingjie Wang"
                    },
                    {
                        "name": "Anna de Graaff"
                    },
                    {
                        "name": "Rebecca L. Davies"
                    },
                    {
                        "name": "Jenny E. Greene"
                    },
                    {
                        "name": "Joel Leja"
                    },
                    {
                        "name": "Gabriel B. Brammer"
                    },
                    {
                        "name": "Andy D. Goulding"
                    },
                    {
                        "name": "Tim B. Miller"
                    },
                    {
                        "name": "Katherine A. Suess"
                    },
                    {
                        "name": "Andrea Weibel"
                    },
                    {
                        "name": "Christina C. Williams"
                    },
                    {
                        "name": "Rachel Bezanson"
                    },
                    {
                        "name": "Leindert A. Boogaard"
                    },
                    {
                        "name": "Nikko J. Cleri"
                    },
                    {
                        "name": "Michaela Hirschmann"
                    },
                    {
                        "name": "Harley Katz"
                    },
                    {
                        "name": "Ivo Labbe"
                    },
                    {
                        "name": "Michael V. Maseda"
                    },
                    {
                        "name": "Jorryt Matthee"
                    },
                    {
                        "name": "Ian McConachie"
                    },
                    {
                        "name": "Rohan P. Naidu"
                    },
                    {
                        "name": "Pascal A. Oesch"
                    },
                    {
                        "name": "Hans-Walter Rix"
                    },
                    {
                        "name": "David J. Setton"
                    },
                    {
                        "name": "Katherine E. Whitaker"
                    }
                ],
                "author_detail": {
                    "name": "Katherine E. Whitaker"
                },
                "author": "Katherine E. Whitaker",
                "arxiv_comment": "Fixed a typo that made Fig 4b corresponded to an incorrect plot. 22\n  pages, 9 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.02304v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.02304v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16803v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16803v3",
                "updated": "2024-12-27T15:32:01Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    15,
                    32,
                    1,
                    4,
                    362,
                    0
                ],
                "published": "2024-10-22T08:28:05Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    8,
                    28,
                    5,
                    1,
                    296,
                    0
                ],
                "title": "Context-aware Inductive Knowledge Graph Completion with Latent Type\n  Constraints and Subgraph Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-aware Inductive Knowledge Graph Completion with Latent Type\n  Constraints and Subgraph Reasoning"
                },
                "summary": "Inductive knowledge graph completion (KGC) aims to predict missing triples\nwith unseen entities. Recent works focus on modeling reasoning paths between\nthe head and tail entity as direct supporting evidence. However, these methods\ndepend heavily on the existence and quality of reasoning paths, which limits\ntheir general applicability in different scenarios. In addition, we observe\nthat latent type constraints and neighboring facts inherent in KGs are also\nvital in inferring missing triples. To effectively utilize all useful\ninformation in KGs, we introduce CATS, a novel context-aware inductive KGC\nsolution. With sufficient guidance from proper prompts and supervised\nfine-tuning, CATS activates the strong semantic understanding and reasoning\ncapabilities of large language models to assess the existence of query triples,\nwhich consist of two modules. First, the type-aware reasoning module evaluates\nwhether the candidate entity matches the latent entity type as required by the\nquery relation. Then, the subgraph reasoning module selects relevant reasoning\npaths and neighboring facts, and evaluates their correlation to the query\ntriple. Experiment results on three widely used datasets demonstrate that CATS\nsignificantly outperforms state-of-the-art methods in 16 out of 18\ntransductive, inductive, and few-shot settings with an average absolute MRR\nimprovement of 7.2%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inductive knowledge graph completion (KGC) aims to predict missing triples\nwith unseen entities. Recent works focus on modeling reasoning paths between\nthe head and tail entity as direct supporting evidence. However, these methods\ndepend heavily on the existence and quality of reasoning paths, which limits\ntheir general applicability in different scenarios. In addition, we observe\nthat latent type constraints and neighboring facts inherent in KGs are also\nvital in inferring missing triples. To effectively utilize all useful\ninformation in KGs, we introduce CATS, a novel context-aware inductive KGC\nsolution. With sufficient guidance from proper prompts and supervised\nfine-tuning, CATS activates the strong semantic understanding and reasoning\ncapabilities of large language models to assess the existence of query triples,\nwhich consist of two modules. First, the type-aware reasoning module evaluates\nwhether the candidate entity matches the latent entity type as required by the\nquery relation. Then, the subgraph reasoning module selects relevant reasoning\npaths and neighboring facts, and evaluates their correlation to the query\ntriple. Experiment results on three widely used datasets demonstrate that CATS\nsignificantly outperforms state-of-the-art methods in 16 out of 18\ntransductive, inductive, and few-shot settings with an average absolute MRR\nimprovement of 7.2%."
                },
                "authors": [
                    {
                        "name": "Muzhi Li"
                    },
                    {
                        "name": "Cehao Yang"
                    },
                    {
                        "name": "Chengjin Xu"
                    },
                    {
                        "name": "Zixing Song"
                    },
                    {
                        "name": "Xuhui Jiang"
                    },
                    {
                        "name": "Jian Guo"
                    },
                    {
                        "name": "Ho-fung Leung"
                    },
                    {
                        "name": "Irwin King"
                    }
                ],
                "author_detail": {
                    "name": "Irwin King"
                },
                "author": "Irwin King",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16803v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16803v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19682v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19682v1",
                "updated": "2024-12-27T15:20:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    15,
                    20,
                    45,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T15:20:45Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    15,
                    20,
                    45,
                    4,
                    362,
                    0
                ],
                "title": "A Hybrid Technique for Plant Disease Identification and Localisation in\n  Real-time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Hybrid Technique for Plant Disease Identification and Localisation in\n  Real-time"
                },
                "summary": "Over the past decade, several image-processing methods and algorithms have\nbeen proposed for identifying plant diseases based on visual data. DNN (Deep\nNeural Networks) have recently become popular for this task. Both traditional\nimage processing and DNN-based methods encounter significant performance issues\nin real-time detection owing to computational limitations and a broad spectrum\nof plant disease features. This article proposes a novel technique for\nidentifying and localising plant disease based on the Quad-Tree decomposition\nof an image and feature learning simultaneously. The proposed algorithm\nsignificantly improves accuracy and faster convergence in high-resolution\nimages with relatively low computational load. Hence it is ideal for deploying\nthe algorithm in a standalone processor in a remotely operated image\nacquisition and disease detection system, ideally mounted on drones and robots\nworking on large agricultural fields. The technique proposed in this article is\nhybrid as it exploits the advantages of traditional image processing methods\nand DNN-based models at different scales, resulting in faster inference. The F1\nscore is approximately 0.80 for four disease classes corresponding to potato\nand tomato crops.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over the past decade, several image-processing methods and algorithms have\nbeen proposed for identifying plant diseases based on visual data. DNN (Deep\nNeural Networks) have recently become popular for this task. Both traditional\nimage processing and DNN-based methods encounter significant performance issues\nin real-time detection owing to computational limitations and a broad spectrum\nof plant disease features. This article proposes a novel technique for\nidentifying and localising plant disease based on the Quad-Tree decomposition\nof an image and feature learning simultaneously. The proposed algorithm\nsignificantly improves accuracy and faster convergence in high-resolution\nimages with relatively low computational load. Hence it is ideal for deploying\nthe algorithm in a standalone processor in a remotely operated image\nacquisition and disease detection system, ideally mounted on drones and robots\nworking on large agricultural fields. The technique proposed in this article is\nhybrid as it exploits the advantages of traditional image processing methods\nand DNN-based models at different scales, resulting in faster inference. The F1\nscore is approximately 0.80 for four disease classes corresponding to potato\nand tomato crops."
                },
                "authors": [
                    {
                        "name": "Mahendra Kumar Gohil"
                    },
                    {
                        "name": "Anirudha Bhattacharjee"
                    },
                    {
                        "name": "Rwik Rana"
                    },
                    {
                        "name": "Kishan Lal"
                    },
                    {
                        "name": "Samir Kumar Biswas"
                    },
                    {
                        "name": "Nachiketa Tiwari"
                    },
                    {
                        "name": "Bishakh Bhattacharya"
                    }
                ],
                "author_detail": {
                    "name": "Bishakh Bhattacharya"
                },
                "author": "Bishakh Bhattacharya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19682v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19682v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15473v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15473v2",
                "updated": "2024-12-27T14:56:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    14,
                    56,
                    10,
                    4,
                    362,
                    0
                ],
                "published": "2024-06-15T17:40:49Z",
                "published_parsed": [
                    2024,
                    6,
                    15,
                    17,
                    40,
                    49,
                    5,
                    167,
                    0
                ],
                "title": "Intertwining CP and NLP: The Generation of Unreasonably Constrained\n  Sentences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intertwining CP and NLP: The Generation of Unreasonably Constrained\n  Sentences"
                },
                "summary": "Constrained text generation remains a challenging task, particularly when\ndealing with hard constraints. Traditional NLP approaches prioritize generating\nmeaningful and coherent output. Also, the current state-of-the-art methods\noften lack the expressiveness and constraint satisfaction capabilities to\nhandle such tasks effectively. Recently, an approach for generating constrained\nsentences in CP has been proposed in (Bonlarron et al, 2023). This ad-hoc model\nto solve the sentences generation problem under MNREAD rules proved\nneithertheless to be computationaly and structuraly unsuitable to deal with\nother more constrained problems. In this paper, a novel more generic approach\nis introduced to tackle many of these previously untractable problems, and\nillustrated here with the quite untractable sentences generation problem\nfollowing RADNER rules.\n  More precisely, this paper presents the CPTextGen Framework. This framework\nconsiders a constrained text generation problem as a discrete combinatorial\noptimization problem. It is solved by a constraint programming method that\ncombines linguistic properties (e.g., n-grams or language level) with other\nmore classical constraints (e.g., the number of characters, syllables).\nEventually, a curation phase allows for selecting the best-generated sentences\naccording to perplexity using an LLM.\n  The effectiveness of this approach is demonstrated by tackling a new, more\ntediously constrained text generation problem: the iconic RADNER sentences\nproblem. This problem aims to generate sentences respecting a set of quite\nstrict rules defined by their use in vision and clinical research. Thanks to\nour CP-based approach, many new strongly constrained sentences have been\nsuccessfully generated. This highlights our approach's potential to handle\nunreasonably constrained text generation scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constrained text generation remains a challenging task, particularly when\ndealing with hard constraints. Traditional NLP approaches prioritize generating\nmeaningful and coherent output. Also, the current state-of-the-art methods\noften lack the expressiveness and constraint satisfaction capabilities to\nhandle such tasks effectively. Recently, an approach for generating constrained\nsentences in CP has been proposed in (Bonlarron et al, 2023). This ad-hoc model\nto solve the sentences generation problem under MNREAD rules proved\nneithertheless to be computationaly and structuraly unsuitable to deal with\nother more constrained problems. In this paper, a novel more generic approach\nis introduced to tackle many of these previously untractable problems, and\nillustrated here with the quite untractable sentences generation problem\nfollowing RADNER rules.\n  More precisely, this paper presents the CPTextGen Framework. This framework\nconsiders a constrained text generation problem as a discrete combinatorial\noptimization problem. It is solved by a constraint programming method that\ncombines linguistic properties (e.g., n-grams or language level) with other\nmore classical constraints (e.g., the number of characters, syllables).\nEventually, a curation phase allows for selecting the best-generated sentences\naccording to perplexity using an LLM.\n  The effectiveness of this approach is demonstrated by tackling a new, more\ntediously constrained text generation problem: the iconic RADNER sentences\nproblem. This problem aims to generate sentences respecting a set of quite\nstrict rules defined by their use in vision and clinical research. Thanks to\nour CP-based approach, many new strongly constrained sentences have been\nsuccessfully generated. This highlights our approach's potential to handle\nunreasonably constrained text generation scenarios."
                },
                "authors": [
                    {
                        "name": "Alexandre Bonlarron"
                    },
                    {
                        "name": "Jean-Charles Régin"
                    }
                ],
                "author_detail": {
                    "name": "Jean-Charles Régin"
                },
                "author": "Jean-Charles Régin",
                "arxiv_doi": "10.24963/ijcai.2024/841",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.24963/ijcai.2024/841",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.15473v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15473v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Disambiguation and additional references",
                "arxiv_journal_ref": "Proceedings of the Thirty-Third International Joint Conference on\n  Artificial Intelligence AI, Arts & Creativity. Pages 7600-7608. Year 2024",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.13362v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.13362v3",
                "updated": "2024-12-27T14:44:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    14,
                    44,
                    30,
                    4,
                    362,
                    0
                ],
                "published": "2024-05-22T05:43:15Z",
                "published_parsed": [
                    2024,
                    5,
                    22,
                    5,
                    43,
                    15,
                    2,
                    143,
                    0
                ],
                "title": "Lusifer: LLM-based User SImulated Feedback Environment for online\n  Recommender systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lusifer: LLM-based User SImulated Feedback Environment for online\n  Recommender systems"
                },
                "summary": "Training reinforcement learning-based recommender systems is often hindered\nby the lack of dynamic and realistic user interactions. To address this\nlimitation, we introduce Lusifer, a novel environment leveraging Large Language\nModels (LLMs) to generate simulated user feedback. Lusifer synthesizes user\nprofiles and interaction histories to simulate responses and behaviors toward\nrecommended items, with profiles updated after each rating to reflect evolving\nuser characteristics. Utilizing the MovieLens dataset as a proof of concept, we\nlimited our implementation to the last 40 interactions for each user,\nrepresenting approximately 39% and 22% of the training sets, to focus on recent\nuser behavior. For consistency and to gain insights into the performance of\ntraditional methods with limited data, we implemented baseline approaches using\nthe same data subset. Our results demonstrate that Lusifer accurately emulates\nuser behavior and preferences, even with reduced training data having an RMSE\nof 1.3 across various test sets. This paper presents Lusifer's operational\npipeline, including prompt generation and iterative user profile updates, and\ncompares its performance against baseline methods. The findings validate\nLusifer's ability to produce realistic dynamic feedback and suggest that it\noffers a scalable and adjustable framework for user simulation in online\nreinforcement learning recommender systems for future studies, particularly\nwhen training data is limited.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training reinforcement learning-based recommender systems is often hindered\nby the lack of dynamic and realistic user interactions. To address this\nlimitation, we introduce Lusifer, a novel environment leveraging Large Language\nModels (LLMs) to generate simulated user feedback. Lusifer synthesizes user\nprofiles and interaction histories to simulate responses and behaviors toward\nrecommended items, with profiles updated after each rating to reflect evolving\nuser characteristics. Utilizing the MovieLens dataset as a proof of concept, we\nlimited our implementation to the last 40 interactions for each user,\nrepresenting approximately 39% and 22% of the training sets, to focus on recent\nuser behavior. For consistency and to gain insights into the performance of\ntraditional methods with limited data, we implemented baseline approaches using\nthe same data subset. Our results demonstrate that Lusifer accurately emulates\nuser behavior and preferences, even with reduced training data having an RMSE\nof 1.3 across various test sets. This paper presents Lusifer's operational\npipeline, including prompt generation and iterative user profile updates, and\ncompares its performance against baseline methods. The findings validate\nLusifer's ability to produce realistic dynamic feedback and suggest that it\noffers a scalable and adjustable framework for user simulation in online\nreinforcement learning recommender systems for future studies, particularly\nwhen training data is limited."
                },
                "authors": [
                    {
                        "name": "Danial Ebrat"
                    },
                    {
                        "name": "Eli Paradalis"
                    },
                    {
                        "name": "Luis Rueda"
                    }
                ],
                "author_detail": {
                    "name": "Luis Rueda"
                },
                "author": "Luis Rueda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.13362v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.13362v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19665v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19665v1",
                "updated": "2024-12-27T14:23:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    14,
                    23,
                    31,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T14:23:31Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    14,
                    23,
                    31,
                    4,
                    362,
                    0
                ],
                "title": "Standardizing reverberation-mapped H$α$ and H$β$ active\n  galactic nuclei using radius--luminosity relations involving monochromatic\n  and broad H$α$ luminosities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Standardizing reverberation-mapped H$α$ and H$β$ active\n  galactic nuclei using radius--luminosity relations involving monochromatic\n  and broad H$α$ luminosities"
                },
                "summary": "We test the standardizability of a homogeneous sample of 41 lower-redshift\n($0.00415\\leq z \\leq 0.474$) active galactic nuclei (AGNs) reverberation-mapped\n(RM) using the broad H$\\alpha$ and H$\\beta$ emission lines. We find that these\nsources can be standardized using four radius$-$luminosity ($R-L$) relations\nincorporating H$\\alpha$ and H$\\beta$ time delays and monochromatic and broad\nH$\\alpha$ luminosities. Although the $R-L$ relation parameters are well\nconstrained and independent of the six cosmological models considered, the\nresulting cosmological constraints are weak. The measured $R-L$ relations\nexhibit slightly steeper slopes than predicted by a simple photoionization\nmodel and steeper than those from previous higher-redshift H$\\beta$ analyses\nbased on larger datasets. These differences likely reflect the absence of\nhigh-accreting sources in our smaller, lower-redshift sample, which primarily\ncomprises lower-accreting AGNs. The inferred cosmological parameters are\nconsistent within 2$\\sigma$ (or better) with those from better-established\ncosmological probes. This contrasts with our earlier findings using a larger,\nheterogeneous sample of 118 H$\\beta$ AGNs, which yielded cosmological\nconstraints differing by $\\gtrsim 2\\sigma$ from better-established cosmological\nprobes. Our analysis demonstrates that sample homogeneity$-$specifically, the\nuse of a consistent time-lag determination method$-$is crucial for developing\nRM AGNs as a cosmological probe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We test the standardizability of a homogeneous sample of 41 lower-redshift\n($0.00415\\leq z \\leq 0.474$) active galactic nuclei (AGNs) reverberation-mapped\n(RM) using the broad H$\\alpha$ and H$\\beta$ emission lines. We find that these\nsources can be standardized using four radius$-$luminosity ($R-L$) relations\nincorporating H$\\alpha$ and H$\\beta$ time delays and monochromatic and broad\nH$\\alpha$ luminosities. Although the $R-L$ relation parameters are well\nconstrained and independent of the six cosmological models considered, the\nresulting cosmological constraints are weak. The measured $R-L$ relations\nexhibit slightly steeper slopes than predicted by a simple photoionization\nmodel and steeper than those from previous higher-redshift H$\\beta$ analyses\nbased on larger datasets. These differences likely reflect the absence of\nhigh-accreting sources in our smaller, lower-redshift sample, which primarily\ncomprises lower-accreting AGNs. The inferred cosmological parameters are\nconsistent within 2$\\sigma$ (or better) with those from better-established\ncosmological probes. This contrasts with our earlier findings using a larger,\nheterogeneous sample of 118 H$\\beta$ AGNs, which yielded cosmological\nconstraints differing by $\\gtrsim 2\\sigma$ from better-established cosmological\nprobes. Our analysis demonstrates that sample homogeneity$-$specifically, the\nuse of a consistent time-lag determination method$-$is crucial for developing\nRM AGNs as a cosmological probe."
                },
                "authors": [
                    {
                        "name": "Shulei Cao"
                    },
                    {
                        "name": "Amit Kumar Mandal"
                    },
                    {
                        "name": "Michal Zajaček"
                    },
                    {
                        "name": "Bożena Czerny"
                    },
                    {
                        "name": "Bharat Ratra"
                    }
                ],
                "author_detail": {
                    "name": "Bharat Ratra"
                },
                "author": "Bharat Ratra",
                "arxiv_comment": "34 pages, 18 figures, 9 tables, submitted to PRD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19665v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19665v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19663v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19663v1",
                "updated": "2024-12-27T14:19:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    14,
                    19,
                    36,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T14:19:36Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    14,
                    19,
                    36,
                    4,
                    362,
                    0
                ],
                "title": "CAD-GPT: Synthesising CAD Construction Sequence with Spatial\n  Reasoning-Enhanced Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAD-GPT: Synthesising CAD Construction Sequence with Spatial\n  Reasoning-Enhanced Multimodal LLMs"
                },
                "summary": "Computer-aided design (CAD) significantly enhances the efficiency, accuracy,\nand innovation of design processes by enabling precise 2D and 3D modeling,\nextensive analysis, and optimization. Existing methods for creating CAD models\nrely on latent vectors or point clouds, which are difficult to obtain and\ncostly to store. Recent advances in Multimodal Large Language Models (MLLMs)\nhave inspired researchers to use natural language instructions and images for\nCAD model construction. However, these models still struggle with inferring\naccurate 3D spatial location and orientation, leading to inaccuracies in\ndetermining the spatial 3D starting points and extrusion directions for\nconstructing geometries. This work introduces CAD-GPT, a CAD synthesis method\nwith spatial reasoning-enhanced MLLM that takes either a single image or a\ntextual description as input. To achieve precise spatial inference, our\napproach introduces a 3D Modeling Spatial Mechanism. This method maps 3D\nspatial positions and 3D sketch plane rotation angles into a 1D linguistic\nfeature space using a specialized spatial unfolding mechanism, while\ndiscretizing 2D sketch coordinates into an appropriate planar space to enable\nprecise determination of spatial starting position, sketch orientation, and 2D\nsketch coordinate translations. Extensive experiments demonstrate that CAD-GPT\nconsistently outperforms existing state-of-the-art methods in CAD model\nsynthesis, both quantitatively and qualitatively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computer-aided design (CAD) significantly enhances the efficiency, accuracy,\nand innovation of design processes by enabling precise 2D and 3D modeling,\nextensive analysis, and optimization. Existing methods for creating CAD models\nrely on latent vectors or point clouds, which are difficult to obtain and\ncostly to store. Recent advances in Multimodal Large Language Models (MLLMs)\nhave inspired researchers to use natural language instructions and images for\nCAD model construction. However, these models still struggle with inferring\naccurate 3D spatial location and orientation, leading to inaccuracies in\ndetermining the spatial 3D starting points and extrusion directions for\nconstructing geometries. This work introduces CAD-GPT, a CAD synthesis method\nwith spatial reasoning-enhanced MLLM that takes either a single image or a\ntextual description as input. To achieve precise spatial inference, our\napproach introduces a 3D Modeling Spatial Mechanism. This method maps 3D\nspatial positions and 3D sketch plane rotation angles into a 1D linguistic\nfeature space using a specialized spatial unfolding mechanism, while\ndiscretizing 2D sketch coordinates into an appropriate planar space to enable\nprecise determination of spatial starting position, sketch orientation, and 2D\nsketch coordinate translations. Extensive experiments demonstrate that CAD-GPT\nconsistently outperforms existing state-of-the-art methods in CAD model\nsynthesis, both quantitatively and qualitatively."
                },
                "authors": [
                    {
                        "name": "Siyu Wang"
                    },
                    {
                        "name": "Cailian Chen"
                    },
                    {
                        "name": "Xinyi Le"
                    },
                    {
                        "name": "Qimin Xu"
                    },
                    {
                        "name": "Lei Xu"
                    },
                    {
                        "name": "Yanzhou Zhang"
                    },
                    {
                        "name": "Jie Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Yang"
                },
                "author": "Jie Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19663v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19663v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11843v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11843v3",
                "updated": "2024-12-27T14:17:05Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    14,
                    17,
                    5,
                    4,
                    362,
                    0
                ],
                "published": "2024-07-16T15:24:44Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    15,
                    24,
                    44,
                    1,
                    198,
                    0
                ],
                "title": "Preemptive Detection and Correction of Misaligned Actions in LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preemptive Detection and Correction of Misaligned Actions in LLM Agents"
                },
                "summary": "Deploying LLM-based agents in real-life applications often faces a critical\nchallenge: the misalignment between agents' behavior and user intent. Such\nmisalignment may lead agents to unintentionally execute critical actions that\ncarry negative outcomes (e.g., accidentally triggering a \"buy-now\" in web\nshopping), resulting in undesirable or even irreversible consequences. Although\naddressing these issues is crucial, the preemptive detection and correction of\nmisaligned actions remains relatively underexplored. To fill this gap, we\nintroduce InferAct, a novel approach that leverages the belief reasoning\nability of LLMs, grounded in Theory-of-Mind, to detect misaligned actions\nbefore execution. Once the misalignment is detected, InferAct alerts users for\ntimely correction, preventing adverse outcomes and enhancing the reliability of\nLLM agents' decision-making processes. Experiments on three widely used tasks\ndemonstrate that InferAct achieves up to 20% improvements on Marco-F1 against\nbaselines in misaligned action detection. An in-depth evaluation of\nmisalignment correction further highlights InferAct's effectiveness in\nimproving agent alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying LLM-based agents in real-life applications often faces a critical\nchallenge: the misalignment between agents' behavior and user intent. Such\nmisalignment may lead agents to unintentionally execute critical actions that\ncarry negative outcomes (e.g., accidentally triggering a \"buy-now\" in web\nshopping), resulting in undesirable or even irreversible consequences. Although\naddressing these issues is crucial, the preemptive detection and correction of\nmisaligned actions remains relatively underexplored. To fill this gap, we\nintroduce InferAct, a novel approach that leverages the belief reasoning\nability of LLMs, grounded in Theory-of-Mind, to detect misaligned actions\nbefore execution. Once the misalignment is detected, InferAct alerts users for\ntimely correction, preventing adverse outcomes and enhancing the reliability of\nLLM agents' decision-making processes. Experiments on three widely used tasks\ndemonstrate that InferAct achieves up to 20% improvements on Marco-F1 against\nbaselines in misaligned action detection. An in-depth evaluation of\nmisalignment correction further highlights InferAct's effectiveness in\nimproving agent alignment."
                },
                "authors": [
                    {
                        "name": "Haishuo Fang"
                    },
                    {
                        "name": "Xiaodan Zhu"
                    },
                    {
                        "name": "Iryna Gurevych"
                    }
                ],
                "author_detail": {
                    "name": "Iryna Gurevych"
                },
                "author": "Iryna Gurevych",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11843v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11843v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2111.08524v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2111.08524v3",
                "updated": "2024-12-27T14:08:01Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    14,
                    8,
                    1,
                    4,
                    362,
                    0
                ],
                "published": "2021-11-16T14:53:19Z",
                "published_parsed": [
                    2021,
                    11,
                    16,
                    14,
                    53,
                    19,
                    1,
                    320,
                    0
                ],
                "title": "Non-separable Spatio-temporal Graph Kernels via SPDEs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-separable Spatio-temporal Graph Kernels via SPDEs"
                },
                "summary": "Gaussian processes (GPs) provide a principled and direct approach for\ninference and learning on graphs. However, the lack of justified graph kernels\nfor spatio-temporal modelling has held back their use in graph problems. We\nleverage an explicit link between stochastic partial differential equations\n(SPDEs) and GPs on graphs, introduce a framework for deriving graph kernels via\nSPDEs, and derive non-separable spatio-temporal graph kernels that capture\ninteraction across space and time. We formulate the graph kernels for the\nstochastic heat equation and wave equation. We show that by providing novel\ntools for spatio-temporal GP modelling on graphs, we outperform pre-existing\ngraph kernels in real-world applications that feature diffusion, oscillation,\nand other complicated interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian processes (GPs) provide a principled and direct approach for\ninference and learning on graphs. However, the lack of justified graph kernels\nfor spatio-temporal modelling has held back their use in graph problems. We\nleverage an explicit link between stochastic partial differential equations\n(SPDEs) and GPs on graphs, introduce a framework for deriving graph kernels via\nSPDEs, and derive non-separable spatio-temporal graph kernels that capture\ninteraction across space and time. We formulate the graph kernels for the\nstochastic heat equation and wave equation. We show that by providing novel\ntools for spatio-temporal GP modelling on graphs, we outperform pre-existing\ngraph kernels in real-world applications that feature diffusion, oscillation,\nand other complicated interactions."
                },
                "authors": [
                    {
                        "name": "Alexander Nikitin"
                    },
                    {
                        "name": "ST John"
                    },
                    {
                        "name": "Arno Solin"
                    },
                    {
                        "name": "Samuel Kaski"
                    }
                ],
                "author_detail": {
                    "name": "Samuel Kaski"
                },
                "author": "Samuel Kaski",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2111.08524v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2111.08524v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19652v1",
                "updated": "2024-12-27T13:56:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    13,
                    56,
                    51,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T13:56:51Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    13,
                    56,
                    51,
                    4,
                    362,
                    0
                ],
                "title": "FreStega: A Plug-and-Play Method for Boosting Imperceptibility and\n  Capacity in Generative Linguistic Steganography for Real-World Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreStega: A Plug-and-Play Method for Boosting Imperceptibility and\n  Capacity in Generative Linguistic Steganography for Real-World Scenarios"
                },
                "summary": "Linguistic steganography embeds secret information in seemingly innocent\ntexts, safeguarding privacy in surveillance environments. Generative linguistic\nsteganography leverages the probability distribution of language models (LMs)\nand applies steganographic algorithms to generate stego tokens, gaining\nattention with recent Large Language Model (LLM) advancements. To enhance\nsecurity, researchers develop distribution-preserving stego algorithms to\nminimize the gap between stego sampling and LM sampling. However, the reliance\non language model distributions, coupled with deviations from real-world cover\ntexts, results in insufficient imperceptibility when facing steganalysis\ndetectors in real-world scenarios. Moreover, LLM distributions tend to be more\ndeterministic, resulting in reduced entropy and, consequently, lower embedding\ncapacity. In this paper, we propose FreStega, a plug-and-play method to\nreconstruct the distribution of language models used for generative linguistic\nsteganography. FreStega dynamically adjusts token probabilities from the\nlanguage model at each step of stegotext auto-regressive generation, leveraging\nboth sequential and spatial dimensions. In sequential adjustment, the\ntemperature is dynamically adjusted based on instantaneous entropy, enhancing\nthe diversity of stego texts and boosting embedding capacity. In the spatial\ndimension, the distribution is aligned with guidance from the target domain\ncorpus, closely mimicking real cover text in the target domain. By reforming\nthe distribution, FreStega enhances the imperceptibility of stego text in\npractical scenarios and improves steganographic capacity by 15.41\\%, all\nwithout compromising the quality of the generated text. FreStega serves as a\nplug-and-play remedy to enhance the imperceptibility and embedding capacity of\nexisting distribution-preserving steganography methods in real-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linguistic steganography embeds secret information in seemingly innocent\ntexts, safeguarding privacy in surveillance environments. Generative linguistic\nsteganography leverages the probability distribution of language models (LMs)\nand applies steganographic algorithms to generate stego tokens, gaining\nattention with recent Large Language Model (LLM) advancements. To enhance\nsecurity, researchers develop distribution-preserving stego algorithms to\nminimize the gap between stego sampling and LM sampling. However, the reliance\non language model distributions, coupled with deviations from real-world cover\ntexts, results in insufficient imperceptibility when facing steganalysis\ndetectors in real-world scenarios. Moreover, LLM distributions tend to be more\ndeterministic, resulting in reduced entropy and, consequently, lower embedding\ncapacity. In this paper, we propose FreStega, a plug-and-play method to\nreconstruct the distribution of language models used for generative linguistic\nsteganography. FreStega dynamically adjusts token probabilities from the\nlanguage model at each step of stegotext auto-regressive generation, leveraging\nboth sequential and spatial dimensions. In sequential adjustment, the\ntemperature is dynamically adjusted based on instantaneous entropy, enhancing\nthe diversity of stego texts and boosting embedding capacity. In the spatial\ndimension, the distribution is aligned with guidance from the target domain\ncorpus, closely mimicking real cover text in the target domain. By reforming\nthe distribution, FreStega enhances the imperceptibility of stego text in\npractical scenarios and improves steganographic capacity by 15.41\\%, all\nwithout compromising the quality of the generated text. FreStega serves as a\nplug-and-play remedy to enhance the imperceptibility and embedding capacity of\nexisting distribution-preserving steganography methods in real-world scenarios."
                },
                "authors": [
                    {
                        "name": "Kaiyi Pang"
                    }
                ],
                "author_detail": {
                    "name": "Kaiyi Pang"
                },
                "author": "Kaiyi Pang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12468v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12468v2",
                "updated": "2024-12-27T13:52:05Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    13,
                    52,
                    5,
                    4,
                    362,
                    0
                ],
                "published": "2024-10-16T11:33:57Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    11,
                    33,
                    57,
                    2,
                    290,
                    0
                ],
                "title": "Evaluating Software Development Agents: Patch Patterns, Code Quality,\n  and Issue Complexity in Real-World GitHub Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Software Development Agents: Patch Patterns, Code Quality,\n  and Issue Complexity in Real-World GitHub Scenarios"
                },
                "summary": "In recent years, AI-based software engineering has progressed from\npre-trained models to advanced agentic workflows, with Software Development\nAgents representing the next major leap. These agents, capable of reasoning,\nplanning, and interacting with external environments, offer promising solutions\nto complex software engineering tasks. However, while much research has\nevaluated code generated by large language models (LLMs), comprehensive studies\non agent-generated patches, particularly in real-world settings, are lacking.\nThis study addresses that gap by evaluating 4,892 patches from 10 top-ranked\nagents on 500 real-world GitHub issues from SWE-Bench Verified, focusing on\ntheir impact on code quality. Our analysis shows no single agent dominated,\nwith 170 issues unresolved, indicating room for improvement. Even for patches\nthat passed unit tests and resolved issues, agents made different file and\nfunction modifications compared to the gold patches from repository developers,\nrevealing limitations in the benchmark's test case coverage. Most agents\nmaintained code reliability and security, avoiding new bugs or vulnerabilities;\nwhile some agents increased code complexity, many reduced code duplication and\nminimized code smells. Finally, agents performed better on simpler codebases,\nsuggesting that breaking complex tasks into smaller sub-tasks could improve\neffectiveness. This study provides the first comprehensive evaluation of\nagent-generated patches on real-world GitHub issues, offering insights to\nadvance AI-driven software development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, AI-based software engineering has progressed from\npre-trained models to advanced agentic workflows, with Software Development\nAgents representing the next major leap. These agents, capable of reasoning,\nplanning, and interacting with external environments, offer promising solutions\nto complex software engineering tasks. However, while much research has\nevaluated code generated by large language models (LLMs), comprehensive studies\non agent-generated patches, particularly in real-world settings, are lacking.\nThis study addresses that gap by evaluating 4,892 patches from 10 top-ranked\nagents on 500 real-world GitHub issues from SWE-Bench Verified, focusing on\ntheir impact on code quality. Our analysis shows no single agent dominated,\nwith 170 issues unresolved, indicating room for improvement. Even for patches\nthat passed unit tests and resolved issues, agents made different file and\nfunction modifications compared to the gold patches from repository developers,\nrevealing limitations in the benchmark's test case coverage. Most agents\nmaintained code reliability and security, avoiding new bugs or vulnerabilities;\nwhile some agents increased code complexity, many reduced code duplication and\nminimized code smells. Finally, agents performed better on simpler codebases,\nsuggesting that breaking complex tasks into smaller sub-tasks could improve\neffectiveness. This study provides the first comprehensive evaluation of\nagent-generated patches on real-world GitHub issues, offering insights to\nadvance AI-driven software development."
                },
                "authors": [
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Lingxiao Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Lingxiao Jiang"
                },
                "author": "Lingxiao Jiang",
                "arxiv_comment": "Paper accepted to the SANER 2025 Conference Research Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12468v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12468v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19637v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19637v1",
                "updated": "2024-12-27T13:31:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    13,
                    31,
                    55,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T13:31:55Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    13,
                    31,
                    55,
                    4,
                    362,
                    0
                ],
                "title": "ReNeg: Learning Negative Embedding with Reward Guidance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReNeg: Learning Negative Embedding with Reward Guidance"
                },
                "summary": "In text-to-image (T2I) generation applications, negative embeddings have\nproven to be a simple yet effective approach for enhancing generation quality.\nTypically, these negative embeddings are derived from user-defined negative\nprompts, which, while being functional, are not necessarily optimal. In this\npaper, we introduce ReNeg, an end-to-end method designed to learn improved\nNegative embeddings guided by a Reward model. We employ a reward feedback\nlearning framework and integrate classifier-free guidance (CFG) into the\ntraining process, which was previously utilized only during inference, thus\nenabling the effective learning of negative embeddings. We also propose two\nstrategies for learning both global and per-sample negative embeddings.\nExtensive experiments show that the learned negative embedding significantly\noutperforms null-text and handcrafted counterparts, achieving substantial\nimprovements in human preference alignment. Additionally, the negative\nembedding learned within the same text embedding space exhibits strong\ngeneralization capabilities. For example, using the same CLIP text encoder, the\nnegative embedding learned on SD1.5 can be seamlessly transferred to\ntext-to-image or even text-to-video models such as ControlNet, ZeroScope, and\nVideoCrafter2, resulting in consistent performance improvements across the\nboard.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In text-to-image (T2I) generation applications, negative embeddings have\nproven to be a simple yet effective approach for enhancing generation quality.\nTypically, these negative embeddings are derived from user-defined negative\nprompts, which, while being functional, are not necessarily optimal. In this\npaper, we introduce ReNeg, an end-to-end method designed to learn improved\nNegative embeddings guided by a Reward model. We employ a reward feedback\nlearning framework and integrate classifier-free guidance (CFG) into the\ntraining process, which was previously utilized only during inference, thus\nenabling the effective learning of negative embeddings. We also propose two\nstrategies for learning both global and per-sample negative embeddings.\nExtensive experiments show that the learned negative embedding significantly\noutperforms null-text and handcrafted counterparts, achieving substantial\nimprovements in human preference alignment. Additionally, the negative\nembedding learned within the same text embedding space exhibits strong\ngeneralization capabilities. For example, using the same CLIP text encoder, the\nnegative embedding learned on SD1.5 can be seamlessly transferred to\ntext-to-image or even text-to-video models such as ControlNet, ZeroScope, and\nVideoCrafter2, resulting in consistent performance improvements across the\nboard."
                },
                "authors": [
                    {
                        "name": "Xiaomin Li"
                    },
                    {
                        "name": "Yixuan Liu"
                    },
                    {
                        "name": "Takashi Isobe"
                    },
                    {
                        "name": "Xu Jia"
                    },
                    {
                        "name": "Qinpeng Cui"
                    },
                    {
                        "name": "Dong Zhou"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "You He"
                    },
                    {
                        "name": "Huchuan Lu"
                    },
                    {
                        "name": "Zhongdao Wang"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19637v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19637v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02572v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02572v4",
                "updated": "2024-12-27T13:29:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    13,
                    29,
                    14,
                    4,
                    362,
                    0
                ],
                "published": "2024-09-04T09:46:33Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    9,
                    46,
                    33,
                    2,
                    248,
                    0
                ],
                "title": "GenDFIR: Advancing Cyber Incident Timeline Analysis Through Retrieval\n  Augmented Generation and Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenDFIR: Advancing Cyber Incident Timeline Analysis Through Retrieval\n  Augmented Generation and Large Language Models"
                },
                "summary": "Cyber timeline analysis, or forensic timeline analysis, is crucial in Digital\nForensics and Incident Response (DFIR). It examines artefacts and events\nparticularly timestamps and metadata to detect anomalies, establish\ncorrelations, and reconstruct incident timelines. Traditional methods rely on\nstructured artefacts, such as logs and filesystem metadata, using specialised\ntools for evidence identification and feature extraction. This paper introduces\nGenDFIR, a framework leveraging large language models (LLMs), specifically\nLlama 3.1 8B in zero shot mode, integrated with a Retrieval-Augmented\nGeneration (RAG) agent. Incident data is preprocessed into a structured\nknowledge base, enabling the RAG agent to retrieve relevant events based on\nuser prompts. The LLM interprets this context, offering semantic enrichment.\nTested on synthetic data in a controlled environment, results demonstrate\nGenDFIR's reliability and robustness, showcasing LLMs potential to automate\ntimeline analysis and advance threat detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cyber timeline analysis, or forensic timeline analysis, is crucial in Digital\nForensics and Incident Response (DFIR). It examines artefacts and events\nparticularly timestamps and metadata to detect anomalies, establish\ncorrelations, and reconstruct incident timelines. Traditional methods rely on\nstructured artefacts, such as logs and filesystem metadata, using specialised\ntools for evidence identification and feature extraction. This paper introduces\nGenDFIR, a framework leveraging large language models (LLMs), specifically\nLlama 3.1 8B in zero shot mode, integrated with a Retrieval-Augmented\nGeneration (RAG) agent. Incident data is preprocessed into a structured\nknowledge base, enabling the RAG agent to retrieve relevant events based on\nuser prompts. The LLM interprets this context, offering semantic enrichment.\nTested on synthetic data in a controlled environment, results demonstrate\nGenDFIR's reliability and robustness, showcasing LLMs potential to automate\ntimeline analysis and advance threat detection."
                },
                "authors": [
                    {
                        "name": "Fatma Yasmine Loumachi"
                    },
                    {
                        "name": "Mohamed Chahine Ghanem"
                    },
                    {
                        "name": "Mohamed Amine Ferrag"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Amine Ferrag"
                },
                "author": "Mohamed Amine Ferrag",
                "arxiv_comment": "24 pages V5.3",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02572v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02572v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19630v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19630v1",
                "updated": "2024-12-27T13:19:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    13,
                    19,
                    35,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T13:19:35Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    13,
                    19,
                    35,
                    4,
                    362,
                    0
                ],
                "title": "IMTP: Search-based Code Generation for In-memory Tensor Programs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IMTP: Search-based Code Generation for In-memory Tensor Programs"
                },
                "summary": "Processing-in-DRAM (DRAM-PIM) has emerged as a promising technology for\naccelerating memory-intensive operations in modern applications, such as Large\nLanguage Models (LLMs). Despite its potential, current software stacks for\nDRAM-PIM face significant challenges, including reliance on hand-tuned\nlibraries that hinder programmability, limited support for high-level\nabstractions, and the lack of systematic optimization frameworks. To address\nthese limitations, we present IMTP, a search-based optimizing tensor compiler\nfor UPMEM. Key features of IMTP include: (1) automated searches of the joint\nsearch space for host and kernel tensor programs, (2) PIM-aware optimizations\nfor efficiently handling boundary conditions, and (3) improved search\nalgorithms for the expanded search space of UPMEM systems. Our experimental\nresults on UPMEM hardware demonstrate performance gains of up to 8.21x for\nvarious UPMEM benchmark kernels and 5.33x for GPT-J layers. To the best of our\nknowledge, IMTP is the first tensor compiler to provide fully automated,\nautotuning-integrated code generation support for a DRAM-PIM system. By\nbridging the gap between high-level tensor computation abstractions and\nlow-level hardware-specific requirements, IMTP establishes a foundation for\nadvancing DRAM-PIM programmability and enabling streamlined optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing-in-DRAM (DRAM-PIM) has emerged as a promising technology for\naccelerating memory-intensive operations in modern applications, such as Large\nLanguage Models (LLMs). Despite its potential, current software stacks for\nDRAM-PIM face significant challenges, including reliance on hand-tuned\nlibraries that hinder programmability, limited support for high-level\nabstractions, and the lack of systematic optimization frameworks. To address\nthese limitations, we present IMTP, a search-based optimizing tensor compiler\nfor UPMEM. Key features of IMTP include: (1) automated searches of the joint\nsearch space for host and kernel tensor programs, (2) PIM-aware optimizations\nfor efficiently handling boundary conditions, and (3) improved search\nalgorithms for the expanded search space of UPMEM systems. Our experimental\nresults on UPMEM hardware demonstrate performance gains of up to 8.21x for\nvarious UPMEM benchmark kernels and 5.33x for GPT-J layers. To the best of our\nknowledge, IMTP is the first tensor compiler to provide fully automated,\nautotuning-integrated code generation support for a DRAM-PIM system. By\nbridging the gap between high-level tensor computation abstractions and\nlow-level hardware-specific requirements, IMTP establishes a foundation for\nadvancing DRAM-PIM programmability and enabling streamlined optimization."
                },
                "authors": [
                    {
                        "name": "Yongwon Shin"
                    },
                    {
                        "name": "Dookyung Kang"
                    },
                    {
                        "name": "Hyojin Sung"
                    }
                ],
                "author_detail": {
                    "name": "Hyojin Sung"
                },
                "author": "Hyojin Sung",
                "arxiv_comment": "13 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19630v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19630v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19622v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19622v1",
                "updated": "2024-12-27T12:49:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    12,
                    49,
                    3,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T12:49:03Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    12,
                    49,
                    3,
                    4,
                    362,
                    0
                ],
                "title": "Signatures of prediction during natural listening in MEG data?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Signatures of prediction during natural listening in MEG data?"
                },
                "summary": "The brain uses contextual information and prior knowledge to anticipate\nupcoming content during language comprehension. Recent research has shown\npredictive signals can be revealed in pre-onset ECoG activity during\nnaturalistic narrative listening, by building encoding models based on word\nembeddings from Large Language Models (LLMs). Similarly, evidence for\nlong-range predictive encoding has been observed in fMRI data, where\nincorporating embeddings for multiple upcoming words in a narrative improves\nalignment with brain activity. This study examines whether similar predictive\ninformation can be detected in MEG, a technique with higher temporal resolution\nthan fMRI but a lower signal-to-noise ratio than ECoG. Our findings indicate\nthat MEG captures pre-onset representations up to 1 second before word onset,\nconsistent with ECoG results. However, unlike fMRI findings, incorporating\nfuture word embeddings did not enhance MEG encoding, even for one word into the\nfuture, which suggests that the pre-onset encoding may not reflect predictive\nprocessing. This work demonstrates that MEG combined with LLMs is a valuable\napproach for studying language processing in naturalistic narratives and\nhighlights the need to study further what constitutes evidence for prediction\nduring natural listening.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The brain uses contextual information and prior knowledge to anticipate\nupcoming content during language comprehension. Recent research has shown\npredictive signals can be revealed in pre-onset ECoG activity during\nnaturalistic narrative listening, by building encoding models based on word\nembeddings from Large Language Models (LLMs). Similarly, evidence for\nlong-range predictive encoding has been observed in fMRI data, where\nincorporating embeddings for multiple upcoming words in a narrative improves\nalignment with brain activity. This study examines whether similar predictive\ninformation can be detected in MEG, a technique with higher temporal resolution\nthan fMRI but a lower signal-to-noise ratio than ECoG. Our findings indicate\nthat MEG captures pre-onset representations up to 1 second before word onset,\nconsistent with ECoG results. However, unlike fMRI findings, incorporating\nfuture word embeddings did not enhance MEG encoding, even for one word into the\nfuture, which suggests that the pre-onset encoding may not reflect predictive\nprocessing. This work demonstrates that MEG combined with LLMs is a valuable\napproach for studying language processing in naturalistic narratives and\nhighlights the need to study further what constitutes evidence for prediction\nduring natural listening."
                },
                "authors": [
                    {
                        "name": "Sahel Azizpour"
                    },
                    {
                        "name": "Britta U. Westner"
                    },
                    {
                        "name": "Jakub Szewczyk"
                    },
                    {
                        "name": "Umut Güçlü"
                    },
                    {
                        "name": "Linda Geerligs"
                    }
                ],
                "author_detail": {
                    "name": "Linda Geerligs"
                },
                "author": "Linda Geerligs",
                "arxiv_comment": "12 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19622v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19622v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19616v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19616v1",
                "updated": "2024-12-27T12:23:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    12,
                    23,
                    39,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T12:23:39Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    12,
                    23,
                    39,
                    4,
                    362,
                    0
                ],
                "title": "Gradient Weight-normalized Low-rank Projection for Efficient LLM\n  Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gradient Weight-normalized Low-rank Projection for Efficient LLM\n  Training"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable performance across various\ntasks, but the escalating demands on computational resources pose significant\nchallenges, particularly in the extensive utilization of full fine-tuning for\ndownstream tasks. To address this, parameter-efficient fine-tuning (PEFT)\nmethods have been developed, but they often underperform compared to full\nfine-tuning and struggle with memory efficiency. In this work, we introduce\nGradient Weight-Normalized Low-Rank Projection (GradNormLoRP), a novel approach\nthat enhances both parameter and memory efficiency while maintaining comparable\nperformance to full fine-tuning. GradNormLoRP normalizes the weight matrix to\nimprove gradient conditioning, facilitating better convergence during\noptimization. Additionally, it applies low-rank approximations to the weight\nand gradient matrices, significantly reducing memory usage during training.\nExtensive experiments demonstrate that our 8-bit GradNormLoRP reduces optimizer\nmemory usage by up to 89.5% and enables the pre-training of large LLMs, such as\nLLaMA 7B, on consumer-level GPUs like the NVIDIA RTX 4090, without additional\ninference costs. Moreover, GradNormLoRP outperforms existing low-rank methods\nin fine-tuning tasks. For instance, when fine-tuning the RoBERTa model on all\nGLUE tasks with a rank of 8, GradNormLoRP achieves an average score of 80.65,\nsurpassing LoRA's score of 79.23. These results underscore GradNormLoRP as a\npromising alternative for efficient LLM pre-training and fine-tuning. Source\ncode and Appendix:\nhttps://github.com/Jhhuangkay/Gradient-Weight-normalized-Low-rank-Projection-for-Efficient-LLM-Training",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable performance across various\ntasks, but the escalating demands on computational resources pose significant\nchallenges, particularly in the extensive utilization of full fine-tuning for\ndownstream tasks. To address this, parameter-efficient fine-tuning (PEFT)\nmethods have been developed, but they often underperform compared to full\nfine-tuning and struggle with memory efficiency. In this work, we introduce\nGradient Weight-Normalized Low-Rank Projection (GradNormLoRP), a novel approach\nthat enhances both parameter and memory efficiency while maintaining comparable\nperformance to full fine-tuning. GradNormLoRP normalizes the weight matrix to\nimprove gradient conditioning, facilitating better convergence during\noptimization. Additionally, it applies low-rank approximations to the weight\nand gradient matrices, significantly reducing memory usage during training.\nExtensive experiments demonstrate that our 8-bit GradNormLoRP reduces optimizer\nmemory usage by up to 89.5% and enables the pre-training of large LLMs, such as\nLLaMA 7B, on consumer-level GPUs like the NVIDIA RTX 4090, without additional\ninference costs. Moreover, GradNormLoRP outperforms existing low-rank methods\nin fine-tuning tasks. For instance, when fine-tuning the RoBERTa model on all\nGLUE tasks with a rank of 8, GradNormLoRP achieves an average score of 80.65,\nsurpassing LoRA's score of 79.23. These results underscore GradNormLoRP as a\npromising alternative for efficient LLM pre-training and fine-tuning. Source\ncode and Appendix:\nhttps://github.com/Jhhuangkay/Gradient-Weight-normalized-Low-rank-Projection-for-Efficient-LLM-Training"
                },
                "authors": [
                    {
                        "name": "Jia-Hong Huang"
                    },
                    {
                        "name": "Yixian Shen"
                    },
                    {
                        "name": "Hongyi Zhu"
                    },
                    {
                        "name": "Stevan Rudinac"
                    },
                    {
                        "name": "Evangelos Kanoulas"
                    }
                ],
                "author_detail": {
                    "name": "Evangelos Kanoulas"
                },
                "author": "Evangelos Kanoulas",
                "arxiv_comment": "Accepted by the 39th AAAI Conference on Artificial Intelligence\n  (AAAI-25) [Main Technical Track]",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19616v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19616v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19610v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19610v1",
                "updated": "2024-12-27T12:11:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    12,
                    11,
                    50,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T12:11:50Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    12,
                    11,
                    50,
                    4,
                    362,
                    0
                ],
                "title": "Machine Generated Product Advertisements: Benchmarking LLMs Against\n  Human Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine Generated Product Advertisements: Benchmarking LLMs Against\n  Human Performance"
                },
                "summary": "This study compares the performance of AI-generated and human-written product\ndescriptions using a multifaceted evaluation model. We analyze descriptions for\n100 products generated by four AI models (Gemma 2B, LLAMA, GPT2, and ChatGPT 4)\nwith and without sample descriptions, against human-written descriptions. Our\nevaluation metrics include sentiment, readability, persuasiveness, Search\nEngine Optimization(SEO), clarity, emotional appeal, and call-to-action\neffectiveness. The results indicate that ChatGPT 4 performs the best. In\ncontrast, other models demonstrate significant shortcomings, producing\nincoherent and illogical output that lacks logical structure and contextual\nrelevance. These models struggle to maintain focus on the product being\ndescribed, resulting in disjointed sentences that do not convey meaningful\ninformation. This research provides insights into the current capabilities and\nlimitations of AI in the creation of content for e-Commerce.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study compares the performance of AI-generated and human-written product\ndescriptions using a multifaceted evaluation model. We analyze descriptions for\n100 products generated by four AI models (Gemma 2B, LLAMA, GPT2, and ChatGPT 4)\nwith and without sample descriptions, against human-written descriptions. Our\nevaluation metrics include sentiment, readability, persuasiveness, Search\nEngine Optimization(SEO), clarity, emotional appeal, and call-to-action\neffectiveness. The results indicate that ChatGPT 4 performs the best. In\ncontrast, other models demonstrate significant shortcomings, producing\nincoherent and illogical output that lacks logical structure and contextual\nrelevance. These models struggle to maintain focus on the product being\ndescribed, resulting in disjointed sentences that do not convey meaningful\ninformation. This research provides insights into the current capabilities and\nlimitations of AI in the creation of content for e-Commerce."
                },
                "authors": [
                    {
                        "name": "Sanjukta Ghosh"
                    }
                ],
                "author_detail": {
                    "name": "Sanjukta Ghosh"
                },
                "author": "Sanjukta Ghosh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19610v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19610v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19595v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19595v1",
                "updated": "2024-12-27T11:33:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    11,
                    33,
                    19,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T11:33:19Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    11,
                    33,
                    19,
                    4,
                    362,
                    0
                ],
                "title": "SocRATES: Towards Automated Scenario-based Testing of Social Navigation\n  Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SocRATES: Towards Automated Scenario-based Testing of Social Navigation\n  Algorithms"
                },
                "summary": "Current social navigation methods and benchmarks primarily focus on proxemics\nand task efficiency. While these factors are important, qualitative aspects\nsuch as perceptions of a robot's social competence are equally crucial for\nsuccessful adoption and integration into human environments. We propose a more\ncomprehensive evaluation of social navigation through scenario-based testing,\nwhere specific human-robot interaction scenarios can reveal key robot\nbehaviors. However, creating such scenarios is often labor-intensive and\ncomplex. In this work, we address this challenge by introducing a pipeline that\nautomates the generation of context-, and location-appropriate social\nnavigation scenarios, ready for simulation. Our pipeline transforms simple\nscenario metadata into detailed textual scenarios, infers pedestrian and robot\ntrajectories, and simulates pedestrian behaviors, which enables more controlled\nevaluation. We leverage the social reasoning and code-generation capabilities\nof Large Language Models (LLMs) to streamline scenario generation and\ntranslation. Our experiments show that our pipeline produces realistic\nscenarios and significantly improves scenario translation over naive LLM\nprompting. Additionally, we present initial feedback from a usability study\nwith social navigation experts and a case-study demonstrating a scenario-based\nevaluation of three navigation algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current social navigation methods and benchmarks primarily focus on proxemics\nand task efficiency. While these factors are important, qualitative aspects\nsuch as perceptions of a robot's social competence are equally crucial for\nsuccessful adoption and integration into human environments. We propose a more\ncomprehensive evaluation of social navigation through scenario-based testing,\nwhere specific human-robot interaction scenarios can reveal key robot\nbehaviors. However, creating such scenarios is often labor-intensive and\ncomplex. In this work, we address this challenge by introducing a pipeline that\nautomates the generation of context-, and location-appropriate social\nnavigation scenarios, ready for simulation. Our pipeline transforms simple\nscenario metadata into detailed textual scenarios, infers pedestrian and robot\ntrajectories, and simulates pedestrian behaviors, which enables more controlled\nevaluation. We leverage the social reasoning and code-generation capabilities\nof Large Language Models (LLMs) to streamline scenario generation and\ntranslation. Our experiments show that our pipeline produces realistic\nscenarios and significantly improves scenario translation over naive LLM\nprompting. Additionally, we present initial feedback from a usability study\nwith social navigation experts and a case-study demonstrating a scenario-based\nevaluation of three navigation algorithms."
                },
                "authors": [
                    {
                        "name": "Shashank Rao Marpally"
                    },
                    {
                        "name": "Pranav Goyal"
                    },
                    {
                        "name": "Harold Soh"
                    }
                ],
                "author_detail": {
                    "name": "Harold Soh"
                },
                "author": "Harold Soh",
                "arxiv_comment": "7 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19595v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19595v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19587v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19587v1",
                "updated": "2024-12-27T11:14:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    11,
                    14,
                    11,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T11:14:11Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    11,
                    14,
                    11,
                    4,
                    362,
                    0
                ],
                "title": "Goal-oriented Communications based on Recursive Early Exit Neural\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Goal-oriented Communications based on Recursive Early Exit Neural\n  Networks"
                },
                "summary": "This paper presents a novel framework for goal-oriented semantic\ncommunications leveraging recursive early exit models. The proposed approach is\nbuilt on two key components. First, we introduce an innovative early exit\nstrategy that dynamically partitions computations, enabling samples to be\noffloaded to a server based on layer-wise recursive prediction dynamics that\ndetect samples for which the confidence is not increasing fast enough over\nlayers. Second, we develop a Reinforcement Learning-based online optimization\nframework that jointly determines early exit points, computation splitting, and\noffloading strategies, while accounting for wireless conditions, inference\naccuracy, and resource costs. Numerical evaluations in an edge inference\nscenario demonstrate the method's adaptability and effectiveness in striking an\nexcellent trade-off between performance, latency, and resource efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a novel framework for goal-oriented semantic\ncommunications leveraging recursive early exit models. The proposed approach is\nbuilt on two key components. First, we introduce an innovative early exit\nstrategy that dynamically partitions computations, enabling samples to be\noffloaded to a server based on layer-wise recursive prediction dynamics that\ndetect samples for which the confidence is not increasing fast enough over\nlayers. Second, we develop a Reinforcement Learning-based online optimization\nframework that jointly determines early exit points, computation splitting, and\noffloading strategies, while accounting for wireless conditions, inference\naccuracy, and resource costs. Numerical evaluations in an edge inference\nscenario demonstrate the method's adaptability and effectiveness in striking an\nexcellent trade-off between performance, latency, and resource efficiency."
                },
                "authors": [
                    {
                        "name": "Jary Pomponi"
                    },
                    {
                        "name": "Mattia Merluzzi"
                    },
                    {
                        "name": "Alessio Devoto"
                    },
                    {
                        "name": "Mateus Pontes Mota"
                    },
                    {
                        "name": "Paolo Di Lorenzo"
                    },
                    {
                        "name": "Simone Scardapane"
                    }
                ],
                "author_detail": {
                    "name": "Simone Scardapane"
                },
                "author": "Simone Scardapane",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19587v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19587v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04920v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04920v2",
                "updated": "2024-12-27T10:50:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    10,
                    50,
                    33,
                    4,
                    362,
                    0
                ],
                "published": "2024-05-08T09:42:08Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    9,
                    42,
                    8,
                    2,
                    129,
                    0
                ],
                "title": "Impact of phylogeny on the inference of functional sectors from protein\n  sequence data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Impact of phylogeny on the inference of functional sectors from protein\n  sequence data"
                },
                "summary": "Statistical analysis of multiple sequence alignments of homologous proteins\nhas revealed groups of coevolving amino acids called sectors. These groups of\namino-acid sites feature collective correlations in their amino-acid usage, and\nthey are associated to functional properties. Modeling showed that nonlinear\nselection on an additive functional trait of a protein is generically expected\nto give rise to a functional sector. These modeling results motivated a\nprincipled method, called ICOD, which is designed to identify functional\nsectors, as well as mutational effects, from sequence data. However, a\nchallenge for all methods aiming to identify sectors from multiple sequence\nalignments is that correlations in amino-acid usage can also arise from the\nmere fact that homologous sequences share common ancestry, i.e. from phylogeny.\nHere, we generate controlled synthetic data from a minimal model comprising\nboth phylogeny and functional sectors. We use this data to dissect the impact\nof phylogeny on sector identification and on mutational effect inference by\ndifferent methods. We find that ICOD is most robust to phylogeny, but that\nconservation is also quite robust. Next, we consider natural multiple sequence\nalignments of protein families for which deep mutational scan experimental data\nis available. We show that in this natural data, conservation and ICOD best\nidentify sites with strong functional roles, in agreement with our results on\nsynthetic data. Importantly, these two methods have different premises, since\nthey respectively focus on conservation and on correlations. Thus, their joint\nuse can reveal complementary information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical analysis of multiple sequence alignments of homologous proteins\nhas revealed groups of coevolving amino acids called sectors. These groups of\namino-acid sites feature collective correlations in their amino-acid usage, and\nthey are associated to functional properties. Modeling showed that nonlinear\nselection on an additive functional trait of a protein is generically expected\nto give rise to a functional sector. These modeling results motivated a\nprincipled method, called ICOD, which is designed to identify functional\nsectors, as well as mutational effects, from sequence data. However, a\nchallenge for all methods aiming to identify sectors from multiple sequence\nalignments is that correlations in amino-acid usage can also arise from the\nmere fact that homologous sequences share common ancestry, i.e. from phylogeny.\nHere, we generate controlled synthetic data from a minimal model comprising\nboth phylogeny and functional sectors. We use this data to dissect the impact\nof phylogeny on sector identification and on mutational effect inference by\ndifferent methods. We find that ICOD is most robust to phylogeny, but that\nconservation is also quite robust. Next, we consider natural multiple sequence\nalignments of protein families for which deep mutational scan experimental data\nis available. We show that in this natural data, conservation and ICOD best\nidentify sites with strong functional roles, in agreement with our results on\nsynthetic data. Importantly, these two methods have different premises, since\nthey respectively focus on conservation and on correlations. Thus, their joint\nuse can reveal complementary information."
                },
                "authors": [
                    {
                        "name": "Nicola Dietler"
                    },
                    {
                        "name": "Alia Abbara"
                    },
                    {
                        "name": "Subham Choudhury"
                    },
                    {
                        "name": "Anne-Florence Bitbol"
                    }
                ],
                "author_detail": {
                    "name": "Anne-Florence Bitbol"
                },
                "author": "Anne-Florence Bitbol",
                "arxiv_doi": "10.1371/journal.pcbi.1012091",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1371/journal.pcbi.1012091",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.04920v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04920v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "40 pages, 27 figures",
                "arxiv_journal_ref": "PLoS Comput. Biol. 20(9): e1012091 (2024)",
                "arxiv_primary_category": {
                    "term": "q-bio.PE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19576v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19576v1",
                "updated": "2024-12-27T10:46:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    10,
                    46,
                    13,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T10:46:13Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    10,
                    46,
                    13,
                    4,
                    362,
                    0
                ],
                "title": "Hybrid Population Monte Carlo",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Population Monte Carlo"
                },
                "summary": "Importance sampling (IS) is a powerful Monte Carlo (MC) technique for\napproximating intractable integrals, for instance in Bayesian inference. The\nperformance of IS relies heavily on the appropriate choice of the so-called\nproposal distribution. Adaptive IS (AIS) methods iteratively improve target\nestimates by adapting the proposal distribution. Recent AIS research focuses on\nenhancing proposal adaptation for high-dimensional problems, while addressing\nthe challenge of multi-modal targets. In this paper, a new class of AIS methods\nis presented, utilizing a hybrid approach that incorporates weighted samples\nand proposal distributions to enhance performance. This approach belongs to the\nfamily of population Monte Carlo (PMC) algorithms, where a population of\nproposals is adapted to better approximate the target distribution. The\nproposed hybrid population Monte Carlo (HPMC) implements a novel two-step\nadaptation mechanism. In the first step, a hybrid method is used to generate\nthe population of the preliminary proposal locations based on both weighted\nsamples and location parameters. We use Hamiltonian Monte Carlo (HMC) to\ngenerate the preliminary proposal locations. HMC has a good exploratory\nbehavior, especially in high dimension scenarios. In the second step, the novel\ncooperation algorithms are performing to find the final proposals for the next\niteration. HPMC achieves a significant performance improvement in\nhigh-dimensional problems when compared to the state-of-the-art algorithms. We\ndiscuss the statistical properties of HPMC and show its high performance in two\nchallenging benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Importance sampling (IS) is a powerful Monte Carlo (MC) technique for\napproximating intractable integrals, for instance in Bayesian inference. The\nperformance of IS relies heavily on the appropriate choice of the so-called\nproposal distribution. Adaptive IS (AIS) methods iteratively improve target\nestimates by adapting the proposal distribution. Recent AIS research focuses on\nenhancing proposal adaptation for high-dimensional problems, while addressing\nthe challenge of multi-modal targets. In this paper, a new class of AIS methods\nis presented, utilizing a hybrid approach that incorporates weighted samples\nand proposal distributions to enhance performance. This approach belongs to the\nfamily of population Monte Carlo (PMC) algorithms, where a population of\nproposals is adapted to better approximate the target distribution. The\nproposed hybrid population Monte Carlo (HPMC) implements a novel two-step\nadaptation mechanism. In the first step, a hybrid method is used to generate\nthe population of the preliminary proposal locations based on both weighted\nsamples and location parameters. We use Hamiltonian Monte Carlo (HMC) to\ngenerate the preliminary proposal locations. HMC has a good exploratory\nbehavior, especially in high dimension scenarios. In the second step, the novel\ncooperation algorithms are performing to find the final proposals for the next\niteration. HPMC achieves a significant performance improvement in\nhigh-dimensional problems when compared to the state-of-the-art algorithms. We\ndiscuss the statistical properties of HPMC and show its high performance in two\nchallenging benchmarks."
                },
                "authors": [
                    {
                        "name": "Ali Mousavi"
                    },
                    {
                        "name": "Víctor Elvira"
                    }
                ],
                "author_detail": {
                    "name": "Víctor Elvira"
                },
                "author": "Víctor Elvira",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19576v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19576v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19493v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19493v3",
                "updated": "2024-12-27T10:34:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    10,
                    34,
                    15,
                    4,
                    362,
                    0
                ],
                "published": "2024-07-28T13:23:43Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    13,
                    23,
                    43,
                    6,
                    210,
                    0
                ],
                "title": "Official-NV: An LLM-Generated News Video Dataset for Multimodal Fake\n  News Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Official-NV: An LLM-Generated News Video Dataset for Multimodal Fake\n  News Detection"
                },
                "summary": "News media, especially video news media, have penetrated into every aspect of\ndaily life, which also brings the risk of fake news. Therefore, multimodal fake\nnews detection has recently garnered increased attention. However, the existing\ndatasets are comprised of user-uploaded videos and contain an excess amounts of\nsuperfluous data, which introduces noise into the model training process. To\naddress this issue, we construct a dataset named Official-NV, comprising\nofficially published news videos. The crawl officially published videos are\naugmented through the use of LLMs-based generation and manual verification,\nthereby expanding the dataset. We also propose a new baseline model called\nOFNVD, which captures key information from multimodal features through a GLU\nattention mechanism and performs feature enhancement and modal aggregation via\na cross-modal Transformer. Benchmarking the dataset and baselines demonstrates\nthe effectiveness of our model in multimodal news detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "News media, especially video news media, have penetrated into every aspect of\ndaily life, which also brings the risk of fake news. Therefore, multimodal fake\nnews detection has recently garnered increased attention. However, the existing\ndatasets are comprised of user-uploaded videos and contain an excess amounts of\nsuperfluous data, which introduces noise into the model training process. To\naddress this issue, we construct a dataset named Official-NV, comprising\nofficially published news videos. The crawl officially published videos are\naugmented through the use of LLMs-based generation and manual verification,\nthereby expanding the dataset. We also propose a new baseline model called\nOFNVD, which captures key information from multimodal features through a GLU\nattention mechanism and performs feature enhancement and modal aggregation via\na cross-modal Transformer. Benchmarking the dataset and baselines demonstrates\nthe effectiveness of our model in multimodal news detection."
                },
                "authors": [
                    {
                        "name": "Yihao Wang"
                    },
                    {
                        "name": "Lizhi Chen"
                    },
                    {
                        "name": "Zhong Qian"
                    },
                    {
                        "name": "Peifeng Li"
                    }
                ],
                "author_detail": {
                    "name": "Peifeng Li"
                },
                "author": "Peifeng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19493v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19493v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14554v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14554v2",
                "updated": "2024-12-27T10:17:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    10,
                    17,
                    12,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-19T06:10:40Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    6,
                    10,
                    40,
                    3,
                    354,
                    0
                ],
                "title": "The Current Challenges of Software Engineering in the Era of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Current Challenges of Software Engineering in the Era of Large\n  Language Models"
                },
                "summary": "With the advent of large language models (LLMs) in the artificial\nintelligence (AI) area, the field of software engineering (SE) has also\nwitnessed a paradigm shift. These models, by leveraging the power of deep\nlearning and massive amounts of data, have demonstrated an unprecedented\ncapacity to understand, generate, and operate programming languages. They can\nassist developers in completing a broad spectrum of software development\nactivities, encompassing software design, automated programming, and\nmaintenance, which potentially reduces huge human efforts. Integrating LLMs\nwithin the SE landscape (LLM4SE) has become a burgeoning trend, necessitating\nexploring this emergent landscape's challenges and opportunities.\n  The paper aims at revisiting the software development life cycle (SDLC) under\nLLMs, and highlighting challenges and opportunities of the new paradigm. The\npaper first summarizes the overall process of LLM4SE, and then elaborates on\nthe current challenges based on a through discussion. The discussion was held\namong more than 20 participants from academia and industry, specializing in\nfields such as software engineering and artificial intelligence. Specifically,\nwe achieve 26 key challenges from seven aspects, including software requirement\n& design, coding assistance, testing code generation, code review, code\nmaintenance, software vulnerability management, and data, training, and\nevaluation. We hope the achieved challenges would benefit future research in\nthe LLM4SE field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advent of large language models (LLMs) in the artificial\nintelligence (AI) area, the field of software engineering (SE) has also\nwitnessed a paradigm shift. These models, by leveraging the power of deep\nlearning and massive amounts of data, have demonstrated an unprecedented\ncapacity to understand, generate, and operate programming languages. They can\nassist developers in completing a broad spectrum of software development\nactivities, encompassing software design, automated programming, and\nmaintenance, which potentially reduces huge human efforts. Integrating LLMs\nwithin the SE landscape (LLM4SE) has become a burgeoning trend, necessitating\nexploring this emergent landscape's challenges and opportunities.\n  The paper aims at revisiting the software development life cycle (SDLC) under\nLLMs, and highlighting challenges and opportunities of the new paradigm. The\npaper first summarizes the overall process of LLM4SE, and then elaborates on\nthe current challenges based on a through discussion. The discussion was held\namong more than 20 participants from academia and industry, specializing in\nfields such as software engineering and artificial intelligence. Specifically,\nwe achieve 26 key challenges from seven aspects, including software requirement\n& design, coding assistance, testing code generation, code review, code\nmaintenance, software vulnerability management, and data, training, and\nevaluation. We hope the achieved challenges would benefit future research in\nthe LLM4SE field."
                },
                "authors": [
                    {
                        "name": "Cuiyun Gao"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Shan Gao"
                    },
                    {
                        "name": "Xin Xia"
                    },
                    {
                        "name": "Zhi Jin"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Jin"
                },
                "author": "Zhi Jin",
                "arxiv_comment": "Accepted by ACM Transactions on Software Engineering and Methodology\n  (TOSEM)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14554v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14554v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.00326v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.00326v6",
                "updated": "2024-12-27T10:12:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    10,
                    12,
                    28,
                    4,
                    362,
                    0
                ],
                "published": "2023-12-01T03:44:54Z",
                "published_parsed": [
                    2023,
                    12,
                    1,
                    3,
                    44,
                    54,
                    4,
                    335,
                    0
                ],
                "title": "Agent-OM: Leveraging LLM Agents for Ontology Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent-OM: Leveraging LLM Agents for Ontology Matching"
                },
                "summary": "Ontology matching (OM) enables semantic interoperability between different\nontologies and resolves their conceptual heterogeneity by aligning related\nentities. OM systems currently have two prevailing design paradigms:\nconventional knowledge-based expert systems and newer machine learning-based\npredictive systems. While large language models (LLMs) and LLM agents have\nrevolutionised data engineering and have been applied creatively in many\ndomains, their potential for OM remains underexplored. This study introduces a\nnovel agent-powered LLM-based design paradigm for OM systems. With\nconsideration of several specific challenges in leveraging LLM agents for OM,\nwe propose a generic framework, namely Agent-OM (Agent for Ontology Matching),\nconsisting of two Siamese agents for retrieval and matching, with a set of OM\ntools. Our framework is implemented in a proof-of-concept system. Evaluations\nof three Ontology Alignment Evaluation Initiative (OAEI) tracks over\nstate-of-the-art OM systems show that our system can achieve results very close\nto the long-standing best performance on simple OM tasks and can significantly\nimprove the performance on complex and few-shot OM tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ontology matching (OM) enables semantic interoperability between different\nontologies and resolves their conceptual heterogeneity by aligning related\nentities. OM systems currently have two prevailing design paradigms:\nconventional knowledge-based expert systems and newer machine learning-based\npredictive systems. While large language models (LLMs) and LLM agents have\nrevolutionised data engineering and have been applied creatively in many\ndomains, their potential for OM remains underexplored. This study introduces a\nnovel agent-powered LLM-based design paradigm for OM systems. With\nconsideration of several specific challenges in leveraging LLM agents for OM,\nwe propose a generic framework, namely Agent-OM (Agent for Ontology Matching),\nconsisting of two Siamese agents for retrieval and matching, with a set of OM\ntools. Our framework is implemented in a proof-of-concept system. Evaluations\nof three Ontology Alignment Evaluation Initiative (OAEI) tracks over\nstate-of-the-art OM systems show that our system can achieve results very close\nto the long-standing best performance on simple OM tasks and can significantly\nimprove the performance on complex and few-shot OM tasks."
                },
                "authors": [
                    {
                        "name": "Zhangcheng Qiang"
                    },
                    {
                        "name": "Weiqing Wang"
                    },
                    {
                        "name": "Kerry Taylor"
                    }
                ],
                "author_detail": {
                    "name": "Kerry Taylor"
                },
                "author": "Kerry Taylor",
                "arxiv_comment": "19 pages, 12 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.00326v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.00326v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19562v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19562v1",
                "updated": "2024-12-27T10:05:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    10,
                    5,
                    45,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T10:05:45Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    10,
                    5,
                    45,
                    4,
                    362,
                    0
                ],
                "title": "Hindsight Planner: A Closed-Loop Few-Shot Planner for Embodied\n  Instruction Following",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hindsight Planner: A Closed-Loop Few-Shot Planner for Embodied\n  Instruction Following"
                },
                "summary": "This work focuses on building a task planner for Embodied Instruction\nFollowing (EIF) using Large Language Models (LLMs). Previous works typically\ntrain a planner to imitate expert trajectories, treating this as a supervised\ntask. While these methods achieve competitive performance, they often lack\nsufficient robustness. When a suboptimal action is taken, the planner may\nencounter an out-of-distribution state, which can lead to task failure. In\ncontrast, we frame the task as a Partially Observable Markov Decision Process\n(POMDP) and aim to develop a robust planner under a few-shot assumption. Thus,\nwe propose a closed-loop planner with an adaptation module and a novel\nhindsight method, aiming to use as much information as possible to assist the\nplanner. Our experiments on the ALFRED dataset indicate that our planner\nachieves competitive performance under a few-shot assumption. For the first\ntime, our few-shot agent's performance approaches and even surpasses that of\nthe full-shot supervised agent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work focuses on building a task planner for Embodied Instruction\nFollowing (EIF) using Large Language Models (LLMs). Previous works typically\ntrain a planner to imitate expert trajectories, treating this as a supervised\ntask. While these methods achieve competitive performance, they often lack\nsufficient robustness. When a suboptimal action is taken, the planner may\nencounter an out-of-distribution state, which can lead to task failure. In\ncontrast, we frame the task as a Partially Observable Markov Decision Process\n(POMDP) and aim to develop a robust planner under a few-shot assumption. Thus,\nwe propose a closed-loop planner with an adaptation module and a novel\nhindsight method, aiming to use as much information as possible to assist the\nplanner. Our experiments on the ALFRED dataset indicate that our planner\nachieves competitive performance under a few-shot assumption. For the first\ntime, our few-shot agent's performance approaches and even surpasses that of\nthe full-shot supervised agent."
                },
                "authors": [
                    {
                        "name": "Yuxiao Yang"
                    },
                    {
                        "name": "Shenao Zhang"
                    },
                    {
                        "name": "Zhihan Liu"
                    },
                    {
                        "name": "Huaxiu Yao"
                    },
                    {
                        "name": "Zhaoran Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhaoran Wang"
                },
                "author": "Zhaoran Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19562v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19562v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.15975v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.15975v3",
                "updated": "2024-12-27T09:52:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    9,
                    52,
                    15,
                    4,
                    362,
                    0
                ],
                "published": "2023-12-26T10:02:10Z",
                "published_parsed": [
                    2023,
                    12,
                    26,
                    10,
                    2,
                    10,
                    1,
                    360,
                    0
                ],
                "title": "Filtered data based estimators for stochastic processes driven by\n  colored noise",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Filtered data based estimators for stochastic processes driven by\n  colored noise"
                },
                "summary": "We consider the problem of estimating unknown parameters in stochastic\ndifferential equations driven by colored noise, which we model as a sequence of\nGaussian stationary processes with decreasing correlation time. We aim to infer\nparameters in the limit equation, driven by white noise, given observations of\nthe colored noise dynamics. We consider both the maximum likelihood and the\nstochastic gradient descent in continuous time estimators, and we propose to\nmodify them by including filtered data. We provide a convergence analysis for\nour estimators showing their asymptotic unbiasedness in a general setting and\nasymptotic normality under a simplified scenario.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the problem of estimating unknown parameters in stochastic\ndifferential equations driven by colored noise, which we model as a sequence of\nGaussian stationary processes with decreasing correlation time. We aim to infer\nparameters in the limit equation, driven by white noise, given observations of\nthe colored noise dynamics. We consider both the maximum likelihood and the\nstochastic gradient descent in continuous time estimators, and we propose to\nmodify them by including filtered data. We provide a convergence analysis for\nour estimators showing their asymptotic unbiasedness in a general setting and\nasymptotic normality under a simplified scenario."
                },
                "authors": [
                    {
                        "name": "Grigorios A. Pavliotis"
                    },
                    {
                        "name": "Sebastian Reich"
                    },
                    {
                        "name": "Andrea Zanoni"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Zanoni"
                },
                "author": "Andrea Zanoni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.15975v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.15975v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20333v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20333v3",
                "updated": "2024-12-27T09:24:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    9,
                    24,
                    45,
                    4,
                    362,
                    0
                ],
                "published": "2024-05-30T17:59:10Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    17,
                    59,
                    10,
                    3,
                    151,
                    0
                ],
                "title": "SurgiTrack: Fine-Grained Multi-Class Multi-Tool Tracking in Surgical\n  Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SurgiTrack: Fine-Grained Multi-Class Multi-Tool Tracking in Surgical\n  Videos"
                },
                "summary": "Accurate tool tracking is essential for the success of computer-assisted\nintervention. Previous efforts often modeled tool trajectories rigidly,\noverlooking the dynamic nature of surgical procedures, especially tracking\nscenarios like out-of-body and out-of-camera views. Addressing this limitation,\nthe new CholecTrack20 dataset provides detailed labels that account for\nmultiple tool trajectories in three perspectives: (1) intraoperative, (2)\nintracorporeal, and (3) visibility, representing the different types of\ntemporal duration of tool tracks. These fine-grained labels enhance tracking\nflexibility but also increase the task complexity. Re-identifying tools after\nocclusion or re-insertion into the body remains challenging due to high visual\nsimilarity, especially among tools of the same category. This work recognizes\nthe critical role of the tool operators in distinguishing tool track instances,\nespecially those belonging to the same tool category. The operators'\ninformation are however not explicitly captured in surgical videos. We\ntherefore propose SurgiTrack, a novel deep learning method that leverages\nYOLOv7 for precise tool detection and employs an attention mechanism to model\nthe originating direction of the tools, as a proxy to their operators, for tool\nre-identification. To handle diverse tool trajectory perspectives, SurgiTrack\nemploys a harmonizing bipartite matching graph, minimizing conflicts and\nensuring accurate tool identity association. Experimental results on\nCholecTrack20 demonstrate SurgiTrack's effectiveness, outperforming baselines\nand state-of-the-art methods with real-time inference capability. This work\nsets a new standard in surgical tool tracking, providing dynamic trajectories\nfor more adaptable and precise assistance in minimally invasive surgeries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate tool tracking is essential for the success of computer-assisted\nintervention. Previous efforts often modeled tool trajectories rigidly,\noverlooking the dynamic nature of surgical procedures, especially tracking\nscenarios like out-of-body and out-of-camera views. Addressing this limitation,\nthe new CholecTrack20 dataset provides detailed labels that account for\nmultiple tool trajectories in three perspectives: (1) intraoperative, (2)\nintracorporeal, and (3) visibility, representing the different types of\ntemporal duration of tool tracks. These fine-grained labels enhance tracking\nflexibility but also increase the task complexity. Re-identifying tools after\nocclusion or re-insertion into the body remains challenging due to high visual\nsimilarity, especially among tools of the same category. This work recognizes\nthe critical role of the tool operators in distinguishing tool track instances,\nespecially those belonging to the same tool category. The operators'\ninformation are however not explicitly captured in surgical videos. We\ntherefore propose SurgiTrack, a novel deep learning method that leverages\nYOLOv7 for precise tool detection and employs an attention mechanism to model\nthe originating direction of the tools, as a proxy to their operators, for tool\nre-identification. To handle diverse tool trajectory perspectives, SurgiTrack\nemploys a harmonizing bipartite matching graph, minimizing conflicts and\nensuring accurate tool identity association. Experimental results on\nCholecTrack20 demonstrate SurgiTrack's effectiveness, outperforming baselines\nand state-of-the-art methods with real-time inference capability. This work\nsets a new standard in surgical tool tracking, providing dynamic trajectories\nfor more adaptable and precise assistance in minimally invasive surgeries."
                },
                "authors": [
                    {
                        "name": "Chinedu Innocent Nwoye"
                    },
                    {
                        "name": "Nicolas Padoy"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Padoy"
                },
                "author": "Nicolas Padoy",
                "arxiv_doi": "10.1016/j.media.2024.103438",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.media.2024.103438",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.20333v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20333v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 7 figures, 7 tables, 1 video. Supplementary video available\n  at: https://vimeo.com/951853260 . Article published in Medical Image Analysis\n  Journal 2025",
                "arxiv_journal_ref": "Medical Image Analysis, Volume 101, Article 103438 (April 2025)",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19535v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19535v1",
                "updated": "2024-12-27T09:01:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    9,
                    1,
                    15,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T09:01:15Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    9,
                    1,
                    15,
                    4,
                    362,
                    0
                ],
                "title": "StyleRWKV: High-Quality and High-Efficiency Style Transfer with\n  RWKV-like Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StyleRWKV: High-Quality and High-Efficiency Style Transfer with\n  RWKV-like Architecture"
                },
                "summary": "Style transfer aims to generate a new image preserving the content but with\nthe artistic representation of the style source. Most of the existing methods\nare based on Transformers or diffusion models, however, they suffer from\nquadratic computational complexity and high inference time. RWKV, as an\nemerging deep sequence models, has shown immense potential for long-context\nsequence modeling in NLP tasks. In this work, we present a novel framework\nStyleRWKV, to achieve high-quality style transfer with limited memory usage and\nlinear time complexity. Specifically, we propose a Recurrent WKV (Re-WKV)\nattention mechanism, which incorporates bidirectional attention to establish a\nglobal receptive field. Additionally, we develop a Deformable Shifting\n(Deform-Shifting) layer that introduces learnable offsets to the sampling grid\nof the convolution kernel, allowing tokens to shift flexibly and adaptively\nfrom the region of interest, thereby enhancing the model's ability to capture\nlocal dependencies. Finally, we propose a Skip Scanning (S-Scanning) method\nthat effectively establishes global contextual dependencies. Extensive\nexperiments with analysis including qualitative and quantitative evaluations\ndemonstrate that our approach outperforms state-of-the-art methods in terms of\nstylization quality, model complexity, and inference efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Style transfer aims to generate a new image preserving the content but with\nthe artistic representation of the style source. Most of the existing methods\nare based on Transformers or diffusion models, however, they suffer from\nquadratic computational complexity and high inference time. RWKV, as an\nemerging deep sequence models, has shown immense potential for long-context\nsequence modeling in NLP tasks. In this work, we present a novel framework\nStyleRWKV, to achieve high-quality style transfer with limited memory usage and\nlinear time complexity. Specifically, we propose a Recurrent WKV (Re-WKV)\nattention mechanism, which incorporates bidirectional attention to establish a\nglobal receptive field. Additionally, we develop a Deformable Shifting\n(Deform-Shifting) layer that introduces learnable offsets to the sampling grid\nof the convolution kernel, allowing tokens to shift flexibly and adaptively\nfrom the region of interest, thereby enhancing the model's ability to capture\nlocal dependencies. Finally, we propose a Skip Scanning (S-Scanning) method\nthat effectively establishes global contextual dependencies. Extensive\nexperiments with analysis including qualitative and quantitative evaluations\ndemonstrate that our approach outperforms state-of-the-art methods in terms of\nstylization quality, model complexity, and inference efficiency."
                },
                "authors": [
                    {
                        "name": "Miaomiao Dai"
                    },
                    {
                        "name": "Qianyu Zhou"
                    },
                    {
                        "name": "Lizhuang Ma"
                    }
                ],
                "author_detail": {
                    "name": "Lizhuang Ma"
                },
                "author": "Lizhuang Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19535v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19535v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15465v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15465v3",
                "updated": "2024-12-27T08:57:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    8,
                    57,
                    26,
                    4,
                    362,
                    0
                ],
                "published": "2024-05-24T11:40:22Z",
                "published_parsed": [
                    2024,
                    5,
                    24,
                    11,
                    40,
                    22,
                    4,
                    145,
                    0
                ],
                "title": "Boost UAV-based Ojbect Detection via Scale-Invariant Feature\n  Disentanglement and Adversarial Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boost UAV-based Ojbect Detection via Scale-Invariant Feature\n  Disentanglement and Adversarial Learning"
                },
                "summary": "Detecting objects from Unmanned Aerial Vehicles (UAV) is often hindered by a\nlarge number of small objects, resulting in low detection accuracy. To address\nthis issue, mainstream approaches typically utilize multi-stage inferences.\nDespite their remarkable detecting accuracies, real-time efficiency is\nsacrificed, making them less practical to handle real applications. To this\nend, we propose to improve the single-stage inference accuracy through learning\nscale-invariant features. Specifically, a Scale-Invariant Feature Disentangling\nmodule is designed to disentangle scale-related and scale-invariant features.\nThen an Adversarial Feature Learning scheme is employed to enhance\ndisentanglement. Finally, scale-invariant features are leveraged for robust\nUAV-based object detection. Furthermore, we construct a multi-modal UAV object\ndetection dataset, State-Air, which incorporates annotated UAV state\nparameters. We apply our approach to three lightweight detection frameworks on\ntwo benchmark datasets. Extensive experiments demonstrate that our approach can\neffectively improve model accuracy and achieve state-of-the-art (SoTA)\nperformance on two datasets. Our code and dataset will be publicly available\nonce the paper is accepted.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting objects from Unmanned Aerial Vehicles (UAV) is often hindered by a\nlarge number of small objects, resulting in low detection accuracy. To address\nthis issue, mainstream approaches typically utilize multi-stage inferences.\nDespite their remarkable detecting accuracies, real-time efficiency is\nsacrificed, making them less practical to handle real applications. To this\nend, we propose to improve the single-stage inference accuracy through learning\nscale-invariant features. Specifically, a Scale-Invariant Feature Disentangling\nmodule is designed to disentangle scale-related and scale-invariant features.\nThen an Adversarial Feature Learning scheme is employed to enhance\ndisentanglement. Finally, scale-invariant features are leveraged for robust\nUAV-based object detection. Furthermore, we construct a multi-modal UAV object\ndetection dataset, State-Air, which incorporates annotated UAV state\nparameters. We apply our approach to three lightweight detection frameworks on\ntwo benchmark datasets. Extensive experiments demonstrate that our approach can\neffectively improve model accuracy and achieve state-of-the-art (SoTA)\nperformance on two datasets. Our code and dataset will be publicly available\nonce the paper is accepted."
                },
                "authors": [
                    {
                        "name": "Fan Liu"
                    },
                    {
                        "name": "Liang Yao"
                    },
                    {
                        "name": "Chuanyi Zhang"
                    },
                    {
                        "name": "Ting Wu"
                    },
                    {
                        "name": "Xinlei Zhang"
                    },
                    {
                        "name": "Xiruo Jiang"
                    },
                    {
                        "name": "Jun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhou"
                },
                "author": "Jun Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15465v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15465v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11843v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11843v2",
                "updated": "2024-12-27T08:32:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    8,
                    32,
                    38,
                    4,
                    362,
                    0
                ],
                "published": "2024-09-23T08:39:16Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    8,
                    39,
                    16,
                    0,
                    267,
                    0
                ],
                "title": "From Commands to Prompts: LLM-based Semantic File System for AIOS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Commands to Prompts: LLM-based Semantic File System for AIOS"
                },
                "summary": "Large language models (LLMs) have demonstrated significant potential in the\ndevelopment of intelligent applications and systems such as LLM-based agents\nand agent operating systems (AIOS). However, when these applications and\nsystems interact with the underlying file system, the file system still remains\nthe traditional paradigm: reliant on manual navigation through precise\ncommands. This paradigm poses a bottleneck to the usability of these systems as\nusers are required to navigate complex folder hierarchies and remember cryptic\nfile names. To address this limitation, we propose an LLM-based semantic file\nsystem ( LSFS ) for prompt-driven file management. Unlike conventional\napproaches, LSFS incorporates LLMs to enable users or agents to interact with\nfiles through natural language prompts, facilitating semantic file management.\nAt the macro-level, we develop a comprehensive API set to achieve semantic file\nmanagement functionalities, such as semantic file retrieval, file update\nmonitoring and summarization, and semantic file rollback). At the micro-level,\nwe store files by constructing semantic indexes for them, design and implement\nsyscalls of different semantic operations (e.g., CRUD, group by, join) powered\nby vector database. Our experiments show that LSFS offers significant\nimprovements over traditional file systems in terms of user convenience, the\ndiversity of supported functions, and the accuracy and efficiency of file\noperations. Additionally, with the integration of LLM, our system enables more\nintelligent file management tasks, such as content summarization and version\ncomparison, further enhancing its capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated significant potential in the\ndevelopment of intelligent applications and systems such as LLM-based agents\nand agent operating systems (AIOS). However, when these applications and\nsystems interact with the underlying file system, the file system still remains\nthe traditional paradigm: reliant on manual navigation through precise\ncommands. This paradigm poses a bottleneck to the usability of these systems as\nusers are required to navigate complex folder hierarchies and remember cryptic\nfile names. To address this limitation, we propose an LLM-based semantic file\nsystem ( LSFS ) for prompt-driven file management. Unlike conventional\napproaches, LSFS incorporates LLMs to enable users or agents to interact with\nfiles through natural language prompts, facilitating semantic file management.\nAt the macro-level, we develop a comprehensive API set to achieve semantic file\nmanagement functionalities, such as semantic file retrieval, file update\nmonitoring and summarization, and semantic file rollback). At the micro-level,\nwe store files by constructing semantic indexes for them, design and implement\nsyscalls of different semantic operations (e.g., CRUD, group by, join) powered\nby vector database. Our experiments show that LSFS offers significant\nimprovements over traditional file systems in terms of user convenience, the\ndiversity of supported functions, and the accuracy and efficiency of file\noperations. Additionally, with the integration of LLM, our system enables more\nintelligent file management tasks, such as content summarization and version\ncomparison, further enhancing its capabilities."
                },
                "authors": [
                    {
                        "name": "Zeru Shi"
                    },
                    {
                        "name": "Kai Mei"
                    },
                    {
                        "name": "Yongye Su"
                    },
                    {
                        "name": "Chaoji Zuo"
                    },
                    {
                        "name": "Wenyue Hua"
                    },
                    {
                        "name": "Wujiang Xu"
                    },
                    {
                        "name": "Yujie Ren"
                    },
                    {
                        "name": "Zirui Liu"
                    },
                    {
                        "name": "Mengnan Du"
                    },
                    {
                        "name": "Dong Deng"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11843v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11843v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19524v1",
                "updated": "2024-12-27T08:31:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    8,
                    31,
                    19,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T08:31:19Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    8,
                    31,
                    19,
                    4,
                    362,
                    0
                ],
                "title": "PLN and NARS Often Yield Similar strength $\\times$ confidence Given\n  Highly Uncertain Term Probabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PLN and NARS Often Yield Similar strength $\\times$ confidence Given\n  Highly Uncertain Term Probabilities"
                },
                "summary": "We provide a comparative analysis of the deduction, induction, and abduction\nformulas used in Probabilistic Logic Networks (PLN) and the Non-Axiomatic\nReasoning System (NARS), two uncertain reasoning frameworks aimed at AGI. One\ndifference between the two systems is that, at the level of individual\ninference rules, PLN directly leverages both term and relationship\nprobabilities, whereas NARS only leverages relationship frequencies and has no\nsimple analogue of term probabilities. Thus we focus here on scenarios where\nthere is high uncertainty about term probabilities, and explore how this\nuncertainty influences the comparative inferential conclusions of the two\nsystems. We compare the product of strength and confidence ($s\\times c$) in PLN\nagainst the product of frequency and confidence ($f\\times c$) in NARS\n(quantities we refer to as measuring the \"power\" of an uncertain statement) in\ncases of high term probability uncertainty, using heuristic analyses and\nelementary numerical computations. We find that in many practical situations\nwith high term probability uncertainty, PLN and NARS formulas give very similar\nresults for the power of an inference conclusion, even though they sometimes\ncome to these similar numbers in quite different ways.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We provide a comparative analysis of the deduction, induction, and abduction\nformulas used in Probabilistic Logic Networks (PLN) and the Non-Axiomatic\nReasoning System (NARS), two uncertain reasoning frameworks aimed at AGI. One\ndifference between the two systems is that, at the level of individual\ninference rules, PLN directly leverages both term and relationship\nprobabilities, whereas NARS only leverages relationship frequencies and has no\nsimple analogue of term probabilities. Thus we focus here on scenarios where\nthere is high uncertainty about term probabilities, and explore how this\nuncertainty influences the comparative inferential conclusions of the two\nsystems. We compare the product of strength and confidence ($s\\times c$) in PLN\nagainst the product of frequency and confidence ($f\\times c$) in NARS\n(quantities we refer to as measuring the \"power\" of an uncertain statement) in\ncases of high term probability uncertainty, using heuristic analyses and\nelementary numerical computations. We find that in many practical situations\nwith high term probability uncertainty, PLN and NARS formulas give very similar\nresults for the power of an inference conclusion, even though they sometimes\ncome to these similar numbers in quite different ways."
                },
                "authors": [
                    {
                        "name": "Ben Goertzel"
                    }
                ],
                "author_detail": {
                    "name": "Ben Goertzel"
                },
                "author": "Ben Goertzel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02415v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02415v3",
                "updated": "2024-12-27T08:13:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    8,
                    13,
                    12,
                    4,
                    362,
                    0
                ],
                "published": "2024-09-04T03:41:42Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    3,
                    41,
                    42,
                    2,
                    248,
                    0
                ],
                "title": "Local Map Construction with SDMap: A Comprehensive Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Local Map Construction with SDMap: A Comprehensive Survey"
                },
                "summary": "Local map construction is a vital component of intelligent driving\nperception, offering necessary reference for vehicle positioning and planning.\nStandard Definition map (SDMap), known for its low cost, accessibility, and\nversatility, has significant potential as prior information for local map\nperception. This paper mainly reviews the local map construction methods with\nSDMap, including definitions, general processing flow, and datasets. Besides,\nthis paper analyzes multimodal data representation and fusion methods in\nSDMap-based local map construction. This paper also discusses key challenges\nand future directions, such as optimizing SDMap processing, enhancing spatial\nalignment with real-time data, and incorporating richer environmental\ninformation. At last, the review looks forward to future research focusing on\nenhancing road topology inference and multimodal data fusion to improve the\nrobustness and scalability of local map perception.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Local map construction is a vital component of intelligent driving\nperception, offering necessary reference for vehicle positioning and planning.\nStandard Definition map (SDMap), known for its low cost, accessibility, and\nversatility, has significant potential as prior information for local map\nperception. This paper mainly reviews the local map construction methods with\nSDMap, including definitions, general processing flow, and datasets. Besides,\nthis paper analyzes multimodal data representation and fusion methods in\nSDMap-based local map construction. This paper also discusses key challenges\nand future directions, such as optimizing SDMap processing, enhancing spatial\nalignment with real-time data, and incorporating richer environmental\ninformation. At last, the review looks forward to future research focusing on\nenhancing road topology inference and multimodal data fusion to improve the\nrobustness and scalability of local map perception."
                },
                "authors": [
                    {
                        "name": "Jiaqi Li"
                    },
                    {
                        "name": "Pingfan Jia"
                    },
                    {
                        "name": "Jiaxing Chen"
                    },
                    {
                        "name": "Jiaxi Liu"
                    },
                    {
                        "name": "Lei He"
                    },
                    {
                        "name": "Keqiang Li"
                    }
                ],
                "author_detail": {
                    "name": "Keqiang Li"
                },
                "author": "Keqiang Li",
                "arxiv_comment": "18 pages, 26 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02415v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02415v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19513v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19513v1",
                "updated": "2024-12-27T08:09:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    8,
                    9,
                    11,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T08:09:11Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    8,
                    9,
                    11,
                    4,
                    362,
                    0
                ],
                "title": "Confidence v.s. Critique: A Decomposition of Self-Correction Capability\n  for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confidence v.s. Critique: A Decomposition of Self-Correction Capability\n  for LLMs"
                },
                "summary": "Large Language Models (LLMs) can correct their self-generated responses, but\na decline in accuracy after self-correction is also witnessed. To have a deeper\nunderstanding of self-correction, we endeavor to decompose, evaluate, and\nanalyze the self-correction behaviors of LLMs. By enumerating and analyzing\nanswer correctness before and after self-correction, we decompose the\nself-correction capability into confidence (being confident to correct answers)\nand critique (turning wrong answers to correct) capabilities, and propose two\nmetrics from a probabilistic perspective to measure these 2 capabilities, along\nwith another metric for overall self-correction capability evaluation. Based on\nour decomposition and evaluation metrics, we conduct extensive experiments and\ndraw some empirical conclusions. For example, we find different models can\nexhibit distinct behaviors: some models are confident while others are more\ncritical. We also find the trade-off between the two capabilities (i.e.\nimproving one can lead to a decline in the other) when manipulating model\nself-correction behavior by prompts or in-context learning. Further, we find a\nsimple yet efficient strategy to improve self-correction capability by\ntransforming Supervision Fine-Tuning (SFT) data format, and our strategy\noutperforms vanilla SFT in both capabilities and achieves much higher accuracy\nafter self-correction. Our code will be publicly available on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can correct their self-generated responses, but\na decline in accuracy after self-correction is also witnessed. To have a deeper\nunderstanding of self-correction, we endeavor to decompose, evaluate, and\nanalyze the self-correction behaviors of LLMs. By enumerating and analyzing\nanswer correctness before and after self-correction, we decompose the\nself-correction capability into confidence (being confident to correct answers)\nand critique (turning wrong answers to correct) capabilities, and propose two\nmetrics from a probabilistic perspective to measure these 2 capabilities, along\nwith another metric for overall self-correction capability evaluation. Based on\nour decomposition and evaluation metrics, we conduct extensive experiments and\ndraw some empirical conclusions. For example, we find different models can\nexhibit distinct behaviors: some models are confident while others are more\ncritical. We also find the trade-off between the two capabilities (i.e.\nimproving one can lead to a decline in the other) when manipulating model\nself-correction behavior by prompts or in-context learning. Further, we find a\nsimple yet efficient strategy to improve self-correction capability by\ntransforming Supervision Fine-Tuning (SFT) data format, and our strategy\noutperforms vanilla SFT in both capabilities and achieves much higher accuracy\nafter self-correction. Our code will be publicly available on GitHub."
                },
                "authors": [
                    {
                        "name": "Zhe Yang"
                    },
                    {
                        "name": "Yichang Zhang"
                    },
                    {
                        "name": "Yudong Wang"
                    },
                    {
                        "name": "Ziyao Xu"
                    },
                    {
                        "name": "Junyang Lin"
                    },
                    {
                        "name": "Zhifang Sui"
                    }
                ],
                "author_detail": {
                    "name": "Zhifang Sui"
                },
                "author": "Zhifang Sui",
                "arxiv_comment": "16 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19513v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19513v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19512v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19512v1",
                "updated": "2024-12-27T08:03:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    8,
                    3,
                    22,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T08:03:22Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    8,
                    3,
                    22,
                    4,
                    362,
                    0
                ],
                "title": "Safeguard Fine-Tuned LLMs Through Pre- and Post-Tuning Model Merging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safeguard Fine-Tuned LLMs Through Pre- and Post-Tuning Model Merging"
                },
                "summary": "Fine-tuning large language models (LLMs) for downstream tasks is a widely\nadopted approach, but it often leads to safety degradation in safety-aligned\nLLMs. Currently, many solutions address this issue by incorporating additional\nsafety data, which can be impractical in many cases. In this paper, we address\nthe question: How can we improve downstream task performance while preserving\nsafety in LLMs without relying on additional safety data? We propose a simple\nand effective method that maintains the inherent safety of LLMs while enhancing\ntheir downstream task performance: merging the weights of pre- and\npost-fine-tuned safety-aligned models. Experimental results across various\ndownstream tasks, models, and merging methods demonstrate that this approach\neffectively mitigates safety degradation while improving downstream task\nperformance, offering a practical solution for adapting safety-aligned LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large language models (LLMs) for downstream tasks is a widely\nadopted approach, but it often leads to safety degradation in safety-aligned\nLLMs. Currently, many solutions address this issue by incorporating additional\nsafety data, which can be impractical in many cases. In this paper, we address\nthe question: How can we improve downstream task performance while preserving\nsafety in LLMs without relying on additional safety data? We propose a simple\nand effective method that maintains the inherent safety of LLMs while enhancing\ntheir downstream task performance: merging the weights of pre- and\npost-fine-tuned safety-aligned models. Experimental results across various\ndownstream tasks, models, and merging methods demonstrate that this approach\neffectively mitigates safety degradation while improving downstream task\nperformance, offering a practical solution for adapting safety-aligned LLMs."
                },
                "authors": [
                    {
                        "name": "Hua Farn"
                    },
                    {
                        "name": "Hsuan Su"
                    },
                    {
                        "name": "Shachi H Kumar"
                    },
                    {
                        "name": "Saurav Sahay"
                    },
                    {
                        "name": "Shang-Tse Chen"
                    },
                    {
                        "name": "Hung-yi Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hung-yi Lee"
                },
                "author": "Hung-yi Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19512v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19512v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19509v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19509v1",
                "updated": "2024-12-27T07:55:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    7,
                    55,
                    36,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T07:55:36Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    7,
                    55,
                    36,
                    4,
                    362,
                    0
                ],
                "title": "MBQ: Modality-Balanced Quantization for Large Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MBQ: Modality-Balanced Quantization for Large Vision-Language Models"
                },
                "summary": "Vision-Language Models (VLMs) have enabled a variety of real-world\napplications. The large parameter size of VLMs brings large memory and\ncomputation overhead which poses significant challenges for deployment.\nPost-Training Quantization (PTQ) is an effective technique to reduce the memory\nand computation overhead. Existing PTQ methods mainly focus on large language\nmodels (LLMs), without considering the differences across other modalities. In\nthis paper, we discover that there is a significant difference in sensitivity\nbetween language and vision tokens in large VLMs. Therefore, treating tokens\nfrom different modalities equally, as in existing PTQ methods, may\nover-emphasize the insensitive modalities, leading to significant accuracy\nloss. To deal with the above issue, we propose a simple yet effective method,\nModality-Balanced Quantization (MBQ), for large VLMs. Specifically, MBQ\nincorporates the different sensitivities across modalities during the\ncalibration process to minimize the reconstruction loss for better quantization\nparameters. Extensive experiments show that MBQ can significantly improve task\naccuracy by up to 4.4% and 11.6% under W3 and W4A8 quantization for 7B to 70B\nVLMs, compared to SOTA baselines. Additionally, we implement a W3 GPU kernel\nthat fuses the dequantization and GEMV operators, achieving a 1.4x speedup on\nLLaVA-onevision-7B on the RTX 4090. The code is available at\nhttps://github.com/thu-nics/MBQ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) have enabled a variety of real-world\napplications. The large parameter size of VLMs brings large memory and\ncomputation overhead which poses significant challenges for deployment.\nPost-Training Quantization (PTQ) is an effective technique to reduce the memory\nand computation overhead. Existing PTQ methods mainly focus on large language\nmodels (LLMs), without considering the differences across other modalities. In\nthis paper, we discover that there is a significant difference in sensitivity\nbetween language and vision tokens in large VLMs. Therefore, treating tokens\nfrom different modalities equally, as in existing PTQ methods, may\nover-emphasize the insensitive modalities, leading to significant accuracy\nloss. To deal with the above issue, we propose a simple yet effective method,\nModality-Balanced Quantization (MBQ), for large VLMs. Specifically, MBQ\nincorporates the different sensitivities across modalities during the\ncalibration process to minimize the reconstruction loss for better quantization\nparameters. Extensive experiments show that MBQ can significantly improve task\naccuracy by up to 4.4% and 11.6% under W3 and W4A8 quantization for 7B to 70B\nVLMs, compared to SOTA baselines. Additionally, we implement a W3 GPU kernel\nthat fuses the dequantization and GEMV operators, achieving a 1.4x speedup on\nLLaVA-onevision-7B on the RTX 4090. The code is available at\nhttps://github.com/thu-nics/MBQ."
                },
                "authors": [
                    {
                        "name": "Shiyao Li"
                    },
                    {
                        "name": "Yingchun Hu"
                    },
                    {
                        "name": "Xuefei Ning"
                    },
                    {
                        "name": "Xihui Liu"
                    },
                    {
                        "name": "Ke Hong"
                    },
                    {
                        "name": "Xiaotao Jia"
                    },
                    {
                        "name": "Xiuhong Li"
                    },
                    {
                        "name": "Yaqi Yan"
                    },
                    {
                        "name": "Pei Ran"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Shengen Yan"
                    },
                    {
                        "name": "Huazhong Yang"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19509v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19509v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19507v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19507v1",
                "updated": "2024-12-27T07:53:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    7,
                    53,
                    59,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T07:53:59Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    7,
                    53,
                    59,
                    4,
                    362,
                    0
                ],
                "title": "Hybrid Local Causal Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Local Causal Discovery"
                },
                "summary": "Local causal discovery aims to learn and distinguish the direct causes and\neffects of a target variable from observed data. Existing constraint-based\nlocal causal discovery methods use AND or OR rules in constructing the local\ncausal skeleton, but using either rule alone is prone to produce cascading\nerrors in the learned local causal skeleton, and thus impacting the inference\nof local causal relationships. On the other hand, directly applying score-based\nglobal causal discovery methods to local causal discovery may randomly return\nincorrect results due to the existence of local equivalence classes. To address\nthe above issues, we propose a Hybrid Local Causal Discovery algorithm, called\nHLCD. Specifically, HLCD initially utilizes a constraint-based approach\ncombined with the OR rule to obtain a candidate skeleton and then employs a\nscore-based method to eliminate redundant portions in the candidate skeleton.\nFurthermore, during the local causal orientation phase, HLCD distinguishes\nbetween V-structures and equivalence classes by comparing the local structure\nscores between the two, thereby avoiding orientation interference caused by\nlocal equivalence classes. We conducted extensive experiments with seven\nstate-of-the-art competitors on 14 benchmark Bayesian network datasets, and the\nexperimental results demonstrate that HLCD significantly outperforms existing\nlocal causal discovery algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Local causal discovery aims to learn and distinguish the direct causes and\neffects of a target variable from observed data. Existing constraint-based\nlocal causal discovery methods use AND or OR rules in constructing the local\ncausal skeleton, but using either rule alone is prone to produce cascading\nerrors in the learned local causal skeleton, and thus impacting the inference\nof local causal relationships. On the other hand, directly applying score-based\nglobal causal discovery methods to local causal discovery may randomly return\nincorrect results due to the existence of local equivalence classes. To address\nthe above issues, we propose a Hybrid Local Causal Discovery algorithm, called\nHLCD. Specifically, HLCD initially utilizes a constraint-based approach\ncombined with the OR rule to obtain a candidate skeleton and then employs a\nscore-based method to eliminate redundant portions in the candidate skeleton.\nFurthermore, during the local causal orientation phase, HLCD distinguishes\nbetween V-structures and equivalence classes by comparing the local structure\nscores between the two, thereby avoiding orientation interference caused by\nlocal equivalence classes. We conducted extensive experiments with seven\nstate-of-the-art competitors on 14 benchmark Bayesian network datasets, and the\nexperimental results demonstrate that HLCD significantly outperforms existing\nlocal causal discovery algorithms."
                },
                "authors": [
                    {
                        "name": "Zhaolong Ling"
                    },
                    {
                        "name": "Honghui Peng"
                    },
                    {
                        "name": "Yiwen Zhang"
                    },
                    {
                        "name": "Peng Zhou"
                    },
                    {
                        "name": "Xingyu Wu"
                    },
                    {
                        "name": "Kui Yu"
                    },
                    {
                        "name": "Xindong Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xindong Wu"
                },
                "author": "Xindong Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19507v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19507v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19498v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19498v1",
                "updated": "2024-12-27T07:33:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    7,
                    33,
                    49,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T07:33:49Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    7,
                    33,
                    49,
                    4,
                    362,
                    0
                ],
                "title": "Casevo: A Cognitive Agents and Social Evolution Simulator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Casevo: A Cognitive Agents and Social Evolution Simulator"
                },
                "summary": "In this paper, we introduce a multi-agent simulation framework Casevo\n(Cognitive Agents and Social Evolution Simulator), that integrates large\nlanguage models (LLMs) to simulate complex social phenomena and decision-making\nprocesses. Casevo is designed as a discrete-event simulator driven by agents\nwith features such as Chain of Thoughts (CoT), Retrieval-Augmented Generation\n(RAG), and Customizable Memory Mechanism. Casevo enables dynamic social\nmodeling, which can support various scenarios such as social network analysis,\npublic opinion dynamics, and behavior prediction in complex social systems. To\ndemonstrate the effectiveness of Casevo, we utilize one of the U.S. 2020\nmidterm election TV debates as a simulation example. Our results show that\nCasevo facilitates more realistic and flexible agent interactions, improving\nthe quality of dynamic social phenomena simulation. This work contributes to\nthe field by providing a robust system for studying large-scale, high-fidelity\nsocial behaviors with advanced LLM-driven agents, expanding the capabilities of\ntraditional agent-based modeling (ABM). The open-source code repository address\nof casevo is https://github.com/rgCASS/casevo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a multi-agent simulation framework Casevo\n(Cognitive Agents and Social Evolution Simulator), that integrates large\nlanguage models (LLMs) to simulate complex social phenomena and decision-making\nprocesses. Casevo is designed as a discrete-event simulator driven by agents\nwith features such as Chain of Thoughts (CoT), Retrieval-Augmented Generation\n(RAG), and Customizable Memory Mechanism. Casevo enables dynamic social\nmodeling, which can support various scenarios such as social network analysis,\npublic opinion dynamics, and behavior prediction in complex social systems. To\ndemonstrate the effectiveness of Casevo, we utilize one of the U.S. 2020\nmidterm election TV debates as a simulation example. Our results show that\nCasevo facilitates more realistic and flexible agent interactions, improving\nthe quality of dynamic social phenomena simulation. This work contributes to\nthe field by providing a robust system for studying large-scale, high-fidelity\nsocial behaviors with advanced LLM-driven agents, expanding the capabilities of\ntraditional agent-based modeling (ABM). The open-source code repository address\nof casevo is https://github.com/rgCASS/casevo."
                },
                "authors": [
                    {
                        "name": "Zexun Jiang"
                    },
                    {
                        "name": "Yafang Shi"
                    },
                    {
                        "name": "Maoxu Li"
                    },
                    {
                        "name": "Hongjiang Xiao"
                    },
                    {
                        "name": "Yunxiao Qin"
                    },
                    {
                        "name": "Qinglan Wei"
                    },
                    {
                        "name": "Ye Wang"
                    },
                    {
                        "name": "Yuan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Zhang"
                },
                "author": "Yuan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19498v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19498v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07111v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07111v2",
                "updated": "2024-12-27T07:29:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    7,
                    29,
                    19,
                    4,
                    362,
                    0
                ],
                "published": "2024-11-11T16:37:40Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    16,
                    37,
                    40,
                    0,
                    316,
                    0
                ],
                "title": "Building a Taiwanese Mandarin Spoken Language Model: A First Attempt",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building a Taiwanese Mandarin Spoken Language Model: A First Attempt"
                },
                "summary": "This technical report presents our initial attempt to build a spoken large\nlanguage model (LLM) for Taiwanese Mandarin, specifically tailored to enable\nreal-time, speech-to-speech interaction in multi-turn conversations. Our\nend-to-end model incorporates a decoder-only transformer architecture and aims\nto achieve seamless interaction while preserving the conversational flow,\nincluding full-duplex capabilities allowing simultaneous speaking and\nlistening. The paper also details the training process, including data\npreparation with synthesized dialogues and adjustments for real-time\ninteraction. We also developed a platform to evaluate conversational fluency\nand response coherence in multi-turn dialogues. We hope the release of the\nreport can contribute to the future development of spoken LLMs in Taiwanese\nMandarin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This technical report presents our initial attempt to build a spoken large\nlanguage model (LLM) for Taiwanese Mandarin, specifically tailored to enable\nreal-time, speech-to-speech interaction in multi-turn conversations. Our\nend-to-end model incorporates a decoder-only transformer architecture and aims\nto achieve seamless interaction while preserving the conversational flow,\nincluding full-duplex capabilities allowing simultaneous speaking and\nlistening. The paper also details the training process, including data\npreparation with synthesized dialogues and adjustments for real-time\ninteraction. We also developed a platform to evaluate conversational fluency\nand response coherence in multi-turn dialogues. We hope the release of the\nreport can contribute to the future development of spoken LLMs in Taiwanese\nMandarin."
                },
                "authors": [
                    {
                        "name": "Chih-Kai Yang"
                    },
                    {
                        "name": "Yu-Kuan Fu"
                    },
                    {
                        "name": "Chen-An Li"
                    },
                    {
                        "name": "Yi-Cheng Lin"
                    },
                    {
                        "name": "Yu-Xiang Lin"
                    },
                    {
                        "name": "Wei-Chih Chen"
                    },
                    {
                        "name": "Ho Lam Chung"
                    },
                    {
                        "name": "Chun-Yi Kuan"
                    },
                    {
                        "name": "Wei-Ping Huang"
                    },
                    {
                        "name": "Ke-Han Lu"
                    },
                    {
                        "name": "Tzu-Quan Lin"
                    },
                    {
                        "name": "Hsiu-Hsuan Wang"
                    },
                    {
                        "name": "En-Pei Hu"
                    },
                    {
                        "name": "Chan-Jan Hsu"
                    },
                    {
                        "name": "Liang-Hsuan Tseng"
                    },
                    {
                        "name": "I-Hsiang Chiu"
                    },
                    {
                        "name": "Ulin Sanga"
                    },
                    {
                        "name": "Xuanjun Chen"
                    },
                    {
                        "name": "Po-chun Hsu"
                    },
                    {
                        "name": "Shu-wen Yang"
                    },
                    {
                        "name": "Hung-yi Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hung-yi Lee"
                },
                "author": "Hung-yi Lee",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07111v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07111v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15862v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15862v3",
                "updated": "2024-12-27T07:04:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    7,
                    4,
                    19,
                    4,
                    362,
                    0
                ],
                "published": "2024-11-24T14:38:59Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    14,
                    38,
                    59,
                    6,
                    329,
                    0
                ],
                "title": "Do LLMs Really Think Step-by-step In Implicit Reasoning?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs Really Think Step-by-step In Implicit Reasoning?"
                },
                "summary": "It has been well-known that Chain-of-Thought can remarkably enhance LLMs'\nperformance on complex tasks. However, because it also introduces slower\ninference speeds and higher computational costs, many researches have attempted\nto use implicit CoT, which does not need LLMs to explicitly generate the\nintermediate steps. However, the invisible reasoning process leaves us a doubt\nthat, can implicit CoT really be equal to explicit CoT? Therefore, in this\nstudy, we address this question through experiments. We probe the information\nof intermediate steps from the model's hidden states when it is either trained\nor prompted to perform implicit CoT. The results surprisingly indicate that\nwhen prompted, LLMs hardly think about intermediate steps, suggesting they may\njust rely on experience rather than strict step-by-step reasoning. But when\ntrained, they indeed calculate intermediate steps. Moreover, in both\nsituations, we find the effect of using implicit CoT is susceptible to the\nformat of the problem, reaffirming the current deficiency of implicit CoT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It has been well-known that Chain-of-Thought can remarkably enhance LLMs'\nperformance on complex tasks. However, because it also introduces slower\ninference speeds and higher computational costs, many researches have attempted\nto use implicit CoT, which does not need LLMs to explicitly generate the\nintermediate steps. However, the invisible reasoning process leaves us a doubt\nthat, can implicit CoT really be equal to explicit CoT? Therefore, in this\nstudy, we address this question through experiments. We probe the information\nof intermediate steps from the model's hidden states when it is either trained\nor prompted to perform implicit CoT. The results surprisingly indicate that\nwhen prompted, LLMs hardly think about intermediate steps, suggesting they may\njust rely on experience rather than strict step-by-step reasoning. But when\ntrained, they indeed calculate intermediate steps. Moreover, in both\nsituations, we find the effect of using implicit CoT is susceptible to the\nformat of the problem, reaffirming the current deficiency of implicit CoT."
                },
                "authors": [
                    {
                        "name": "Yijiong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Yijiong Yu"
                },
                "author": "Yijiong Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15862v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15862v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.08914v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.08914v3",
                "updated": "2024-12-27T06:56:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    6,
                    56,
                    18,
                    4,
                    362,
                    0
                ],
                "published": "2023-12-14T13:20:57Z",
                "published_parsed": [
                    2023,
                    12,
                    14,
                    13,
                    20,
                    57,
                    3,
                    348,
                    0
                ],
                "title": "CogAgent: A Visual Language Model for GUI Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CogAgent: A Visual Language Model for GUI Agents"
                },
                "summary": "People are spending an enormous amount of time on digital devices through\ngraphical user interfaces (GUIs), e.g., computer or smartphone screens. Large\nlanguage models (LLMs) such as ChatGPT can assist people in tasks like writing\nemails, but struggle to understand and interact with GUIs, thus limiting their\npotential to increase automation levels. In this paper, we introduce CogAgent,\nan 18-billion-parameter visual language model (VLM) specializing in GUI\nunderstanding and navigation. By utilizing both low-resolution and\nhigh-resolution image encoders, CogAgent supports input at a resolution of\n1120*1120, enabling it to recognize tiny page elements and text. As a\ngeneralist visual language model, CogAgent achieves the state of the art on\nfive text-rich and four general VQA benchmarks, including VQAv2, OK-VQA,\nText-VQA, ST-VQA, ChartQA, infoVQA, DocVQA, MM-Vet, and POPE. CogAgent, using\nonly screenshots as input, outperforms LLM-based methods that consume extracted\nHTML text on both PC and Android GUI navigation tasks -- Mind2Web and AITW,\nadvancing the state of the art. The model and codes are available at\nhttps://github.com/THUDM/CogVLM, with a new version of CogAgent-9B-20241220\navailable at https://github.com/THUDM/CogAgent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "People are spending an enormous amount of time on digital devices through\ngraphical user interfaces (GUIs), e.g., computer or smartphone screens. Large\nlanguage models (LLMs) such as ChatGPT can assist people in tasks like writing\nemails, but struggle to understand and interact with GUIs, thus limiting their\npotential to increase automation levels. In this paper, we introduce CogAgent,\nan 18-billion-parameter visual language model (VLM) specializing in GUI\nunderstanding and navigation. By utilizing both low-resolution and\nhigh-resolution image encoders, CogAgent supports input at a resolution of\n1120*1120, enabling it to recognize tiny page elements and text. As a\ngeneralist visual language model, CogAgent achieves the state of the art on\nfive text-rich and four general VQA benchmarks, including VQAv2, OK-VQA,\nText-VQA, ST-VQA, ChartQA, infoVQA, DocVQA, MM-Vet, and POPE. CogAgent, using\nonly screenshots as input, outperforms LLM-based methods that consume extracted\nHTML text on both PC and Android GUI navigation tasks -- Mind2Web and AITW,\nadvancing the state of the art. The model and codes are available at\nhttps://github.com/THUDM/CogVLM, with a new version of CogAgent-9B-20241220\navailable at https://github.com/THUDM/CogAgent."
                },
                "authors": [
                    {
                        "name": "Wenyi Hong"
                    },
                    {
                        "name": "Weihan Wang"
                    },
                    {
                        "name": "Qingsong Lv"
                    },
                    {
                        "name": "Jiazheng Xu"
                    },
                    {
                        "name": "Wenmeng Yu"
                    },
                    {
                        "name": "Junhui Ji"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Zihan Wang"
                    },
                    {
                        "name": "Yuxuan Zhang"
                    },
                    {
                        "name": "Juanzi Li"
                    },
                    {
                        "name": "Bin Xu"
                    },
                    {
                        "name": "Yuxiao Dong"
                    },
                    {
                        "name": "Ming Ding"
                    },
                    {
                        "name": "Jie Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Tang"
                },
                "author": "Jie Tang",
                "arxiv_comment": "CVPR 2024 (Highlight), 27 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.08914v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.08914v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19475v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19475v1",
                "updated": "2024-12-27T06:00:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    6,
                    0,
                    30,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T06:00:30Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    6,
                    0,
                    30,
                    4,
                    362,
                    0
                ],
                "title": "Exploiting Dynamic Sparsity for Near-Field Spatial Non-Stationary\n  XL-MIMO Channel Tracking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploiting Dynamic Sparsity for Near-Field Spatial Non-Stationary\n  XL-MIMO Channel Tracking"
                },
                "summary": "This work considers a spatial non-stationary channel tracking problem in\nbroadband extremely large-scale multiple-input-multiple-output (XL-MIMO)\nsystems. In the case of spatial non-stationary, each scatterer has a certain\nvisibility region (VR) over antennas and power change may occur among visible\nantennas. Concentrating on the temporal correlation of XL-MIMO channels, we\ndesign a three-layer Markov prior model and hierarchical two-dimensional (2D)\nMarkov model to exploit the dynamic sparsity of sparse channel vectors and VRs,\nrespectively. Then, we formulate the channel tracking problem as a bilinear\nmeasurement process, and a novel dynamic alternating maximum a posteriori\n(DA-MAP) framework is developed to solve the problem. The DA-MAP contains four\nbasic modules: channel estimation module, VR detection module, grid update\nmodule, and temporal correlated module. Specifically, the first module is an\ninverse-free variational Bayesian inference (IF-VBI) estimator that avoids\ncomputational intensive matrix inverse each iteration; the second module is a\nturbo compressive sensing (Turbo-CS) algorithm that only needs small-scale\nmatrix operations in a parallel fashion; the third module refines the\npolar-delay domain grid; and the fourth module can process the temporal prior\ninformation to ensure high-efficiency channel tracking. Simulations show that\nthe proposed method can achieve a significant channel tracking performance\nwhile achieving low computational overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work considers a spatial non-stationary channel tracking problem in\nbroadband extremely large-scale multiple-input-multiple-output (XL-MIMO)\nsystems. In the case of spatial non-stationary, each scatterer has a certain\nvisibility region (VR) over antennas and power change may occur among visible\nantennas. Concentrating on the temporal correlation of XL-MIMO channels, we\ndesign a three-layer Markov prior model and hierarchical two-dimensional (2D)\nMarkov model to exploit the dynamic sparsity of sparse channel vectors and VRs,\nrespectively. Then, we formulate the channel tracking problem as a bilinear\nmeasurement process, and a novel dynamic alternating maximum a posteriori\n(DA-MAP) framework is developed to solve the problem. The DA-MAP contains four\nbasic modules: channel estimation module, VR detection module, grid update\nmodule, and temporal correlated module. Specifically, the first module is an\ninverse-free variational Bayesian inference (IF-VBI) estimator that avoids\ncomputational intensive matrix inverse each iteration; the second module is a\nturbo compressive sensing (Turbo-CS) algorithm that only needs small-scale\nmatrix operations in a parallel fashion; the third module refines the\npolar-delay domain grid; and the fourth module can process the temporal prior\ninformation to ensure high-efficiency channel tracking. Simulations show that\nthe proposed method can achieve a significant channel tracking performance\nwhile achieving low computational overhead."
                },
                "authors": [
                    {
                        "name": "Wenkang Xu amd An Liu"
                    },
                    {
                        "name": "Min-jian Zhao"
                    },
                    {
                        "name": "Giuseppe Caire"
                    },
                    {
                        "name": "Yik-Chung Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yik-Chung Wu"
                },
                "author": "Yik-Chung Wu",
                "arxiv_comment": "13 pages, 11 figures,Submitted to IEEE TSP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19475v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19475v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06997v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06997v3",
                "updated": "2024-12-27T06:00:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    6,
                    0,
                    28,
                    4,
                    362,
                    0
                ],
                "published": "2024-10-09T15:44:34Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    15,
                    44,
                    34,
                    2,
                    283,
                    0
                ],
                "title": "Feasibility Study of a Diffusion-Based Model for Cross-Modal Generation\n  of Knee MRI from X-ray: Integrating Radiographic Feature Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feasibility Study of a Diffusion-Based Model for Cross-Modal Generation\n  of Knee MRI from X-ray: Integrating Radiographic Feature Information"
                },
                "summary": "Knee osteoarthritis (KOA) is a prevalent musculoskeletal disorder, often\ndiagnosed using X-rays due to its cost-effectiveness. While Magnetic Resonance\nImaging (MRI) provides superior soft tissue visualization and serves as a\nvaluable supplementary diagnostic tool, its high cost and limited accessibility\nsignificantly restrict its widespread use. To explore the feasibility of\nbridging this imaging gap, we conducted a feasibility study leveraging a\ndiffusion-based model that uses an X-ray image as conditional input, alongside\ntarget depth and additional patient-specific feature information, to generate\ncorresponding MRI sequences. Our findings demonstrate that the MRI volumes\ngenerated by our approach is visually closer to real MRI scans. Moreover,\nincreasing inference steps enhances the continuity and smoothness of the\nsynthesized MRI sequences. Through ablation studies, we further validate that\nintegrating supplementary patient-specific information, beyond what X-rays\nalone can provide, enhances the accuracy and clinical relevance of the\ngenerated MRI, which underscores the potential of leveraging external\npatient-specific information to improve the MRI generation. This study is\navailable at https://zwang78.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knee osteoarthritis (KOA) is a prevalent musculoskeletal disorder, often\ndiagnosed using X-rays due to its cost-effectiveness. While Magnetic Resonance\nImaging (MRI) provides superior soft tissue visualization and serves as a\nvaluable supplementary diagnostic tool, its high cost and limited accessibility\nsignificantly restrict its widespread use. To explore the feasibility of\nbridging this imaging gap, we conducted a feasibility study leveraging a\ndiffusion-based model that uses an X-ray image as conditional input, alongside\ntarget depth and additional patient-specific feature information, to generate\ncorresponding MRI sequences. Our findings demonstrate that the MRI volumes\ngenerated by our approach is visually closer to real MRI scans. Moreover,\nincreasing inference steps enhances the continuity and smoothness of the\nsynthesized MRI sequences. Through ablation studies, we further validate that\nintegrating supplementary patient-specific information, beyond what X-rays\nalone can provide, enhances the accuracy and clinical relevance of the\ngenerated MRI, which underscores the potential of leveraging external\npatient-specific information to improve the MRI generation. This study is\navailable at https://zwang78.github.io/."
                },
                "authors": [
                    {
                        "name": "Zhe Wang"
                    },
                    {
                        "name": "Yung Hsin Chen"
                    },
                    {
                        "name": "Aladine Chetouani"
                    },
                    {
                        "name": "Fabian Bauer"
                    },
                    {
                        "name": "Yuhua Ru"
                    },
                    {
                        "name": "Fang Chen"
                    },
                    {
                        "name": "Liping Zhang"
                    },
                    {
                        "name": "Rachid Jennane"
                    },
                    {
                        "name": "Mohamed Jarraya"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Jarraya"
                },
                "author": "Mohamed Jarraya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06997v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06997v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11913v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11913v2",
                "updated": "2024-12-27T05:56:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    5,
                    56,
                    51,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-16T15:56:04Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    15,
                    56,
                    4,
                    0,
                    351,
                    0
                ],
                "title": "Learning Human-Aware Robot Policies for Adaptive Assistance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Human-Aware Robot Policies for Adaptive Assistance"
                },
                "summary": "Developing robots that can assist humans efficiently, safely, and adaptively\nis crucial for real-world applications such as healthcare. While previous work\noften assumes a centralized system for co-optimizing human-robot interactions,\nwe argue that real-world scenarios are much more complicated, as humans have\nindividual preferences regarding how tasks are performed. Robots typically lack\ndirect access to these implicit preferences. However, to provide effective\nassistance, robots must still be able to recognize and adapt to the individual\nneeds and preferences of different users. To address these challenges, we\npropose a novel framework in which robots infer human intentions and reason\nabout human utilities through interaction. Our approach features two critical\nmodules: the anticipation module is a motion predictor that captures the\nspatial-temporal relationship between the robot agent and user agent, which\ncontributes to predicting human behavior; the utility module infers the\nunderlying human utility functions through progressive task demonstration\nsampling. Extensive experiments across various robot types and assistive tasks\ndemonstrate that the proposed framework not only enhances task success and\nefficiency but also significantly improves user satisfaction, paving the way\nfor more personalized and adaptive assistive robotic systems. Code and demos\nare available at https://asonin.github.io/Human-Aware-Assistance/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing robots that can assist humans efficiently, safely, and adaptively\nis crucial for real-world applications such as healthcare. While previous work\noften assumes a centralized system for co-optimizing human-robot interactions,\nwe argue that real-world scenarios are much more complicated, as humans have\nindividual preferences regarding how tasks are performed. Robots typically lack\ndirect access to these implicit preferences. However, to provide effective\nassistance, robots must still be able to recognize and adapt to the individual\nneeds and preferences of different users. To address these challenges, we\npropose a novel framework in which robots infer human intentions and reason\nabout human utilities through interaction. Our approach features two critical\nmodules: the anticipation module is a motion predictor that captures the\nspatial-temporal relationship between the robot agent and user agent, which\ncontributes to predicting human behavior; the utility module infers the\nunderlying human utility functions through progressive task demonstration\nsampling. Extensive experiments across various robot types and assistive tasks\ndemonstrate that the proposed framework not only enhances task success and\nefficiency but also significantly improves user satisfaction, paving the way\nfor more personalized and adaptive assistive robotic systems. Code and demos\nare available at https://asonin.github.io/Human-Aware-Assistance/."
                },
                "authors": [
                    {
                        "name": "Jason Qin"
                    },
                    {
                        "name": "Shikun Ban"
                    },
                    {
                        "name": "Wentao Zhu"
                    },
                    {
                        "name": "Yizhou Wang"
                    },
                    {
                        "name": "Dimitris Samaras"
                    }
                ],
                "author_detail": {
                    "name": "Dimitris Samaras"
                },
                "author": "Dimitris Samaras",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11913v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11913v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17378v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17378v3",
                "updated": "2024-12-27T05:56:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    5,
                    56,
                    37,
                    4,
                    362,
                    0
                ],
                "published": "2024-06-25T08:55:12Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    8,
                    55,
                    12,
                    1,
                    177,
                    0
                ],
                "title": "A Text is Worth Several Tokens: Text Embedding from LLMs Secretly Aligns\n  Well with The Key Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Text is Worth Several Tokens: Text Embedding from LLMs Secretly Aligns\n  Well with The Key Tokens"
                },
                "summary": "Text embeddings from large language models (LLMs) have achieved excellent\nresults in tasks such as information retrieval, semantic textual similarity,\netc. In this work, we show an interesting finding: when feeding a text into the\nLLM-based embedder, the obtained text embedding will be able to be aligned with\nthe key tokens in the input text. We first fully analyze this phenomenon on\neight LLM-based embedders and show that this phenomenon is universal and is not\naffected by model architecture, training strategy, and embedding method. With a\ndeeper analysis, we find that the main change in embedding space between these\nembedders and their LLM backbones is in the first principal component. By\nadjusting the first principal component, we can align text embedding with the\nkey tokens. Finally, we give several examples to demonstrate the vast\napplication potential of this finding: (1) we propose a simple and practical\nsparse retrieval method based on the aligned tokens, which can achieve 80% of\nthe dense retrieval effect of the same model while reducing the computation\nsignificantly; (2) we show that our findings provide a novel perspective to\nhelp understand novel technologies (e.g., instruction-following embedding) and\nfuzzy concepts (e.g., semantic relatedness vs. similarity) in this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text embeddings from large language models (LLMs) have achieved excellent\nresults in tasks such as information retrieval, semantic textual similarity,\netc. In this work, we show an interesting finding: when feeding a text into the\nLLM-based embedder, the obtained text embedding will be able to be aligned with\nthe key tokens in the input text. We first fully analyze this phenomenon on\neight LLM-based embedders and show that this phenomenon is universal and is not\naffected by model architecture, training strategy, and embedding method. With a\ndeeper analysis, we find that the main change in embedding space between these\nembedders and their LLM backbones is in the first principal component. By\nadjusting the first principal component, we can align text embedding with the\nkey tokens. Finally, we give several examples to demonstrate the vast\napplication potential of this finding: (1) we propose a simple and practical\nsparse retrieval method based on the aligned tokens, which can achieve 80% of\nthe dense retrieval effect of the same model while reducing the computation\nsignificantly; (2) we show that our findings provide a novel perspective to\nhelp understand novel technologies (e.g., instruction-following embedding) and\nfuzzy concepts (e.g., semantic relatedness vs. similarity) in this field."
                },
                "authors": [
                    {
                        "name": "Zhijie Nie"
                    },
                    {
                        "name": "Richong Zhang"
                    },
                    {
                        "name": "Zhanyu Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhanyu Wu"
                },
                "author": "Zhanyu Wu",
                "arxiv_comment": "Work in Progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17378v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17378v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17335v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17335v2",
                "updated": "2024-12-27T05:51:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    5,
                    51,
                    50,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-23T07:01:29Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    7,
                    1,
                    29,
                    0,
                    358,
                    0
                ],
                "title": "Hierarchical Dirichlet Process Mixture of Products of Multinomial\n  Distributions: Applications to Survey Data with Potentially Missing Values",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Dirichlet Process Mixture of Products of Multinomial\n  Distributions: Applications to Survey Data with Potentially Missing Values"
                },
                "summary": "In social science research, understanding latent structures in populations\nthrough survey data with categorical responses is a common and important task.\nTraditional methods like Factor Analysis and Latent Class Analysis have\nlimitations, particularly in handling categorical data and accommodating mixed\nmemberships in latent structures, respectively. Moreover, choosing the number\nof factors or latent classes is often subjective and can be challenging in the\npresence of missing values. This study introduces a Hierarchical Dirichlet\nProcess Mixture of Products of Multinomial Distributions (HDPMPM) model, which\nleverages the flexibility of nonparametric Bayesian methods to address these\nlimitations. The HDPMPM model allows for multiple latent classes within\nindividuals and avoids fixing the number of mixture components at an arbitrary\nnumber. Additionally, it incorporates missing data imputation directly into the\nmodel's Gibbs sampling process. By applying a truncated stick-breaking\nrepresentation of the Dirichlet process, we can derive a Gibbs sampling scheme\nfor posterior inference. An application of the HDPMPM model to the 2016\nAmerican National Election Study (ANES) data demonstrates its effectiveness in\nidentifying political profiles and handling missing data scenarios, including\nthose that are missing at random (MAR) and missing completely at random (MCAR).\nThe results show that the HDPMPM model successfully recovers dominant profiles\nand manages complex latent structures in survey data, providing an alternative\ntool for social science researchers in dealing with categorical data with\nmissing values.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In social science research, understanding latent structures in populations\nthrough survey data with categorical responses is a common and important task.\nTraditional methods like Factor Analysis and Latent Class Analysis have\nlimitations, particularly in handling categorical data and accommodating mixed\nmemberships in latent structures, respectively. Moreover, choosing the number\nof factors or latent classes is often subjective and can be challenging in the\npresence of missing values. This study introduces a Hierarchical Dirichlet\nProcess Mixture of Products of Multinomial Distributions (HDPMPM) model, which\nleverages the flexibility of nonparametric Bayesian methods to address these\nlimitations. The HDPMPM model allows for multiple latent classes within\nindividuals and avoids fixing the number of mixture components at an arbitrary\nnumber. Additionally, it incorporates missing data imputation directly into the\nmodel's Gibbs sampling process. By applying a truncated stick-breaking\nrepresentation of the Dirichlet process, we can derive a Gibbs sampling scheme\nfor posterior inference. An application of the HDPMPM model to the 2016\nAmerican National Election Study (ANES) data demonstrates its effectiveness in\nidentifying political profiles and handling missing data scenarios, including\nthose that are missing at random (MAR) and missing completely at random (MCAR).\nThe results show that the HDPMPM model successfully recovers dominant profiles\nand manages complex latent structures in survey data, providing an alternative\ntool for social science researchers in dealing with categorical data with\nmissing values."
                },
                "authors": [
                    {
                        "name": "Chayut Wongkamthong"
                    }
                ],
                "author_detail": {
                    "name": "Chayut Wongkamthong"
                },
                "author": "Chayut Wongkamthong",
                "arxiv_comment": "This manuscript is currently undergoing the journal submission\n  process",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17335v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17335v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02115v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02115v2",
                "updated": "2024-12-27T05:48:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    5,
                    48,
                    14,
                    4,
                    362,
                    0
                ],
                "published": "2024-11-04T14:29:04Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    14,
                    29,
                    4,
                    0,
                    309,
                    0
                ],
                "title": "FedMoE-DA: Federated Mixture of Experts via Domain Aware Fine-grained\n  Aggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedMoE-DA: Federated Mixture of Experts via Domain Aware Fine-grained\n  Aggregation"
                },
                "summary": "Federated learning (FL) is a collaborative machine learning approach that\nenables multiple clients to train models without sharing their private data.\nWith the rise of deep learning, large-scale models have garnered significant\nattention due to their exceptional performance. However, a key challenge in FL\nis the limitation imposed by clients with constrained computational and\ncommunication resources, which hampers the deployment of these large models.\nThe Mixture of Experts (MoE) architecture addresses this challenge with its\nsparse activation property, which reduces computational workload and\ncommunication demands during inference and updates. Additionally, MoE\nfacilitates better personalization by allowing each expert to specialize in\ndifferent subsets of the data distribution. To alleviate the communication\nburdens between the server and clients, we propose FedMoE-DA, a new FL model\ntraining framework that leverages the MoE architecture and incorporates a novel\ndomain-aware, fine-grained aggregation strategy to enhance the robustness,\npersonalizability, and communication efficiency simultaneously. Specifically,\nthe correlation between both intra-client expert models and inter-client data\nheterogeneity is exploited. Moreover, we utilize peer-to-peer (P2P)\ncommunication between clients for selective expert model synchronization, thus\nsignificantly reducing the server-client transmissions. Experiments demonstrate\nthat our FedMoE-DA achieves excellent performance while reducing the\ncommunication pressure on the server.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated learning (FL) is a collaborative machine learning approach that\nenables multiple clients to train models without sharing their private data.\nWith the rise of deep learning, large-scale models have garnered significant\nattention due to their exceptional performance. However, a key challenge in FL\nis the limitation imposed by clients with constrained computational and\ncommunication resources, which hampers the deployment of these large models.\nThe Mixture of Experts (MoE) architecture addresses this challenge with its\nsparse activation property, which reduces computational workload and\ncommunication demands during inference and updates. Additionally, MoE\nfacilitates better personalization by allowing each expert to specialize in\ndifferent subsets of the data distribution. To alleviate the communication\nburdens between the server and clients, we propose FedMoE-DA, a new FL model\ntraining framework that leverages the MoE architecture and incorporates a novel\ndomain-aware, fine-grained aggregation strategy to enhance the robustness,\npersonalizability, and communication efficiency simultaneously. Specifically,\nthe correlation between both intra-client expert models and inter-client data\nheterogeneity is exploited. Moreover, we utilize peer-to-peer (P2P)\ncommunication between clients for selective expert model synchronization, thus\nsignificantly reducing the server-client transmissions. Experiments demonstrate\nthat our FedMoE-DA achieves excellent performance while reducing the\ncommunication pressure on the server."
                },
                "authors": [
                    {
                        "name": "Ziwei Zhan"
                    },
                    {
                        "name": "Wenkuan Zhao"
                    },
                    {
                        "name": "Yuanqing Li"
                    },
                    {
                        "name": "Weijie Liu"
                    },
                    {
                        "name": "Xiaoxi Zhang"
                    },
                    {
                        "name": "Chee Wei Tan"
                    },
                    {
                        "name": "Chuan Wu"
                    },
                    {
                        "name": "Deke Guo"
                    },
                    {
                        "name": "Xu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xu Chen"
                },
                "author": "Xu Chen",
                "arxiv_comment": "8 pages, 5 figures, accepted by The 20th International Conference on\n  Mobility, Sensing and Networking (MSN 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02115v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02115v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00652v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00652v2",
                "updated": "2024-12-27T05:32:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    5,
                    32,
                    11,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-01T03:12:26Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    3,
                    12,
                    26,
                    6,
                    336,
                    0
                ],
                "title": "Multi-Agent Collaboration in Incident Response with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Collaboration in Incident Response with Large Language\n  Models"
                },
                "summary": "Incident response (IR) is a critical aspect of cybersecurity, requiring rapid\ndecision-making and coordinated efforts to address cyberattacks effectively.\nLeveraging large language models (LLMs) as intelligent agents offers a novel\napproach to enhancing collaboration and efficiency in IR scenarios. This paper\nexplores the application of LLM-based multi-agent collaboration using the\nBackdoors & Breaches framework, a tabletop game designed for cybersecurity\ntraining. We simulate real-world IR dynamics through various team structures,\nincluding centralized, decentralized, and hybrid configurations. By analyzing\nagent interactions and performance across these setups, we provide insights\ninto optimizing multi-agent collaboration for incident response. Our findings\nhighlight the potential of LLMs to enhance decision-making, improve\nadaptability, and streamline IR processes, paving the way for more effective\nand coordinated responses to cyber threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incident response (IR) is a critical aspect of cybersecurity, requiring rapid\ndecision-making and coordinated efforts to address cyberattacks effectively.\nLeveraging large language models (LLMs) as intelligent agents offers a novel\napproach to enhancing collaboration and efficiency in IR scenarios. This paper\nexplores the application of LLM-based multi-agent collaboration using the\nBackdoors & Breaches framework, a tabletop game designed for cybersecurity\ntraining. We simulate real-world IR dynamics through various team structures,\nincluding centralized, decentralized, and hybrid configurations. By analyzing\nagent interactions and performance across these setups, we provide insights\ninto optimizing multi-agent collaboration for incident response. Our findings\nhighlight the potential of LLMs to enhance decision-making, improve\nadaptability, and streamline IR processes, paving the way for more effective\nand coordinated responses to cyber threats."
                },
                "authors": [
                    {
                        "name": "Zefang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zefang Liu"
                },
                "author": "Zefang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00652v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00652v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.10462v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.10462v3",
                "updated": "2024-12-27T05:30:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    5,
                    30,
                    0,
                    4,
                    362,
                    0
                ],
                "published": "2023-08-21T04:31:06Z",
                "published_parsed": [
                    2023,
                    8,
                    21,
                    4,
                    31,
                    6,
                    0,
                    233,
                    0
                ],
                "title": "Exploring Parameter-Efficient Fine-Tuning Techniques for Code Generation\n  with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Parameter-Efficient Fine-Tuning Techniques for Code Generation\n  with Large Language Models"
                },
                "summary": "Large language models (LLMs) demonstrate impressive capabilities to generate\naccurate code snippets given natural language intents in a zero-shot manner,\ni.e., without the need for specific fine-tuning. While prior studies have\nhighlighted the advantages of fine-tuning LLMs, this process incurs high\ncomputational costs, making it impractical in resource-scarce environments,\nparticularly for models with billions of parameters. To address these\nchallenges, previous research explored in-context learning (ICL) and\nretrieval-augmented generation (RAG) as strategies to guide the LLM generative\nprocess with task-specific prompt examples. However, ICL and RAG introduce\ninconveniences, such as the need for designing contextually relevant prompts\nand the absence of learning task-specific parameters, thereby limiting\ndownstream task performance. In this context, we foresee parameter-efficient\nfine-tuning (PEFT) as a promising approach to efficiently specialize LLMs to\ntask-specific data while maintaining reasonable resource consumption. In this\npaper, we deliver a comprehensive study of PEFT techniques for LLMs in the\ncontext of automated code generation. Our comprehensive investigation of PEFT\ntechniques for LLMs reveals their superiority and potential over ICL and RAG\nacross a diverse set of LLMs and three representative Python code generation\ndatasets: Conala, CodeAlpacaPy, and APPS. Furthermore, our study highlights the\npotential for tuning larger LLMs and significant reductions in memory usage by\ncombining PEFT with quantization. Therefore, this study opens opportunities for\nbroader applications of PEFT in software engineering scenarios. Our code is\navailable at https://github.com/martin-wey/peft-llm-code/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate impressive capabilities to generate\naccurate code snippets given natural language intents in a zero-shot manner,\ni.e., without the need for specific fine-tuning. While prior studies have\nhighlighted the advantages of fine-tuning LLMs, this process incurs high\ncomputational costs, making it impractical in resource-scarce environments,\nparticularly for models with billions of parameters. To address these\nchallenges, previous research explored in-context learning (ICL) and\nretrieval-augmented generation (RAG) as strategies to guide the LLM generative\nprocess with task-specific prompt examples. However, ICL and RAG introduce\ninconveniences, such as the need for designing contextually relevant prompts\nand the absence of learning task-specific parameters, thereby limiting\ndownstream task performance. In this context, we foresee parameter-efficient\nfine-tuning (PEFT) as a promising approach to efficiently specialize LLMs to\ntask-specific data while maintaining reasonable resource consumption. In this\npaper, we deliver a comprehensive study of PEFT techniques for LLMs in the\ncontext of automated code generation. Our comprehensive investigation of PEFT\ntechniques for LLMs reveals their superiority and potential over ICL and RAG\nacross a diverse set of LLMs and three representative Python code generation\ndatasets: Conala, CodeAlpacaPy, and APPS. Furthermore, our study highlights the\npotential for tuning larger LLMs and significant reductions in memory usage by\ncombining PEFT with quantization. Therefore, this study opens opportunities for\nbroader applications of PEFT in software engineering scenarios. Our code is\navailable at https://github.com/martin-wey/peft-llm-code/."
                },
                "authors": [
                    {
                        "name": "Martin Weyssow"
                    },
                    {
                        "name": "Xin Zhou"
                    },
                    {
                        "name": "Kisub Kim"
                    },
                    {
                        "name": "David Lo"
                    },
                    {
                        "name": "Houari Sahraoui"
                    }
                ],
                "author_detail": {
                    "name": "Houari Sahraoui"
                },
                "author": "Houari Sahraoui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.10462v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.10462v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.09032v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.09032v3",
                "updated": "2024-12-27T05:13:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    5,
                    13,
                    23,
                    4,
                    362,
                    0
                ],
                "published": "2024-03-14T01:51:35Z",
                "published_parsed": [
                    2024,
                    3,
                    14,
                    1,
                    51,
                    35,
                    3,
                    74,
                    0
                ],
                "title": "CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language\n  Models to Coding Preferences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language\n  Models to Coding Preferences"
                },
                "summary": "Evaluating the alignment of large language models (LLMs) with user-defined\ncoding preferences is a challenging endeavour that requires a deep assessment\nof LLMs' outputs. Existing methods and benchmarks rely primarily on automated\nmetrics and static analysis tools, which often fail to capture the nuances of\nuser instructions and LLM outputs. To address this gap, we propose using the\nLLM-as-a-Judge methodology to evaluate the alignment of LLMs with coding\npreferences. Based on this approach, we present CodeUltraFeedback, a\ncomprehensive dataset designed to facilitate the evaluation and improvement of\nLLM alignment. CodeUltraFeedback consists of 10,000 coding instructions, each\nannotated with four responses generated from a diverse pool of 14 LLMs. These\nresponses are ranked based on five distinct coding preferences using GPT-3.5 as\na judge, providing both numerical scores and detailed textual feedback. Our\nanalysis of CodeUltraFeedback reveals that responses from GPT-3.5 and GPT-4 are\ngenerally preferred over those from open-weight LLMs, highlighting significant\ndifferences in alignment between closed and open-weight models. In turn, we\nexplore the usage of CodeUltraFeedback as feedback data to fine-tune and align\nCodeLlama-7B-Instruct using supervised fine-tuning (SFT) and reinforcement\nlearning from AI feedback (RLAIF) with direct preference optimization (DPO).\nThe resulting aligned CodeLlama-7B-Instruct model outperforms larger LLMs in\nterms of alignment with coding preferences and shows improved functional\ncorrectness on the HumanEval+ benchmark compared to the original instruct\nmodel. Therefore, our contributions bridge the gap in preference tuning of LLMs\nfor code and set the stage for further advancements in model alignment and\nRLAIF in automated software engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the alignment of large language models (LLMs) with user-defined\ncoding preferences is a challenging endeavour that requires a deep assessment\nof LLMs' outputs. Existing methods and benchmarks rely primarily on automated\nmetrics and static analysis tools, which often fail to capture the nuances of\nuser instructions and LLM outputs. To address this gap, we propose using the\nLLM-as-a-Judge methodology to evaluate the alignment of LLMs with coding\npreferences. Based on this approach, we present CodeUltraFeedback, a\ncomprehensive dataset designed to facilitate the evaluation and improvement of\nLLM alignment. CodeUltraFeedback consists of 10,000 coding instructions, each\nannotated with four responses generated from a diverse pool of 14 LLMs. These\nresponses are ranked based on five distinct coding preferences using GPT-3.5 as\na judge, providing both numerical scores and detailed textual feedback. Our\nanalysis of CodeUltraFeedback reveals that responses from GPT-3.5 and GPT-4 are\ngenerally preferred over those from open-weight LLMs, highlighting significant\ndifferences in alignment between closed and open-weight models. In turn, we\nexplore the usage of CodeUltraFeedback as feedback data to fine-tune and align\nCodeLlama-7B-Instruct using supervised fine-tuning (SFT) and reinforcement\nlearning from AI feedback (RLAIF) with direct preference optimization (DPO).\nThe resulting aligned CodeLlama-7B-Instruct model outperforms larger LLMs in\nterms of alignment with coding preferences and shows improved functional\ncorrectness on the HumanEval+ benchmark compared to the original instruct\nmodel. Therefore, our contributions bridge the gap in preference tuning of LLMs\nfor code and set the stage for further advancements in model alignment and\nRLAIF in automated software engineering."
                },
                "authors": [
                    {
                        "name": "Martin Weyssow"
                    },
                    {
                        "name": "Aton Kamanda"
                    },
                    {
                        "name": "Xin Zhou"
                    },
                    {
                        "name": "Houari Sahraoui"
                    }
                ],
                "author_detail": {
                    "name": "Houari Sahraoui"
                },
                "author": "Houari Sahraoui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.09032v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.09032v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19450v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19450v1",
                "updated": "2024-12-27T04:37:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    4,
                    37,
                    39,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T04:37:39Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    4,
                    37,
                    39,
                    4,
                    362,
                    0
                ],
                "title": "Find the Intention of Instruction: Comprehensive Evaluation of\n  Instruction Understanding for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Find the Intention of Instruction: Comprehensive Evaluation of\n  Instruction Understanding for Large Language Models"
                },
                "summary": "One of the key strengths of Large Language Models (LLMs) is their ability to\ninteract with humans by generating appropriate responses to given instructions.\nThis ability, known as instruction-following capability, has established a\nfoundation for the use of LLMs across various fields and serves as a crucial\nmetric for evaluating their performance. While numerous evaluation benchmarks\nhave been developed, most focus solely on clear and coherent instructions.\nHowever, we have noted that LLMs can become easily distracted by\ninstruction-formatted statements, which may lead to an oversight of their\ninstruction comprehension skills. To address this issue, we introduce the\nIntention of Instruction (IoInst) benchmark. This benchmark evaluates LLMs'\ncapacity to remain focused and understand instructions without being misled by\nextraneous instructions. The primary objective of this benchmark is to identify\nthe appropriate instruction that accurately guides the generation of a given\ncontext. Our findings suggest that even recently introduced state-of-the-art\nmodels still lack instruction understanding capability. Along with the\nproposition of IoInst in this study, we also present broad analyses of the\nseveral strategies potentially applicable to IoInst.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One of the key strengths of Large Language Models (LLMs) is their ability to\ninteract with humans by generating appropriate responses to given instructions.\nThis ability, known as instruction-following capability, has established a\nfoundation for the use of LLMs across various fields and serves as a crucial\nmetric for evaluating their performance. While numerous evaluation benchmarks\nhave been developed, most focus solely on clear and coherent instructions.\nHowever, we have noted that LLMs can become easily distracted by\ninstruction-formatted statements, which may lead to an oversight of their\ninstruction comprehension skills. To address this issue, we introduce the\nIntention of Instruction (IoInst) benchmark. This benchmark evaluates LLMs'\ncapacity to remain focused and understand instructions without being misled by\nextraneous instructions. The primary objective of this benchmark is to identify\nthe appropriate instruction that accurately guides the generation of a given\ncontext. Our findings suggest that even recently introduced state-of-the-art\nmodels still lack instruction understanding capability. Along with the\nproposition of IoInst in this study, we also present broad analyses of the\nseveral strategies potentially applicable to IoInst."
                },
                "authors": [
                    {
                        "name": "Hyeonseok Moon"
                    },
                    {
                        "name": "Jaehyung Seo"
                    },
                    {
                        "name": "Seungyoon Lee"
                    },
                    {
                        "name": "Chanjun Park"
                    },
                    {
                        "name": "Heuiseok Lim"
                    }
                ],
                "author_detail": {
                    "name": "Heuiseok Lim"
                },
                "author": "Heuiseok Lim",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19450v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19450v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19442v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19442v1",
                "updated": "2024-12-27T04:17:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    4,
                    17,
                    57,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T04:17:57Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    4,
                    17,
                    57,
                    4,
                    362,
                    0
                ],
                "title": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management"
                },
                "summary": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}."
                },
                "authors": [
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Anxin Tian"
                    },
                    {
                        "name": "Tianhao Tang"
                    },
                    {
                        "name": "Zhanchao Xu"
                    },
                    {
                        "name": "Xuejia Chen"
                    },
                    {
                        "name": "Nicole Hu"
                    },
                    {
                        "name": "Wei Dong"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19442v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19442v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19437v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19437v1",
                "updated": "2024-12-27T04:03:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    4,
                    3,
                    16,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T04:03:16Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    4,
                    3,
                    16,
                    4,
                    362,
                    0
                ],
                "title": "DeepSeek-V3 Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepSeek-V3 Technical Report"
                },
                "summary": "We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with\n671B total parameters with 37B activated for each token. To achieve efficient\ninference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent\nAttention (MLA) and DeepSeekMoE architectures, which were thoroughly validated\nin DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free\nstrategy for load balancing and sets a multi-token prediction training\nobjective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion\ndiverse and high-quality tokens, followed by Supervised Fine-Tuning and\nReinforcement Learning stages to fully harness its capabilities. Comprehensive\nevaluations reveal that DeepSeek-V3 outperforms other open-source models and\nachieves performance comparable to leading closed-source models. Despite its\nexcellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its\nfull training. In addition, its training process is remarkably stable.\nThroughout the entire training process, we did not experience any irrecoverable\nloss spikes or perform any rollbacks. The model checkpoints are available at\nhttps://github.com/deepseek-ai/DeepSeek-V3.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with\n671B total parameters with 37B activated for each token. To achieve efficient\ninference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent\nAttention (MLA) and DeepSeekMoE architectures, which were thoroughly validated\nin DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free\nstrategy for load balancing and sets a multi-token prediction training\nobjective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion\ndiverse and high-quality tokens, followed by Supervised Fine-Tuning and\nReinforcement Learning stages to fully harness its capabilities. Comprehensive\nevaluations reveal that DeepSeek-V3 outperforms other open-source models and\nachieves performance comparable to leading closed-source models. Despite its\nexcellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its\nfull training. In addition, its training process is remarkably stable.\nThroughout the entire training process, we did not experience any irrecoverable\nloss spikes or perform any rollbacks. The model checkpoints are available at\nhttps://github.com/deepseek-ai/DeepSeek-V3."
                },
                "authors": [
                    {
                        "name": "DeepSeek-AI"
                    },
                    {
                        "name": "Aixin Liu"
                    },
                    {
                        "name": "Bei Feng"
                    },
                    {
                        "name": "Bing Xue"
                    },
                    {
                        "name": "Bingxuan Wang"
                    },
                    {
                        "name": "Bochao Wu"
                    },
                    {
                        "name": "Chengda Lu"
                    },
                    {
                        "name": "Chenggang Zhao"
                    },
                    {
                        "name": "Chengqi Deng"
                    },
                    {
                        "name": "Chenyu Zhang"
                    },
                    {
                        "name": "Chong Ruan"
                    },
                    {
                        "name": "Damai Dai"
                    },
                    {
                        "name": "Daya Guo"
                    },
                    {
                        "name": "Dejian Yang"
                    },
                    {
                        "name": "Deli Chen"
                    },
                    {
                        "name": "Dongjie Ji"
                    },
                    {
                        "name": "Erhang Li"
                    },
                    {
                        "name": "Fangyun Lin"
                    },
                    {
                        "name": "Fucong Dai"
                    },
                    {
                        "name": "Fuli Luo"
                    },
                    {
                        "name": "Guangbo Hao"
                    },
                    {
                        "name": "Guanting Chen"
                    },
                    {
                        "name": "Guowei Li"
                    },
                    {
                        "name": "H. Zhang"
                    },
                    {
                        "name": "Han Bao"
                    },
                    {
                        "name": "Hanwei Xu"
                    },
                    {
                        "name": "Haocheng Wang"
                    },
                    {
                        "name": "Haowei Zhang"
                    },
                    {
                        "name": "Honghui Ding"
                    },
                    {
                        "name": "Huajian Xin"
                    },
                    {
                        "name": "Huazuo Gao"
                    },
                    {
                        "name": "Hui Li"
                    },
                    {
                        "name": "Hui Qu"
                    },
                    {
                        "name": "J. L. Cai"
                    },
                    {
                        "name": "Jian Liang"
                    },
                    {
                        "name": "Jianzhong Guo"
                    },
                    {
                        "name": "Jiaqi Ni"
                    },
                    {
                        "name": "Jiashi Li"
                    },
                    {
                        "name": "Jiawei Wang"
                    },
                    {
                        "name": "Jin Chen"
                    },
                    {
                        "name": "Jingchang Chen"
                    },
                    {
                        "name": "Jingyang Yuan"
                    },
                    {
                        "name": "Junjie Qiu"
                    },
                    {
                        "name": "Junlong Li"
                    },
                    {
                        "name": "Junxiao Song"
                    },
                    {
                        "name": "Kai Dong"
                    },
                    {
                        "name": "Kai Hu"
                    },
                    {
                        "name": "Kaige Gao"
                    },
                    {
                        "name": "Kang Guan"
                    },
                    {
                        "name": "Kexin Huang"
                    },
                    {
                        "name": "Kuai Yu"
                    },
                    {
                        "name": "Lean Wang"
                    },
                    {
                        "name": "Lecong Zhang"
                    },
                    {
                        "name": "Lei Xu"
                    },
                    {
                        "name": "Leyi Xia"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Litong Wang"
                    },
                    {
                        "name": "Liyue Zhang"
                    },
                    {
                        "name": "Meng Li"
                    },
                    {
                        "name": "Miaojun Wang"
                    },
                    {
                        "name": "Mingchuan Zhang"
                    },
                    {
                        "name": "Minghua Zhang"
                    },
                    {
                        "name": "Minghui Tang"
                    },
                    {
                        "name": "Mingming Li"
                    },
                    {
                        "name": "Ning Tian"
                    },
                    {
                        "name": "Panpan Huang"
                    },
                    {
                        "name": "Peiyi Wang"
                    },
                    {
                        "name": "Peng Zhang"
                    },
                    {
                        "name": "Qiancheng Wang"
                    },
                    {
                        "name": "Qihao Zhu"
                    },
                    {
                        "name": "Qinyu Chen"
                    },
                    {
                        "name": "Qiushi Du"
                    },
                    {
                        "name": "R. J. Chen"
                    },
                    {
                        "name": "R. L. Jin"
                    },
                    {
                        "name": "Ruiqi Ge"
                    },
                    {
                        "name": "Ruisong Zhang"
                    },
                    {
                        "name": "Ruizhe Pan"
                    },
                    {
                        "name": "Runji Wang"
                    },
                    {
                        "name": "Runxin Xu"
                    },
                    {
                        "name": "Ruoyu Zhang"
                    },
                    {
                        "name": "Ruyi Chen"
                    },
                    {
                        "name": "S. S. Li"
                    },
                    {
                        "name": "Shanghao Lu"
                    },
                    {
                        "name": "Shangyan Zhou"
                    },
                    {
                        "name": "Shanhuang Chen"
                    },
                    {
                        "name": "Shaoqing Wu"
                    },
                    {
                        "name": "Shengfeng Ye"
                    },
                    {
                        "name": "Shengfeng Ye"
                    },
                    {
                        "name": "Shirong Ma"
                    },
                    {
                        "name": "Shiyu Wang"
                    },
                    {
                        "name": "Shuang Zhou"
                    },
                    {
                        "name": "Shuiping Yu"
                    },
                    {
                        "name": "Shunfeng Zhou"
                    },
                    {
                        "name": "Shuting Pan"
                    },
                    {
                        "name": "T. Wang"
                    },
                    {
                        "name": "Tao Yun"
                    },
                    {
                        "name": "Tian Pei"
                    },
                    {
                        "name": "Tianyu Sun"
                    },
                    {
                        "name": "W. L. Xiao"
                    },
                    {
                        "name": "Wangding Zeng"
                    },
                    {
                        "name": "Wanjia Zhao"
                    },
                    {
                        "name": "Wei An"
                    },
                    {
                        "name": "Wen Liu"
                    },
                    {
                        "name": "Wenfeng Liang"
                    },
                    {
                        "name": "Wenjun Gao"
                    },
                    {
                        "name": "Wenqin Yu"
                    },
                    {
                        "name": "Wentao Zhang"
                    },
                    {
                        "name": "X. Q. Li"
                    },
                    {
                        "name": "Xiangyue Jin"
                    },
                    {
                        "name": "Xianzu Wang"
                    },
                    {
                        "name": "Xiao Bi"
                    },
                    {
                        "name": "Xiaodong Liu"
                    },
                    {
                        "name": "Xiaohan Wang"
                    },
                    {
                        "name": "Xiaojin Shen"
                    },
                    {
                        "name": "Xiaokang Chen"
                    },
                    {
                        "name": "Xiaokang Zhang"
                    },
                    {
                        "name": "Xiaosha Chen"
                    },
                    {
                        "name": "Xiaotao Nie"
                    },
                    {
                        "name": "Xiaowen Sun"
                    },
                    {
                        "name": "Xiaoxiang Wang"
                    },
                    {
                        "name": "Xin Cheng"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Xin Xie"
                    },
                    {
                        "name": "Xingchao Liu"
                    },
                    {
                        "name": "Xingkai Yu"
                    },
                    {
                        "name": "Xinnan Song"
                    },
                    {
                        "name": "Xinxia Shan"
                    },
                    {
                        "name": "Xinyi Zhou"
                    },
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Xinyuan Li"
                    },
                    {
                        "name": "Xuecheng Su"
                    },
                    {
                        "name": "Xuheng Lin"
                    },
                    {
                        "name": "Y. K. Li"
                    },
                    {
                        "name": "Y. Q. Wang"
                    },
                    {
                        "name": "Y. X. Wei"
                    },
                    {
                        "name": "Y. X. Zhu"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Yanhong Xu"
                    },
                    {
                        "name": "Yanhong Xu"
                    },
                    {
                        "name": "Yanping Huang"
                    },
                    {
                        "name": "Yao Li"
                    },
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Yaofeng Sun"
                    },
                    {
                        "name": "Yaohui Li"
                    },
                    {
                        "name": "Yaohui Wang"
                    },
                    {
                        "name": "Yi Yu"
                    },
                    {
                        "name": "Yi Zheng"
                    },
                    {
                        "name": "Yichao Zhang"
                    },
                    {
                        "name": "Yifan Shi"
                    },
                    {
                        "name": "Yiliang Xiong"
                    },
                    {
                        "name": "Ying He"
                    },
                    {
                        "name": "Ying Tang"
                    },
                    {
                        "name": "Yishi Piao"
                    },
                    {
                        "name": "Yisong Wang"
                    },
                    {
                        "name": "Yixuan Tan"
                    },
                    {
                        "name": "Yiyang Ma"
                    },
                    {
                        "name": "Yiyuan Liu"
                    },
                    {
                        "name": "Yongqiang Guo"
                    },
                    {
                        "name": "Yu Wu"
                    },
                    {
                        "name": "Yuan Ou"
                    },
                    {
                        "name": "Yuchen Zhu"
                    },
                    {
                        "name": "Yuduan Wang"
                    },
                    {
                        "name": "Yue Gong"
                    },
                    {
                        "name": "Yuheng Zou"
                    },
                    {
                        "name": "Yujia He"
                    },
                    {
                        "name": "Yukun Zha"
                    },
                    {
                        "name": "Yunfan Xiong"
                    },
                    {
                        "name": "Yunxian Ma"
                    },
                    {
                        "name": "Yuting Yan"
                    },
                    {
                        "name": "Yuxiang Luo"
                    },
                    {
                        "name": "Yuxiang You"
                    },
                    {
                        "name": "Yuxuan Liu"
                    },
                    {
                        "name": "Yuyang Zhou"
                    },
                    {
                        "name": "Z. F. Wu"
                    },
                    {
                        "name": "Z. Z. Ren"
                    },
                    {
                        "name": "Zehui Ren"
                    },
                    {
                        "name": "Zhangli Sha"
                    },
                    {
                        "name": "Zhe Fu"
                    },
                    {
                        "name": "Zhean Xu"
                    },
                    {
                        "name": "Zhen Huang"
                    },
                    {
                        "name": "Zhen Zhang"
                    },
                    {
                        "name": "Zhenda Xie"
                    },
                    {
                        "name": "Zhengyan Zhang"
                    },
                    {
                        "name": "Zhewen Hao"
                    },
                    {
                        "name": "Zhibin Gou"
                    },
                    {
                        "name": "Zhicheng Ma"
                    },
                    {
                        "name": "Zhigang Yan"
                    },
                    {
                        "name": "Zhihong Shao"
                    },
                    {
                        "name": "Zhipeng Xu"
                    },
                    {
                        "name": "Zhiyu Wu"
                    },
                    {
                        "name": "Zhongyu Zhang"
                    },
                    {
                        "name": "Zhuoshu Li"
                    },
                    {
                        "name": "Zihui Gu"
                    },
                    {
                        "name": "Zijia Zhu"
                    },
                    {
                        "name": "Zijun Liu"
                    },
                    {
                        "name": "Zilin Li"
                    },
                    {
                        "name": "Ziwei Xie"
                    },
                    {
                        "name": "Ziyang Song"
                    },
                    {
                        "name": "Ziyi Gao"
                    },
                    {
                        "name": "Zizheng Pan"
                    }
                ],
                "author_detail": {
                    "name": "Zizheng Pan"
                },
                "author": "Zizheng Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19437v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19437v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19424v1",
                "updated": "2024-12-27T03:29:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    3,
                    29,
                    10,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T03:29:10Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    3,
                    29,
                    10,
                    4,
                    362,
                    0
                ],
                "title": "Temporal Context Consistency Above All: Enhancing Long-Term Anticipation\n  by Learning and Enforcing Temporal Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Context Consistency Above All: Enhancing Long-Term Anticipation\n  by Learning and Enforcing Temporal Constraints"
                },
                "summary": "This paper proposes a method for long-term action anticipation (LTA), the\ntask of predicting action labels and their duration in a video given the\nobservation of an initial untrimmed video interval. We build on an\nencoder-decoder architecture with parallel decoding and make two key\ncontributions. First, we introduce a bi-directional action context regularizer\nmodule on the top of the decoder that ensures temporal context coherence in\ntemporally adjacent segments. Second, we learn from classified segments a\ntransition matrix that models the probability of transitioning from one action\nto another and the sequence is optimized globally over the full prediction\ninterval. In addition, we use a specialized encoder for the task of action\nsegmentation to increase the quality of the predictions in the observation\ninterval at inference time, leading to a better understanding of the past. We\nvalidate our methods on four benchmark datasets for LTA, the EpicKitchen-55,\nEGTEA+, 50Salads and Breakfast demonstrating superior or comparable performance\nto state-of-the-art methods, including probabilistic models and also those\nbased on Large Language Models, that assume trimmed video as input. The code\nwill be released upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a method for long-term action anticipation (LTA), the\ntask of predicting action labels and their duration in a video given the\nobservation of an initial untrimmed video interval. We build on an\nencoder-decoder architecture with parallel decoding and make two key\ncontributions. First, we introduce a bi-directional action context regularizer\nmodule on the top of the decoder that ensures temporal context coherence in\ntemporally adjacent segments. Second, we learn from classified segments a\ntransition matrix that models the probability of transitioning from one action\nto another and the sequence is optimized globally over the full prediction\ninterval. In addition, we use a specialized encoder for the task of action\nsegmentation to increase the quality of the predictions in the observation\ninterval at inference time, leading to a better understanding of the past. We\nvalidate our methods on four benchmark datasets for LTA, the EpicKitchen-55,\nEGTEA+, 50Salads and Breakfast demonstrating superior or comparable performance\nto state-of-the-art methods, including probabilistic models and also those\nbased on Large Language Models, that assume trimmed video as input. The code\nwill be released upon acceptance."
                },
                "authors": [
                    {
                        "name": "Alberto Maté"
                    },
                    {
                        "name": "Mariella Dimiccoli"
                    }
                ],
                "author_detail": {
                    "name": "Mariella Dimiccoli"
                },
                "author": "Mariella Dimiccoli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19423v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19423v1",
                "updated": "2024-12-27T03:17:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    3,
                    17,
                    26,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T03:17:26Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    3,
                    17,
                    26,
                    4,
                    362,
                    0
                ],
                "title": "Revisiting PCA for time series reduction in temporal dimension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting PCA for time series reduction in temporal dimension"
                },
                "summary": "Revisiting PCA for Time Series Reduction in Temporal Dimension; Jiaxin Gao,\nWenbo Hu, Yuntian Chen; Deep learning has significantly advanced time series\nanalysis (TSA), enabling the extraction of complex patterns for tasks like\nclassification, forecasting, and regression. Although dimensionality reduction\nhas traditionally focused on the variable space-achieving notable success in\nminimizing data redundancy and computational complexity-less attention has been\npaid to reducing the temporal dimension. In this study, we revisit Principal\nComponent Analysis (PCA), a classical dimensionality reduction technique, to\nexplore its utility in temporal dimension reduction for time series data. It is\ngenerally thought that applying PCA to the temporal dimension would disrupt\ntemporal dependencies, leading to limited exploration in this area. However,\nour theoretical analysis and extensive experiments demonstrate that applying\nPCA to sliding series windows not only maintains model performance, but also\nenhances computational efficiency. In auto-regressive forecasting, the temporal\nstructure is partially preserved through windowing, and PCA is applied within\nthese windows to denoise the time series while retaining their statistical\ninformation. By preprocessing time-series data with PCA, we reduce the temporal\ndimensionality before feeding it into TSA models such as Linear, Transformer,\nCNN, and RNN architectures. This approach accelerates training and inference\nand reduces resource consumption. Notably, PCA improves Informer training and\ninference speed by up to 40% and decreases GPU memory usage of TimesNet by 30%,\nwithout sacrificing model accuracy. Comparative analysis against other\nreduction methods further highlights the effectiveness of PCA in improving the\nefficiency of TSA models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting PCA for Time Series Reduction in Temporal Dimension; Jiaxin Gao,\nWenbo Hu, Yuntian Chen; Deep learning has significantly advanced time series\nanalysis (TSA), enabling the extraction of complex patterns for tasks like\nclassification, forecasting, and regression. Although dimensionality reduction\nhas traditionally focused on the variable space-achieving notable success in\nminimizing data redundancy and computational complexity-less attention has been\npaid to reducing the temporal dimension. In this study, we revisit Principal\nComponent Analysis (PCA), a classical dimensionality reduction technique, to\nexplore its utility in temporal dimension reduction for time series data. It is\ngenerally thought that applying PCA to the temporal dimension would disrupt\ntemporal dependencies, leading to limited exploration in this area. However,\nour theoretical analysis and extensive experiments demonstrate that applying\nPCA to sliding series windows not only maintains model performance, but also\nenhances computational efficiency. In auto-regressive forecasting, the temporal\nstructure is partially preserved through windowing, and PCA is applied within\nthese windows to denoise the time series while retaining their statistical\ninformation. By preprocessing time-series data with PCA, we reduce the temporal\ndimensionality before feeding it into TSA models such as Linear, Transformer,\nCNN, and RNN architectures. This approach accelerates training and inference\nand reduces resource consumption. Notably, PCA improves Informer training and\ninference speed by up to 40% and decreases GPU memory usage of TimesNet by 30%,\nwithout sacrificing model accuracy. Comparative analysis against other\nreduction methods further highlights the effectiveness of PCA in improving the\nefficiency of TSA models."
                },
                "authors": [
                    {
                        "name": "Jiaxin Gao"
                    },
                    {
                        "name": "Wenbo Hu"
                    },
                    {
                        "name": "Yuntian Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yuntian Chen"
                },
                "author": "Yuntian Chen",
                "arxiv_comment": "13 pages, 5 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19423v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19423v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13949v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13949v2",
                "updated": "2024-12-27T03:00:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    3,
                    0,
                    19,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-18T15:29:30Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    15,
                    29,
                    30,
                    2,
                    353,
                    0
                ],
                "title": "Cracking the Code of Hallucination in LVLMs with Vision-aware Head\n  Divergence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cracking the Code of Hallucination in LVLMs with Vision-aware Head\n  Divergence"
                },
                "summary": "Large vision-language models (LVLMs) have made substantial progress in\nintegrating large language models (LLMs) with visual inputs, enabling advanced\nmultimodal reasoning. Despite their success, a persistent challenge is\nhallucination-where generated text fails to accurately reflect visual\ncontent-undermining both accuracy and reliability. Existing methods focus on\nalignment training or decoding refinements but primarily address symptoms at\nthe generation stage without probing the underlying causes. In this work, we\ninvestigate the internal mechanisms driving hallucination in LVLMs, with an\nemphasis on the multi-head attention module. Specifically, we introduce\nVision-aware Head Divergence (VHD), a metric that quantifies the sensitivity of\nattention head outputs to visual context. Based on this, our findings reveal\nthe presence of vision-aware attention heads that are more attuned to visual\ninformation; however, the model's overreliance on its prior language patterns\nis closely related to hallucinations. Building on these insights, we propose\nVision-aware Head Reinforcement (VHR), a training-free approach to mitigate\nhallucination by enhancing the role of vision-aware attention heads. Extensive\nexperiments demonstrate that our method achieves superior performance compared\nto state-of-the-art approaches in mitigating hallucinations, while maintaining\nhigh efficiency with negligible additional time overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large vision-language models (LVLMs) have made substantial progress in\nintegrating large language models (LLMs) with visual inputs, enabling advanced\nmultimodal reasoning. Despite their success, a persistent challenge is\nhallucination-where generated text fails to accurately reflect visual\ncontent-undermining both accuracy and reliability. Existing methods focus on\nalignment training or decoding refinements but primarily address symptoms at\nthe generation stage without probing the underlying causes. In this work, we\ninvestigate the internal mechanisms driving hallucination in LVLMs, with an\nemphasis on the multi-head attention module. Specifically, we introduce\nVision-aware Head Divergence (VHD), a metric that quantifies the sensitivity of\nattention head outputs to visual context. Based on this, our findings reveal\nthe presence of vision-aware attention heads that are more attuned to visual\ninformation; however, the model's overreliance on its prior language patterns\nis closely related to hallucinations. Building on these insights, we propose\nVision-aware Head Reinforcement (VHR), a training-free approach to mitigate\nhallucination by enhancing the role of vision-aware attention heads. Extensive\nexperiments demonstrate that our method achieves superior performance compared\nto state-of-the-art approaches in mitigating hallucinations, while maintaining\nhigh efficiency with negligible additional time overhead."
                },
                "authors": [
                    {
                        "name": "Jinghan He"
                    },
                    {
                        "name": "Kuan Zhu"
                    },
                    {
                        "name": "Haiyun Guo"
                    },
                    {
                        "name": "Junfeng Fang"
                    },
                    {
                        "name": "Zhenglin Hua"
                    },
                    {
                        "name": "Yuheng Jia"
                    },
                    {
                        "name": "Ming Tang"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    },
                    {
                        "name": "Jinqiao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jinqiao Wang"
                },
                "author": "Jinqiao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13949v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13949v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15519v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15519v2",
                "updated": "2024-12-27T02:48:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    2,
                    48,
                    4,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-20T03:15:02Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    3,
                    15,
                    2,
                    4,
                    355,
                    0
                ],
                "title": "PreNeT: Leveraging Computational Features to Predict Deep Neural Network\n  Training Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PreNeT: Leveraging Computational Features to Predict Deep Neural Network\n  Training Time"
                },
                "summary": "Training deep learning models, particularly Transformer-based architectures\nsuch as Large Language Models (LLMs), demands substantial computational\nresources and extended training periods. While optimal configuration and\ninfrastructure selection can significantly reduce associated costs, this\noptimization requires preliminary analysis tools. This paper introduces PreNeT,\na novel predictive framework designed to address this optimization challenge.\nPreNeT facilitates training optimization by integrating comprehensive\ncomputational metrics, including layer-specific parameters, arithmetic\noperations and memory utilization. A key feature of PreNeT is its capacity to\naccurately predict training duration on previously unexamined hardware\ninfrastructures, including novel accelerator architectures. This framework\nemploys a sophisticated approach to capture and analyze the distinct\ncharacteristics of various neural network layers, thereby enhancing existing\nprediction methodologies. Through proactive implementation of PreNeT,\nresearchers and practitioners can determine optimal configurations, parameter\nsettings, and hardware specifications to maximize cost-efficiency and minimize\ntraining duration. Experimental results demonstrate that PreNeT achieves up to\n72% improvement in prediction accuracy compared to contemporary\nstate-of-the-art frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training deep learning models, particularly Transformer-based architectures\nsuch as Large Language Models (LLMs), demands substantial computational\nresources and extended training periods. While optimal configuration and\ninfrastructure selection can significantly reduce associated costs, this\noptimization requires preliminary analysis tools. This paper introduces PreNeT,\na novel predictive framework designed to address this optimization challenge.\nPreNeT facilitates training optimization by integrating comprehensive\ncomputational metrics, including layer-specific parameters, arithmetic\noperations and memory utilization. A key feature of PreNeT is its capacity to\naccurately predict training duration on previously unexamined hardware\ninfrastructures, including novel accelerator architectures. This framework\nemploys a sophisticated approach to capture and analyze the distinct\ncharacteristics of various neural network layers, thereby enhancing existing\nprediction methodologies. Through proactive implementation of PreNeT,\nresearchers and practitioners can determine optimal configurations, parameter\nsettings, and hardware specifications to maximize cost-efficiency and minimize\ntraining duration. Experimental results demonstrate that PreNeT achieves up to\n72% improvement in prediction accuracy compared to contemporary\nstate-of-the-art frameworks."
                },
                "authors": [
                    {
                        "name": "Alireza Pourali"
                    },
                    {
                        "name": "Arian Boukani"
                    },
                    {
                        "name": "Hamzeh Khazaei"
                    }
                ],
                "author_detail": {
                    "name": "Hamzeh Khazaei"
                },
                "author": "Hamzeh Khazaei",
                "arxiv_comment": "11 pages, Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15519v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15519v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00840v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00840v3",
                "updated": "2024-12-27T02:37:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    2,
                    37,
                    36,
                    4,
                    362,
                    0
                ],
                "published": "2024-08-01T18:00:07Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    18,
                    0,
                    7,
                    3,
                    214,
                    0
                ],
                "title": "Merging White Dwarf Binaries Produce Type Ia Supernovae in Elliptical\n  Galaxies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Merging White Dwarf Binaries Produce Type Ia Supernovae in Elliptical\n  Galaxies"
                },
                "summary": "I find that Type Ia supernovae (SNe Ia) with bimodal nebular emission\nprofiles occur almost exclusively in massive ($M_\\star \\gtrsim\n10^{11}~M_\\odot$) galaxies with low star-formation rates (SFR~$\\lesssim\n0.5~M_\\odot$/yr). The bimodal profiles are likely produced by two white dwarfs\nthat exploded during a merger or collision, supported by a correlation between\nthe peak-to-peak velocity separation ($v_{\\rm sep}$) and the SN Ia peak\nluminosity ($M_V$) which arises naturally from more massive white dwarf\nbinaries synthesizing more $^{56}$Ni during the explosion. The quiescent hosts\nare consistent with the long delay times required to form double white dwarf\nbinaries. The distributions of SNe Ia with and without bimodal nebular lines\ndiffer in host mass, SFR, and specific SFR with K-S test probabilities of\n$3.1\\%$, $0.03\\%$, and $0.02\\%$, respectively. Viewing angle effects can fully\nexplain the SNe Ia in quiescent hosts without bimodal emission profiles and the\ndearth of merger/collision driven SNe Ia in star-forming hosts requires at\nleast two distinct progenitor channels for normal SNe Ia. $30-40\\%$ of all SNe\nIa originate from mergers or collisions depending on how cleanly host\nenvironment distinguishes progenitor scenarios. The bimodal SNe Ia share some\ncharacteristics with the underluminous 91bg-like SNe Ia that also prefer older\npopulations, but there is no unambiguous connection between the two\nclassifications. This may suggest separate processes or multiple axes of ejecta\n(a)symmetry. Existing models for white dwarf mergers and collisions broadly\nreproduce the $v_{\\rm sep} - M_V$ correlation and future analyses may be able\nto infer the masses/mass-ratios of merging white dwarfs in external galaxies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I find that Type Ia supernovae (SNe Ia) with bimodal nebular emission\nprofiles occur almost exclusively in massive ($M_\\star \\gtrsim\n10^{11}~M_\\odot$) galaxies with low star-formation rates (SFR~$\\lesssim\n0.5~M_\\odot$/yr). The bimodal profiles are likely produced by two white dwarfs\nthat exploded during a merger or collision, supported by a correlation between\nthe peak-to-peak velocity separation ($v_{\\rm sep}$) and the SN Ia peak\nluminosity ($M_V$) which arises naturally from more massive white dwarf\nbinaries synthesizing more $^{56}$Ni during the explosion. The quiescent hosts\nare consistent with the long delay times required to form double white dwarf\nbinaries. The distributions of SNe Ia with and without bimodal nebular lines\ndiffer in host mass, SFR, and specific SFR with K-S test probabilities of\n$3.1\\%$, $0.03\\%$, and $0.02\\%$, respectively. Viewing angle effects can fully\nexplain the SNe Ia in quiescent hosts without bimodal emission profiles and the\ndearth of merger/collision driven SNe Ia in star-forming hosts requires at\nleast two distinct progenitor channels for normal SNe Ia. $30-40\\%$ of all SNe\nIa originate from mergers or collisions depending on how cleanly host\nenvironment distinguishes progenitor scenarios. The bimodal SNe Ia share some\ncharacteristics with the underluminous 91bg-like SNe Ia that also prefer older\npopulations, but there is no unambiguous connection between the two\nclassifications. This may suggest separate processes or multiple axes of ejecta\n(a)symmetry. Existing models for white dwarf mergers and collisions broadly\nreproduce the $v_{\\rm sep} - M_V$ correlation and future analyses may be able\nto infer the masses/mass-ratios of merging white dwarfs in external galaxies."
                },
                "authors": [
                    {
                        "name": "Michael A. Tucker"
                    }
                ],
                "author_detail": {
                    "name": "Michael A. Tucker"
                },
                "author": "Michael A. Tucker",
                "arxiv_doi": "10.1093/mnrasl/slae121",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/mnrasl/slae121",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.00840v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00840v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "7 pages, 4 figures, 1 table. Accepted to MNRAS Letters",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17647v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17647v4",
                "updated": "2024-12-27T02:20:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    2,
                    20,
                    36,
                    4,
                    362,
                    0
                ],
                "published": "2024-09-26T08:51:29Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    8,
                    51,
                    29,
                    3,
                    270,
                    0
                ],
                "title": "MECD: Unlocking Multi-Event Causal Discovery in Video Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MECD: Unlocking Multi-Event Causal Discovery in Video Reasoning"
                },
                "summary": "Video causal reasoning aims to achieve a high-level understanding of video\ncontent from a causal perspective. However, current video reasoning tasks are\nlimited in scope, primarily executed in a question-answering paradigm and\nfocusing on short videos containing only a single event and simple causal\nrelationships, lacking comprehensive and structured causality analysis for\nvideos with multiple events. To fill this gap, we introduce a new task and\ndataset, Multi-Event Causal Discovery (MECD). It aims to uncover the causal\nrelationships between events distributed chronologically across long videos.\nGiven visual segments and textual descriptions of events, MECD requires\nidentifying the causal associations between these events to derive a\ncomprehensive, structured event-level video causal diagram explaining why and\nhow the final result event occurred. To address MECD, we devise a novel\nframework inspired by the Granger Causality method, using an efficient\nmask-based event prediction model to perform an Event Granger Test, which\nestimates causality by comparing the predicted result event when premise events\nare masked versus unmasked. Furthermore, we integrate causal inference\ntechniques such as front-door adjustment and counterfactual inference to\naddress challenges in MECD like causality confounding and illusory causality.\nExperiments validate the effectiveness of our framework in providing causal\nrelationships in multi-event videos, outperforming GPT-4o and VideoLLaVA by\n5.7% and 4.1%, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video causal reasoning aims to achieve a high-level understanding of video\ncontent from a causal perspective. However, current video reasoning tasks are\nlimited in scope, primarily executed in a question-answering paradigm and\nfocusing on short videos containing only a single event and simple causal\nrelationships, lacking comprehensive and structured causality analysis for\nvideos with multiple events. To fill this gap, we introduce a new task and\ndataset, Multi-Event Causal Discovery (MECD). It aims to uncover the causal\nrelationships between events distributed chronologically across long videos.\nGiven visual segments and textual descriptions of events, MECD requires\nidentifying the causal associations between these events to derive a\ncomprehensive, structured event-level video causal diagram explaining why and\nhow the final result event occurred. To address MECD, we devise a novel\nframework inspired by the Granger Causality method, using an efficient\nmask-based event prediction model to perform an Event Granger Test, which\nestimates causality by comparing the predicted result event when premise events\nare masked versus unmasked. Furthermore, we integrate causal inference\ntechniques such as front-door adjustment and counterfactual inference to\naddress challenges in MECD like causality confounding and illusory causality.\nExperiments validate the effectiveness of our framework in providing causal\nrelationships in multi-event videos, outperforming GPT-4o and VideoLLaVA by\n5.7% and 4.1%, respectively."
                },
                "authors": [
                    {
                        "name": "Tieyuan Chen"
                    },
                    {
                        "name": "Huabin Liu"
                    },
                    {
                        "name": "Tianyao He"
                    },
                    {
                        "name": "Yihang Chen"
                    },
                    {
                        "name": "Chaofan Gan"
                    },
                    {
                        "name": "Xiao Ma"
                    },
                    {
                        "name": "Cheng Zhong"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Yingxue Wang"
                    },
                    {
                        "name": "Hui Lin"
                    },
                    {
                        "name": "Weiyao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Weiyao Lin"
                },
                "author": "Weiyao Lin",
                "arxiv_comment": "Accepted at NeurIPS 2024 as a spotlight paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17647v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17647v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19394v1",
                "updated": "2024-12-27T01:00:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    1,
                    0,
                    23,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T01:00:23Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    1,
                    0,
                    23,
                    4,
                    362,
                    0
                ],
                "title": "An Engorgio Prompt Makes Large Language Model Babble on",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Engorgio Prompt Makes Large Language Model Babble on"
                },
                "summary": "Auto-regressive large language models (LLMs) have yielded impressive\nperformance in many real-world tasks. However, the new paradigm of these LLMs\nalso exposes novel threats. In this paper, we explore their vulnerability to\ninference cost attacks, where a malicious user crafts Engorgio prompts to\nintentionally increase the computation cost and latency of the inference\nprocess. We design Engorgio, a novel methodology, to efficiently generate\nadversarial Engorgio prompts to affect the target LLM's service availability.\nEngorgio has the following two technical contributions. (1) We employ a\nparameterized distribution to track LLMs' prediction trajectory. (2) Targeting\nthe auto-regressive nature of LLMs' inference process, we propose novel loss\nfunctions to stably suppress the appearance of the <EOS> token, whose\noccurrence will interrupt the LLM's generation process. We conduct extensive\nexperiments on 13 open-sourced LLMs with parameters ranging from 125M to 30B.\nThe results show that Engorgio prompts can successfully induce LLMs to generate\nabnormally long outputs (i.e., roughly 2-13$\\times$ longer to reach 90%+ of the\noutput length limit) in a white-box scenario and our real-world experiment\ndemonstrates Engergio's threat to LLM service with limited computing resources.\nThe code is accessible at https://github.com/jianshuod/Engorgio-prompt.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive large language models (LLMs) have yielded impressive\nperformance in many real-world tasks. However, the new paradigm of these LLMs\nalso exposes novel threats. In this paper, we explore their vulnerability to\ninference cost attacks, where a malicious user crafts Engorgio prompts to\nintentionally increase the computation cost and latency of the inference\nprocess. We design Engorgio, a novel methodology, to efficiently generate\nadversarial Engorgio prompts to affect the target LLM's service availability.\nEngorgio has the following two technical contributions. (1) We employ a\nparameterized distribution to track LLMs' prediction trajectory. (2) Targeting\nthe auto-regressive nature of LLMs' inference process, we propose novel loss\nfunctions to stably suppress the appearance of the <EOS> token, whose\noccurrence will interrupt the LLM's generation process. We conduct extensive\nexperiments on 13 open-sourced LLMs with parameters ranging from 125M to 30B.\nThe results show that Engorgio prompts can successfully induce LLMs to generate\nabnormally long outputs (i.e., roughly 2-13$\\times$ longer to reach 90%+ of the\noutput length limit) in a white-box scenario and our real-world experiment\ndemonstrates Engergio's threat to LLM service with limited computing resources.\nThe code is accessible at https://github.com/jianshuod/Engorgio-prompt."
                },
                "authors": [
                    {
                        "name": "Jianshuo Dong"
                    },
                    {
                        "name": "Ziyuan Zhang"
                    },
                    {
                        "name": "Qingjie Zhang"
                    },
                    {
                        "name": "Han Qiu"
                    },
                    {
                        "name": "Tianwei Zhang"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Hewu Li"
                    },
                    {
                        "name": "Qi Li"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Ke Xu"
                    }
                ],
                "author_detail": {
                    "name": "Ke Xu"
                },
                "author": "Ke Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19381v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19381v1",
                "updated": "2024-12-26T23:50:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    23,
                    50,
                    2,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T23:50:02Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    23,
                    50,
                    2,
                    3,
                    361,
                    0
                ],
                "title": "Reconstruction of non-trivial magnetization textures from magnetic field\n  images using neural networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconstruction of non-trivial magnetization textures from magnetic field\n  images using neural networks"
                },
                "summary": "Spatial imaging of magnetic stray fields from magnetic materials is a useful\ntool for identifying the underlying magnetic configurations of the material.\nHowever, transforming the magnetic image into a magnetization image is an\nill-poised problem, which can result in artefacts that limit the inferences\nthat can be made on the material under investigation. In this work, we develop\na neural network fitting approach that approximates this transformation,\nreducing these artefacts. Additionally, we demonstrate that this approach\nallows the inclusion of additional models and bounds that are not possible with\ntraditional reconstruction methods. These advantages allow for the\nreconstruction of non-trivial magnetization textures with varying magnetization\ndirections in thin-film magnets, which was not possible previously. We\ndemonstrate this new capability by performing magnetization reconstructions on\na variety of topological spin textures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatial imaging of magnetic stray fields from magnetic materials is a useful\ntool for identifying the underlying magnetic configurations of the material.\nHowever, transforming the magnetic image into a magnetization image is an\nill-poised problem, which can result in artefacts that limit the inferences\nthat can be made on the material under investigation. In this work, we develop\na neural network fitting approach that approximates this transformation,\nreducing these artefacts. Additionally, we demonstrate that this approach\nallows the inclusion of additional models and bounds that are not possible with\ntraditional reconstruction methods. These advantages allow for the\nreconstruction of non-trivial magnetization textures with varying magnetization\ndirections in thin-film magnets, which was not possible previously. We\ndemonstrate this new capability by performing magnetization reconstructions on\na variety of topological spin textures."
                },
                "authors": [
                    {
                        "name": "David A. Broadway"
                    },
                    {
                        "name": "Mykhailo Flaks"
                    },
                    {
                        "name": "Adrien E. E. Dubois"
                    },
                    {
                        "name": "Patrick Maletinsky"
                    }
                ],
                "author_detail": {
                    "name": "Patrick Maletinsky"
                },
                "author": "Patrick Maletinsky",
                "arxiv_comment": "14 pages, 9 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19381v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19381v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19363v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19363v1",
                "updated": "2024-12-26T22:06:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    22,
                    6,
                    29,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T22:06:29Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    22,
                    6,
                    29,
                    3,
                    361,
                    0
                ],
                "title": "Large Language Models for Market Research: A Data-augmentation Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Market Research: A Data-augmentation Approach"
                },
                "summary": "Large Language Models (LLMs) have transformed artificial intelligence by\nexcelling in complex natural language processing tasks. Their ability to\ngenerate human-like text has opened new possibilities for market research,\nparticularly in conjoint analysis, where understanding consumer preferences is\nessential but often resource-intensive. Traditional survey-based methods face\nlimitations in scalability and cost, making LLM-generated data a promising\nalternative. However, while LLMs have the potential to simulate real consumer\nbehavior, recent studies highlight a significant gap between LLM-generated and\nhuman data, with biases introduced when substituting between the two. In this\npaper, we address this gap by proposing a novel statistical data augmentation\napproach that efficiently integrates LLM-generated data with real data in\nconjoint analysis. Our method leverages transfer learning principles to debias\nthe LLM-generated data using a small amount of human data. This results in\nstatistically robust estimators with consistent and asymptotically normal\nproperties, in contrast to naive approaches that simply substitute human data\nwith LLM-generated data, which can exacerbate bias. We validate our framework\nthrough an empirical study on COVID-19 vaccine preferences, demonstrating its\nsuperior ability to reduce estimation error and save data and costs by 24.9\\%\nto 79.8\\%. In contrast, naive approaches fail to save data due to the inherent\nbiases in LLM-generated data compared to human data. Another empirical study on\nsports car choices validates the robustness of our results. Our findings\nsuggest that while LLM-generated data is not a direct substitute for human\nresponses, it can serve as a valuable complement when used within a robust\nstatistical framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have transformed artificial intelligence by\nexcelling in complex natural language processing tasks. Their ability to\ngenerate human-like text has opened new possibilities for market research,\nparticularly in conjoint analysis, where understanding consumer preferences is\nessential but often resource-intensive. Traditional survey-based methods face\nlimitations in scalability and cost, making LLM-generated data a promising\nalternative. However, while LLMs have the potential to simulate real consumer\nbehavior, recent studies highlight a significant gap between LLM-generated and\nhuman data, with biases introduced when substituting between the two. In this\npaper, we address this gap by proposing a novel statistical data augmentation\napproach that efficiently integrates LLM-generated data with real data in\nconjoint analysis. Our method leverages transfer learning principles to debias\nthe LLM-generated data using a small amount of human data. This results in\nstatistically robust estimators with consistent and asymptotically normal\nproperties, in contrast to naive approaches that simply substitute human data\nwith LLM-generated data, which can exacerbate bias. We validate our framework\nthrough an empirical study on COVID-19 vaccine preferences, demonstrating its\nsuperior ability to reduce estimation error and save data and costs by 24.9\\%\nto 79.8\\%. In contrast, naive approaches fail to save data due to the inherent\nbiases in LLM-generated data compared to human data. Another empirical study on\nsports car choices validates the robustness of our results. Our findings\nsuggest that while LLM-generated data is not a direct substitute for human\nresponses, it can serve as a valuable complement when used within a robust\nstatistical framework."
                },
                "authors": [
                    {
                        "name": "Mengxin Wang"
                    },
                    {
                        "name": "Dennis J. Zhang"
                    },
                    {
                        "name": "Heng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Heng Zhang"
                },
                "arxiv_affiliation": "W. P. Carey School of Business, Arizona State University",
                "author": "Heng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19363v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19363v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50, 90B60, 62F12",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; J.4; G.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19361v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19361v1",
                "updated": "2024-12-26T22:04:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    22,
                    4,
                    23,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T22:04:23Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    22,
                    4,
                    23,
                    3,
                    361,
                    0
                ],
                "title": "Dynamic Skill Adaptation for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Skill Adaptation for Large Language Models"
                },
                "summary": "We present Dynamic Skill Adaptation (DSA), an adaptive and dynamic framework\nto adapt novel and complex skills to Large Language Models (LLMs). Compared\nwith previous work which learns from human-curated and static data in random\norders, we propose to first automatically generate and organize the training\ndata by mimicking the learning pathways of human and then dynamically tailor\nthe training data based on the training dynamics. Specifically, inspired by the\nlearning structures and teaching strategies in the human education system, we\nfirst construct a skill graph by decomposing complex skills into sub-skills and\narranging them based on their dependencies in human syllables. For every skill,\nwe utilize LLMs to generate both textbook-like data which contains detailed\ndescriptions of skills for pre-training and exercise-like data which targets at\nexplicitly utilizing the skills to solve problems for instruction-tuning.\nFurthermore, during the instruction-tuning, we dynamically update the training\ndata which down-weight easy-to-learn examples, generate more complex examples,\nand filter out data with errors. Experiments on large language models such as\nLLAMA and Mistral demonstrate the effectiveness of our proposed methods in\nadapting math reasoning skills and social study skills.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Dynamic Skill Adaptation (DSA), an adaptive and dynamic framework\nto adapt novel and complex skills to Large Language Models (LLMs). Compared\nwith previous work which learns from human-curated and static data in random\norders, we propose to first automatically generate and organize the training\ndata by mimicking the learning pathways of human and then dynamically tailor\nthe training data based on the training dynamics. Specifically, inspired by the\nlearning structures and teaching strategies in the human education system, we\nfirst construct a skill graph by decomposing complex skills into sub-skills and\narranging them based on their dependencies in human syllables. For every skill,\nwe utilize LLMs to generate both textbook-like data which contains detailed\ndescriptions of skills for pre-training and exercise-like data which targets at\nexplicitly utilizing the skills to solve problems for instruction-tuning.\nFurthermore, during the instruction-tuning, we dynamically update the training\ndata which down-weight easy-to-learn examples, generate more complex examples,\nand filter out data with errors. Experiments on large language models such as\nLLAMA and Mistral demonstrate the effectiveness of our proposed methods in\nadapting math reasoning skills and social study skills."
                },
                "authors": [
                    {
                        "name": "Jiaao Chen"
                    },
                    {
                        "name": "Diyi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Diyi Yang"
                },
                "author": "Diyi Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19361v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04565v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04565v2",
                "updated": "2024-12-26T21:57:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    21,
                    57,
                    29,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-05T19:13:17Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    19,
                    13,
                    17,
                    3,
                    340,
                    0
                ],
                "title": "Solving High-dimensional Inverse Problems Using Amortized\n  Likelihood-free Inference with Noisy and Incomplete Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solving High-dimensional Inverse Problems Using Amortized\n  Likelihood-free Inference with Noisy and Incomplete Data"
                },
                "summary": "We present a likelihood-free probabilistic inversion method based on\nnormalizing flows for high-dimensional inverse problems. The proposed method is\ncomposed of two complementary networks: a summary network for data compression\nand an inference network for parameter estimation. The summary network encodes\nraw observations into a fixed-size vector of summary features, while the\ninference network generates samples of the approximate posterior distribution\nof the model parameters based on these summary features. The posterior samples\nare produced in a deep generative fashion by sampling from a latent Gaussian\ndistribution and passing these samples through an invertible transformation. We\nconstruct this invertible transformation by sequentially alternating\nconditional invertible neural network and conditional neural spline flow\nlayers. The summary and inference networks are trained simultaneously. We apply\nthe proposed method to an inversion problem in groundwater hydrology to\nestimate the posterior distribution of the log-conductivity field conditioned\non spatially sparse time-series observations of the system's hydraulic head\nresponses.The conductivity field is represented with 706 degrees of freedom in\nthe considered problem.The comparison with the likelihood-based iterative\nensemble smoother PEST-IES method demonstrates that the proposed method\naccurately estimates the parameter posterior distribution and the observations'\npredictive posterior distribution at a fraction of the inference time of\nPEST-IES.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a likelihood-free probabilistic inversion method based on\nnormalizing flows for high-dimensional inverse problems. The proposed method is\ncomposed of two complementary networks: a summary network for data compression\nand an inference network for parameter estimation. The summary network encodes\nraw observations into a fixed-size vector of summary features, while the\ninference network generates samples of the approximate posterior distribution\nof the model parameters based on these summary features. The posterior samples\nare produced in a deep generative fashion by sampling from a latent Gaussian\ndistribution and passing these samples through an invertible transformation. We\nconstruct this invertible transformation by sequentially alternating\nconditional invertible neural network and conditional neural spline flow\nlayers. The summary and inference networks are trained simultaneously. We apply\nthe proposed method to an inversion problem in groundwater hydrology to\nestimate the posterior distribution of the log-conductivity field conditioned\non spatially sparse time-series observations of the system's hydraulic head\nresponses.The conductivity field is represented with 706 degrees of freedom in\nthe considered problem.The comparison with the likelihood-based iterative\nensemble smoother PEST-IES method demonstrates that the proposed method\naccurately estimates the parameter posterior distribution and the observations'\npredictive posterior distribution at a fraction of the inference time of\nPEST-IES."
                },
                "authors": [
                    {
                        "name": "Jice Zeng"
                    },
                    {
                        "name": "Yuanzhe Wang"
                    },
                    {
                        "name": "Alexandre M. Tartakovsky"
                    },
                    {
                        "name": "David Barajas-Solano"
                    }
                ],
                "author_detail": {
                    "name": "David Barajas-Solano"
                },
                "author": "David Barajas-Solano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04565v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04565v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19351v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19351v1",
                "updated": "2024-12-26T21:13:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    21,
                    13,
                    12,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T21:13:12Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    21,
                    13,
                    12,
                    3,
                    361,
                    0
                ],
                "title": "ETTA: Elucidating the Design Space of Text-to-Audio Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ETTA: Elucidating the Design Space of Text-to-Audio Models"
                },
                "summary": "Recent years have seen significant progress in Text-To-Audio (TTA) synthesis,\nenabling users to enrich their creative workflows with synthetic audio\ngenerated from natural language prompts. Despite this progress, the effects of\ndata, model architecture, training objective functions, and sampling strategies\non target benchmarks are not well understood. With the purpose of providing a\nholistic understanding of the design space of TTA models, we set up a\nlarge-scale empirical experiment focused on diffusion and flow matching models.\nOur contributions include: 1) AF-Synthetic, a large dataset of high quality\nsynthetic captions obtained from an audio understanding model; 2) a systematic\ncomparison of different architectural, training, and inference design choices\nfor TTA models; 3) an analysis of sampling methods and their Pareto curves with\nrespect to generation quality and inference speed. We leverage the knowledge\nobtained from this extensive analysis to propose our best model dubbed\nElucidated Text-To-Audio (ETTA). When evaluated on AudioCaps and MusicCaps,\nETTA provides improvements over the baselines trained on publicly available\ndata, while being competitive with models trained on proprietary data. Finally,\nwe show ETTA's improved ability to generate creative audio following complex\nand imaginative captions -- a task that is more challenging than current\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent years have seen significant progress in Text-To-Audio (TTA) synthesis,\nenabling users to enrich their creative workflows with synthetic audio\ngenerated from natural language prompts. Despite this progress, the effects of\ndata, model architecture, training objective functions, and sampling strategies\non target benchmarks are not well understood. With the purpose of providing a\nholistic understanding of the design space of TTA models, we set up a\nlarge-scale empirical experiment focused on diffusion and flow matching models.\nOur contributions include: 1) AF-Synthetic, a large dataset of high quality\nsynthetic captions obtained from an audio understanding model; 2) a systematic\ncomparison of different architectural, training, and inference design choices\nfor TTA models; 3) an analysis of sampling methods and their Pareto curves with\nrespect to generation quality and inference speed. We leverage the knowledge\nobtained from this extensive analysis to propose our best model dubbed\nElucidated Text-To-Audio (ETTA). When evaluated on AudioCaps and MusicCaps,\nETTA provides improvements over the baselines trained on publicly available\ndata, while being competitive with models trained on proprietary data. Finally,\nwe show ETTA's improved ability to generate creative audio following complex\nand imaginative captions -- a task that is more challenging than current\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Sang-gil Lee"
                    },
                    {
                        "name": "Zhifeng Kong"
                    },
                    {
                        "name": "Arushi Goel"
                    },
                    {
                        "name": "Sungwon Kim"
                    },
                    {
                        "name": "Rafael Valle"
                    },
                    {
                        "name": "Bryan Catanzaro"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Catanzaro"
                },
                "author": "Bryan Catanzaro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19351v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19351v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19350v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19350v1",
                "updated": "2024-12-26T20:53:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    20,
                    53,
                    4,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T20:53:04Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    20,
                    53,
                    4,
                    3,
                    361,
                    0
                ],
                "title": "On the Expressiveness and Length Generalization of Selective State-Space\n  Models on Regular Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Expressiveness and Length Generalization of Selective State-Space\n  Models on Regular Languages"
                },
                "summary": "Selective state-space models (SSMs) are an emerging alternative to the\nTransformer, offering the unique advantage of parallel training and sequential\ninference. Although these models have shown promising performance on a variety\nof tasks, their formal expressiveness and length generalization properties\nremain underexplored. In this work, we provide insight into the workings of\nselective SSMs by analyzing their expressiveness and length generalization\nperformance on regular language tasks, i.e., finite-state automaton (FSA)\nemulation. We address certain limitations of modern SSM-based architectures by\nintroducing the Selective Dense State-Space Model (SD-SSM), the first selective\nSSM that exhibits perfect length generalization on a set of various regular\nlanguage tasks using a single layer. It utilizes a dictionary of dense\ntransition matrices, a softmax selection mechanism that creates a convex\ncombination of dictionary matrices at each time step, and a readout consisting\nof layer normalization followed by a linear map. We then proceed to evaluate\nvariants of diagonal selective SSMs by considering their empirical performance\non commutative and non-commutative automata. We explain the experimental\nresults with theoretical considerations. Our code is available at\nhttps://github.com/IBM/selective-dense-state-space-model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Selective state-space models (SSMs) are an emerging alternative to the\nTransformer, offering the unique advantage of parallel training and sequential\ninference. Although these models have shown promising performance on a variety\nof tasks, their formal expressiveness and length generalization properties\nremain underexplored. In this work, we provide insight into the workings of\nselective SSMs by analyzing their expressiveness and length generalization\nperformance on regular language tasks, i.e., finite-state automaton (FSA)\nemulation. We address certain limitations of modern SSM-based architectures by\nintroducing the Selective Dense State-Space Model (SD-SSM), the first selective\nSSM that exhibits perfect length generalization on a set of various regular\nlanguage tasks using a single layer. It utilizes a dictionary of dense\ntransition matrices, a softmax selection mechanism that creates a convex\ncombination of dictionary matrices at each time step, and a readout consisting\nof layer normalization followed by a linear map. We then proceed to evaluate\nvariants of diagonal selective SSMs by considering their empirical performance\non commutative and non-commutative automata. We explain the experimental\nresults with theoretical considerations. Our code is available at\nhttps://github.com/IBM/selective-dense-state-space-model."
                },
                "authors": [
                    {
                        "name": "Aleksandar Terzić"
                    },
                    {
                        "name": "Michael Hersche"
                    },
                    {
                        "name": "Giacomo Camposampiero"
                    },
                    {
                        "name": "Thomas Hofmann"
                    },
                    {
                        "name": "Abu Sebastian"
                    },
                    {
                        "name": "Abbas Rahimi"
                    }
                ],
                "author_detail": {
                    "name": "Abbas Rahimi"
                },
                "author": "Abbas Rahimi",
                "arxiv_comment": "13 pages, 7 figures, to be published in AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19350v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19350v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.19619v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.19619v2",
                "updated": "2024-12-26T20:04:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    20,
                    4,
                    21,
                    3,
                    361,
                    0
                ],
                "published": "2023-10-30T15:12:09Z",
                "published_parsed": [
                    2023,
                    10,
                    30,
                    15,
                    12,
                    9,
                    0,
                    303,
                    0
                ],
                "title": "Towards A Holistic Landscape of Situated Theory of Mind in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards A Holistic Landscape of Situated Theory of Mind in Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) have generated considerable interest and debate\nregarding their potential emergence of Theory of Mind (ToM). Several recent\ninquiries reveal a lack of robust ToM in these models and pose a pressing\ndemand to develop new benchmarks, as current ones primarily focus on different\naspects of ToM and are prone to shortcuts and data leakage. In this position\npaper, we seek to answer two road-blocking questions: (1) How can we taxonomize\na holistic landscape of machine ToM? (2) What is a more effective evaluation\nprotocol for machine ToM? Following psychological studies, we taxonomize\nmachine ToM into 7 mental state categories and delineate existing benchmarks to\nidentify under-explored aspects of ToM. We argue for a holistic and situated\nevaluation of ToM to break ToM into individual components and treat LLMs as an\nagent who is physically situated in environments and socially situated in\ninteractions with humans. Such situated evaluation provides a more\ncomprehensive assessment of mental states and potentially mitigates the risk of\nshortcuts and data leakage. We further present a pilot study in a grid world\nsetup as a proof of concept. We hope this position paper can facilitate future\nresearch to integrate ToM with LLMs and offer an intuitive means for\nresearchers to better position their work in the landscape of ToM. Project\npage: https://github.com/Mars-tin/awesome-theory-of-mind",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have generated considerable interest and debate\nregarding their potential emergence of Theory of Mind (ToM). Several recent\ninquiries reveal a lack of robust ToM in these models and pose a pressing\ndemand to develop new benchmarks, as current ones primarily focus on different\naspects of ToM and are prone to shortcuts and data leakage. In this position\npaper, we seek to answer two road-blocking questions: (1) How can we taxonomize\na holistic landscape of machine ToM? (2) What is a more effective evaluation\nprotocol for machine ToM? Following psychological studies, we taxonomize\nmachine ToM into 7 mental state categories and delineate existing benchmarks to\nidentify under-explored aspects of ToM. We argue for a holistic and situated\nevaluation of ToM to break ToM into individual components and treat LLMs as an\nagent who is physically situated in environments and socially situated in\ninteractions with humans. Such situated evaluation provides a more\ncomprehensive assessment of mental states and potentially mitigates the risk of\nshortcuts and data leakage. We further present a pilot study in a grid world\nsetup as a proof of concept. We hope this position paper can facilitate future\nresearch to integrate ToM with LLMs and offer an intuitive means for\nresearchers to better position their work in the landscape of ToM. Project\npage: https://github.com/Mars-tin/awesome-theory-of-mind"
                },
                "authors": [
                    {
                        "name": "Ziqiao Ma"
                    },
                    {
                        "name": "Jacob Sansom"
                    },
                    {
                        "name": "Run Peng"
                    },
                    {
                        "name": "Joyce Chai"
                    }
                ],
                "author_detail": {
                    "name": "Joyce Chai"
                },
                "author": "Joyce Chai",
                "arxiv_comment": "EMNLP 2023 (Findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.19619v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.19619v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19329v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19329v1",
                "updated": "2024-12-26T18:58:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    18,
                    58,
                    38,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T18:58:38Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    18,
                    58,
                    38,
                    3,
                    361,
                    0
                ],
                "title": "Deep learning and whole-brain networks for biomarker discovery: modeling\n  the dynamics of brain fluctuations in resting-state and cognitive tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning and whole-brain networks for biomarker discovery: modeling\n  the dynamics of brain fluctuations in resting-state and cognitive tasks"
                },
                "summary": "Background: Brain network models offer insights into brain dynamics, but the\nutility of model-derived bifurcation parameters as biomarkers remains\nunderexplored. Objective: This study evaluates bifurcation parameters from a\nwhole-brain network model as biomarkers for distinguishing brain states\nassociated with resting-state and task-based cognitive conditions. Methods:\nSynthetic BOLD signals were generated using a supercritical Hopf brain network\nmodel to train deep learning models for bifurcation parameter prediction.\nInference was performed on Human Connectome Project data, including both\nresting-state and task-based conditions. Statistical analyses assessed the\nseparability of brain states based on bifurcation parameter distributions.\nResults: Bifurcation parameter distributions differed significantly across task\nand resting-state conditions ($p < 0.0001$ for all but one comparison).\nTask-based brain states exhibited higher bifurcation values compared to rest.\nConclusion: Bifurcation parameters effectively differentiate cognitive and\nresting states, warranting further investigation as biomarkers for brain state\ncharacterization and neurological disorder assessment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Brain network models offer insights into brain dynamics, but the\nutility of model-derived bifurcation parameters as biomarkers remains\nunderexplored. Objective: This study evaluates bifurcation parameters from a\nwhole-brain network model as biomarkers for distinguishing brain states\nassociated with resting-state and task-based cognitive conditions. Methods:\nSynthetic BOLD signals were generated using a supercritical Hopf brain network\nmodel to train deep learning models for bifurcation parameter prediction.\nInference was performed on Human Connectome Project data, including both\nresting-state and task-based conditions. Statistical analyses assessed the\nseparability of brain states based on bifurcation parameter distributions.\nResults: Bifurcation parameter distributions differed significantly across task\nand resting-state conditions ($p < 0.0001$ for all but one comparison).\nTask-based brain states exhibited higher bifurcation values compared to rest.\nConclusion: Bifurcation parameters effectively differentiate cognitive and\nresting states, warranting further investigation as biomarkers for brain state\ncharacterization and neurological disorder assessment."
                },
                "authors": [
                    {
                        "name": "Facundo Roffet"
                    },
                    {
                        "name": "Gustavo Deco"
                    },
                    {
                        "name": "Claudio Delrieux"
                    },
                    {
                        "name": "Gustavo Patow"
                    }
                ],
                "author_detail": {
                    "name": "Gustavo Patow"
                },
                "author": "Gustavo Patow",
                "arxiv_comment": "12 pages, 4 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19329v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19329v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15188v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15188v2",
                "updated": "2024-12-26T18:56:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    18,
                    56,
                    18,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-19T18:56:24Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    56,
                    24,
                    3,
                    354,
                    0
                ],
                "title": "LMFusion: Adapting Pretrained Language Models for Multimodal Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LMFusion: Adapting Pretrained Language Models for Multimodal Generation"
                },
                "summary": "We present LMFusion, a framework for empowering pretrained text-only large\nlanguage models (LLMs) with multimodal generative capabilities, enabling them\nto understand and generate both text and images in arbitrary sequences.\nLMFusion leverages existing Llama-3's weights for processing texts\nautoregressively while introducing additional and parallel transformer modules\nfor processing images with diffusion. During training, the data from each\nmodality is routed to its dedicated modules: modality-specific feedforward\nlayers, query-key-value projections, and normalization layers process each\nmodality independently, while the shared self-attention layers allow\ninteractions across text and image features. By freezing the text-specific\nmodules and only training the image-specific modules, LMFusion preserves the\nlanguage capabilities of text-only LLMs while developing strong visual\nunderstanding and generation abilities. Compared to methods that pretrain\nmultimodal generative models from scratch, our experiments demonstrate that,\nLMFusion improves image understanding by 20% and image generation by 3.6% using\nonly 50% of the FLOPs while maintaining Llama-3's language capabilities. We\nalso demonstrate that this framework can adapt existing vision-language models\nwith multimodal generation ability. Overall, this framework not only leverages\nexisting computational investments in text-only LLMs but also enables the\nparallel development of language and vision capabilities, presenting a\npromising direction for efficient multimodal model development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present LMFusion, a framework for empowering pretrained text-only large\nlanguage models (LLMs) with multimodal generative capabilities, enabling them\nto understand and generate both text and images in arbitrary sequences.\nLMFusion leverages existing Llama-3's weights for processing texts\nautoregressively while introducing additional and parallel transformer modules\nfor processing images with diffusion. During training, the data from each\nmodality is routed to its dedicated modules: modality-specific feedforward\nlayers, query-key-value projections, and normalization layers process each\nmodality independently, while the shared self-attention layers allow\ninteractions across text and image features. By freezing the text-specific\nmodules and only training the image-specific modules, LMFusion preserves the\nlanguage capabilities of text-only LLMs while developing strong visual\nunderstanding and generation abilities. Compared to methods that pretrain\nmultimodal generative models from scratch, our experiments demonstrate that,\nLMFusion improves image understanding by 20% and image generation by 3.6% using\nonly 50% of the FLOPs while maintaining Llama-3's language capabilities. We\nalso demonstrate that this framework can adapt existing vision-language models\nwith multimodal generation ability. Overall, this framework not only leverages\nexisting computational investments in text-only LLMs but also enables the\nparallel development of language and vision capabilities, presenting a\npromising direction for efficient multimodal model development."
                },
                "authors": [
                    {
                        "name": "Weijia Shi"
                    },
                    {
                        "name": "Xiaochuang Han"
                    },
                    {
                        "name": "Chunting Zhou"
                    },
                    {
                        "name": "Weixin Liang"
                    },
                    {
                        "name": "Xi Victoria Lin"
                    },
                    {
                        "name": "Luke Zettlemoyer"
                    },
                    {
                        "name": "Lili Yu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Yu"
                },
                "author": "Lili Yu",
                "arxiv_comment": "Name change: LlamaFusion to LMFusion",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15188v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15188v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.13168v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.13168v4",
                "updated": "2024-12-26T18:54:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    18,
                    54,
                    53,
                    3,
                    361,
                    0
                ],
                "published": "2023-05-22T15:56:44Z",
                "published_parsed": [
                    2023,
                    5,
                    22,
                    15,
                    56,
                    44,
                    0,
                    142,
                    0
                ],
                "title": "LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities\n  and Future Opportunities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities\n  and Future Opportunities"
                },
                "summary": "This paper presents an exhaustive quantitative and qualitative evaluation of\nLarge Language Models (LLMs) for Knowledge Graph (KG) construction and\nreasoning. We engage in experiments across eight diverse datasets, focusing on\nfour representative tasks encompassing entity and relation extraction, event\nextraction, link prediction, and question-answering, thereby thoroughly\nexploring LLMs' performance in the domain of construction and inference.\nEmpirically, our findings suggest that LLMs, represented by GPT-4, are more\nsuited as inference assistants rather than few-shot information extractors.\nSpecifically, while GPT-4 exhibits good performance in tasks related to KG\nconstruction, it excels further in reasoning tasks, surpassing fine-tuned\nmodels in certain cases. Moreover, our investigation extends to the potential\ngeneralization ability of LLMs for information extraction, leading to the\nproposition of a Virtual Knowledge Extraction task and the development of the\ncorresponding VINE dataset. Based on these empirical findings, we further\npropose AutoKG, a multi-agent-based approach employing LLMs and external\nsources for KG construction and reasoning. We anticipate that this research can\nprovide invaluable insights for future undertakings in the field of knowledge\ngraphs. The code and datasets are in https://github.com/zjunlp/AutoKG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an exhaustive quantitative and qualitative evaluation of\nLarge Language Models (LLMs) for Knowledge Graph (KG) construction and\nreasoning. We engage in experiments across eight diverse datasets, focusing on\nfour representative tasks encompassing entity and relation extraction, event\nextraction, link prediction, and question-answering, thereby thoroughly\nexploring LLMs' performance in the domain of construction and inference.\nEmpirically, our findings suggest that LLMs, represented by GPT-4, are more\nsuited as inference assistants rather than few-shot information extractors.\nSpecifically, while GPT-4 exhibits good performance in tasks related to KG\nconstruction, it excels further in reasoning tasks, surpassing fine-tuned\nmodels in certain cases. Moreover, our investigation extends to the potential\ngeneralization ability of LLMs for information extraction, leading to the\nproposition of a Virtual Knowledge Extraction task and the development of the\ncorresponding VINE dataset. Based on these empirical findings, we further\npropose AutoKG, a multi-agent-based approach employing LLMs and external\nsources for KG construction and reasoning. We anticipate that this research can\nprovide invaluable insights for future undertakings in the field of knowledge\ngraphs. The code and datasets are in https://github.com/zjunlp/AutoKG."
                },
                "authors": [
                    {
                        "name": "Yuqi Zhu"
                    },
                    {
                        "name": "Xiaohan Wang"
                    },
                    {
                        "name": "Jing Chen"
                    },
                    {
                        "name": "Shuofei Qiao"
                    },
                    {
                        "name": "Yixin Ou"
                    },
                    {
                        "name": "Yunzhi Yao"
                    },
                    {
                        "name": "Shumin Deng"
                    },
                    {
                        "name": "Huajun Chen"
                    },
                    {
                        "name": "Ningyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ningyu Zhang"
                },
                "author": "Ningyu Zhang",
                "arxiv_comment": "World Wide Web Journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.13168v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.13168v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19325v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19325v1",
                "updated": "2024-12-26T18:54:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    18,
                    54,
                    32,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T18:54:32Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    18,
                    54,
                    32,
                    3,
                    361,
                    0
                ],
                "title": "Performance Control in Early Exiting to Deploy Large Models at the Same\n  Cost of Smaller Ones",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Control in Early Exiting to Deploy Large Models at the Same\n  Cost of Smaller Ones"
                },
                "summary": "Early Exiting (EE) is a promising technique for speeding up inference by\nadaptively allocating compute resources to data points based on their\ndifficulty. The approach enables predictions to exit at earlier layers for\nsimpler samples while reserving more computation for challenging ones. In this\nstudy, we first present a novel perspective on the EE approach, showing that\nlarger models deployed with EE can achieve higher performance than smaller\nmodels while maintaining similar computational costs. As existing EE approaches\nrely on confidence estimation at each exit point, we further study the impact\nof overconfidence on the controllability of the compute-performance trade-off.\nWe introduce Performance Control Early Exiting (PCEE), a method that enables\naccuracy thresholding by basing decisions not on a data point's confidence but\non the average accuracy of samples with similar confidence levels from a\nheld-out validation set. In our experiments, we show that PCEE offers a simple\nyet computationally efficient approach that provides better control over\nperformance than standard confidence-based approaches, and allows us to scale\nup model sizes to yield performance gain while reducing the computational cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Early Exiting (EE) is a promising technique for speeding up inference by\nadaptively allocating compute resources to data points based on their\ndifficulty. The approach enables predictions to exit at earlier layers for\nsimpler samples while reserving more computation for challenging ones. In this\nstudy, we first present a novel perspective on the EE approach, showing that\nlarger models deployed with EE can achieve higher performance than smaller\nmodels while maintaining similar computational costs. As existing EE approaches\nrely on confidence estimation at each exit point, we further study the impact\nof overconfidence on the controllability of the compute-performance trade-off.\nWe introduce Performance Control Early Exiting (PCEE), a method that enables\naccuracy thresholding by basing decisions not on a data point's confidence but\non the average accuracy of samples with similar confidence levels from a\nheld-out validation set. In our experiments, we show that PCEE offers a simple\nyet computationally efficient approach that provides better control over\nperformance than standard confidence-based approaches, and allows us to scale\nup model sizes to yield performance gain while reducing the computational cost."
                },
                "authors": [
                    {
                        "name": "Mehrnaz Mofakhami"
                    },
                    {
                        "name": "Reza Bayat"
                    },
                    {
                        "name": "Ioannis Mitliagkas"
                    },
                    {
                        "name": "Joao Monteiro"
                    },
                    {
                        "name": "Valentina Zantedeschi"
                    }
                ],
                "author_detail": {
                    "name": "Valentina Zantedeschi"
                },
                "author": "Valentina Zantedeschi",
                "arxiv_comment": "Appeared at ICML 2024 Workshop on Efficient Systems for Foundation\n  Models (ES-FoMo-II)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19325v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19325v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19318v1",
                "updated": "2024-12-26T18:42:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    18,
                    42,
                    8,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T18:42:08Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    18,
                    42,
                    8,
                    3,
                    361,
                    0
                ],
                "title": "Adaptive Conformal Inference by Betting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Conformal Inference by Betting"
                },
                "summary": "Conformal prediction is a valuable tool for quantifying predictive\nuncertainty of machine learning models. However, its applicability relies on\nthe assumption of data exchangeability, a condition which is often not met in\nreal-world scenarios. In this paper, we consider the problem of adaptive\nconformal inference without any assumptions about the data generating process.\nExisting approaches for adaptive conformal inference are based on optimizing\nthe pinball loss using variants of online gradient descent. A notable\nshortcoming of such approaches is in their explicit dependence on and\nsensitivity to the choice of the learning rates. In this paper, we propose a\ndifferent approach for adaptive conformal inference that leverages\nparameter-free online convex optimization techniques. We prove that our method\ncontrols long-term miscoverage frequency at a nominal level and demonstrate its\nconvincing empirical performance without any need of performing cumbersome\nparameter tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conformal prediction is a valuable tool for quantifying predictive\nuncertainty of machine learning models. However, its applicability relies on\nthe assumption of data exchangeability, a condition which is often not met in\nreal-world scenarios. In this paper, we consider the problem of adaptive\nconformal inference without any assumptions about the data generating process.\nExisting approaches for adaptive conformal inference are based on optimizing\nthe pinball loss using variants of online gradient descent. A notable\nshortcoming of such approaches is in their explicit dependence on and\nsensitivity to the choice of the learning rates. In this paper, we propose a\ndifferent approach for adaptive conformal inference that leverages\nparameter-free online convex optimization techniques. We prove that our method\ncontrols long-term miscoverage frequency at a nominal level and demonstrate its\nconvincing empirical performance without any need of performing cumbersome\nparameter tuning."
                },
                "authors": [
                    {
                        "name": "Aleksandr Podkopaev"
                    },
                    {
                        "name": "Darren Xu"
                    },
                    {
                        "name": "Kuang-Chih Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kuang-Chih Lee"
                },
                "author": "Kuang-Chih Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19312v1",
                "updated": "2024-12-26T18:19:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    18,
                    19,
                    53,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T18:19:53Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    18,
                    19,
                    53,
                    3,
                    361,
                    0
                ],
                "title": "From Interets to Insights: An LLM Approach to Course Recommendations\n  Using Natural Language Queries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Interets to Insights: An LLM Approach to Course Recommendations\n  Using Natural Language Queries"
                },
                "summary": "Most universities in the United States encourage their students to explore\nacademic areas before declaring a major and to acquire academic breadth by\nsatisfying a variety of requirements. Each term, students must choose among\nmany thousands of offerings, spanning dozens of subject areas, a handful of\ncourses to take. The curricular environment is also dynamic, and poor\ncommunication and search functions on campus can limit a student's ability to\ndiscover new courses of interest. To support both students and their advisers\nin such a setting, we explore a novel Large Language Model (LLM) course\nrecommendation system that applies a Retrieval Augmented Generation (RAG)\nmethod to the corpus of course descriptions. The system first generates an\n'ideal' course description based on the user's query. This description is\nconverted into a search vector using embeddings, which is then used to find\nactual courses with similar content by comparing embedding similarities. We\ndescribe the method and assess the quality and fairness of some example\nprompts. Steps to deploy a pilot system on campus are discussed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most universities in the United States encourage their students to explore\nacademic areas before declaring a major and to acquire academic breadth by\nsatisfying a variety of requirements. Each term, students must choose among\nmany thousands of offerings, spanning dozens of subject areas, a handful of\ncourses to take. The curricular environment is also dynamic, and poor\ncommunication and search functions on campus can limit a student's ability to\ndiscover new courses of interest. To support both students and their advisers\nin such a setting, we explore a novel Large Language Model (LLM) course\nrecommendation system that applies a Retrieval Augmented Generation (RAG)\nmethod to the corpus of course descriptions. The system first generates an\n'ideal' course description based on the user's query. This description is\nconverted into a search vector using embeddings, which is then used to find\nactual courses with similar content by comparing embedding similarities. We\ndescribe the method and assess the quality and fairness of some example\nprompts. Steps to deploy a pilot system on campus are discussed."
                },
                "authors": [
                    {
                        "name": "Hugh Van Deventer"
                    },
                    {
                        "name": "Mark Mills"
                    },
                    {
                        "name": "August Evrard"
                    }
                ],
                "author_detail": {
                    "name": "August Evrard"
                },
                "author": "August Evrard",
                "arxiv_comment": "17 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19304v1",
                "updated": "2024-12-26T17:53:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    17,
                    53,
                    14,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T17:53:14Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    17,
                    53,
                    14,
                    3,
                    361,
                    0
                ],
                "title": "Perceive, Query & Reason: Enhancing Video QA with Question-Guided\n  Temporal Queries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perceive, Query & Reason: Enhancing Video QA with Question-Guided\n  Temporal Queries"
                },
                "summary": "Video Question Answering (Video QA) is a challenging video understanding task\nthat requires models to comprehend entire videos, identify the most relevant\ninformation based on contextual cues from a given question, and reason\naccurately to provide answers. Recent advancements in Multimodal Large Language\nModels (MLLMs) have transformed video QA by leveraging their exceptional\ncommonsense reasoning capabilities. This progress is largely driven by the\neffective alignment between visual data and the language space of MLLMs.\nHowever, for video QA, an additional space-time alignment poses a considerable\nchallenge for extracting question-relevant information across frames. In this\nwork, we investigate diverse temporal modeling techniques to integrate with\nMLLMs, aiming to achieve question-guided temporal modeling that leverages\npre-trained visual and textual alignment in MLLMs. We propose T-Former, a novel\ntemporal modeling method that creates a question-guided temporal bridge between\nframe-wise visual perception and the reasoning capabilities of LLMs. Our\nevaluation across multiple video QA benchmarks demonstrates that T-Former\ncompetes favorably with existing temporal modeling approaches and aligns with\nrecent advancements in video QA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Question Answering (Video QA) is a challenging video understanding task\nthat requires models to comprehend entire videos, identify the most relevant\ninformation based on contextual cues from a given question, and reason\naccurately to provide answers. Recent advancements in Multimodal Large Language\nModels (MLLMs) have transformed video QA by leveraging their exceptional\ncommonsense reasoning capabilities. This progress is largely driven by the\neffective alignment between visual data and the language space of MLLMs.\nHowever, for video QA, an additional space-time alignment poses a considerable\nchallenge for extracting question-relevant information across frames. In this\nwork, we investigate diverse temporal modeling techniques to integrate with\nMLLMs, aiming to achieve question-guided temporal modeling that leverages\npre-trained visual and textual alignment in MLLMs. We propose T-Former, a novel\ntemporal modeling method that creates a question-guided temporal bridge between\nframe-wise visual perception and the reasoning capabilities of LLMs. Our\nevaluation across multiple video QA benchmarks demonstrates that T-Former\ncompetes favorably with existing temporal modeling approaches and aligns with\nrecent advancements in video QA."
                },
                "authors": [
                    {
                        "name": "Roberto Amoroso"
                    },
                    {
                        "name": "Gengyuan Zhang"
                    },
                    {
                        "name": "Rajat Koner"
                    },
                    {
                        "name": "Lorenzo Baraldi"
                    },
                    {
                        "name": "Rita Cucchiara"
                    },
                    {
                        "name": "Volker Tresp"
                    }
                ],
                "author_detail": {
                    "name": "Volker Tresp"
                },
                "author": "Volker Tresp",
                "arxiv_comment": "WACV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19291v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19291v1",
                "updated": "2024-12-26T17:34:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    17,
                    34,
                    26,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T17:34:26Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    17,
                    34,
                    26,
                    3,
                    361,
                    0
                ],
                "title": "RAG with Differential Privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAG with Differential Privacy"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has emerged as the dominant technique to\nprovide *Large Language Models* (LLM) with fresh and relevant context,\nmitigating the risk of hallucinations and improving the overall quality of\nresponses in environments with large and fast moving knowledge bases. However,\nthe integration of external documents into the generation process raises\nsignificant privacy concerns. Indeed, when added to a prompt, it is not\npossible to guarantee a response will not inadvertently expose confidential\ndata, leading to potential breaches of privacy and ethical dilemmas. This paper\nexplores a practical solution to this problem suitable to general knowledge\nextraction from personal data. It shows *differentially private token\ngeneration* is a viable approach to private RAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has emerged as the dominant technique to\nprovide *Large Language Models* (LLM) with fresh and relevant context,\nmitigating the risk of hallucinations and improving the overall quality of\nresponses in environments with large and fast moving knowledge bases. However,\nthe integration of external documents into the generation process raises\nsignificant privacy concerns. Indeed, when added to a prompt, it is not\npossible to guarantee a response will not inadvertently expose confidential\ndata, leading to potential breaches of privacy and ethical dilemmas. This paper\nexplores a practical solution to this problem suitable to general knowledge\nextraction from personal data. It shows *differentially private token\ngeneration* is a viable approach to private RAG."
                },
                "authors": [
                    {
                        "name": "Nicolas Grislain"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Grislain"
                },
                "author": "Nicolas Grislain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19291v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15648v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15648v2",
                "updated": "2024-12-26T17:00:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    17,
                    0,
                    51,
                    3,
                    361,
                    0
                ],
                "published": "2024-03-22T23:12:28Z",
                "published_parsed": [
                    2024,
                    3,
                    22,
                    23,
                    12,
                    28,
                    4,
                    82,
                    0
                ],
                "title": "SRLM: Human-in-Loop Interactive Social Robot Navigation with Large\n  Language Model and Deep Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SRLM: Human-in-Loop Interactive Social Robot Navigation with Large\n  Language Model and Deep Reinforcement Learning"
                },
                "summary": "An interactive social robotic assistant must provide services in complex and\ncrowded spaces while adapting its behavior based on real-time human language\ncommands or feedback. In this paper, we propose a novel hybrid approach called\nSocial Robot Planner (SRLM), which integrates Large Language Models (LLM) and\nDeep Reinforcement Learning (DRL) to navigate through human-filled public\nspaces and provide multiple social services. SRLM infers global planning from\nhuman-in-loop commands in real-time, and encodes social information into a\nLLM-based large navigation model (LNM) for low-level motion execution.\nMoreover, a DRL-based planner is designed to maintain benchmarking performance,\nwhich is blended with LNM by a large feedback model (LFM) to address the\ninstability of current text and LLM-driven LNM. Finally, SRLM demonstrates\noutstanding performance in extensive experiments. More details about this work\nare available at: https://sites.google.com/view/navi-srlm",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An interactive social robotic assistant must provide services in complex and\ncrowded spaces while adapting its behavior based on real-time human language\ncommands or feedback. In this paper, we propose a novel hybrid approach called\nSocial Robot Planner (SRLM), which integrates Large Language Models (LLM) and\nDeep Reinforcement Learning (DRL) to navigate through human-filled public\nspaces and provide multiple social services. SRLM infers global planning from\nhuman-in-loop commands in real-time, and encodes social information into a\nLLM-based large navigation model (LNM) for low-level motion execution.\nMoreover, a DRL-based planner is designed to maintain benchmarking performance,\nwhich is blended with LNM by a large feedback model (LFM) to address the\ninstability of current text and LLM-driven LNM. Finally, SRLM demonstrates\noutstanding performance in extensive experiments. More details about this work\nare available at: https://sites.google.com/view/navi-srlm"
                },
                "authors": [
                    {
                        "name": "Weizheng Wang"
                    },
                    {
                        "name": "Ike Obi"
                    },
                    {
                        "name": "Byung-Cheol Min"
                    }
                ],
                "author_detail": {
                    "name": "Byung-Cheol Min"
                },
                "author": "Byung-Cheol Min",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15648v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15648v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19260v1",
                "updated": "2024-12-26T15:54:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    15,
                    54,
                    10,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T15:54:10Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    15,
                    54,
                    10,
                    3,
                    361,
                    0
                ],
                "title": "MEDEC: A Benchmark for Medical Error Detection and Correction in\n  Clinical Notes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEDEC: A Benchmark for Medical Error Detection and Correction in\n  Clinical Notes"
                },
                "summary": "Several studies showed that Large Language Models (LLMs) can answer medical\nquestions correctly, even outperforming the average human score in some medical\nexams. However, to our knowledge, no study has been conducted to assess the\nability of language models to validate existing or generated medical text for\ncorrectness and consistency. In this paper, we introduce MEDEC\n(https://github.com/abachaa/MEDEC), the first publicly available benchmark for\nmedical error detection and correction in clinical notes, covering five types\nof errors (Diagnosis, Management, Treatment, Pharmacotherapy, and Causal\nOrganism). MEDEC consists of 3,848 clinical texts, including 488 clinical notes\nfrom three US hospital systems that were not previously seen by any LLM. The\ndataset has been used for the MEDIQA-CORR shared task to evaluate seventeen\nparticipating systems [Ben Abacha et al., 2024]. In this paper, we describe the\ndata creation methods and we evaluate recent LLMs (e.g., o1-preview, GPT-4,\nClaude 3.5 Sonnet, and Gemini 2.0 Flash) for the tasks of detecting and\ncorrecting medical errors requiring both medical knowledge and reasoning\ncapabilities. We also conducted a comparative study where two medical doctors\nperformed the same task on the MEDEC test set. The results showed that MEDEC is\na sufficiently challenging benchmark to assess the ability of models to\nvalidate existing or generated notes and to correct medical errors. We also\nfound that although recent LLMs have a good performance in error detection and\ncorrection, they are still outperformed by medical doctors in these tasks. We\ndiscuss the potential factors behind this gap, the insights from our\nexperiments, the limitations of current evaluation metrics, and share potential\npointers for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several studies showed that Large Language Models (LLMs) can answer medical\nquestions correctly, even outperforming the average human score in some medical\nexams. However, to our knowledge, no study has been conducted to assess the\nability of language models to validate existing or generated medical text for\ncorrectness and consistency. In this paper, we introduce MEDEC\n(https://github.com/abachaa/MEDEC), the first publicly available benchmark for\nmedical error detection and correction in clinical notes, covering five types\nof errors (Diagnosis, Management, Treatment, Pharmacotherapy, and Causal\nOrganism). MEDEC consists of 3,848 clinical texts, including 488 clinical notes\nfrom three US hospital systems that were not previously seen by any LLM. The\ndataset has been used for the MEDIQA-CORR shared task to evaluate seventeen\nparticipating systems [Ben Abacha et al., 2024]. In this paper, we describe the\ndata creation methods and we evaluate recent LLMs (e.g., o1-preview, GPT-4,\nClaude 3.5 Sonnet, and Gemini 2.0 Flash) for the tasks of detecting and\ncorrecting medical errors requiring both medical knowledge and reasoning\ncapabilities. We also conducted a comparative study where two medical doctors\nperformed the same task on the MEDEC test set. The results showed that MEDEC is\na sufficiently challenging benchmark to assess the ability of models to\nvalidate existing or generated notes and to correct medical errors. We also\nfound that although recent LLMs have a good performance in error detection and\ncorrection, they are still outperformed by medical doctors in these tasks. We\ndiscuss the potential factors behind this gap, the insights from our\nexperiments, the limitations of current evaluation metrics, and share potential\npointers for future research."
                },
                "authors": [
                    {
                        "name": "Asma Ben Abacha"
                    },
                    {
                        "name": "Wen-wai Yim"
                    },
                    {
                        "name": "Yujuan Fu"
                    },
                    {
                        "name": "Zhaoyi Sun"
                    },
                    {
                        "name": "Meliha Yetisgen"
                    },
                    {
                        "name": "Fei Xia"
                    },
                    {
                        "name": "Thomas Lin"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Lin"
                },
                "author": "Thomas Lin",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08753v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08753v2",
                "updated": "2024-12-26T15:49:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    15,
                    49,
                    20,
                    3,
                    361,
                    0
                ],
                "published": "2024-11-13T16:31:08Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    16,
                    31,
                    8,
                    2,
                    318,
                    0
                ],
                "title": "Which Viewpoint Shows it Best? Language for Weakly Supervising View\n  Selection in Multi-view Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Which Viewpoint Shows it Best? Language for Weakly Supervising View\n  Selection in Multi-view Videos"
                },
                "summary": "Given a multi-view video, which viewpoint is most informative for a human\nobserver? Existing methods rely on heuristics or expensive \"best-view\"\nsupervision to answer this question, limiting their applicability. We propose a\nweakly supervised approach that leverages language accompanying an\ninstructional multi-view video as a means to recover its most informative\nviewpoint(s). Our key hypothesis is that the more accurately an individual view\ncan predict a view-agnostic text summary, the more informative it is. To put\nthis into action, we propose a framework that uses the relative accuracy of\nview-dependent caption predictions as a proxy for best view pseudo-labels.\nThen, those pseudo-labels are used to train a view selector, together with an\nauxiliary camera pose predictor that enhances view-sensitivity. During\ninference, our model takes as input only a multi-view video--no language or\ncamera poses--and returns the best viewpoint to watch at each timestep. On two\nchallenging datasets comprised of diverse multi-camera setups and how-to\nactivities, our model consistently outperforms state-of-the-art baselines, both\nwith quantitative metrics and human evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given a multi-view video, which viewpoint is most informative for a human\nobserver? Existing methods rely on heuristics or expensive \"best-view\"\nsupervision to answer this question, limiting their applicability. We propose a\nweakly supervised approach that leverages language accompanying an\ninstructional multi-view video as a means to recover its most informative\nviewpoint(s). Our key hypothesis is that the more accurately an individual view\ncan predict a view-agnostic text summary, the more informative it is. To put\nthis into action, we propose a framework that uses the relative accuracy of\nview-dependent caption predictions as a proxy for best view pseudo-labels.\nThen, those pseudo-labels are used to train a view selector, together with an\nauxiliary camera pose predictor that enhances view-sensitivity. During\ninference, our model takes as input only a multi-view video--no language or\ncamera poses--and returns the best viewpoint to watch at each timestep. On two\nchallenging datasets comprised of diverse multi-camera setups and how-to\nactivities, our model consistently outperforms state-of-the-art baselines, both\nwith quantitative metrics and human evaluation."
                },
                "authors": [
                    {
                        "name": "Sagnik Majumder"
                    },
                    {
                        "name": "Tushar Nagarajan"
                    },
                    {
                        "name": "Ziad Al-Halah"
                    },
                    {
                        "name": "Reina Pradhan"
                    },
                    {
                        "name": "Kristen Grauman"
                    }
                ],
                "author_detail": {
                    "name": "Kristen Grauman"
                },
                "author": "Kristen Grauman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08753v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08753v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.19157v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.19157v5",
                "updated": "2024-12-26T15:18:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    15,
                    18,
                    54,
                    3,
                    361,
                    0
                ],
                "published": "2024-02-29T13:40:04Z",
                "published_parsed": [
                    2024,
                    2,
                    29,
                    13,
                    40,
                    4,
                    3,
                    60,
                    0
                ],
                "title": "Broken detailed balance and entropy production in directed networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Broken detailed balance and entropy production in directed networks"
                },
                "summary": "The structure of a complex network plays a crucial role in determining its\ndynamical properties. In this work, we show that the the degree to which a\nnetwork is directed and hierarchically organised is closely associated with the\ndegree to which its dynamics break detailed balance and produce entropy. We\nconsider a range of dynamical processes and show how different directed network\nfeatures affect their entropy production rate. We begin with an analytical\ntreatment of a 2-node network followed by numerical simulations of synthetic\nnetworks using the preferential attachment and Erd\\\"os-Renyi algorithms. Next,\nwe analyse a collection of 97 empirical networks to determine the effect of\ncomplex real-world topologies. Finally, we present a simple method for\ninferring broken detailed balance and directed network structure from\nmultivariate time-series and apply our method to identify non-equilibrium\ndynamics and hierarchical organisation in both human neuroimaging and financial\ntime-series. Overall, our results shed light on the consequences of directed\nnetwork structure on non-equilibrium dynamics and highlight the importance and\nubiquity of hierarchical organisation and non-equilibrium dynamics in\nreal-world systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The structure of a complex network plays a crucial role in determining its\ndynamical properties. In this work, we show that the the degree to which a\nnetwork is directed and hierarchically organised is closely associated with the\ndegree to which its dynamics break detailed balance and produce entropy. We\nconsider a range of dynamical processes and show how different directed network\nfeatures affect their entropy production rate. We begin with an analytical\ntreatment of a 2-node network followed by numerical simulations of synthetic\nnetworks using the preferential attachment and Erd\\\"os-Renyi algorithms. Next,\nwe analyse a collection of 97 empirical networks to determine the effect of\ncomplex real-world topologies. Finally, we present a simple method for\ninferring broken detailed balance and directed network structure from\nmultivariate time-series and apply our method to identify non-equilibrium\ndynamics and hierarchical organisation in both human neuroimaging and financial\ntime-series. Overall, our results shed light on the consequences of directed\nnetwork structure on non-equilibrium dynamics and highlight the importance and\nubiquity of hierarchical organisation and non-equilibrium dynamics in\nreal-world systems."
                },
                "authors": [
                    {
                        "name": "Ramón Nartallo-Kaluarachchi"
                    },
                    {
                        "name": "Malbor Asllani"
                    },
                    {
                        "name": "Gustavo Deco"
                    },
                    {
                        "name": "Morten L. Kringelbach"
                    },
                    {
                        "name": "Alain Goriely"
                    },
                    {
                        "name": "Renaud Lambiotte"
                    }
                ],
                "author_detail": {
                    "name": "Renaud Lambiotte"
                },
                "author": "Renaud Lambiotte",
                "arxiv_doi": "10.1103/PhysRevE.110.034313",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevE.110.034313",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.19157v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.19157v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "36 pages, 16 figures",
                "arxiv_journal_ref": "Phys. Rev. E 110, 034313 (2024) Erratum: Phys. Rev. E 110, 069901\n  (2024)",
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19245v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19245v1",
                "updated": "2024-12-26T15:01:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    15,
                    1,
                    24,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T15:01:24Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    15,
                    1,
                    24,
                    3,
                    361,
                    0
                ],
                "title": "Sentiment trading with large language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sentiment trading with large language models"
                },
                "summary": "We investigate the efficacy of large language models (LLMs) in sentiment\nanalysis of U.S. financial news and their potential in predicting stock market\nreturns. We analyze a dataset comprising 965,375 news articles that span from\nJanuary 1, 2010, to June 30, 2023; we focus on the performance of various LLMs,\nincluding BERT, OPT, FINBERT, and the traditional Loughran-McDonald dictionary\nmodel, which has been a dominant methodology in the finance literature. The\nstudy documents a significant association between LLM scores and subsequent\ndaily stock returns. Specifically, OPT, which is a GPT-3 based LLM, shows the\nhighest accuracy in sentiment prediction with an accuracy of 74.4%, slightly\nahead of BERT (72.5%) and FINBERT (72.2%). In contrast, the Loughran-McDonald\ndictionary model demonstrates considerably lower effectiveness with only 50.1%\naccuracy. Regression analyses highlight a robust positive impact of OPT model\nscores on next-day stock returns, with coefficients of 0.274 and 0.254 in\ndifferent model specifications. BERT and FINBERT also exhibit predictive\nrelevance, though to a lesser extent. Notably, we do not observe a significant\nrelationship between the Loughran-McDonald dictionary model scores and stock\nreturns, challenging the efficacy of this traditional method in the current\nfinancial context. In portfolio performance, the long-short OPT strategy excels\nwith a Sharpe ratio of 3.05, compared to 2.11 for BERT and 2.07 for FINBERT\nlong-short strategies. Strategies based on the Loughran-McDonald dictionary\nyield the lowest Sharpe ratio of 1.23. Our findings emphasize the superior\nperformance of advanced LLMs, especially OPT, in financial market prediction\nand portfolio management, marking a significant shift in the landscape of\nfinancial analysis tools with implications to financial regulation and policy\nanalysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the efficacy of large language models (LLMs) in sentiment\nanalysis of U.S. financial news and their potential in predicting stock market\nreturns. We analyze a dataset comprising 965,375 news articles that span from\nJanuary 1, 2010, to June 30, 2023; we focus on the performance of various LLMs,\nincluding BERT, OPT, FINBERT, and the traditional Loughran-McDonald dictionary\nmodel, which has been a dominant methodology in the finance literature. The\nstudy documents a significant association between LLM scores and subsequent\ndaily stock returns. Specifically, OPT, which is a GPT-3 based LLM, shows the\nhighest accuracy in sentiment prediction with an accuracy of 74.4%, slightly\nahead of BERT (72.5%) and FINBERT (72.2%). In contrast, the Loughran-McDonald\ndictionary model demonstrates considerably lower effectiveness with only 50.1%\naccuracy. Regression analyses highlight a robust positive impact of OPT model\nscores on next-day stock returns, with coefficients of 0.274 and 0.254 in\ndifferent model specifications. BERT and FINBERT also exhibit predictive\nrelevance, though to a lesser extent. Notably, we do not observe a significant\nrelationship between the Loughran-McDonald dictionary model scores and stock\nreturns, challenging the efficacy of this traditional method in the current\nfinancial context. In portfolio performance, the long-short OPT strategy excels\nwith a Sharpe ratio of 3.05, compared to 2.11 for BERT and 2.07 for FINBERT\nlong-short strategies. Strategies based on the Loughran-McDonald dictionary\nyield the lowest Sharpe ratio of 1.23. Our findings emphasize the superior\nperformance of advanced LLMs, especially OPT, in financial market prediction\nand portfolio management, marking a significant shift in the landscape of\nfinancial analysis tools with implications to financial regulation and policy\nanalysis."
                },
                "authors": [
                    {
                        "name": "Kemal Kirtac"
                    },
                    {
                        "name": "Guido Germano"
                    }
                ],
                "author_detail": {
                    "name": "Guido Germano"
                },
                "author": "Guido Germano",
                "arxiv_doi": "10.1016/j.frl.2024.105227",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.frl.2024.105227",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.19245v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19245v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Finance Research Letters, 62, p.105227 (2024)",
                "arxiv_primary_category": {
                    "term": "q-fin.CP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.PM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.TR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19241v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19241v1",
                "updated": "2024-12-26T14:51:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    14,
                    51,
                    24,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T14:51:24Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    14,
                    51,
                    24,
                    3,
                    361,
                    0
                ],
                "title": "Latenrgy: Model Agnostic Latency and Energy Consumption Prediction for\n  Binary Classifiers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latenrgy: Model Agnostic Latency and Energy Consumption Prediction for\n  Binary Classifiers"
                },
                "summary": "Machine learning systems increasingly drive innovation across scientific\nfields and industry, yet challenges in compute overhead, specifically during\ninference, limit their scalability and sustainability. Responsible AI\nguardrails, essential for ensuring fairness, transparency, and privacy, further\nexacerbate these computational demands. This study addresses critical gaps in\nthe literature, chiefly the lack of generalized predictive techniques for\nlatency and energy consumption, limited cross-comparisons of classifiers, and\nunquantified impacts of RAI guardrails on inference performance. Using Theory\nConstruction Methodology, this work constructed a model-agnostic theoretical\nframework for predicting latency and energy consumption in binary\nclassification models during inference. The framework synthesizes classifier\ncharacteristics, dataset properties, and RAI guardrails into a unified\nanalytical instrument. Two predictive equations are derived that capture the\ninterplay between these factors while offering generalizability across diverse\nclassifiers. The proposed framework provides foundational insights for\ndesigning efficient, responsible ML systems. It enables researchers to\nbenchmark and optimize inference performance and assists practitioners in\ndeploying scalable solutions. Finally, this work establishes a theoretical\nfoundation for balancing computational efficiency with ethical AI principles,\npaving the way for future empirical validation and broader applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning systems increasingly drive innovation across scientific\nfields and industry, yet challenges in compute overhead, specifically during\ninference, limit their scalability and sustainability. Responsible AI\nguardrails, essential for ensuring fairness, transparency, and privacy, further\nexacerbate these computational demands. This study addresses critical gaps in\nthe literature, chiefly the lack of generalized predictive techniques for\nlatency and energy consumption, limited cross-comparisons of classifiers, and\nunquantified impacts of RAI guardrails on inference performance. Using Theory\nConstruction Methodology, this work constructed a model-agnostic theoretical\nframework for predicting latency and energy consumption in binary\nclassification models during inference. The framework synthesizes classifier\ncharacteristics, dataset properties, and RAI guardrails into a unified\nanalytical instrument. Two predictive equations are derived that capture the\ninterplay between these factors while offering generalizability across diverse\nclassifiers. The proposed framework provides foundational insights for\ndesigning efficient, responsible ML systems. It enables researchers to\nbenchmark and optimize inference performance and assists practitioners in\ndeploying scalable solutions. Finally, this work establishes a theoretical\nfoundation for balancing computational efficiency with ethical AI principles,\npaving the way for future empirical validation and broader applications."
                },
                "authors": [
                    {
                        "name": "Jason M. Pittman"
                    }
                ],
                "author_detail": {
                    "name": "Jason M. Pittman"
                },
                "author": "Jason M. Pittman",
                "arxiv_comment": "8 pages, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19241v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19241v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18116v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18116v2",
                "updated": "2024-12-26T13:52:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    13,
                    52,
                    48,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-24T02:54:56Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    2,
                    54,
                    56,
                    1,
                    359,
                    0
                ],
                "title": "AutoDroid-V2: Boosting SLM-based GUI Agents via Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoDroid-V2: Boosting SLM-based GUI Agents via Code Generation"
                },
                "summary": "Large language models (LLMs) have brought exciting new advances to mobile UI\nagents, a long-standing research field that aims to complete arbitrary natural\nlanguage tasks through mobile UI interactions. However, existing UI agents\nusually demand high reasoning capabilities of powerful large models that are\ndifficult to be deployed locally on end-users' devices, which raises huge\nconcerns about user privacy and centralized serving cost. One way to reduce the\nrequired model size is to customize a smaller domain-specific model with\nhigh-quality training data, e.g. large-scale human demonstrations of diverse\ntypes of apps and tasks, while such datasets are extremely difficult to obtain.\nInspired by the remarkable coding abilities of recent small language models\n(SLMs), we propose to convert the UI task automation problem to a code\ngeneration problem, which can be effectively solved by an on-device SLM and\nefficiently executed with an on-device code interpreter. Unlike normal coding\ntasks that can be extensively pretrained with public datasets, generating UI\nautomation code is challenging due to the diversity, complexity, and\nvariability of target apps. Therefore, we adopt a document-centered approach\nthat automatically builds fine-grained API documentation for each app and\ngenerates diverse task samples based on this documentation. By guiding the\nagent with the synthetic documents and task samples, it learns to generate\nprecise and efficient scripts to complete unseen tasks. Based on detailed\ncomparisons with state-of-the-art mobile UI agents, our approach effectively\nimproves the mobile task automation with significantly higher success rates and\nlower latency/token consumption. Code will be open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have brought exciting new advances to mobile UI\nagents, a long-standing research field that aims to complete arbitrary natural\nlanguage tasks through mobile UI interactions. However, existing UI agents\nusually demand high reasoning capabilities of powerful large models that are\ndifficult to be deployed locally on end-users' devices, which raises huge\nconcerns about user privacy and centralized serving cost. One way to reduce the\nrequired model size is to customize a smaller domain-specific model with\nhigh-quality training data, e.g. large-scale human demonstrations of diverse\ntypes of apps and tasks, while such datasets are extremely difficult to obtain.\nInspired by the remarkable coding abilities of recent small language models\n(SLMs), we propose to convert the UI task automation problem to a code\ngeneration problem, which can be effectively solved by an on-device SLM and\nefficiently executed with an on-device code interpreter. Unlike normal coding\ntasks that can be extensively pretrained with public datasets, generating UI\nautomation code is challenging due to the diversity, complexity, and\nvariability of target apps. Therefore, we adopt a document-centered approach\nthat automatically builds fine-grained API documentation for each app and\ngenerates diverse task samples based on this documentation. By guiding the\nagent with the synthetic documents and task samples, it learns to generate\nprecise and efficient scripts to complete unseen tasks. Based on detailed\ncomparisons with state-of-the-art mobile UI agents, our approach effectively\nimproves the mobile task automation with significantly higher success rates and\nlower latency/token consumption. Code will be open-sourced."
                },
                "authors": [
                    {
                        "name": "Hao Wen"
                    },
                    {
                        "name": "Shizuo Tian"
                    },
                    {
                        "name": "Borislav Pavlov"
                    },
                    {
                        "name": "Wenjie Du"
                    },
                    {
                        "name": "Yixuan Li"
                    },
                    {
                        "name": "Ge Chang"
                    },
                    {
                        "name": "Shanhui Zhao"
                    },
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Yunxin Liu"
                    },
                    {
                        "name": "Ya-Qin Zhang"
                    },
                    {
                        "name": "Yuanchun Li"
                    }
                ],
                "author_detail": {
                    "name": "Yuanchun Li"
                },
                "author": "Yuanchun Li",
                "arxiv_comment": "15 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18116v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18116v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2208.06528v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2208.06528v4",
                "updated": "2024-12-26T13:40:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    13,
                    40,
                    43,
                    3,
                    361,
                    0
                ],
                "published": "2022-08-12T23:17:46Z",
                "published_parsed": [
                    2022,
                    8,
                    12,
                    23,
                    17,
                    46,
                    4,
                    224,
                    0
                ],
                "title": "Dynamic Bayesian Learning for Spatiotemporal Mechanistic Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Bayesian Learning for Spatiotemporal Mechanistic Models"
                },
                "summary": "We develop an approach for Bayesian learning of spatiotemporal dynamical\nmechanistic models. Such learning consists of statistical emulation of the\nmechanistic system that can efficiently interpolate the output of the system\nfrom arbitrary inputs. The emulated learner can then be used to train the\nsystem from noisy data achieved by melding information from observed data with\nthe emulated mechanistic system. This joint melding of mechanistic systems\nemploy hierarchical state-space models with Gaussian process regression.\nAssuming the dynamical system is controlled by a finite collection of inputs,\nGaussian process regression learns the effect of these parameters through a\nnumber of training runs, driving the stochastic innovations of the\nspatiotemporal state-space component. This enables efficient modeling of the\ndynamics over space and time. This article details exact inference with\nanalytically accessible posterior distributions in hierarchical matrix-variate\nNormal and Wishart models in designing the emulator. This step obviates\nexpensive iterative algorithms such as Markov chain Monte Carlo or variational\napproximations. We also show how emulation is applicable to large-scale\nemulation by designing a dynamic Bayesian transfer learning framework.\nInference on $\\bm \\eta$ proceeds using Markov chain Monte Carlo as a\npost-emulation step using the emulator as a regression component. We\ndemonstrate this framework through solving inverse problems arising in the\nanalysis of ordinary and partial nonlinear differential equations and, in\naddition, to a black-box computer model generating spatiotemporal dynamics\nacross a graphical model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop an approach for Bayesian learning of spatiotemporal dynamical\nmechanistic models. Such learning consists of statistical emulation of the\nmechanistic system that can efficiently interpolate the output of the system\nfrom arbitrary inputs. The emulated learner can then be used to train the\nsystem from noisy data achieved by melding information from observed data with\nthe emulated mechanistic system. This joint melding of mechanistic systems\nemploy hierarchical state-space models with Gaussian process regression.\nAssuming the dynamical system is controlled by a finite collection of inputs,\nGaussian process regression learns the effect of these parameters through a\nnumber of training runs, driving the stochastic innovations of the\nspatiotemporal state-space component. This enables efficient modeling of the\ndynamics over space and time. This article details exact inference with\nanalytically accessible posterior distributions in hierarchical matrix-variate\nNormal and Wishart models in designing the emulator. This step obviates\nexpensive iterative algorithms such as Markov chain Monte Carlo or variational\napproximations. We also show how emulation is applicable to large-scale\nemulation by designing a dynamic Bayesian transfer learning framework.\nInference on $\\bm \\eta$ proceeds using Markov chain Monte Carlo as a\npost-emulation step using the emulator as a regression component. We\ndemonstrate this framework through solving inverse problems arising in the\nanalysis of ordinary and partial nonlinear differential equations and, in\naddition, to a black-box computer model generating spatiotemporal dynamics\nacross a graphical model."
                },
                "authors": [
                    {
                        "name": "Sudipto Banerjee"
                    },
                    {
                        "name": "Xiang Chen"
                    },
                    {
                        "name": "Ian Frankenburg"
                    },
                    {
                        "name": "Daniel Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Zhou"
                },
                "author": "Daniel Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2208.06528v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2208.06528v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19211v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19211v1",
                "updated": "2024-12-26T13:21:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    13,
                    21,
                    9,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T13:21:09Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    13,
                    21,
                    9,
                    3,
                    361,
                    0
                ],
                "title": "Large Language Models Meet Graph Neural Networks: A Perspective of Graph\n  Mining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Meet Graph Neural Networks: A Perspective of Graph\n  Mining"
                },
                "summary": "Graph mining is an important area in data mining and machine learning that\ninvolves extracting valuable information from graph-structured data. In recent\nyears, significant progress has been made in this field through the development\nof graph neural networks (GNNs). However, GNNs are still deficient in\ngeneralizing to diverse graph data. Aiming to this issue, Large Language Models\n(LLMs) could provide new solutions for graph mining tasks with their superior\nsemantic understanding. In this review, we systematically review the\ncombination and application techniques of LLMs and GNNs and present a novel\ntaxonomy for research in this interdisciplinary field, which involves three\nmain categories: GNN-driving-LLM, LLM-driving-GNN, and GNN-LLM-co-driving.\nWithin this framework, we reveal the capabilities of LLMs in enhancing graph\nfeature extraction as well as improving the effectiveness of downstream tasks\nsuch as node classification, link prediction, and community detection. Although\nLLMs have demonstrated their great potential in handling graph-structured data,\ntheir high computational requirements and complexity remain challenges. Future\nresearch needs to continue to explore how to efficiently fuse LLMs and GNNs to\nachieve more powerful graph learning and reasoning capabilities and provide new\nimpetus for the development of graph mining techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph mining is an important area in data mining and machine learning that\ninvolves extracting valuable information from graph-structured data. In recent\nyears, significant progress has been made in this field through the development\nof graph neural networks (GNNs). However, GNNs are still deficient in\ngeneralizing to diverse graph data. Aiming to this issue, Large Language Models\n(LLMs) could provide new solutions for graph mining tasks with their superior\nsemantic understanding. In this review, we systematically review the\ncombination and application techniques of LLMs and GNNs and present a novel\ntaxonomy for research in this interdisciplinary field, which involves three\nmain categories: GNN-driving-LLM, LLM-driving-GNN, and GNN-LLM-co-driving.\nWithin this framework, we reveal the capabilities of LLMs in enhancing graph\nfeature extraction as well as improving the effectiveness of downstream tasks\nsuch as node classification, link prediction, and community detection. Although\nLLMs have demonstrated their great potential in handling graph-structured data,\ntheir high computational requirements and complexity remain challenges. Future\nresearch needs to continue to explore how to efficiently fuse LLMs and GNNs to\nachieve more powerful graph learning and reasoning capabilities and provide new\nimpetus for the development of graph mining techniques."
                },
                "authors": [
                    {
                        "name": "Yuxin You"
                    },
                    {
                        "name": "Zhen Liu"
                    },
                    {
                        "name": "Xiangchao Wen"
                    },
                    {
                        "name": "Yongtao Zhang"
                    },
                    {
                        "name": "Wei Ai"
                    }
                ],
                "author_detail": {
                    "name": "Wei Ai"
                },
                "author": "Wei Ai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19211v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19211v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.15351v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.15351v2",
                "updated": "2024-12-26T13:14:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    13,
                    14,
                    16,
                    3,
                    361,
                    0
                ],
                "published": "2024-02-23T14:38:19Z",
                "published_parsed": [
                    2024,
                    2,
                    23,
                    14,
                    38,
                    19,
                    4,
                    54,
                    0
                ],
                "title": "AutoMMLab: Automatically Generating Deployable Models from Language\n  Instructions for Computer Vision Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoMMLab: Automatically Generating Deployable Models from Language\n  Instructions for Computer Vision Tasks"
                },
                "summary": "Automated machine learning (AutoML) is a collection of techniques designed to\nautomate the machine learning development process. While traditional AutoML\napproaches have been successfully applied in several critical steps of model\ndevelopment (e.g. hyperparameter optimization), there lacks a AutoML system\nthat automates the entire end-to-end model production workflow for computer\nvision. To fill this blank, we propose a novel request-to-model task, which\ninvolves understanding the user's natural language request and execute the\nentire workflow to output production-ready models. This empowers non-expert\nindividuals to easily build task-specific models via a user-friendly language\ninterface. To facilitate development and evaluation, we develop a new\nexperimental platform called AutoMMLab and a new benchmark called LAMP for\nstudying key components in the end-to-end request-to-model pipeline.\nHyperparameter optimization (HPO) is one of the most important components for\nAutoML. Traditional approaches mostly rely on trial-and-error, leading to\ninefficient parameter search. To solve this problem, we propose a novel\nLLM-based HPO algorithm, called HPO-LLaMA. Equipped with extensive knowledge\nand experience in model hyperparameter tuning, HPO-LLaMA achieves significant\nimprovement of HPO efficiency. Dataset and code are available at\nhttps://github.com/yang-ze-kang/AutoMMLab.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated machine learning (AutoML) is a collection of techniques designed to\nautomate the machine learning development process. While traditional AutoML\napproaches have been successfully applied in several critical steps of model\ndevelopment (e.g. hyperparameter optimization), there lacks a AutoML system\nthat automates the entire end-to-end model production workflow for computer\nvision. To fill this blank, we propose a novel request-to-model task, which\ninvolves understanding the user's natural language request and execute the\nentire workflow to output production-ready models. This empowers non-expert\nindividuals to easily build task-specific models via a user-friendly language\ninterface. To facilitate development and evaluation, we develop a new\nexperimental platform called AutoMMLab and a new benchmark called LAMP for\nstudying key components in the end-to-end request-to-model pipeline.\nHyperparameter optimization (HPO) is one of the most important components for\nAutoML. Traditional approaches mostly rely on trial-and-error, leading to\ninefficient parameter search. To solve this problem, we propose a novel\nLLM-based HPO algorithm, called HPO-LLaMA. Equipped with extensive knowledge\nand experience in model hyperparameter tuning, HPO-LLaMA achieves significant\nimprovement of HPO efficiency. Dataset and code are available at\nhttps://github.com/yang-ze-kang/AutoMMLab."
                },
                "authors": [
                    {
                        "name": "Zekang Yang"
                    },
                    {
                        "name": "Wang Zeng"
                    },
                    {
                        "name": "Sheng Jin"
                    },
                    {
                        "name": "Chen Qian"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Wentao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Liu"
                },
                "author": "Wentao Liu",
                "arxiv_comment": "Accepted by AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.15351v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.15351v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19206v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19206v1",
                "updated": "2024-12-26T13:07:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    13,
                    7,
                    3,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T13:07:03Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    13,
                    7,
                    3,
                    3,
                    361,
                    0
                ],
                "title": "NADER: Neural Architecture Design via Multi-Agent Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NADER: Neural Architecture Design via Multi-Agent Collaboration"
                },
                "summary": "Designing effective neural architectures poses a significant challenge in\ndeep learning. While Neural Architecture Search (NAS) automates the search for\noptimal architectures, existing methods are often constrained by predetermined\nsearch spaces and may miss critical neural architectures. In this paper, we\nintroduce NADER (Neural Architecture Design via multi-agEnt collaboRation), a\nnovel framework that formulates neural architecture design (NAD) as a LLM-based\nmulti-agent collaboration problem. NADER employs a team of specialized agents\nto enhance a base architecture through iterative modification. Current\nLLM-based NAD methods typically operate independently, lacking the ability to\nlearn from past experiences, which results in repeated mistakes and inefficient\nexploration. To address this issue, we propose the Reflector, which effectively\nlearns from immediate feedback and long-term experiences. Additionally, unlike\nprevious LLM-based methods that use code to represent neural architectures, we\nutilize a graph-based representation. This approach allows agents to focus on\ndesign aspects without being distracted by coding. We demonstrate the\neffectiveness of NADER in discovering high-performing architectures beyond\npredetermined search spaces through extensive experiments on benchmark tasks,\nshowcasing its advantages over state-of-the-art methods. The codes will be\nreleased soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing effective neural architectures poses a significant challenge in\ndeep learning. While Neural Architecture Search (NAS) automates the search for\noptimal architectures, existing methods are often constrained by predetermined\nsearch spaces and may miss critical neural architectures. In this paper, we\nintroduce NADER (Neural Architecture Design via multi-agEnt collaboRation), a\nnovel framework that formulates neural architecture design (NAD) as a LLM-based\nmulti-agent collaboration problem. NADER employs a team of specialized agents\nto enhance a base architecture through iterative modification. Current\nLLM-based NAD methods typically operate independently, lacking the ability to\nlearn from past experiences, which results in repeated mistakes and inefficient\nexploration. To address this issue, we propose the Reflector, which effectively\nlearns from immediate feedback and long-term experiences. Additionally, unlike\nprevious LLM-based methods that use code to represent neural architectures, we\nutilize a graph-based representation. This approach allows agents to focus on\ndesign aspects without being distracted by coding. We demonstrate the\neffectiveness of NADER in discovering high-performing architectures beyond\npredetermined search spaces through extensive experiments on benchmark tasks,\nshowcasing its advantages over state-of-the-art methods. The codes will be\nreleased soon."
                },
                "authors": [
                    {
                        "name": "Zekang Yang"
                    },
                    {
                        "name": "Wang Zeng"
                    },
                    {
                        "name": "Sheng Jin"
                    },
                    {
                        "name": "Chen Qian"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Wentao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Liu"
                },
                "author": "Wentao Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19206v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19206v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19198v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19198v1",
                "updated": "2024-12-26T12:36:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    12,
                    36,
                    39,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T12:36:39Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    12,
                    36,
                    39,
                    3,
                    361,
                    0
                ],
                "title": "Multi-Attribute Constraint Satisfaction via Language Model Rewriting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Attribute Constraint Satisfaction via Language Model Rewriting"
                },
                "summary": "Obeying precise constraints on top of multiple external attributes is a\ncommon computational problem underlying seemingly different domains, from\ncontrolled text generation to protein engineering. Existing language model (LM)\ncontrollability methods for multi-attribute constraint satisfaction often rely\non specialized architectures or gradient-based classifiers, limiting their\nflexibility to work with arbitrary black-box evaluators and pretrained models.\nCurrent general-purpose large language models, while capable, cannot achieve\nfine-grained multi-attribute control over external attributes. Thus, we create\nMulti-Attribute Constraint Satisfaction (MACS), a generalized method capable of\nfinetuning language models on any sequential domain to satisfy user-specified\nconstraints on multiple external real-value attributes. Our method trains LMs\nas editors by sampling diverse multi-attribute edit pairs from an initial set\nof paraphrased outputs. During inference, LM iteratively improves upon its\nprevious solution to satisfy constraints for all attributes by leveraging our\ndesigned constraint satisfaction reward. We additionally experiment with\nreward-weighted behavior cloning to further improve the constraint satisfaction\nrate of LMs. To evaluate our approach, we present a new Fine-grained Constraint\nSatisfaction (FineCS) benchmark, featuring two challenging tasks: (1) Text\nStyle Transfer, where the goal is to simultaneously modify the sentiment and\ncomplexity of reviews, and (2) Protein Design, focusing on modulating\nfluorescence and stability of Green Fluorescent Proteins (GFP). Our empirical\nresults show that MACS achieves the highest threshold satisfaction in both\nFineCS tasks, outperforming strong domain-specific baselines. Our work opens\nnew avenues for generalized and real-value multi-attribute control, with\nimplications for diverse applications spanning NLP and bioinformatics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Obeying precise constraints on top of multiple external attributes is a\ncommon computational problem underlying seemingly different domains, from\ncontrolled text generation to protein engineering. Existing language model (LM)\ncontrollability methods for multi-attribute constraint satisfaction often rely\non specialized architectures or gradient-based classifiers, limiting their\nflexibility to work with arbitrary black-box evaluators and pretrained models.\nCurrent general-purpose large language models, while capable, cannot achieve\nfine-grained multi-attribute control over external attributes. Thus, we create\nMulti-Attribute Constraint Satisfaction (MACS), a generalized method capable of\nfinetuning language models on any sequential domain to satisfy user-specified\nconstraints on multiple external real-value attributes. Our method trains LMs\nas editors by sampling diverse multi-attribute edit pairs from an initial set\nof paraphrased outputs. During inference, LM iteratively improves upon its\nprevious solution to satisfy constraints for all attributes by leveraging our\ndesigned constraint satisfaction reward. We additionally experiment with\nreward-weighted behavior cloning to further improve the constraint satisfaction\nrate of LMs. To evaluate our approach, we present a new Fine-grained Constraint\nSatisfaction (FineCS) benchmark, featuring two challenging tasks: (1) Text\nStyle Transfer, where the goal is to simultaneously modify the sentiment and\ncomplexity of reviews, and (2) Protein Design, focusing on modulating\nfluorescence and stability of Green Fluorescent Proteins (GFP). Our empirical\nresults show that MACS achieves the highest threshold satisfaction in both\nFineCS tasks, outperforming strong domain-specific baselines. Our work opens\nnew avenues for generalized and real-value multi-attribute control, with\nimplications for diverse applications spanning NLP and bioinformatics."
                },
                "authors": [
                    {
                        "name": "Ashutosh Baheti"
                    },
                    {
                        "name": "Debanjana Chakraborty"
                    },
                    {
                        "name": "Faeze Brahman"
                    },
                    {
                        "name": "Ronan Le Bras"
                    },
                    {
                        "name": "Ximing Lu"
                    },
                    {
                        "name": "Nouha Dziri"
                    },
                    {
                        "name": "Yejin Choi"
                    },
                    {
                        "name": "Mark Riedl"
                    },
                    {
                        "name": "Maarten Sap"
                    }
                ],
                "author_detail": {
                    "name": "Maarten Sap"
                },
                "author": "Maarten Sap",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19198v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19198v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19191v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19191v1",
                "updated": "2024-12-26T12:12:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    12,
                    12,
                    23,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T12:12:23Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    12,
                    12,
                    23,
                    3,
                    361,
                    0
                ],
                "title": "Biology Instructions: A Dataset and Benchmark for Multi-Omics Sequence\n  Understanding Capability of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Biology Instructions: A Dataset and Benchmark for Multi-Omics Sequence\n  Understanding Capability of Large Language Models"
                },
                "summary": "Large language models have already demonstrated their formidable capabilities\nin general domains, ushering in a revolutionary transformation. However,\nexploring and exploiting the extensive knowledge of these models to comprehend\nmulti-omics biology remains underexplored. To fill this research gap, we first\nintroduce Biology-Instructions, the first large-scale multi-omics biological\nsequences-related instruction-tuning dataset including DNA, RNA, proteins, and\nmulti-molecules, designed to bridge the gap between large language models\n(LLMs) and complex biological sequences-related tasks. This dataset can enhance\nthe versatility of LLMs by integrating diverse biological sequenced-based\nprediction tasks with advanced reasoning capabilities, while maintaining\nconversational fluency. Additionally, we reveal significant performance\nlimitations in even state-of-the-art LLMs on biological sequence-related\nmulti-omics tasks without specialized pre-training and instruction-tuning. We\nfurther develop a strong baseline called ChatMultiOmics with a novel\nthree-stage training pipeline, demonstrating the powerful ability to understand\nbiology by using Biology-Instructions. Biology-Instructions and ChatMultiOmics\nare publicly available and crucial resources for enabling more effective\nintegration of LLMs with multi-omics sequence analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have already demonstrated their formidable capabilities\nin general domains, ushering in a revolutionary transformation. However,\nexploring and exploiting the extensive knowledge of these models to comprehend\nmulti-omics biology remains underexplored. To fill this research gap, we first\nintroduce Biology-Instructions, the first large-scale multi-omics biological\nsequences-related instruction-tuning dataset including DNA, RNA, proteins, and\nmulti-molecules, designed to bridge the gap between large language models\n(LLMs) and complex biological sequences-related tasks. This dataset can enhance\nthe versatility of LLMs by integrating diverse biological sequenced-based\nprediction tasks with advanced reasoning capabilities, while maintaining\nconversational fluency. Additionally, we reveal significant performance\nlimitations in even state-of-the-art LLMs on biological sequence-related\nmulti-omics tasks without specialized pre-training and instruction-tuning. We\nfurther develop a strong baseline called ChatMultiOmics with a novel\nthree-stage training pipeline, demonstrating the powerful ability to understand\nbiology by using Biology-Instructions. Biology-Instructions and ChatMultiOmics\nare publicly available and crucial resources for enabling more effective\nintegration of LLMs with multi-omics sequence analysis."
                },
                "authors": [
                    {
                        "name": "Haonan He"
                    },
                    {
                        "name": "Yuchen Ren"
                    },
                    {
                        "name": "Yining Tang"
                    },
                    {
                        "name": "Ziyang Xu"
                    },
                    {
                        "name": "Junxian Li"
                    },
                    {
                        "name": "Minghao Yang"
                    },
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Dong Yuan"
                    },
                    {
                        "name": "Tao Chen"
                    },
                    {
                        "name": "Shufei Zhang"
                    },
                    {
                        "name": "Yuqiang Li"
                    },
                    {
                        "name": "Nanqing Dong"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    },
                    {
                        "name": "Peng Ye"
                    }
                ],
                "author_detail": {
                    "name": "Peng Ye"
                },
                "author": "Peng Ye",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19191v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19191v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.BM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2402.09614v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.09614v3",
                "updated": "2024-12-27T18:43:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    18,
                    43,
                    59,
                    4,
                    362,
                    0
                ],
                "published": "2024-02-14T23:05:44Z",
                "published_parsed": [
                    2024,
                    2,
                    14,
                    23,
                    5,
                    44,
                    2,
                    45,
                    0
                ],
                "title": "Reasoning over Uncertain Text by Generative Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning over Uncertain Text by Generative Large Language Models"
                },
                "summary": "This paper considers the challenges Large Language Models (LLMs) face when\nreasoning over text that includes information involving uncertainty explicitly\nquantified via probability values. This type of reasoning is relevant to a\nvariety of contexts ranging from everyday conversations to medical\ndecision-making. Despite improvements in the mathematical reasoning\ncapabilities of LLMs, they still exhibit significant difficulties when it comes\nto probabilistic reasoning. To deal with this problem, we introduce the\nBayesian Linguistic Inference Dataset (BLInD), a new dataset specifically\ndesigned to test the probabilistic reasoning capabilities of LLMs. We use BLInD\nto find out the limitations of LLMs for tasks involving probabilistic\nreasoning. In addition, we present several prompting strategies that map the\nproblem to different formal representations, including Python code,\nprobabilistic algorithms, and probabilistic logical programming. We conclude by\nproviding an evaluation of our methods on BLInD and an adaptation of a causal\nreasoning question-answering dataset. Our empirical results highlight the\neffectiveness of our proposed strategies for multiple LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper considers the challenges Large Language Models (LLMs) face when\nreasoning over text that includes information involving uncertainty explicitly\nquantified via probability values. This type of reasoning is relevant to a\nvariety of contexts ranging from everyday conversations to medical\ndecision-making. Despite improvements in the mathematical reasoning\ncapabilities of LLMs, they still exhibit significant difficulties when it comes\nto probabilistic reasoning. To deal with this problem, we introduce the\nBayesian Linguistic Inference Dataset (BLInD), a new dataset specifically\ndesigned to test the probabilistic reasoning capabilities of LLMs. We use BLInD\nto find out the limitations of LLMs for tasks involving probabilistic\nreasoning. In addition, we present several prompting strategies that map the\nproblem to different formal representations, including Python code,\nprobabilistic algorithms, and probabilistic logical programming. We conclude by\nproviding an evaluation of our methods on BLInD and an adaptation of a causal\nreasoning question-answering dataset. Our empirical results highlight the\neffectiveness of our proposed strategies for multiple LLMs."
                },
                "authors": [
                    {
                        "name": "Aliakbar Nafar"
                    },
                    {
                        "name": "Kristen Brent Venable"
                    },
                    {
                        "name": "Parisa Kordjamshidi"
                    }
                ],
                "author_detail": {
                    "name": "Parisa Kordjamshidi"
                },
                "author": "Parisa Kordjamshidi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.09614v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.09614v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19784v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19784v1",
                "updated": "2024-12-27T18:25:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    18,
                    25,
                    27,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T18:25:27Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    18,
                    25,
                    27,
                    4,
                    362,
                    0
                ],
                "title": "Can AI Help with Your Personal Finances?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can AI Help with Your Personal Finances?"
                },
                "summary": "In recent years, Large Language Models (LLMs) have emerged as a\ntransformative development in artificial intelligence (AI), drawing significant\nattention from industry and academia. Trained on vast datasets, these\nsophisticated AI systems exhibit impressive natural language processing and\ncontent generation capabilities. This paper explores the potential of LLMs to\naddress key challenges in personal finance, focusing on the United States. We\nevaluate several leading LLMs, including OpenAI's ChatGPT, Google's Gemini,\nAnthropic's Claude, and Meta's Llama, to assess their effectiveness in\nproviding accurate financial advice on topics such as mortgages, taxes, loans,\nand investments. Our findings show that while these models achieve an average\naccuracy rate of approximately 70%, they also display notable limitations in\ncertain areas. Specifically, LLMs struggle to provide accurate responses for\ncomplex financial queries, with performance varying significantly across\ndifferent topics. Despite these limitations, the analysis reveals notable\nimprovements in newer versions of these models, highlighting their growing\nutility for individuals and financial advisors. As these AI systems continue to\nevolve, their potential for advancing AI-driven applications in personal\nfinance becomes increasingly promising.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Large Language Models (LLMs) have emerged as a\ntransformative development in artificial intelligence (AI), drawing significant\nattention from industry and academia. Trained on vast datasets, these\nsophisticated AI systems exhibit impressive natural language processing and\ncontent generation capabilities. This paper explores the potential of LLMs to\naddress key challenges in personal finance, focusing on the United States. We\nevaluate several leading LLMs, including OpenAI's ChatGPT, Google's Gemini,\nAnthropic's Claude, and Meta's Llama, to assess their effectiveness in\nproviding accurate financial advice on topics such as mortgages, taxes, loans,\nand investments. Our findings show that while these models achieve an average\naccuracy rate of approximately 70%, they also display notable limitations in\ncertain areas. Specifically, LLMs struggle to provide accurate responses for\ncomplex financial queries, with performance varying significantly across\ndifferent topics. Despite these limitations, the analysis reveals notable\nimprovements in newer versions of these models, highlighting their growing\nutility for individuals and financial advisors. As these AI systems continue to\nevolve, their potential for advancing AI-driven applications in personal\nfinance becomes increasingly promising."
                },
                "authors": [
                    {
                        "name": "Oudom Hean"
                    },
                    {
                        "name": "Utsha Saha"
                    },
                    {
                        "name": "Binita Saha"
                    }
                ],
                "author_detail": {
                    "name": "Binita Saha"
                },
                "author": "Binita Saha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19784v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19784v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19770v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19770v1",
                "updated": "2024-12-27T18:06:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    18,
                    6,
                    25,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T18:06:25Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    18,
                    6,
                    25,
                    4,
                    362,
                    0
                ],
                "title": "Fortran2CPP: Automating Fortran-to-C++ Migration using LLMs via\n  Multi-Turn Dialogue and Dual-Agent Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fortran2CPP: Automating Fortran-to-C++ Migration using LLMs via\n  Multi-Turn Dialogue and Dual-Agent Integration"
                },
                "summary": "Migrating Fortran code to C++ is a common task for many scientific computing\nteams, driven by the need to leverage modern programming paradigms, enhance\ncross-platform compatibility, and improve maintainability. Automating this\ntranslation process using large language models (LLMs) has shown promise, but\nthe lack of high-quality, specialized datasets has hindered their\neffectiveness. In this paper, we address this challenge by introducing a novel\nmulti-turn dialogue dataset, Fortran2CPP, specifically designed for\nFortran-to-C++ code migration. Our dataset, significantly larger than existing\nalternatives, is generated using a unique LLM-driven, dual-agent pipeline\nincorporating iterative compilation, execution, and code repair to ensure high\nquality and functional correctness. To demonstrate the effectiveness of our\ndataset, we fine-tuned several open-weight LLMs on Fortran2CPP and evaluated\ntheir performance on two independent benchmarks. Fine-tuning on our dataset led\nto remarkable gains, with models achieving up to a 3.31x increase in CodeBLEU\nscore and a 92\\% improvement in compilation success rate. This highlights the\ndataset's ability to enhance both the syntactic accuracy and compilability of\nthe translated C++ code. Our dataset and model have been open-sourced and are\navailable on our public GitHub\nrepository\\footnote{\\url{https://github.com/HPC-Fortran2CPP/Fortran2Cpp}}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Migrating Fortran code to C++ is a common task for many scientific computing\nteams, driven by the need to leverage modern programming paradigms, enhance\ncross-platform compatibility, and improve maintainability. Automating this\ntranslation process using large language models (LLMs) has shown promise, but\nthe lack of high-quality, specialized datasets has hindered their\neffectiveness. In this paper, we address this challenge by introducing a novel\nmulti-turn dialogue dataset, Fortran2CPP, specifically designed for\nFortran-to-C++ code migration. Our dataset, significantly larger than existing\nalternatives, is generated using a unique LLM-driven, dual-agent pipeline\nincorporating iterative compilation, execution, and code repair to ensure high\nquality and functional correctness. To demonstrate the effectiveness of our\ndataset, we fine-tuned several open-weight LLMs on Fortran2CPP and evaluated\ntheir performance on two independent benchmarks. Fine-tuning on our dataset led\nto remarkable gains, with models achieving up to a 3.31x increase in CodeBLEU\nscore and a 92\\% improvement in compilation success rate. This highlights the\ndataset's ability to enhance both the syntactic accuracy and compilability of\nthe translated C++ code. Our dataset and model have been open-sourced and are\navailable on our public GitHub\nrepository\\footnote{\\url{https://github.com/HPC-Fortran2CPP/Fortran2Cpp}}."
                },
                "authors": [
                    {
                        "name": "Le Chen"
                    },
                    {
                        "name": "Bin Lei"
                    },
                    {
                        "name": "Dunzhi Zhou"
                    },
                    {
                        "name": "Pei-Hung Lin"
                    },
                    {
                        "name": "Chunhua Liao"
                    },
                    {
                        "name": "Caiwen Ding"
                    },
                    {
                        "name": "Ali Jannesari"
                    }
                ],
                "author_detail": {
                    "name": "Ali Jannesari"
                },
                "author": "Ali Jannesari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19770v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19770v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01366v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01366v2",
                "updated": "2024-12-27T17:49:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    17,
                    49,
                    34,
                    4,
                    362,
                    0
                ],
                "published": "2024-09-02T16:41:44Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    16,
                    41,
                    44,
                    0,
                    246,
                    0
                ],
                "title": "CHESS: Optimizing LLM Inference via Channel-Wise Thresholding and\n  Selective Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CHESS: Optimizing LLM Inference via Channel-Wise Thresholding and\n  Selective Sparsification"
                },
                "summary": "Deploying large language models (LLMs) on edge devices presents significant\nchallenges due to the substantial computational overhead and memory\nrequirements. Activation sparsification can mitigate these resource challenges\nby reducing the number of activated neurons during inference. Existing methods\ntypically employ thresholding-based sparsification based on the statistics of\nactivation tensors. However, they do not model the impact of activation\nsparsification on performance, resulting in suboptimal performance degradation.\nTo address the limitations, this paper reformulates the activation\nsparsification problem to explicitly capture the relationship between\nactivation sparsity and model performance. Then, this paper proposes CHESS, a\ngeneral activation sparsification approach via CHannel-wise thrEsholding and\nSelective Sparsification. First, channel-wise thresholding assigns a unique\nthreshold to each activation channel in the feed-forward network (FFN) layers.\nThen, selective sparsification involves applying thresholding-based activation\nsparsification to specific layers within the attention modules. Finally, we\ndetail the implementation of sparse kernels to accelerate LLM inference.\nExperimental results demonstrate that the proposed CHESS achieves lower\nperformance degradation over eight downstream tasks while activating fewer\nparameters than existing methods, thus speeding up the LLM inference by up to\n1.27x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying large language models (LLMs) on edge devices presents significant\nchallenges due to the substantial computational overhead and memory\nrequirements. Activation sparsification can mitigate these resource challenges\nby reducing the number of activated neurons during inference. Existing methods\ntypically employ thresholding-based sparsification based on the statistics of\nactivation tensors. However, they do not model the impact of activation\nsparsification on performance, resulting in suboptimal performance degradation.\nTo address the limitations, this paper reformulates the activation\nsparsification problem to explicitly capture the relationship between\nactivation sparsity and model performance. Then, this paper proposes CHESS, a\ngeneral activation sparsification approach via CHannel-wise thrEsholding and\nSelective Sparsification. First, channel-wise thresholding assigns a unique\nthreshold to each activation channel in the feed-forward network (FFN) layers.\nThen, selective sparsification involves applying thresholding-based activation\nsparsification to specific layers within the attention modules. Finally, we\ndetail the implementation of sparse kernels to accelerate LLM inference.\nExperimental results demonstrate that the proposed CHESS achieves lower\nperformance degradation over eight downstream tasks while activating fewer\nparameters than existing methods, thus speeding up the LLM inference by up to\n1.27x."
                },
                "authors": [
                    {
                        "name": "Junhui He"
                    },
                    {
                        "name": "Shangyu Wu"
                    },
                    {
                        "name": "Weidong Wen"
                    },
                    {
                        "name": "Chun Jason Xue"
                    },
                    {
                        "name": "Qingan Li"
                    }
                ],
                "author_detail": {
                    "name": "Qingan Li"
                },
                "author": "Qingan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01366v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01366v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19755v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19755v1",
                "updated": "2024-12-27T17:33:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    17,
                    33,
                    39,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T17:33:39Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    17,
                    33,
                    39,
                    4,
                    362,
                    0
                ],
                "title": "\"Did my figure do justice to the answer?\" : Towards Multimodal Short\n  Answer Grading with Feedback (MMSAF)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Did my figure do justice to the answer?\" : Towards Multimodal Short\n  Answer Grading with Feedback (MMSAF)"
                },
                "summary": "Personalized feedback plays a vital role in a student's learning process.\nWhile existing systems are adept at providing feedback over MCQ-based\nevaluation, this work focuses more on subjective and open-ended questions,\nwhich is similar to the problem of Automatic Short Answer Grading (ASAG) with\nfeedback. Additionally, we introduce the Multimodal Short Answer grading with\nFeedback (MMSAF) problem over the traditional ASAG feedback problem to address\nthe scenario where the student answer and reference answer might contain\nimages. Moreover, we introduce the MMSAF dataset with 2197 data points along\nwith an automated framework for generating such data sets. Our evaluations on\nexisting LLMs over this dataset achieved an overall accuracy of 55\\% on Level\nof Correctness labels, 75\\% on Image Relevance labels and a score of 4.27 out\nof 5 in correctness level of LLM generated feedback as rated by experts. As per\nexperts, Pixtral achieved a rating of above 4 out of all metrics, indicating\nthat it is more aligned to human judgement, and that it is the best solution\nfor assisting students.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized feedback plays a vital role in a student's learning process.\nWhile existing systems are adept at providing feedback over MCQ-based\nevaluation, this work focuses more on subjective and open-ended questions,\nwhich is similar to the problem of Automatic Short Answer Grading (ASAG) with\nfeedback. Additionally, we introduce the Multimodal Short Answer grading with\nFeedback (MMSAF) problem over the traditional ASAG feedback problem to address\nthe scenario where the student answer and reference answer might contain\nimages. Moreover, we introduce the MMSAF dataset with 2197 data points along\nwith an automated framework for generating such data sets. Our evaluations on\nexisting LLMs over this dataset achieved an overall accuracy of 55\\% on Level\nof Correctness labels, 75\\% on Image Relevance labels and a score of 4.27 out\nof 5 in correctness level of LLM generated feedback as rated by experts. As per\nexperts, Pixtral achieved a rating of above 4 out of all metrics, indicating\nthat it is more aligned to human judgement, and that it is the best solution\nfor assisting students."
                },
                "authors": [
                    {
                        "name": "Pritam Sil"
                    },
                    {
                        "name": "Bhaskaran Raman"
                    },
                    {
                        "name": "Pushpak Bhattacharyya"
                    }
                ],
                "author_detail": {
                    "name": "Pushpak Bhattacharyya"
                },
                "author": "Pushpak Bhattacharyya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19755v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19755v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19726v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19726v1",
                "updated": "2024-12-27T16:30:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    16,
                    30,
                    12,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T16:30:12Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    16,
                    30,
                    12,
                    4,
                    362,
                    0
                ],
                "title": "Can Large Language Models Adapt to Other Agents In-Context?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Adapt to Other Agents In-Context?"
                },
                "summary": "As the research community aims to build better AI assistants that are more\ndynamic and personalized to the diversity of humans that they interact with,\nthere is increased interest in evaluating the theory of mind capabilities of\nlarge language models (LLMs). Indeed, several recent studies suggest that LLM\ntheory of mind capabilities are quite impressive, approximating human-level\nperformance. Our paper aims to rebuke this narrative and argues instead that\npast studies were not directly measuring agent performance, potentially leading\nto findings that are illusory in nature as a result. We draw a strong\ndistinction between what we call literal theory of mind i.e. measuring the\nagent's ability to predict the behavior of others and functional theory of mind\ni.e. adapting to agents in-context based on a rational response to predictions\nof their behavior. We find that top performing open source LLMs may display\nstrong capabilities in literal theory of mind, depending on how they are\nprompted, but seem to struggle with functional theory of mind -- even when\npartner policies are exceedingly simple. Our work serves to highlight the\ndouble sided nature of inductive bias in LLMs when adapting to new situations.\nWhile this bias can lead to strong performance over limited horizons, it often\nhinders convergence to optimal long-term behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the research community aims to build better AI assistants that are more\ndynamic and personalized to the diversity of humans that they interact with,\nthere is increased interest in evaluating the theory of mind capabilities of\nlarge language models (LLMs). Indeed, several recent studies suggest that LLM\ntheory of mind capabilities are quite impressive, approximating human-level\nperformance. Our paper aims to rebuke this narrative and argues instead that\npast studies were not directly measuring agent performance, potentially leading\nto findings that are illusory in nature as a result. We draw a strong\ndistinction between what we call literal theory of mind i.e. measuring the\nagent's ability to predict the behavior of others and functional theory of mind\ni.e. adapting to agents in-context based on a rational response to predictions\nof their behavior. We find that top performing open source LLMs may display\nstrong capabilities in literal theory of mind, depending on how they are\nprompted, but seem to struggle with functional theory of mind -- even when\npartner policies are exceedingly simple. Our work serves to highlight the\ndouble sided nature of inductive bias in LLMs when adapting to new situations.\nWhile this bias can lead to strong performance over limited horizons, it often\nhinders convergence to optimal long-term behavior."
                },
                "authors": [
                    {
                        "name": "Matthew Riemer"
                    },
                    {
                        "name": "Zahra Ashktorab"
                    },
                    {
                        "name": "Djallel Bouneffouf"
                    },
                    {
                        "name": "Payel Das"
                    },
                    {
                        "name": "Miao Liu"
                    },
                    {
                        "name": "Justin D. Weisz"
                    },
                    {
                        "name": "Murray Campbell"
                    }
                ],
                "author_detail": {
                    "name": "Murray Campbell"
                },
                "author": "Murray Campbell",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19726v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19726v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07343v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07343v4",
                "updated": "2024-12-27T16:29:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    16,
                    29,
                    25,
                    4,
                    362,
                    0
                ],
                "published": "2024-08-14T07:37:07Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    7,
                    37,
                    7,
                    2,
                    227,
                    0
                ],
                "title": "Gradient Alignment Improves Test-Time Adaptation for Medical Image\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gradient Alignment Improves Test-Time Adaptation for Medical Image\n  Segmentation"
                },
                "summary": "Although recent years have witnessed significant advancements in medical\nimage segmentation, the pervasive issue of domain shift among medical images\nfrom diverse centres hinders the effective deployment of pre-trained models.\nMany Test-time Adaptation (TTA) methods have been proposed to address this\nissue by fine-tuning pre-trained models with test data during inference. These\nmethods, however, often suffer from less-satisfactory optimization due to\nsuboptimal optimization direction (dictated by the gradient) and fixed\nstep-size (predicated on the learning rate). In this paper, we propose the\nGradient alignment-based Test-time adaptation (GraTa) method to improve both\nthe gradient direction and learning rate in the optimization procedure. Unlike\nconventional TTA methods, which primarily optimize the pseudo gradient derived\nfrom a self-supervised objective, our method incorporates an auxiliary gradient\nwith the pseudo one to facilitate gradient alignment. Such gradient alignment\nenables the model to excavate the similarities between different gradients and\ncorrect the gradient direction to approximate the empirical gradient related to\nthe current segmentation task. Additionally, we design a dynamic learning rate\nbased on the cosine similarity between the pseudo and auxiliary gradients,\nthereby empowering the adaptive fine-tuning of pre-trained models on diverse\ntest data. Extensive experiments establish the effectiveness of the proposed\ngradient alignment and dynamic learning rate and substantiate the superiority\nof our GraTa method over other state-of-the-art TTA methods on a benchmark\nmedical image segmentation task. The code and weights of pre-trained source\nmodels are available at https://github.com/Chen-Ziyang/GraTa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although recent years have witnessed significant advancements in medical\nimage segmentation, the pervasive issue of domain shift among medical images\nfrom diverse centres hinders the effective deployment of pre-trained models.\nMany Test-time Adaptation (TTA) methods have been proposed to address this\nissue by fine-tuning pre-trained models with test data during inference. These\nmethods, however, often suffer from less-satisfactory optimization due to\nsuboptimal optimization direction (dictated by the gradient) and fixed\nstep-size (predicated on the learning rate). In this paper, we propose the\nGradient alignment-based Test-time adaptation (GraTa) method to improve both\nthe gradient direction and learning rate in the optimization procedure. Unlike\nconventional TTA methods, which primarily optimize the pseudo gradient derived\nfrom a self-supervised objective, our method incorporates an auxiliary gradient\nwith the pseudo one to facilitate gradient alignment. Such gradient alignment\nenables the model to excavate the similarities between different gradients and\ncorrect the gradient direction to approximate the empirical gradient related to\nthe current segmentation task. Additionally, we design a dynamic learning rate\nbased on the cosine similarity between the pseudo and auxiliary gradients,\nthereby empowering the adaptive fine-tuning of pre-trained models on diverse\ntest data. Extensive experiments establish the effectiveness of the proposed\ngradient alignment and dynamic learning rate and substantiate the superiority\nof our GraTa method over other state-of-the-art TTA methods on a benchmark\nmedical image segmentation task. The code and weights of pre-trained source\nmodels are available at https://github.com/Chen-Ziyang/GraTa."
                },
                "authors": [
                    {
                        "name": "Ziyang Chen"
                    },
                    {
                        "name": "Yiwen Ye"
                    },
                    {
                        "name": "Yongsheng Pan"
                    },
                    {
                        "name": "Yong Xia"
                    }
                ],
                "author_detail": {
                    "name": "Yong Xia"
                },
                "author": "Yong Xia",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07343v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07343v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19719v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19719v1",
                "updated": "2024-12-27T16:18:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    16,
                    18,
                    35,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T16:18:35Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    16,
                    18,
                    35,
                    4,
                    362,
                    0
                ],
                "title": "Trading Off Energy Storage and Payload -- An Analytical Model for\n  Freight Train Configuration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trading Off Energy Storage and Payload -- An Analytical Model for\n  Freight Train Configuration"
                },
                "summary": "To support planning of alternative fuel technology (e.g., battery-electric\nlocomotives) deployment for decarbonizing non-electrified freight rail, we\ndevelop a convex optimization formulation with a closed-form solution to\ndetermine the optimal number of energy storage tender cars in a train. The\nformulation shares a similar structure to an Economic Order Quantity (EOQ)\nmodel. For given market characteristics, cost forecasts, and technology\nparameters, our model captures the trade-offs between inventory carrying costs\nassociated with trip times (including delays due to charging/refueling) and\nordering costs associated with train dispatch and operation (energy, amortized\nequipment, and labor costs). To illustrate the framework, we find the optimal\nnumber of battery-electric energy tender cars in 22,501 freight markets\n(origin-destination pairs and commodities) for U.S. Class I railroads. The\nresults display heterogeneity in optimal configurations with lighter, yet more\ntime-sensitive shipments (e.g., intermodal) utilizing more battery tender cars.\nFor heavier commodities (e.g., coal) with lower holding costs, single battery\ntender car configurations are generally optimal. The results also show that the\noptimal train configurations are sensitive to delays associated with recharging\nor swapping tender cars.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To support planning of alternative fuel technology (e.g., battery-electric\nlocomotives) deployment for decarbonizing non-electrified freight rail, we\ndevelop a convex optimization formulation with a closed-form solution to\ndetermine the optimal number of energy storage tender cars in a train. The\nformulation shares a similar structure to an Economic Order Quantity (EOQ)\nmodel. For given market characteristics, cost forecasts, and technology\nparameters, our model captures the trade-offs between inventory carrying costs\nassociated with trip times (including delays due to charging/refueling) and\nordering costs associated with train dispatch and operation (energy, amortized\nequipment, and labor costs). To illustrate the framework, we find the optimal\nnumber of battery-electric energy tender cars in 22,501 freight markets\n(origin-destination pairs and commodities) for U.S. Class I railroads. The\nresults display heterogeneity in optimal configurations with lighter, yet more\ntime-sensitive shipments (e.g., intermodal) utilizing more battery tender cars.\nFor heavier commodities (e.g., coal) with lower holding costs, single battery\ntender car configurations are generally optimal. The results also show that the\noptimal train configurations are sensitive to delays associated with recharging\nor swapping tender cars."
                },
                "authors": [
                    {
                        "name": "Max T. M. Ng"
                    },
                    {
                        "name": "Adrian Hernandez"
                    },
                    {
                        "name": "Pablo L. Durango-Cohen"
                    },
                    {
                        "name": "Hani S. Mahmassani"
                    }
                ],
                "author_detail": {
                    "name": "Hani S. Mahmassani"
                },
                "author": "Hani S. Mahmassani",
                "arxiv_doi": "10.1016/j.tre.2024.103601",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.tre.2024.103601",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.19719v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19719v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "42 pages, 19 figures. This is the accepted version of a work that was\n  published in Transportation Research Part E: Logistics and Transportation\n  Review",
                "arxiv_journal_ref": "Transportation Research Part E: Logistics and Transportation\n  Review Volume 187, July 2024, 103601",
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19707v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19707v1",
                "updated": "2024-12-27T16:02:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    16,
                    2,
                    34,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T16:02:34Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    16,
                    2,
                    34,
                    4,
                    362,
                    0
                ],
                "title": "Toward Adaptive Reasoning in Large Language Models with Thought Rollback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Adaptive Reasoning in Large Language Models with Thought Rollback"
                },
                "summary": "Large language models (LLMs) have been routinely used to solve various tasks\nusing step-by-step reasoning. However, the structure of intermediate reasoning\nsteps, or thoughts, is rigid and unidirectional, such as chains, trees, or\nacyclic-directed graphs. Consequently, the resulting inflexible and\nforward-only reasoning may not address challenging tasks and fail when the LLM\nfrequently gives false responses, i.e., ``hallucinations''. This paper proposes\na new reasoning framework, called Thought Rollback (TR), allowing LLMs to\nadaptively build thought structure while maintaining effective reasoning toward\nproblem-solving under ``hallucinations''. The core mechanism of TR is rolling\nback thoughts, which allows LLMs to perform error analysis on thoughts, and\nthus roll back to any previously mistaken thought for revision. Subsequently,\nby including such trial-and-error in the prompt to guide the LLM, each rollback\nleads to one more reliable reasoning path. Therefore, starting with a simple\nprompt without human annotations, LLM with TR adaptively and gradually explores\nthoughts for a correct solution. Comprehensive experiments on mathematical\nproblems and multi-task reasoning demonstrate the state-of-the-art performance\nof TR in terms of problem-solving rate and interaction cost. For instance, the\nsolving rate of GPT-4 with TR outperforms the current best by $9\\%$ on the MATH\ndataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been routinely used to solve various tasks\nusing step-by-step reasoning. However, the structure of intermediate reasoning\nsteps, or thoughts, is rigid and unidirectional, such as chains, trees, or\nacyclic-directed graphs. Consequently, the resulting inflexible and\nforward-only reasoning may not address challenging tasks and fail when the LLM\nfrequently gives false responses, i.e., ``hallucinations''. This paper proposes\na new reasoning framework, called Thought Rollback (TR), allowing LLMs to\nadaptively build thought structure while maintaining effective reasoning toward\nproblem-solving under ``hallucinations''. The core mechanism of TR is rolling\nback thoughts, which allows LLMs to perform error analysis on thoughts, and\nthus roll back to any previously mistaken thought for revision. Subsequently,\nby including such trial-and-error in the prompt to guide the LLM, each rollback\nleads to one more reliable reasoning path. Therefore, starting with a simple\nprompt without human annotations, LLM with TR adaptively and gradually explores\nthoughts for a correct solution. Comprehensive experiments on mathematical\nproblems and multi-task reasoning demonstrate the state-of-the-art performance\nof TR in terms of problem-solving rate and interaction cost. For instance, the\nsolving rate of GPT-4 with TR outperforms the current best by $9\\%$ on the MATH\ndataset."
                },
                "authors": [
                    {
                        "name": "Sijia Chen"
                    },
                    {
                        "name": "Baochun Li"
                    }
                ],
                "author_detail": {
                    "name": "Baochun Li"
                },
                "author": "Baochun Li",
                "arxiv_comment": "ICML 2024 camera-ready version with 24 pages and 12 figures. Code\n  repo with all prompts:\n  https://github.com/iQua/llmpebase/tree/main/examples/ThoughtRollback",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19707v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19707v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19689v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19689v1",
                "updated": "2024-12-27T15:33:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    15,
                    33,
                    14,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T15:33:14Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    15,
                    33,
                    14,
                    4,
                    362,
                    0
                ],
                "title": "Electric Vehicle Charging Network Design under Congestion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electric Vehicle Charging Network Design under Congestion"
                },
                "summary": "This paper presents an extension of a recently introduced multistage\nstochastic integer model designed for optimizing the deployment of charging\nstations under uncertainty. A key contribution of this work is incorporating\nadditional constraints accounting for congestion management at charging\nstations. The solution approach combines a greedy heuristic with a\nbranch-and-price algorithm, enabling the efficient handling of medium\ninstances. In the branch-and-price algorithm, when the solution to the\nrestricted master problem is not integer, a greedy heuristic and a local search\nprocedure are conducted to obtain feasible solutions. Computational experiments\nillustrate the effectiveness of the proposed framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an extension of a recently introduced multistage\nstochastic integer model designed for optimizing the deployment of charging\nstations under uncertainty. A key contribution of this work is incorporating\nadditional constraints accounting for congestion management at charging\nstations. The solution approach combines a greedy heuristic with a\nbranch-and-price algorithm, enabling the efficient handling of medium\ninstances. In the branch-and-price algorithm, when the solution to the\nrestricted master problem is not integer, a greedy heuristic and a local search\nprocedure are conducted to obtain feasible solutions. Computational experiments\nillustrate the effectiveness of the proposed framework."
                },
                "authors": [
                    {
                        "name": "Antoine Deza"
                    },
                    {
                        "name": "Kai Huang"
                    },
                    {
                        "name": "Carlos Aníbal Suárez"
                    }
                ],
                "author_detail": {
                    "name": "Carlos Aníbal Suárez"
                },
                "author": "Carlos Aníbal Suárez",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19689v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19689v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15473v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15473v2",
                "updated": "2024-12-27T14:56:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    14,
                    56,
                    10,
                    4,
                    362,
                    0
                ],
                "published": "2024-06-15T17:40:49Z",
                "published_parsed": [
                    2024,
                    6,
                    15,
                    17,
                    40,
                    49,
                    5,
                    167,
                    0
                ],
                "title": "Intertwining CP and NLP: The Generation of Unreasonably Constrained\n  Sentences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intertwining CP and NLP: The Generation of Unreasonably Constrained\n  Sentences"
                },
                "summary": "Constrained text generation remains a challenging task, particularly when\ndealing with hard constraints. Traditional NLP approaches prioritize generating\nmeaningful and coherent output. Also, the current state-of-the-art methods\noften lack the expressiveness and constraint satisfaction capabilities to\nhandle such tasks effectively. Recently, an approach for generating constrained\nsentences in CP has been proposed in (Bonlarron et al, 2023). This ad-hoc model\nto solve the sentences generation problem under MNREAD rules proved\nneithertheless to be computationaly and structuraly unsuitable to deal with\nother more constrained problems. In this paper, a novel more generic approach\nis introduced to tackle many of these previously untractable problems, and\nillustrated here with the quite untractable sentences generation problem\nfollowing RADNER rules.\n  More precisely, this paper presents the CPTextGen Framework. This framework\nconsiders a constrained text generation problem as a discrete combinatorial\noptimization problem. It is solved by a constraint programming method that\ncombines linguistic properties (e.g., n-grams or language level) with other\nmore classical constraints (e.g., the number of characters, syllables).\nEventually, a curation phase allows for selecting the best-generated sentences\naccording to perplexity using an LLM.\n  The effectiveness of this approach is demonstrated by tackling a new, more\ntediously constrained text generation problem: the iconic RADNER sentences\nproblem. This problem aims to generate sentences respecting a set of quite\nstrict rules defined by their use in vision and clinical research. Thanks to\nour CP-based approach, many new strongly constrained sentences have been\nsuccessfully generated. This highlights our approach's potential to handle\nunreasonably constrained text generation scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constrained text generation remains a challenging task, particularly when\ndealing with hard constraints. Traditional NLP approaches prioritize generating\nmeaningful and coherent output. Also, the current state-of-the-art methods\noften lack the expressiveness and constraint satisfaction capabilities to\nhandle such tasks effectively. Recently, an approach for generating constrained\nsentences in CP has been proposed in (Bonlarron et al, 2023). This ad-hoc model\nto solve the sentences generation problem under MNREAD rules proved\nneithertheless to be computationaly and structuraly unsuitable to deal with\nother more constrained problems. In this paper, a novel more generic approach\nis introduced to tackle many of these previously untractable problems, and\nillustrated here with the quite untractable sentences generation problem\nfollowing RADNER rules.\n  More precisely, this paper presents the CPTextGen Framework. This framework\nconsiders a constrained text generation problem as a discrete combinatorial\noptimization problem. It is solved by a constraint programming method that\ncombines linguistic properties (e.g., n-grams or language level) with other\nmore classical constraints (e.g., the number of characters, syllables).\nEventually, a curation phase allows for selecting the best-generated sentences\naccording to perplexity using an LLM.\n  The effectiveness of this approach is demonstrated by tackling a new, more\ntediously constrained text generation problem: the iconic RADNER sentences\nproblem. This problem aims to generate sentences respecting a set of quite\nstrict rules defined by their use in vision and clinical research. Thanks to\nour CP-based approach, many new strongly constrained sentences have been\nsuccessfully generated. This highlights our approach's potential to handle\nunreasonably constrained text generation scenarios."
                },
                "authors": [
                    {
                        "name": "Alexandre Bonlarron"
                    },
                    {
                        "name": "Jean-Charles Régin"
                    }
                ],
                "author_detail": {
                    "name": "Jean-Charles Régin"
                },
                "author": "Jean-Charles Régin",
                "arxiv_doi": "10.24963/ijcai.2024/841",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.24963/ijcai.2024/841",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.15473v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15473v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Disambiguation and additional references",
                "arxiv_journal_ref": "Proceedings of the Thirty-Third International Joint Conference on\n  Artificial Intelligence AI, Arts & Creativity. Pages 7600-7608. Year 2024",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.13362v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.13362v3",
                "updated": "2024-12-27T14:44:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    14,
                    44,
                    30,
                    4,
                    362,
                    0
                ],
                "published": "2024-05-22T05:43:15Z",
                "published_parsed": [
                    2024,
                    5,
                    22,
                    5,
                    43,
                    15,
                    2,
                    143,
                    0
                ],
                "title": "Lusifer: LLM-based User SImulated Feedback Environment for online\n  Recommender systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lusifer: LLM-based User SImulated Feedback Environment for online\n  Recommender systems"
                },
                "summary": "Training reinforcement learning-based recommender systems is often hindered\nby the lack of dynamic and realistic user interactions. To address this\nlimitation, we introduce Lusifer, a novel environment leveraging Large Language\nModels (LLMs) to generate simulated user feedback. Lusifer synthesizes user\nprofiles and interaction histories to simulate responses and behaviors toward\nrecommended items, with profiles updated after each rating to reflect evolving\nuser characteristics. Utilizing the MovieLens dataset as a proof of concept, we\nlimited our implementation to the last 40 interactions for each user,\nrepresenting approximately 39% and 22% of the training sets, to focus on recent\nuser behavior. For consistency and to gain insights into the performance of\ntraditional methods with limited data, we implemented baseline approaches using\nthe same data subset. Our results demonstrate that Lusifer accurately emulates\nuser behavior and preferences, even with reduced training data having an RMSE\nof 1.3 across various test sets. This paper presents Lusifer's operational\npipeline, including prompt generation and iterative user profile updates, and\ncompares its performance against baseline methods. The findings validate\nLusifer's ability to produce realistic dynamic feedback and suggest that it\noffers a scalable and adjustable framework for user simulation in online\nreinforcement learning recommender systems for future studies, particularly\nwhen training data is limited.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training reinforcement learning-based recommender systems is often hindered\nby the lack of dynamic and realistic user interactions. To address this\nlimitation, we introduce Lusifer, a novel environment leveraging Large Language\nModels (LLMs) to generate simulated user feedback. Lusifer synthesizes user\nprofiles and interaction histories to simulate responses and behaviors toward\nrecommended items, with profiles updated after each rating to reflect evolving\nuser characteristics. Utilizing the MovieLens dataset as a proof of concept, we\nlimited our implementation to the last 40 interactions for each user,\nrepresenting approximately 39% and 22% of the training sets, to focus on recent\nuser behavior. For consistency and to gain insights into the performance of\ntraditional methods with limited data, we implemented baseline approaches using\nthe same data subset. Our results demonstrate that Lusifer accurately emulates\nuser behavior and preferences, even with reduced training data having an RMSE\nof 1.3 across various test sets. This paper presents Lusifer's operational\npipeline, including prompt generation and iterative user profile updates, and\ncompares its performance against baseline methods. The findings validate\nLusifer's ability to produce realistic dynamic feedback and suggest that it\noffers a scalable and adjustable framework for user simulation in online\nreinforcement learning recommender systems for future studies, particularly\nwhen training data is limited."
                },
                "authors": [
                    {
                        "name": "Danial Ebrat"
                    },
                    {
                        "name": "Eli Paradalis"
                    },
                    {
                        "name": "Luis Rueda"
                    }
                ],
                "author_detail": {
                    "name": "Luis Rueda"
                },
                "author": "Luis Rueda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.13362v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.13362v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19669v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19669v1",
                "updated": "2024-12-27T14:31:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    14,
                    31,
                    52,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T14:31:52Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    14,
                    31,
                    52,
                    4,
                    362,
                    0
                ],
                "title": "Toward Scalable Multirobot Control: Fast Policy Learning in Distributed\n  MPC",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Scalable Multirobot Control: Fast Policy Learning in Distributed\n  MPC"
                },
                "summary": "Distributed model predictive control (DMPC) is promising in achieving optimal\ncooperative control in multirobot systems (MRS). However, real-time DMPC\nimplementation relies on numerical optimization tools to periodically calculate\nlocal control sequences online. This process is computationally demanding and\nlacks scalability for large-scale, nonlinear MRS. This article proposes a novel\ndistributed learning-based predictive control (DLPC) framework for scalable\nmultirobot control. Unlike conventional DMPC methods that calculate open-loop\ncontrol sequences, our approach centers around a computationally fast and\nefficient distributed policy learning algorithm that generates explicit\nclosed-loop DMPC policies for MRS without using numerical solvers. The policy\nlearning is executed incrementally and forward in time in each prediction\ninterval through an online distributed actor-critic implementation. The control\npolicies are successively updated in a receding-horizon manner, enabling fast\nand efficient policy learning with the closed-loop stability guarantee. The\nlearned control policies could be deployed online to MRS with varying robot\nscales, enhancing scalability and transferability for large-scale MRS.\nFurthermore, we extend our methodology to address the multirobot safe learning\nchallenge through a force field-inspired policy learning approach. We validate\nour approach's effectiveness, scalability, and efficiency through extensive\nexperiments on cooperative tasks of large-scale wheeled robots and multirotor\ndrones. Our results demonstrate the rapid learning and deployment of DMPC\npolicies for MRS with scales up to 10,000 units.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed model predictive control (DMPC) is promising in achieving optimal\ncooperative control in multirobot systems (MRS). However, real-time DMPC\nimplementation relies on numerical optimization tools to periodically calculate\nlocal control sequences online. This process is computationally demanding and\nlacks scalability for large-scale, nonlinear MRS. This article proposes a novel\ndistributed learning-based predictive control (DLPC) framework for scalable\nmultirobot control. Unlike conventional DMPC methods that calculate open-loop\ncontrol sequences, our approach centers around a computationally fast and\nefficient distributed policy learning algorithm that generates explicit\nclosed-loop DMPC policies for MRS without using numerical solvers. The policy\nlearning is executed incrementally and forward in time in each prediction\ninterval through an online distributed actor-critic implementation. The control\npolicies are successively updated in a receding-horizon manner, enabling fast\nand efficient policy learning with the closed-loop stability guarantee. The\nlearned control policies could be deployed online to MRS with varying robot\nscales, enhancing scalability and transferability for large-scale MRS.\nFurthermore, we extend our methodology to address the multirobot safe learning\nchallenge through a force field-inspired policy learning approach. We validate\nour approach's effectiveness, scalability, and efficiency through extensive\nexperiments on cooperative tasks of large-scale wheeled robots and multirotor\ndrones. Our results demonstrate the rapid learning and deployment of DMPC\npolicies for MRS with scales up to 10,000 units."
                },
                "authors": [
                    {
                        "name": "Xinglong Zhang"
                    },
                    {
                        "name": "Wei Pan"
                    },
                    {
                        "name": "Cong Li"
                    },
                    {
                        "name": "Xin Xu"
                    },
                    {
                        "name": "Xiangke Wang"
                    },
                    {
                        "name": "Ronghua Zhang"
                    },
                    {
                        "name": "Dewen Hu"
                    }
                ],
                "author_detail": {
                    "name": "Dewen Hu"
                },
                "author": "Dewen Hu",
                "arxiv_comment": "26 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19669v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19669v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19663v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19663v1",
                "updated": "2024-12-27T14:19:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    14,
                    19,
                    36,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T14:19:36Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    14,
                    19,
                    36,
                    4,
                    362,
                    0
                ],
                "title": "CAD-GPT: Synthesising CAD Construction Sequence with Spatial\n  Reasoning-Enhanced Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAD-GPT: Synthesising CAD Construction Sequence with Spatial\n  Reasoning-Enhanced Multimodal LLMs"
                },
                "summary": "Computer-aided design (CAD) significantly enhances the efficiency, accuracy,\nand innovation of design processes by enabling precise 2D and 3D modeling,\nextensive analysis, and optimization. Existing methods for creating CAD models\nrely on latent vectors or point clouds, which are difficult to obtain and\ncostly to store. Recent advances in Multimodal Large Language Models (MLLMs)\nhave inspired researchers to use natural language instructions and images for\nCAD model construction. However, these models still struggle with inferring\naccurate 3D spatial location and orientation, leading to inaccuracies in\ndetermining the spatial 3D starting points and extrusion directions for\nconstructing geometries. This work introduces CAD-GPT, a CAD synthesis method\nwith spatial reasoning-enhanced MLLM that takes either a single image or a\ntextual description as input. To achieve precise spatial inference, our\napproach introduces a 3D Modeling Spatial Mechanism. This method maps 3D\nspatial positions and 3D sketch plane rotation angles into a 1D linguistic\nfeature space using a specialized spatial unfolding mechanism, while\ndiscretizing 2D sketch coordinates into an appropriate planar space to enable\nprecise determination of spatial starting position, sketch orientation, and 2D\nsketch coordinate translations. Extensive experiments demonstrate that CAD-GPT\nconsistently outperforms existing state-of-the-art methods in CAD model\nsynthesis, both quantitatively and qualitatively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computer-aided design (CAD) significantly enhances the efficiency, accuracy,\nand innovation of design processes by enabling precise 2D and 3D modeling,\nextensive analysis, and optimization. Existing methods for creating CAD models\nrely on latent vectors or point clouds, which are difficult to obtain and\ncostly to store. Recent advances in Multimodal Large Language Models (MLLMs)\nhave inspired researchers to use natural language instructions and images for\nCAD model construction. However, these models still struggle with inferring\naccurate 3D spatial location and orientation, leading to inaccuracies in\ndetermining the spatial 3D starting points and extrusion directions for\nconstructing geometries. This work introduces CAD-GPT, a CAD synthesis method\nwith spatial reasoning-enhanced MLLM that takes either a single image or a\ntextual description as input. To achieve precise spatial inference, our\napproach introduces a 3D Modeling Spatial Mechanism. This method maps 3D\nspatial positions and 3D sketch plane rotation angles into a 1D linguistic\nfeature space using a specialized spatial unfolding mechanism, while\ndiscretizing 2D sketch coordinates into an appropriate planar space to enable\nprecise determination of spatial starting position, sketch orientation, and 2D\nsketch coordinate translations. Extensive experiments demonstrate that CAD-GPT\nconsistently outperforms existing state-of-the-art methods in CAD model\nsynthesis, both quantitatively and qualitatively."
                },
                "authors": [
                    {
                        "name": "Siyu Wang"
                    },
                    {
                        "name": "Cailian Chen"
                    },
                    {
                        "name": "Xinyi Le"
                    },
                    {
                        "name": "Qimin Xu"
                    },
                    {
                        "name": "Lei Xu"
                    },
                    {
                        "name": "Yanzhou Zhang"
                    },
                    {
                        "name": "Jie Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Yang"
                },
                "author": "Jie Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19663v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19663v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11843v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11843v3",
                "updated": "2024-12-27T14:17:05Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    14,
                    17,
                    5,
                    4,
                    362,
                    0
                ],
                "published": "2024-07-16T15:24:44Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    15,
                    24,
                    44,
                    1,
                    198,
                    0
                ],
                "title": "Preemptive Detection and Correction of Misaligned Actions in LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preemptive Detection and Correction of Misaligned Actions in LLM Agents"
                },
                "summary": "Deploying LLM-based agents in real-life applications often faces a critical\nchallenge: the misalignment between agents' behavior and user intent. Such\nmisalignment may lead agents to unintentionally execute critical actions that\ncarry negative outcomes (e.g., accidentally triggering a \"buy-now\" in web\nshopping), resulting in undesirable or even irreversible consequences. Although\naddressing these issues is crucial, the preemptive detection and correction of\nmisaligned actions remains relatively underexplored. To fill this gap, we\nintroduce InferAct, a novel approach that leverages the belief reasoning\nability of LLMs, grounded in Theory-of-Mind, to detect misaligned actions\nbefore execution. Once the misalignment is detected, InferAct alerts users for\ntimely correction, preventing adverse outcomes and enhancing the reliability of\nLLM agents' decision-making processes. Experiments on three widely used tasks\ndemonstrate that InferAct achieves up to 20% improvements on Marco-F1 against\nbaselines in misaligned action detection. An in-depth evaluation of\nmisalignment correction further highlights InferAct's effectiveness in\nimproving agent alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying LLM-based agents in real-life applications often faces a critical\nchallenge: the misalignment between agents' behavior and user intent. Such\nmisalignment may lead agents to unintentionally execute critical actions that\ncarry negative outcomes (e.g., accidentally triggering a \"buy-now\" in web\nshopping), resulting in undesirable or even irreversible consequences. Although\naddressing these issues is crucial, the preemptive detection and correction of\nmisaligned actions remains relatively underexplored. To fill this gap, we\nintroduce InferAct, a novel approach that leverages the belief reasoning\nability of LLMs, grounded in Theory-of-Mind, to detect misaligned actions\nbefore execution. Once the misalignment is detected, InferAct alerts users for\ntimely correction, preventing adverse outcomes and enhancing the reliability of\nLLM agents' decision-making processes. Experiments on three widely used tasks\ndemonstrate that InferAct achieves up to 20% improvements on Marco-F1 against\nbaselines in misaligned action detection. An in-depth evaluation of\nmisalignment correction further highlights InferAct's effectiveness in\nimproving agent alignment."
                },
                "authors": [
                    {
                        "name": "Haishuo Fang"
                    },
                    {
                        "name": "Xiaodan Zhu"
                    },
                    {
                        "name": "Iryna Gurevych"
                    }
                ],
                "author_detail": {
                    "name": "Iryna Gurevych"
                },
                "author": "Iryna Gurevych",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11843v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11843v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19652v1",
                "updated": "2024-12-27T13:56:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    13,
                    56,
                    51,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T13:56:51Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    13,
                    56,
                    51,
                    4,
                    362,
                    0
                ],
                "title": "FreStega: A Plug-and-Play Method for Boosting Imperceptibility and\n  Capacity in Generative Linguistic Steganography for Real-World Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreStega: A Plug-and-Play Method for Boosting Imperceptibility and\n  Capacity in Generative Linguistic Steganography for Real-World Scenarios"
                },
                "summary": "Linguistic steganography embeds secret information in seemingly innocent\ntexts, safeguarding privacy in surveillance environments. Generative linguistic\nsteganography leverages the probability distribution of language models (LMs)\nand applies steganographic algorithms to generate stego tokens, gaining\nattention with recent Large Language Model (LLM) advancements. To enhance\nsecurity, researchers develop distribution-preserving stego algorithms to\nminimize the gap between stego sampling and LM sampling. However, the reliance\non language model distributions, coupled with deviations from real-world cover\ntexts, results in insufficient imperceptibility when facing steganalysis\ndetectors in real-world scenarios. Moreover, LLM distributions tend to be more\ndeterministic, resulting in reduced entropy and, consequently, lower embedding\ncapacity. In this paper, we propose FreStega, a plug-and-play method to\nreconstruct the distribution of language models used for generative linguistic\nsteganography. FreStega dynamically adjusts token probabilities from the\nlanguage model at each step of stegotext auto-regressive generation, leveraging\nboth sequential and spatial dimensions. In sequential adjustment, the\ntemperature is dynamically adjusted based on instantaneous entropy, enhancing\nthe diversity of stego texts and boosting embedding capacity. In the spatial\ndimension, the distribution is aligned with guidance from the target domain\ncorpus, closely mimicking real cover text in the target domain. By reforming\nthe distribution, FreStega enhances the imperceptibility of stego text in\npractical scenarios and improves steganographic capacity by 15.41\\%, all\nwithout compromising the quality of the generated text. FreStega serves as a\nplug-and-play remedy to enhance the imperceptibility and embedding capacity of\nexisting distribution-preserving steganography methods in real-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linguistic steganography embeds secret information in seemingly innocent\ntexts, safeguarding privacy in surveillance environments. Generative linguistic\nsteganography leverages the probability distribution of language models (LMs)\nand applies steganographic algorithms to generate stego tokens, gaining\nattention with recent Large Language Model (LLM) advancements. To enhance\nsecurity, researchers develop distribution-preserving stego algorithms to\nminimize the gap between stego sampling and LM sampling. However, the reliance\non language model distributions, coupled with deviations from real-world cover\ntexts, results in insufficient imperceptibility when facing steganalysis\ndetectors in real-world scenarios. Moreover, LLM distributions tend to be more\ndeterministic, resulting in reduced entropy and, consequently, lower embedding\ncapacity. In this paper, we propose FreStega, a plug-and-play method to\nreconstruct the distribution of language models used for generative linguistic\nsteganography. FreStega dynamically adjusts token probabilities from the\nlanguage model at each step of stegotext auto-regressive generation, leveraging\nboth sequential and spatial dimensions. In sequential adjustment, the\ntemperature is dynamically adjusted based on instantaneous entropy, enhancing\nthe diversity of stego texts and boosting embedding capacity. In the spatial\ndimension, the distribution is aligned with guidance from the target domain\ncorpus, closely mimicking real cover text in the target domain. By reforming\nthe distribution, FreStega enhances the imperceptibility of stego text in\npractical scenarios and improves steganographic capacity by 15.41\\%, all\nwithout compromising the quality of the generated text. FreStega serves as a\nplug-and-play remedy to enhance the imperceptibility and embedding capacity of\nexisting distribution-preserving steganography methods in real-world scenarios."
                },
                "authors": [
                    {
                        "name": "Kaiyi Pang"
                    }
                ],
                "author_detail": {
                    "name": "Kaiyi Pang"
                },
                "author": "Kaiyi Pang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12468v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12468v2",
                "updated": "2024-12-27T13:52:05Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    13,
                    52,
                    5,
                    4,
                    362,
                    0
                ],
                "published": "2024-10-16T11:33:57Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    11,
                    33,
                    57,
                    2,
                    290,
                    0
                ],
                "title": "Evaluating Software Development Agents: Patch Patterns, Code Quality,\n  and Issue Complexity in Real-World GitHub Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Software Development Agents: Patch Patterns, Code Quality,\n  and Issue Complexity in Real-World GitHub Scenarios"
                },
                "summary": "In recent years, AI-based software engineering has progressed from\npre-trained models to advanced agentic workflows, with Software Development\nAgents representing the next major leap. These agents, capable of reasoning,\nplanning, and interacting with external environments, offer promising solutions\nto complex software engineering tasks. However, while much research has\nevaluated code generated by large language models (LLMs), comprehensive studies\non agent-generated patches, particularly in real-world settings, are lacking.\nThis study addresses that gap by evaluating 4,892 patches from 10 top-ranked\nagents on 500 real-world GitHub issues from SWE-Bench Verified, focusing on\ntheir impact on code quality. Our analysis shows no single agent dominated,\nwith 170 issues unresolved, indicating room for improvement. Even for patches\nthat passed unit tests and resolved issues, agents made different file and\nfunction modifications compared to the gold patches from repository developers,\nrevealing limitations in the benchmark's test case coverage. Most agents\nmaintained code reliability and security, avoiding new bugs or vulnerabilities;\nwhile some agents increased code complexity, many reduced code duplication and\nminimized code smells. Finally, agents performed better on simpler codebases,\nsuggesting that breaking complex tasks into smaller sub-tasks could improve\neffectiveness. This study provides the first comprehensive evaluation of\nagent-generated patches on real-world GitHub issues, offering insights to\nadvance AI-driven software development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, AI-based software engineering has progressed from\npre-trained models to advanced agentic workflows, with Software Development\nAgents representing the next major leap. These agents, capable of reasoning,\nplanning, and interacting with external environments, offer promising solutions\nto complex software engineering tasks. However, while much research has\nevaluated code generated by large language models (LLMs), comprehensive studies\non agent-generated patches, particularly in real-world settings, are lacking.\nThis study addresses that gap by evaluating 4,892 patches from 10 top-ranked\nagents on 500 real-world GitHub issues from SWE-Bench Verified, focusing on\ntheir impact on code quality. Our analysis shows no single agent dominated,\nwith 170 issues unresolved, indicating room for improvement. Even for patches\nthat passed unit tests and resolved issues, agents made different file and\nfunction modifications compared to the gold patches from repository developers,\nrevealing limitations in the benchmark's test case coverage. Most agents\nmaintained code reliability and security, avoiding new bugs or vulnerabilities;\nwhile some agents increased code complexity, many reduced code duplication and\nminimized code smells. Finally, agents performed better on simpler codebases,\nsuggesting that breaking complex tasks into smaller sub-tasks could improve\neffectiveness. This study provides the first comprehensive evaluation of\nagent-generated patches on real-world GitHub issues, offering insights to\nadvance AI-driven software development."
                },
                "authors": [
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Lingxiao Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Lingxiao Jiang"
                },
                "author": "Lingxiao Jiang",
                "arxiv_comment": "Paper accepted to the SANER 2025 Conference Research Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12468v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12468v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02572v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02572v4",
                "updated": "2024-12-27T13:29:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    13,
                    29,
                    14,
                    4,
                    362,
                    0
                ],
                "published": "2024-09-04T09:46:33Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    9,
                    46,
                    33,
                    2,
                    248,
                    0
                ],
                "title": "GenDFIR: Advancing Cyber Incident Timeline Analysis Through Retrieval\n  Augmented Generation and Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenDFIR: Advancing Cyber Incident Timeline Analysis Through Retrieval\n  Augmented Generation and Large Language Models"
                },
                "summary": "Cyber timeline analysis, or forensic timeline analysis, is crucial in Digital\nForensics and Incident Response (DFIR). It examines artefacts and events\nparticularly timestamps and metadata to detect anomalies, establish\ncorrelations, and reconstruct incident timelines. Traditional methods rely on\nstructured artefacts, such as logs and filesystem metadata, using specialised\ntools for evidence identification and feature extraction. This paper introduces\nGenDFIR, a framework leveraging large language models (LLMs), specifically\nLlama 3.1 8B in zero shot mode, integrated with a Retrieval-Augmented\nGeneration (RAG) agent. Incident data is preprocessed into a structured\nknowledge base, enabling the RAG agent to retrieve relevant events based on\nuser prompts. The LLM interprets this context, offering semantic enrichment.\nTested on synthetic data in a controlled environment, results demonstrate\nGenDFIR's reliability and robustness, showcasing LLMs potential to automate\ntimeline analysis and advance threat detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cyber timeline analysis, or forensic timeline analysis, is crucial in Digital\nForensics and Incident Response (DFIR). It examines artefacts and events\nparticularly timestamps and metadata to detect anomalies, establish\ncorrelations, and reconstruct incident timelines. Traditional methods rely on\nstructured artefacts, such as logs and filesystem metadata, using specialised\ntools for evidence identification and feature extraction. This paper introduces\nGenDFIR, a framework leveraging large language models (LLMs), specifically\nLlama 3.1 8B in zero shot mode, integrated with a Retrieval-Augmented\nGeneration (RAG) agent. Incident data is preprocessed into a structured\nknowledge base, enabling the RAG agent to retrieve relevant events based on\nuser prompts. The LLM interprets this context, offering semantic enrichment.\nTested on synthetic data in a controlled environment, results demonstrate\nGenDFIR's reliability and robustness, showcasing LLMs potential to automate\ntimeline analysis and advance threat detection."
                },
                "authors": [
                    {
                        "name": "Fatma Yasmine Loumachi"
                    },
                    {
                        "name": "Mohamed Chahine Ghanem"
                    },
                    {
                        "name": "Mohamed Amine Ferrag"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Amine Ferrag"
                },
                "author": "Mohamed Amine Ferrag",
                "arxiv_comment": "24 pages V5.3",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02572v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02572v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19630v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19630v1",
                "updated": "2024-12-27T13:19:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    13,
                    19,
                    35,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T13:19:35Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    13,
                    19,
                    35,
                    4,
                    362,
                    0
                ],
                "title": "IMTP: Search-based Code Generation for In-memory Tensor Programs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IMTP: Search-based Code Generation for In-memory Tensor Programs"
                },
                "summary": "Processing-in-DRAM (DRAM-PIM) has emerged as a promising technology for\naccelerating memory-intensive operations in modern applications, such as Large\nLanguage Models (LLMs). Despite its potential, current software stacks for\nDRAM-PIM face significant challenges, including reliance on hand-tuned\nlibraries that hinder programmability, limited support for high-level\nabstractions, and the lack of systematic optimization frameworks. To address\nthese limitations, we present IMTP, a search-based optimizing tensor compiler\nfor UPMEM. Key features of IMTP include: (1) automated searches of the joint\nsearch space for host and kernel tensor programs, (2) PIM-aware optimizations\nfor efficiently handling boundary conditions, and (3) improved search\nalgorithms for the expanded search space of UPMEM systems. Our experimental\nresults on UPMEM hardware demonstrate performance gains of up to 8.21x for\nvarious UPMEM benchmark kernels and 5.33x for GPT-J layers. To the best of our\nknowledge, IMTP is the first tensor compiler to provide fully automated,\nautotuning-integrated code generation support for a DRAM-PIM system. By\nbridging the gap between high-level tensor computation abstractions and\nlow-level hardware-specific requirements, IMTP establishes a foundation for\nadvancing DRAM-PIM programmability and enabling streamlined optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing-in-DRAM (DRAM-PIM) has emerged as a promising technology for\naccelerating memory-intensive operations in modern applications, such as Large\nLanguage Models (LLMs). Despite its potential, current software stacks for\nDRAM-PIM face significant challenges, including reliance on hand-tuned\nlibraries that hinder programmability, limited support for high-level\nabstractions, and the lack of systematic optimization frameworks. To address\nthese limitations, we present IMTP, a search-based optimizing tensor compiler\nfor UPMEM. Key features of IMTP include: (1) automated searches of the joint\nsearch space for host and kernel tensor programs, (2) PIM-aware optimizations\nfor efficiently handling boundary conditions, and (3) improved search\nalgorithms for the expanded search space of UPMEM systems. Our experimental\nresults on UPMEM hardware demonstrate performance gains of up to 8.21x for\nvarious UPMEM benchmark kernels and 5.33x for GPT-J layers. To the best of our\nknowledge, IMTP is the first tensor compiler to provide fully automated,\nautotuning-integrated code generation support for a DRAM-PIM system. By\nbridging the gap between high-level tensor computation abstractions and\nlow-level hardware-specific requirements, IMTP establishes a foundation for\nadvancing DRAM-PIM programmability and enabling streamlined optimization."
                },
                "authors": [
                    {
                        "name": "Yongwon Shin"
                    },
                    {
                        "name": "Dookyung Kang"
                    },
                    {
                        "name": "Hyojin Sung"
                    }
                ],
                "author_detail": {
                    "name": "Hyojin Sung"
                },
                "author": "Hyojin Sung",
                "arxiv_comment": "13 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19630v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19630v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19622v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19622v1",
                "updated": "2024-12-27T12:49:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    12,
                    49,
                    3,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T12:49:03Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    12,
                    49,
                    3,
                    4,
                    362,
                    0
                ],
                "title": "Signatures of prediction during natural listening in MEG data?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Signatures of prediction during natural listening in MEG data?"
                },
                "summary": "The brain uses contextual information and prior knowledge to anticipate\nupcoming content during language comprehension. Recent research has shown\npredictive signals can be revealed in pre-onset ECoG activity during\nnaturalistic narrative listening, by building encoding models based on word\nembeddings from Large Language Models (LLMs). Similarly, evidence for\nlong-range predictive encoding has been observed in fMRI data, where\nincorporating embeddings for multiple upcoming words in a narrative improves\nalignment with brain activity. This study examines whether similar predictive\ninformation can be detected in MEG, a technique with higher temporal resolution\nthan fMRI but a lower signal-to-noise ratio than ECoG. Our findings indicate\nthat MEG captures pre-onset representations up to 1 second before word onset,\nconsistent with ECoG results. However, unlike fMRI findings, incorporating\nfuture word embeddings did not enhance MEG encoding, even for one word into the\nfuture, which suggests that the pre-onset encoding may not reflect predictive\nprocessing. This work demonstrates that MEG combined with LLMs is a valuable\napproach for studying language processing in naturalistic narratives and\nhighlights the need to study further what constitutes evidence for prediction\nduring natural listening.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The brain uses contextual information and prior knowledge to anticipate\nupcoming content during language comprehension. Recent research has shown\npredictive signals can be revealed in pre-onset ECoG activity during\nnaturalistic narrative listening, by building encoding models based on word\nembeddings from Large Language Models (LLMs). Similarly, evidence for\nlong-range predictive encoding has been observed in fMRI data, where\nincorporating embeddings for multiple upcoming words in a narrative improves\nalignment with brain activity. This study examines whether similar predictive\ninformation can be detected in MEG, a technique with higher temporal resolution\nthan fMRI but a lower signal-to-noise ratio than ECoG. Our findings indicate\nthat MEG captures pre-onset representations up to 1 second before word onset,\nconsistent with ECoG results. However, unlike fMRI findings, incorporating\nfuture word embeddings did not enhance MEG encoding, even for one word into the\nfuture, which suggests that the pre-onset encoding may not reflect predictive\nprocessing. This work demonstrates that MEG combined with LLMs is a valuable\napproach for studying language processing in naturalistic narratives and\nhighlights the need to study further what constitutes evidence for prediction\nduring natural listening."
                },
                "authors": [
                    {
                        "name": "Sahel Azizpour"
                    },
                    {
                        "name": "Britta U. Westner"
                    },
                    {
                        "name": "Jakub Szewczyk"
                    },
                    {
                        "name": "Umut Güçlü"
                    },
                    {
                        "name": "Linda Geerligs"
                    }
                ],
                "author_detail": {
                    "name": "Linda Geerligs"
                },
                "author": "Linda Geerligs",
                "arxiv_comment": "12 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19622v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19622v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19616v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19616v1",
                "updated": "2024-12-27T12:23:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    12,
                    23,
                    39,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T12:23:39Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    12,
                    23,
                    39,
                    4,
                    362,
                    0
                ],
                "title": "Gradient Weight-normalized Low-rank Projection for Efficient LLM\n  Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gradient Weight-normalized Low-rank Projection for Efficient LLM\n  Training"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable performance across various\ntasks, but the escalating demands on computational resources pose significant\nchallenges, particularly in the extensive utilization of full fine-tuning for\ndownstream tasks. To address this, parameter-efficient fine-tuning (PEFT)\nmethods have been developed, but they often underperform compared to full\nfine-tuning and struggle with memory efficiency. In this work, we introduce\nGradient Weight-Normalized Low-Rank Projection (GradNormLoRP), a novel approach\nthat enhances both parameter and memory efficiency while maintaining comparable\nperformance to full fine-tuning. GradNormLoRP normalizes the weight matrix to\nimprove gradient conditioning, facilitating better convergence during\noptimization. Additionally, it applies low-rank approximations to the weight\nand gradient matrices, significantly reducing memory usage during training.\nExtensive experiments demonstrate that our 8-bit GradNormLoRP reduces optimizer\nmemory usage by up to 89.5% and enables the pre-training of large LLMs, such as\nLLaMA 7B, on consumer-level GPUs like the NVIDIA RTX 4090, without additional\ninference costs. Moreover, GradNormLoRP outperforms existing low-rank methods\nin fine-tuning tasks. For instance, when fine-tuning the RoBERTa model on all\nGLUE tasks with a rank of 8, GradNormLoRP achieves an average score of 80.65,\nsurpassing LoRA's score of 79.23. These results underscore GradNormLoRP as a\npromising alternative for efficient LLM pre-training and fine-tuning. Source\ncode and Appendix:\nhttps://github.com/Jhhuangkay/Gradient-Weight-normalized-Low-rank-Projection-for-Efficient-LLM-Training",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable performance across various\ntasks, but the escalating demands on computational resources pose significant\nchallenges, particularly in the extensive utilization of full fine-tuning for\ndownstream tasks. To address this, parameter-efficient fine-tuning (PEFT)\nmethods have been developed, but they often underperform compared to full\nfine-tuning and struggle with memory efficiency. In this work, we introduce\nGradient Weight-Normalized Low-Rank Projection (GradNormLoRP), a novel approach\nthat enhances both parameter and memory efficiency while maintaining comparable\nperformance to full fine-tuning. GradNormLoRP normalizes the weight matrix to\nimprove gradient conditioning, facilitating better convergence during\noptimization. Additionally, it applies low-rank approximations to the weight\nand gradient matrices, significantly reducing memory usage during training.\nExtensive experiments demonstrate that our 8-bit GradNormLoRP reduces optimizer\nmemory usage by up to 89.5% and enables the pre-training of large LLMs, such as\nLLaMA 7B, on consumer-level GPUs like the NVIDIA RTX 4090, without additional\ninference costs. Moreover, GradNormLoRP outperforms existing low-rank methods\nin fine-tuning tasks. For instance, when fine-tuning the RoBERTa model on all\nGLUE tasks with a rank of 8, GradNormLoRP achieves an average score of 80.65,\nsurpassing LoRA's score of 79.23. These results underscore GradNormLoRP as a\npromising alternative for efficient LLM pre-training and fine-tuning. Source\ncode and Appendix:\nhttps://github.com/Jhhuangkay/Gradient-Weight-normalized-Low-rank-Projection-for-Efficient-LLM-Training"
                },
                "authors": [
                    {
                        "name": "Jia-Hong Huang"
                    },
                    {
                        "name": "Yixian Shen"
                    },
                    {
                        "name": "Hongyi Zhu"
                    },
                    {
                        "name": "Stevan Rudinac"
                    },
                    {
                        "name": "Evangelos Kanoulas"
                    }
                ],
                "author_detail": {
                    "name": "Evangelos Kanoulas"
                },
                "author": "Evangelos Kanoulas",
                "arxiv_comment": "Accepted by the 39th AAAI Conference on Artificial Intelligence\n  (AAAI-25) [Main Technical Track]",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19616v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19616v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19610v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19610v1",
                "updated": "2024-12-27T12:11:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    12,
                    11,
                    50,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T12:11:50Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    12,
                    11,
                    50,
                    4,
                    362,
                    0
                ],
                "title": "Machine Generated Product Advertisements: Benchmarking LLMs Against\n  Human Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine Generated Product Advertisements: Benchmarking LLMs Against\n  Human Performance"
                },
                "summary": "This study compares the performance of AI-generated and human-written product\ndescriptions using a multifaceted evaluation model. We analyze descriptions for\n100 products generated by four AI models (Gemma 2B, LLAMA, GPT2, and ChatGPT 4)\nwith and without sample descriptions, against human-written descriptions. Our\nevaluation metrics include sentiment, readability, persuasiveness, Search\nEngine Optimization(SEO), clarity, emotional appeal, and call-to-action\neffectiveness. The results indicate that ChatGPT 4 performs the best. In\ncontrast, other models demonstrate significant shortcomings, producing\nincoherent and illogical output that lacks logical structure and contextual\nrelevance. These models struggle to maintain focus on the product being\ndescribed, resulting in disjointed sentences that do not convey meaningful\ninformation. This research provides insights into the current capabilities and\nlimitations of AI in the creation of content for e-Commerce.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study compares the performance of AI-generated and human-written product\ndescriptions using a multifaceted evaluation model. We analyze descriptions for\n100 products generated by four AI models (Gemma 2B, LLAMA, GPT2, and ChatGPT 4)\nwith and without sample descriptions, against human-written descriptions. Our\nevaluation metrics include sentiment, readability, persuasiveness, Search\nEngine Optimization(SEO), clarity, emotional appeal, and call-to-action\neffectiveness. The results indicate that ChatGPT 4 performs the best. In\ncontrast, other models demonstrate significant shortcomings, producing\nincoherent and illogical output that lacks logical structure and contextual\nrelevance. These models struggle to maintain focus on the product being\ndescribed, resulting in disjointed sentences that do not convey meaningful\ninformation. This research provides insights into the current capabilities and\nlimitations of AI in the creation of content for e-Commerce."
                },
                "authors": [
                    {
                        "name": "Sanjukta Ghosh"
                    }
                ],
                "author_detail": {
                    "name": "Sanjukta Ghosh"
                },
                "author": "Sanjukta Ghosh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19610v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19610v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19607v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19607v1",
                "updated": "2024-12-27T12:09:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    12,
                    9,
                    28,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T12:09:28Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    12,
                    9,
                    28,
                    4,
                    362,
                    0
                ],
                "title": "Photonic classification on a single diffractive layer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Photonic classification on a single diffractive layer"
                },
                "summary": "Photonic computation started to shape the future of fast, efficient and\naccessible computation. The advantages brought by light based Diffractive Deep\nNeural Networks (D2NN), are shown to be overwhelmingly advantageous especially\nin targeting classification problems. However, cost and complexity of\nmulti-layer systems are the main challenges that reduce the deployment of this\ntechnology. In this study, we develop a simple yet extremely efficient way to\nachieve optical classification using a single diffractive optical layer. A\nspatial light modulator is used not only to emulate the classifying system but\nalso the input medium for the objects to be classified by the system. Using our\napproach, we classify road traffic signs which has a direct application on\ndaily life and safety. We perform classification of road signs under the effect\nof noise and show that we can successfully classify road signs with more than\n75% accuracy under 20% noise/imperfection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Photonic computation started to shape the future of fast, efficient and\naccessible computation. The advantages brought by light based Diffractive Deep\nNeural Networks (D2NN), are shown to be overwhelmingly advantageous especially\nin targeting classification problems. However, cost and complexity of\nmulti-layer systems are the main challenges that reduce the deployment of this\ntechnology. In this study, we develop a simple yet extremely efficient way to\nachieve optical classification using a single diffractive optical layer. A\nspatial light modulator is used not only to emulate the classifying system but\nalso the input medium for the objects to be classified by the system. Using our\napproach, we classify road traffic signs which has a direct application on\ndaily life and safety. We perform classification of road signs under the effect\nof noise and show that we can successfully classify road signs with more than\n75% accuracy under 20% noise/imperfection."
                },
                "authors": [
                    {
                        "name": "Anil J. Pekgöz"
                    },
                    {
                        "name": "Emre Yüce"
                    }
                ],
                "author_detail": {
                    "name": "Emre Yüce"
                },
                "author": "Emre Yüce",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19607v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19607v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19595v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19595v1",
                "updated": "2024-12-27T11:33:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    11,
                    33,
                    19,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T11:33:19Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    11,
                    33,
                    19,
                    4,
                    362,
                    0
                ],
                "title": "SocRATES: Towards Automated Scenario-based Testing of Social Navigation\n  Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SocRATES: Towards Automated Scenario-based Testing of Social Navigation\n  Algorithms"
                },
                "summary": "Current social navigation methods and benchmarks primarily focus on proxemics\nand task efficiency. While these factors are important, qualitative aspects\nsuch as perceptions of a robot's social competence are equally crucial for\nsuccessful adoption and integration into human environments. We propose a more\ncomprehensive evaluation of social navigation through scenario-based testing,\nwhere specific human-robot interaction scenarios can reveal key robot\nbehaviors. However, creating such scenarios is often labor-intensive and\ncomplex. In this work, we address this challenge by introducing a pipeline that\nautomates the generation of context-, and location-appropriate social\nnavigation scenarios, ready for simulation. Our pipeline transforms simple\nscenario metadata into detailed textual scenarios, infers pedestrian and robot\ntrajectories, and simulates pedestrian behaviors, which enables more controlled\nevaluation. We leverage the social reasoning and code-generation capabilities\nof Large Language Models (LLMs) to streamline scenario generation and\ntranslation. Our experiments show that our pipeline produces realistic\nscenarios and significantly improves scenario translation over naive LLM\nprompting. Additionally, we present initial feedback from a usability study\nwith social navigation experts and a case-study demonstrating a scenario-based\nevaluation of three navigation algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current social navigation methods and benchmarks primarily focus on proxemics\nand task efficiency. While these factors are important, qualitative aspects\nsuch as perceptions of a robot's social competence are equally crucial for\nsuccessful adoption and integration into human environments. We propose a more\ncomprehensive evaluation of social navigation through scenario-based testing,\nwhere specific human-robot interaction scenarios can reveal key robot\nbehaviors. However, creating such scenarios is often labor-intensive and\ncomplex. In this work, we address this challenge by introducing a pipeline that\nautomates the generation of context-, and location-appropriate social\nnavigation scenarios, ready for simulation. Our pipeline transforms simple\nscenario metadata into detailed textual scenarios, infers pedestrian and robot\ntrajectories, and simulates pedestrian behaviors, which enables more controlled\nevaluation. We leverage the social reasoning and code-generation capabilities\nof Large Language Models (LLMs) to streamline scenario generation and\ntranslation. Our experiments show that our pipeline produces realistic\nscenarios and significantly improves scenario translation over naive LLM\nprompting. Additionally, we present initial feedback from a usability study\nwith social navigation experts and a case-study demonstrating a scenario-based\nevaluation of three navigation algorithms."
                },
                "authors": [
                    {
                        "name": "Shashank Rao Marpally"
                    },
                    {
                        "name": "Pranav Goyal"
                    },
                    {
                        "name": "Harold Soh"
                    }
                ],
                "author_detail": {
                    "name": "Harold Soh"
                },
                "author": "Harold Soh",
                "arxiv_comment": "7 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19595v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19595v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19585v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19585v1",
                "updated": "2024-12-27T11:03:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    11,
                    3,
                    26,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T11:03:26Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    11,
                    3,
                    26,
                    4,
                    362,
                    0
                ],
                "title": "Ultralight Signal Classification Model for Automatic Modulation\n  Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultralight Signal Classification Model for Automatic Modulation\n  Recognition"
                },
                "summary": "The growing complexity of radar signals demands responsive and accurate\ndetection systems that can operate efficiently on resource-constrained edge\ndevices. Existing models, while effective, often rely on substantial\ncomputational resources and large datasets, making them impractical for edge\ndeployment. In this work, we propose an ultralight hybrid neural network\noptimized for edge applications, delivering robust performance across\nunfavorable signal-to-noise ratios (mean accuracy of 96.3% at 0 dB) using less\nthan 100 samples per class, and significantly reducing computational overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing complexity of radar signals demands responsive and accurate\ndetection systems that can operate efficiently on resource-constrained edge\ndevices. Existing models, while effective, often rely on substantial\ncomputational resources and large datasets, making them impractical for edge\ndeployment. In this work, we propose an ultralight hybrid neural network\noptimized for edge applications, delivering robust performance across\nunfavorable signal-to-noise ratios (mean accuracy of 96.3% at 0 dB) using less\nthan 100 samples per class, and significantly reducing computational overhead."
                },
                "authors": [
                    {
                        "name": "Alessandro Daniele Genuardi Oquendo"
                    },
                    {
                        "name": "Agustín Matías Galante Cerviño"
                    },
                    {
                        "name": "Nilotpal Sinha"
                    },
                    {
                        "name": "Luc Andrea"
                    },
                    {
                        "name": "Sam Mugel"
                    },
                    {
                        "name": "Román Orús"
                    }
                ],
                "author_detail": {
                    "name": "Román Orús"
                },
                "author": "Román Orús",
                "arxiv_comment": "8 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19585v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19585v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19582v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19582v1",
                "updated": "2024-12-27T10:57:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    10,
                    57,
                    17,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T10:57:17Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    10,
                    57,
                    17,
                    4,
                    362,
                    0
                ],
                "title": "An Actionable Hierarchical Scene Representation Enhancing Autonomous\n  Inspection Missions in Unknown Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Actionable Hierarchical Scene Representation Enhancing Autonomous\n  Inspection Missions in Unknown Environments"
                },
                "summary": "In this article, we present the Layered Semantic Graphs (LSG), a novel\nactionable hierarchical scene graph, fully integrated with a multi-modal\nmission planner, the FLIE: A First-Look based Inspection and Exploration\nplanner. The novelty of this work stems from aiming to address the task of\nmaintaining an intuitive and multi-resolution scene representation, while\nsimultaneously offering a tractable foundation for planning and scene\nunderstanding during an ongoing inspection mission of apriori unknown\ntargets-of-interest in an unknown environment. The proposed LSG scheme is\ncomposed of locally nested hierarchical graphs, at multiple layers of\nabstraction, with the abstract concepts grounded on the functionality of the\nintegrated FLIE planner. Furthermore, LSG encapsulates real-time semantic\nsegmentation models that offer extraction and localization of desired semantic\nelements within the hierarchical representation. This extends the capability of\nthe inspection planner, which can then leverage LSG to make an informed\ndecision to inspect a particular semantic of interest. We also emphasize the\nhierarchical and semantic path-planning capabilities of LSG, which can extend\ninspection missions by improving situational awareness for human operators in\nan unknown environment. The validity of the proposed scheme is proven through\nextensive evaluations of the proposed architecture in simulations, as well as\nexperimental field deployments on a Boston Dynamics Spot quadruped robot in\nurban outdoor environment settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this article, we present the Layered Semantic Graphs (LSG), a novel\nactionable hierarchical scene graph, fully integrated with a multi-modal\nmission planner, the FLIE: A First-Look based Inspection and Exploration\nplanner. The novelty of this work stems from aiming to address the task of\nmaintaining an intuitive and multi-resolution scene representation, while\nsimultaneously offering a tractable foundation for planning and scene\nunderstanding during an ongoing inspection mission of apriori unknown\ntargets-of-interest in an unknown environment. The proposed LSG scheme is\ncomposed of locally nested hierarchical graphs, at multiple layers of\nabstraction, with the abstract concepts grounded on the functionality of the\nintegrated FLIE planner. Furthermore, LSG encapsulates real-time semantic\nsegmentation models that offer extraction and localization of desired semantic\nelements within the hierarchical representation. This extends the capability of\nthe inspection planner, which can then leverage LSG to make an informed\ndecision to inspect a particular semantic of interest. We also emphasize the\nhierarchical and semantic path-planning capabilities of LSG, which can extend\ninspection missions by improving situational awareness for human operators in\nan unknown environment. The validity of the proposed scheme is proven through\nextensive evaluations of the proposed architecture in simulations, as well as\nexperimental field deployments on a Boston Dynamics Spot quadruped robot in\nurban outdoor environment settings."
                },
                "authors": [
                    {
                        "name": "Vignesh Kottayam Viswanathan"
                    },
                    {
                        "name": "Mario Alberto Valdes Saucedo"
                    },
                    {
                        "name": "Sumeet Gajanan Satpute"
                    },
                    {
                        "name": "Christoforos Kanellakis"
                    },
                    {
                        "name": "George Nikolakopoulos"
                    }
                ],
                "author_detail": {
                    "name": "George Nikolakopoulos"
                },
                "author": "George Nikolakopoulos",
                "arxiv_comment": "7 pages, 7 figures, submitted to ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19582v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19582v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19493v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19493v3",
                "updated": "2024-12-27T10:34:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    10,
                    34,
                    15,
                    4,
                    362,
                    0
                ],
                "published": "2024-07-28T13:23:43Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    13,
                    23,
                    43,
                    6,
                    210,
                    0
                ],
                "title": "Official-NV: An LLM-Generated News Video Dataset for Multimodal Fake\n  News Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Official-NV: An LLM-Generated News Video Dataset for Multimodal Fake\n  News Detection"
                },
                "summary": "News media, especially video news media, have penetrated into every aspect of\ndaily life, which also brings the risk of fake news. Therefore, multimodal fake\nnews detection has recently garnered increased attention. However, the existing\ndatasets are comprised of user-uploaded videos and contain an excess amounts of\nsuperfluous data, which introduces noise into the model training process. To\naddress this issue, we construct a dataset named Official-NV, comprising\nofficially published news videos. The crawl officially published videos are\naugmented through the use of LLMs-based generation and manual verification,\nthereby expanding the dataset. We also propose a new baseline model called\nOFNVD, which captures key information from multimodal features through a GLU\nattention mechanism and performs feature enhancement and modal aggregation via\na cross-modal Transformer. Benchmarking the dataset and baselines demonstrates\nthe effectiveness of our model in multimodal news detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "News media, especially video news media, have penetrated into every aspect of\ndaily life, which also brings the risk of fake news. Therefore, multimodal fake\nnews detection has recently garnered increased attention. However, the existing\ndatasets are comprised of user-uploaded videos and contain an excess amounts of\nsuperfluous data, which introduces noise into the model training process. To\naddress this issue, we construct a dataset named Official-NV, comprising\nofficially published news videos. The crawl officially published videos are\naugmented through the use of LLMs-based generation and manual verification,\nthereby expanding the dataset. We also propose a new baseline model called\nOFNVD, which captures key information from multimodal features through a GLU\nattention mechanism and performs feature enhancement and modal aggregation via\na cross-modal Transformer. Benchmarking the dataset and baselines demonstrates\nthe effectiveness of our model in multimodal news detection."
                },
                "authors": [
                    {
                        "name": "Yihao Wang"
                    },
                    {
                        "name": "Lizhi Chen"
                    },
                    {
                        "name": "Zhong Qian"
                    },
                    {
                        "name": "Peifeng Li"
                    }
                ],
                "author_detail": {
                    "name": "Peifeng Li"
                },
                "author": "Peifeng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19493v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19493v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14554v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14554v2",
                "updated": "2024-12-27T10:17:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    10,
                    17,
                    12,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-19T06:10:40Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    6,
                    10,
                    40,
                    3,
                    354,
                    0
                ],
                "title": "The Current Challenges of Software Engineering in the Era of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Current Challenges of Software Engineering in the Era of Large\n  Language Models"
                },
                "summary": "With the advent of large language models (LLMs) in the artificial\nintelligence (AI) area, the field of software engineering (SE) has also\nwitnessed a paradigm shift. These models, by leveraging the power of deep\nlearning and massive amounts of data, have demonstrated an unprecedented\ncapacity to understand, generate, and operate programming languages. They can\nassist developers in completing a broad spectrum of software development\nactivities, encompassing software design, automated programming, and\nmaintenance, which potentially reduces huge human efforts. Integrating LLMs\nwithin the SE landscape (LLM4SE) has become a burgeoning trend, necessitating\nexploring this emergent landscape's challenges and opportunities.\n  The paper aims at revisiting the software development life cycle (SDLC) under\nLLMs, and highlighting challenges and opportunities of the new paradigm. The\npaper first summarizes the overall process of LLM4SE, and then elaborates on\nthe current challenges based on a through discussion. The discussion was held\namong more than 20 participants from academia and industry, specializing in\nfields such as software engineering and artificial intelligence. Specifically,\nwe achieve 26 key challenges from seven aspects, including software requirement\n& design, coding assistance, testing code generation, code review, code\nmaintenance, software vulnerability management, and data, training, and\nevaluation. We hope the achieved challenges would benefit future research in\nthe LLM4SE field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advent of large language models (LLMs) in the artificial\nintelligence (AI) area, the field of software engineering (SE) has also\nwitnessed a paradigm shift. These models, by leveraging the power of deep\nlearning and massive amounts of data, have demonstrated an unprecedented\ncapacity to understand, generate, and operate programming languages. They can\nassist developers in completing a broad spectrum of software development\nactivities, encompassing software design, automated programming, and\nmaintenance, which potentially reduces huge human efforts. Integrating LLMs\nwithin the SE landscape (LLM4SE) has become a burgeoning trend, necessitating\nexploring this emergent landscape's challenges and opportunities.\n  The paper aims at revisiting the software development life cycle (SDLC) under\nLLMs, and highlighting challenges and opportunities of the new paradigm. The\npaper first summarizes the overall process of LLM4SE, and then elaborates on\nthe current challenges based on a through discussion. The discussion was held\namong more than 20 participants from academia and industry, specializing in\nfields such as software engineering and artificial intelligence. Specifically,\nwe achieve 26 key challenges from seven aspects, including software requirement\n& design, coding assistance, testing code generation, code review, code\nmaintenance, software vulnerability management, and data, training, and\nevaluation. We hope the achieved challenges would benefit future research in\nthe LLM4SE field."
                },
                "authors": [
                    {
                        "name": "Cuiyun Gao"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Shan Gao"
                    },
                    {
                        "name": "Xin Xia"
                    },
                    {
                        "name": "Zhi Jin"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Jin"
                },
                "author": "Zhi Jin",
                "arxiv_comment": "Accepted by ACM Transactions on Software Engineering and Methodology\n  (TOSEM)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14554v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14554v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.00326v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.00326v6",
                "updated": "2024-12-27T10:12:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    10,
                    12,
                    28,
                    4,
                    362,
                    0
                ],
                "published": "2023-12-01T03:44:54Z",
                "published_parsed": [
                    2023,
                    12,
                    1,
                    3,
                    44,
                    54,
                    4,
                    335,
                    0
                ],
                "title": "Agent-OM: Leveraging LLM Agents for Ontology Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent-OM: Leveraging LLM Agents for Ontology Matching"
                },
                "summary": "Ontology matching (OM) enables semantic interoperability between different\nontologies and resolves their conceptual heterogeneity by aligning related\nentities. OM systems currently have two prevailing design paradigms:\nconventional knowledge-based expert systems and newer machine learning-based\npredictive systems. While large language models (LLMs) and LLM agents have\nrevolutionised data engineering and have been applied creatively in many\ndomains, their potential for OM remains underexplored. This study introduces a\nnovel agent-powered LLM-based design paradigm for OM systems. With\nconsideration of several specific challenges in leveraging LLM agents for OM,\nwe propose a generic framework, namely Agent-OM (Agent for Ontology Matching),\nconsisting of two Siamese agents for retrieval and matching, with a set of OM\ntools. Our framework is implemented in a proof-of-concept system. Evaluations\nof three Ontology Alignment Evaluation Initiative (OAEI) tracks over\nstate-of-the-art OM systems show that our system can achieve results very close\nto the long-standing best performance on simple OM tasks and can significantly\nimprove the performance on complex and few-shot OM tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ontology matching (OM) enables semantic interoperability between different\nontologies and resolves their conceptual heterogeneity by aligning related\nentities. OM systems currently have two prevailing design paradigms:\nconventional knowledge-based expert systems and newer machine learning-based\npredictive systems. While large language models (LLMs) and LLM agents have\nrevolutionised data engineering and have been applied creatively in many\ndomains, their potential for OM remains underexplored. This study introduces a\nnovel agent-powered LLM-based design paradigm for OM systems. With\nconsideration of several specific challenges in leveraging LLM agents for OM,\nwe propose a generic framework, namely Agent-OM (Agent for Ontology Matching),\nconsisting of two Siamese agents for retrieval and matching, with a set of OM\ntools. Our framework is implemented in a proof-of-concept system. Evaluations\nof three Ontology Alignment Evaluation Initiative (OAEI) tracks over\nstate-of-the-art OM systems show that our system can achieve results very close\nto the long-standing best performance on simple OM tasks and can significantly\nimprove the performance on complex and few-shot OM tasks."
                },
                "authors": [
                    {
                        "name": "Zhangcheng Qiang"
                    },
                    {
                        "name": "Weiqing Wang"
                    },
                    {
                        "name": "Kerry Taylor"
                    }
                ],
                "author_detail": {
                    "name": "Kerry Taylor"
                },
                "author": "Kerry Taylor",
                "arxiv_comment": "19 pages, 12 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.00326v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.00326v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19562v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19562v1",
                "updated": "2024-12-27T10:05:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    10,
                    5,
                    45,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T10:05:45Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    10,
                    5,
                    45,
                    4,
                    362,
                    0
                ],
                "title": "Hindsight Planner: A Closed-Loop Few-Shot Planner for Embodied\n  Instruction Following",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hindsight Planner: A Closed-Loop Few-Shot Planner for Embodied\n  Instruction Following"
                },
                "summary": "This work focuses on building a task planner for Embodied Instruction\nFollowing (EIF) using Large Language Models (LLMs). Previous works typically\ntrain a planner to imitate expert trajectories, treating this as a supervised\ntask. While these methods achieve competitive performance, they often lack\nsufficient robustness. When a suboptimal action is taken, the planner may\nencounter an out-of-distribution state, which can lead to task failure. In\ncontrast, we frame the task as a Partially Observable Markov Decision Process\n(POMDP) and aim to develop a robust planner under a few-shot assumption. Thus,\nwe propose a closed-loop planner with an adaptation module and a novel\nhindsight method, aiming to use as much information as possible to assist the\nplanner. Our experiments on the ALFRED dataset indicate that our planner\nachieves competitive performance under a few-shot assumption. For the first\ntime, our few-shot agent's performance approaches and even surpasses that of\nthe full-shot supervised agent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work focuses on building a task planner for Embodied Instruction\nFollowing (EIF) using Large Language Models (LLMs). Previous works typically\ntrain a planner to imitate expert trajectories, treating this as a supervised\ntask. While these methods achieve competitive performance, they often lack\nsufficient robustness. When a suboptimal action is taken, the planner may\nencounter an out-of-distribution state, which can lead to task failure. In\ncontrast, we frame the task as a Partially Observable Markov Decision Process\n(POMDP) and aim to develop a robust planner under a few-shot assumption. Thus,\nwe propose a closed-loop planner with an adaptation module and a novel\nhindsight method, aiming to use as much information as possible to assist the\nplanner. Our experiments on the ALFRED dataset indicate that our planner\nachieves competitive performance under a few-shot assumption. For the first\ntime, our few-shot agent's performance approaches and even surpasses that of\nthe full-shot supervised agent."
                },
                "authors": [
                    {
                        "name": "Yuxiao Yang"
                    },
                    {
                        "name": "Shenao Zhang"
                    },
                    {
                        "name": "Zhihan Liu"
                    },
                    {
                        "name": "Huaxiu Yao"
                    },
                    {
                        "name": "Zhaoran Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhaoran Wang"
                },
                "author": "Zhaoran Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19562v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19562v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11843v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11843v2",
                "updated": "2024-12-27T08:32:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    8,
                    32,
                    38,
                    4,
                    362,
                    0
                ],
                "published": "2024-09-23T08:39:16Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    8,
                    39,
                    16,
                    0,
                    267,
                    0
                ],
                "title": "From Commands to Prompts: LLM-based Semantic File System for AIOS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Commands to Prompts: LLM-based Semantic File System for AIOS"
                },
                "summary": "Large language models (LLMs) have demonstrated significant potential in the\ndevelopment of intelligent applications and systems such as LLM-based agents\nand agent operating systems (AIOS). However, when these applications and\nsystems interact with the underlying file system, the file system still remains\nthe traditional paradigm: reliant on manual navigation through precise\ncommands. This paradigm poses a bottleneck to the usability of these systems as\nusers are required to navigate complex folder hierarchies and remember cryptic\nfile names. To address this limitation, we propose an LLM-based semantic file\nsystem ( LSFS ) for prompt-driven file management. Unlike conventional\napproaches, LSFS incorporates LLMs to enable users or agents to interact with\nfiles through natural language prompts, facilitating semantic file management.\nAt the macro-level, we develop a comprehensive API set to achieve semantic file\nmanagement functionalities, such as semantic file retrieval, file update\nmonitoring and summarization, and semantic file rollback). At the micro-level,\nwe store files by constructing semantic indexes for them, design and implement\nsyscalls of different semantic operations (e.g., CRUD, group by, join) powered\nby vector database. Our experiments show that LSFS offers significant\nimprovements over traditional file systems in terms of user convenience, the\ndiversity of supported functions, and the accuracy and efficiency of file\noperations. Additionally, with the integration of LLM, our system enables more\nintelligent file management tasks, such as content summarization and version\ncomparison, further enhancing its capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated significant potential in the\ndevelopment of intelligent applications and systems such as LLM-based agents\nand agent operating systems (AIOS). However, when these applications and\nsystems interact with the underlying file system, the file system still remains\nthe traditional paradigm: reliant on manual navigation through precise\ncommands. This paradigm poses a bottleneck to the usability of these systems as\nusers are required to navigate complex folder hierarchies and remember cryptic\nfile names. To address this limitation, we propose an LLM-based semantic file\nsystem ( LSFS ) for prompt-driven file management. Unlike conventional\napproaches, LSFS incorporates LLMs to enable users or agents to interact with\nfiles through natural language prompts, facilitating semantic file management.\nAt the macro-level, we develop a comprehensive API set to achieve semantic file\nmanagement functionalities, such as semantic file retrieval, file update\nmonitoring and summarization, and semantic file rollback). At the micro-level,\nwe store files by constructing semantic indexes for them, design and implement\nsyscalls of different semantic operations (e.g., CRUD, group by, join) powered\nby vector database. Our experiments show that LSFS offers significant\nimprovements over traditional file systems in terms of user convenience, the\ndiversity of supported functions, and the accuracy and efficiency of file\noperations. Additionally, with the integration of LLM, our system enables more\nintelligent file management tasks, such as content summarization and version\ncomparison, further enhancing its capabilities."
                },
                "authors": [
                    {
                        "name": "Zeru Shi"
                    },
                    {
                        "name": "Kai Mei"
                    },
                    {
                        "name": "Yongye Su"
                    },
                    {
                        "name": "Chaoji Zuo"
                    },
                    {
                        "name": "Wenyue Hua"
                    },
                    {
                        "name": "Wujiang Xu"
                    },
                    {
                        "name": "Yujie Ren"
                    },
                    {
                        "name": "Zirui Liu"
                    },
                    {
                        "name": "Mengnan Du"
                    },
                    {
                        "name": "Dong Deng"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11843v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11843v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19515v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19515v1",
                "updated": "2024-12-27T08:14:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    8,
                    14,
                    28,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T08:14:28Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    8,
                    14,
                    28,
                    4,
                    362,
                    0
                ],
                "title": "Real-time classification of EEG signals using Machine Learning\n  deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time classification of EEG signals using Machine Learning\n  deployment"
                },
                "summary": "The prevailing educational methods predominantly rely on traditional\nclassroom instruction or online delivery, often limiting the teachers' ability\nto engage effectively with all the students simultaneously. A more intrinsic\nmethod of evaluating student attentiveness during lectures can enable the\neducators to tailor the course materials and their teaching styles in order to\nbetter meet the students' needs. The aim of this paper is to enhance teaching\nquality in real time, thereby fostering a higher student engagement in the\nclassroom activities. By monitoring the students' electroencephalography (EEG)\nsignals and employing machine learning algorithms, this study proposes a\ncomprehensive solution for addressing this challenge. Machine learning has\nemerged as a powerful tool for simplifying the analysis of complex variables,\nenabling the effective assessment of the students' concentration levels based\non specific parameters. However, the real-time impact of machine learning\nmodels necessitates a careful consideration as their deployment is concerned.\nThis study proposes a machine learning-based approach for predicting the level\nof students' comprehension with regard to a certain topic. A browser interface\nwas introduced that accesses the values of the system's parameters to determine\na student's level of concentration on a chosen topic. The deployment of the\nproposed system made it necessary to address the real-time challenges faced by\nthe students, consider the system's cost, and establish trust in its efficacy.\nThis paper presents the efforts made for approaching this pertinent issue\nthrough the implementation of innovative technologies and provides a framework\nfor addressing key considerations for future research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The prevailing educational methods predominantly rely on traditional\nclassroom instruction or online delivery, often limiting the teachers' ability\nto engage effectively with all the students simultaneously. A more intrinsic\nmethod of evaluating student attentiveness during lectures can enable the\neducators to tailor the course materials and their teaching styles in order to\nbetter meet the students' needs. The aim of this paper is to enhance teaching\nquality in real time, thereby fostering a higher student engagement in the\nclassroom activities. By monitoring the students' electroencephalography (EEG)\nsignals and employing machine learning algorithms, this study proposes a\ncomprehensive solution for addressing this challenge. Machine learning has\nemerged as a powerful tool for simplifying the analysis of complex variables,\nenabling the effective assessment of the students' concentration levels based\non specific parameters. However, the real-time impact of machine learning\nmodels necessitates a careful consideration as their deployment is concerned.\nThis study proposes a machine learning-based approach for predicting the level\nof students' comprehension with regard to a certain topic. A browser interface\nwas introduced that accesses the values of the system's parameters to determine\na student's level of concentration on a chosen topic. The deployment of the\nproposed system made it necessary to address the real-time challenges faced by\nthe students, consider the system's cost, and establish trust in its efficacy.\nThis paper presents the efforts made for approaching this pertinent issue\nthrough the implementation of innovative technologies and provides a framework\nfor addressing key considerations for future research directions."
                },
                "authors": [
                    {
                        "name": "Swati Chowdhuri"
                    },
                    {
                        "name": "Satadip Saha"
                    },
                    {
                        "name": "Samadrita Karmakar"
                    },
                    {
                        "name": "Ankur Chanda"
                    }
                ],
                "author_detail": {
                    "name": "Ankur Chanda"
                },
                "author": "Ankur Chanda",
                "arxiv_doi": "10.33436/v34i4y202401",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.33436/v34i4y202401",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.19515v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19515v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in Romanian Journal of Information Technology and Automatic\n  Control",
                "arxiv_journal_ref": "Vol. 34, No. 4, 7-18, 2024",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19513v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19513v1",
                "updated": "2024-12-27T08:09:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    8,
                    9,
                    11,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T08:09:11Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    8,
                    9,
                    11,
                    4,
                    362,
                    0
                ],
                "title": "Confidence v.s. Critique: A Decomposition of Self-Correction Capability\n  for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confidence v.s. Critique: A Decomposition of Self-Correction Capability\n  for LLMs"
                },
                "summary": "Large Language Models (LLMs) can correct their self-generated responses, but\na decline in accuracy after self-correction is also witnessed. To have a deeper\nunderstanding of self-correction, we endeavor to decompose, evaluate, and\nanalyze the self-correction behaviors of LLMs. By enumerating and analyzing\nanswer correctness before and after self-correction, we decompose the\nself-correction capability into confidence (being confident to correct answers)\nand critique (turning wrong answers to correct) capabilities, and propose two\nmetrics from a probabilistic perspective to measure these 2 capabilities, along\nwith another metric for overall self-correction capability evaluation. Based on\nour decomposition and evaluation metrics, we conduct extensive experiments and\ndraw some empirical conclusions. For example, we find different models can\nexhibit distinct behaviors: some models are confident while others are more\ncritical. We also find the trade-off between the two capabilities (i.e.\nimproving one can lead to a decline in the other) when manipulating model\nself-correction behavior by prompts or in-context learning. Further, we find a\nsimple yet efficient strategy to improve self-correction capability by\ntransforming Supervision Fine-Tuning (SFT) data format, and our strategy\noutperforms vanilla SFT in both capabilities and achieves much higher accuracy\nafter self-correction. Our code will be publicly available on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can correct their self-generated responses, but\na decline in accuracy after self-correction is also witnessed. To have a deeper\nunderstanding of self-correction, we endeavor to decompose, evaluate, and\nanalyze the self-correction behaviors of LLMs. By enumerating and analyzing\nanswer correctness before and after self-correction, we decompose the\nself-correction capability into confidence (being confident to correct answers)\nand critique (turning wrong answers to correct) capabilities, and propose two\nmetrics from a probabilistic perspective to measure these 2 capabilities, along\nwith another metric for overall self-correction capability evaluation. Based on\nour decomposition and evaluation metrics, we conduct extensive experiments and\ndraw some empirical conclusions. For example, we find different models can\nexhibit distinct behaviors: some models are confident while others are more\ncritical. We also find the trade-off between the two capabilities (i.e.\nimproving one can lead to a decline in the other) when manipulating model\nself-correction behavior by prompts or in-context learning. Further, we find a\nsimple yet efficient strategy to improve self-correction capability by\ntransforming Supervision Fine-Tuning (SFT) data format, and our strategy\noutperforms vanilla SFT in both capabilities and achieves much higher accuracy\nafter self-correction. Our code will be publicly available on GitHub."
                },
                "authors": [
                    {
                        "name": "Zhe Yang"
                    },
                    {
                        "name": "Yichang Zhang"
                    },
                    {
                        "name": "Yudong Wang"
                    },
                    {
                        "name": "Ziyao Xu"
                    },
                    {
                        "name": "Junyang Lin"
                    },
                    {
                        "name": "Zhifang Sui"
                    }
                ],
                "author_detail": {
                    "name": "Zhifang Sui"
                },
                "author": "Zhifang Sui",
                "arxiv_comment": "16 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19513v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19513v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19512v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19512v1",
                "updated": "2024-12-27T08:03:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    8,
                    3,
                    22,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T08:03:22Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    8,
                    3,
                    22,
                    4,
                    362,
                    0
                ],
                "title": "Safeguard Fine-Tuned LLMs Through Pre- and Post-Tuning Model Merging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safeguard Fine-Tuned LLMs Through Pre- and Post-Tuning Model Merging"
                },
                "summary": "Fine-tuning large language models (LLMs) for downstream tasks is a widely\nadopted approach, but it often leads to safety degradation in safety-aligned\nLLMs. Currently, many solutions address this issue by incorporating additional\nsafety data, which can be impractical in many cases. In this paper, we address\nthe question: How can we improve downstream task performance while preserving\nsafety in LLMs without relying on additional safety data? We propose a simple\nand effective method that maintains the inherent safety of LLMs while enhancing\ntheir downstream task performance: merging the weights of pre- and\npost-fine-tuned safety-aligned models. Experimental results across various\ndownstream tasks, models, and merging methods demonstrate that this approach\neffectively mitigates safety degradation while improving downstream task\nperformance, offering a practical solution for adapting safety-aligned LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large language models (LLMs) for downstream tasks is a widely\nadopted approach, but it often leads to safety degradation in safety-aligned\nLLMs. Currently, many solutions address this issue by incorporating additional\nsafety data, which can be impractical in many cases. In this paper, we address\nthe question: How can we improve downstream task performance while preserving\nsafety in LLMs without relying on additional safety data? We propose a simple\nand effective method that maintains the inherent safety of LLMs while enhancing\ntheir downstream task performance: merging the weights of pre- and\npost-fine-tuned safety-aligned models. Experimental results across various\ndownstream tasks, models, and merging methods demonstrate that this approach\neffectively mitigates safety degradation while improving downstream task\nperformance, offering a practical solution for adapting safety-aligned LLMs."
                },
                "authors": [
                    {
                        "name": "Hua Farn"
                    },
                    {
                        "name": "Hsuan Su"
                    },
                    {
                        "name": "Shachi H Kumar"
                    },
                    {
                        "name": "Saurav Sahay"
                    },
                    {
                        "name": "Shang-Tse Chen"
                    },
                    {
                        "name": "Hung-yi Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hung-yi Lee"
                },
                "author": "Hung-yi Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19512v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19512v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19509v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19509v1",
                "updated": "2024-12-27T07:55:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    7,
                    55,
                    36,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T07:55:36Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    7,
                    55,
                    36,
                    4,
                    362,
                    0
                ],
                "title": "MBQ: Modality-Balanced Quantization for Large Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MBQ: Modality-Balanced Quantization for Large Vision-Language Models"
                },
                "summary": "Vision-Language Models (VLMs) have enabled a variety of real-world\napplications. The large parameter size of VLMs brings large memory and\ncomputation overhead which poses significant challenges for deployment.\nPost-Training Quantization (PTQ) is an effective technique to reduce the memory\nand computation overhead. Existing PTQ methods mainly focus on large language\nmodels (LLMs), without considering the differences across other modalities. In\nthis paper, we discover that there is a significant difference in sensitivity\nbetween language and vision tokens in large VLMs. Therefore, treating tokens\nfrom different modalities equally, as in existing PTQ methods, may\nover-emphasize the insensitive modalities, leading to significant accuracy\nloss. To deal with the above issue, we propose a simple yet effective method,\nModality-Balanced Quantization (MBQ), for large VLMs. Specifically, MBQ\nincorporates the different sensitivities across modalities during the\ncalibration process to minimize the reconstruction loss for better quantization\nparameters. Extensive experiments show that MBQ can significantly improve task\naccuracy by up to 4.4% and 11.6% under W3 and W4A8 quantization for 7B to 70B\nVLMs, compared to SOTA baselines. Additionally, we implement a W3 GPU kernel\nthat fuses the dequantization and GEMV operators, achieving a 1.4x speedup on\nLLaVA-onevision-7B on the RTX 4090. The code is available at\nhttps://github.com/thu-nics/MBQ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) have enabled a variety of real-world\napplications. The large parameter size of VLMs brings large memory and\ncomputation overhead which poses significant challenges for deployment.\nPost-Training Quantization (PTQ) is an effective technique to reduce the memory\nand computation overhead. Existing PTQ methods mainly focus on large language\nmodels (LLMs), without considering the differences across other modalities. In\nthis paper, we discover that there is a significant difference in sensitivity\nbetween language and vision tokens in large VLMs. Therefore, treating tokens\nfrom different modalities equally, as in existing PTQ methods, may\nover-emphasize the insensitive modalities, leading to significant accuracy\nloss. To deal with the above issue, we propose a simple yet effective method,\nModality-Balanced Quantization (MBQ), for large VLMs. Specifically, MBQ\nincorporates the different sensitivities across modalities during the\ncalibration process to minimize the reconstruction loss for better quantization\nparameters. Extensive experiments show that MBQ can significantly improve task\naccuracy by up to 4.4% and 11.6% under W3 and W4A8 quantization for 7B to 70B\nVLMs, compared to SOTA baselines. Additionally, we implement a W3 GPU kernel\nthat fuses the dequantization and GEMV operators, achieving a 1.4x speedup on\nLLaVA-onevision-7B on the RTX 4090. The code is available at\nhttps://github.com/thu-nics/MBQ."
                },
                "authors": [
                    {
                        "name": "Shiyao Li"
                    },
                    {
                        "name": "Yingchun Hu"
                    },
                    {
                        "name": "Xuefei Ning"
                    },
                    {
                        "name": "Xihui Liu"
                    },
                    {
                        "name": "Ke Hong"
                    },
                    {
                        "name": "Xiaotao Jia"
                    },
                    {
                        "name": "Xiuhong Li"
                    },
                    {
                        "name": "Yaqi Yan"
                    },
                    {
                        "name": "Pei Ran"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Shengen Yan"
                    },
                    {
                        "name": "Huazhong Yang"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19509v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19509v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19498v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19498v1",
                "updated": "2024-12-27T07:33:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    7,
                    33,
                    49,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T07:33:49Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    7,
                    33,
                    49,
                    4,
                    362,
                    0
                ],
                "title": "Casevo: A Cognitive Agents and Social Evolution Simulator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Casevo: A Cognitive Agents and Social Evolution Simulator"
                },
                "summary": "In this paper, we introduce a multi-agent simulation framework Casevo\n(Cognitive Agents and Social Evolution Simulator), that integrates large\nlanguage models (LLMs) to simulate complex social phenomena and decision-making\nprocesses. Casevo is designed as a discrete-event simulator driven by agents\nwith features such as Chain of Thoughts (CoT), Retrieval-Augmented Generation\n(RAG), and Customizable Memory Mechanism. Casevo enables dynamic social\nmodeling, which can support various scenarios such as social network analysis,\npublic opinion dynamics, and behavior prediction in complex social systems. To\ndemonstrate the effectiveness of Casevo, we utilize one of the U.S. 2020\nmidterm election TV debates as a simulation example. Our results show that\nCasevo facilitates more realistic and flexible agent interactions, improving\nthe quality of dynamic social phenomena simulation. This work contributes to\nthe field by providing a robust system for studying large-scale, high-fidelity\nsocial behaviors with advanced LLM-driven agents, expanding the capabilities of\ntraditional agent-based modeling (ABM). The open-source code repository address\nof casevo is https://github.com/rgCASS/casevo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a multi-agent simulation framework Casevo\n(Cognitive Agents and Social Evolution Simulator), that integrates large\nlanguage models (LLMs) to simulate complex social phenomena and decision-making\nprocesses. Casevo is designed as a discrete-event simulator driven by agents\nwith features such as Chain of Thoughts (CoT), Retrieval-Augmented Generation\n(RAG), and Customizable Memory Mechanism. Casevo enables dynamic social\nmodeling, which can support various scenarios such as social network analysis,\npublic opinion dynamics, and behavior prediction in complex social systems. To\ndemonstrate the effectiveness of Casevo, we utilize one of the U.S. 2020\nmidterm election TV debates as a simulation example. Our results show that\nCasevo facilitates more realistic and flexible agent interactions, improving\nthe quality of dynamic social phenomena simulation. This work contributes to\nthe field by providing a robust system for studying large-scale, high-fidelity\nsocial behaviors with advanced LLM-driven agents, expanding the capabilities of\ntraditional agent-based modeling (ABM). The open-source code repository address\nof casevo is https://github.com/rgCASS/casevo."
                },
                "authors": [
                    {
                        "name": "Zexun Jiang"
                    },
                    {
                        "name": "Yafang Shi"
                    },
                    {
                        "name": "Maoxu Li"
                    },
                    {
                        "name": "Hongjiang Xiao"
                    },
                    {
                        "name": "Yunxiao Qin"
                    },
                    {
                        "name": "Qinglan Wei"
                    },
                    {
                        "name": "Ye Wang"
                    },
                    {
                        "name": "Yuan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yuan Zhang"
                },
                "author": "Yuan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19498v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19498v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07111v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07111v2",
                "updated": "2024-12-27T07:29:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    7,
                    29,
                    19,
                    4,
                    362,
                    0
                ],
                "published": "2024-11-11T16:37:40Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    16,
                    37,
                    40,
                    0,
                    316,
                    0
                ],
                "title": "Building a Taiwanese Mandarin Spoken Language Model: A First Attempt",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building a Taiwanese Mandarin Spoken Language Model: A First Attempt"
                },
                "summary": "This technical report presents our initial attempt to build a spoken large\nlanguage model (LLM) for Taiwanese Mandarin, specifically tailored to enable\nreal-time, speech-to-speech interaction in multi-turn conversations. Our\nend-to-end model incorporates a decoder-only transformer architecture and aims\nto achieve seamless interaction while preserving the conversational flow,\nincluding full-duplex capabilities allowing simultaneous speaking and\nlistening. The paper also details the training process, including data\npreparation with synthesized dialogues and adjustments for real-time\ninteraction. We also developed a platform to evaluate conversational fluency\nand response coherence in multi-turn dialogues. We hope the release of the\nreport can contribute to the future development of spoken LLMs in Taiwanese\nMandarin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This technical report presents our initial attempt to build a spoken large\nlanguage model (LLM) for Taiwanese Mandarin, specifically tailored to enable\nreal-time, speech-to-speech interaction in multi-turn conversations. Our\nend-to-end model incorporates a decoder-only transformer architecture and aims\nto achieve seamless interaction while preserving the conversational flow,\nincluding full-duplex capabilities allowing simultaneous speaking and\nlistening. The paper also details the training process, including data\npreparation with synthesized dialogues and adjustments for real-time\ninteraction. We also developed a platform to evaluate conversational fluency\nand response coherence in multi-turn dialogues. We hope the release of the\nreport can contribute to the future development of spoken LLMs in Taiwanese\nMandarin."
                },
                "authors": [
                    {
                        "name": "Chih-Kai Yang"
                    },
                    {
                        "name": "Yu-Kuan Fu"
                    },
                    {
                        "name": "Chen-An Li"
                    },
                    {
                        "name": "Yi-Cheng Lin"
                    },
                    {
                        "name": "Yu-Xiang Lin"
                    },
                    {
                        "name": "Wei-Chih Chen"
                    },
                    {
                        "name": "Ho Lam Chung"
                    },
                    {
                        "name": "Chun-Yi Kuan"
                    },
                    {
                        "name": "Wei-Ping Huang"
                    },
                    {
                        "name": "Ke-Han Lu"
                    },
                    {
                        "name": "Tzu-Quan Lin"
                    },
                    {
                        "name": "Hsiu-Hsuan Wang"
                    },
                    {
                        "name": "En-Pei Hu"
                    },
                    {
                        "name": "Chan-Jan Hsu"
                    },
                    {
                        "name": "Liang-Hsuan Tseng"
                    },
                    {
                        "name": "I-Hsiang Chiu"
                    },
                    {
                        "name": "Ulin Sanga"
                    },
                    {
                        "name": "Xuanjun Chen"
                    },
                    {
                        "name": "Po-chun Hsu"
                    },
                    {
                        "name": "Shu-wen Yang"
                    },
                    {
                        "name": "Hung-yi Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hung-yi Lee"
                },
                "author": "Hung-yi Lee",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07111v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07111v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15862v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15862v3",
                "updated": "2024-12-27T07:04:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    7,
                    4,
                    19,
                    4,
                    362,
                    0
                ],
                "published": "2024-11-24T14:38:59Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    14,
                    38,
                    59,
                    6,
                    329,
                    0
                ],
                "title": "Do LLMs Really Think Step-by-step In Implicit Reasoning?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs Really Think Step-by-step In Implicit Reasoning?"
                },
                "summary": "It has been well-known that Chain-of-Thought can remarkably enhance LLMs'\nperformance on complex tasks. However, because it also introduces slower\ninference speeds and higher computational costs, many researches have attempted\nto use implicit CoT, which does not need LLMs to explicitly generate the\nintermediate steps. However, the invisible reasoning process leaves us a doubt\nthat, can implicit CoT really be equal to explicit CoT? Therefore, in this\nstudy, we address this question through experiments. We probe the information\nof intermediate steps from the model's hidden states when it is either trained\nor prompted to perform implicit CoT. The results surprisingly indicate that\nwhen prompted, LLMs hardly think about intermediate steps, suggesting they may\njust rely on experience rather than strict step-by-step reasoning. But when\ntrained, they indeed calculate intermediate steps. Moreover, in both\nsituations, we find the effect of using implicit CoT is susceptible to the\nformat of the problem, reaffirming the current deficiency of implicit CoT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It has been well-known that Chain-of-Thought can remarkably enhance LLMs'\nperformance on complex tasks. However, because it also introduces slower\ninference speeds and higher computational costs, many researches have attempted\nto use implicit CoT, which does not need LLMs to explicitly generate the\nintermediate steps. However, the invisible reasoning process leaves us a doubt\nthat, can implicit CoT really be equal to explicit CoT? Therefore, in this\nstudy, we address this question through experiments. We probe the information\nof intermediate steps from the model's hidden states when it is either trained\nor prompted to perform implicit CoT. The results surprisingly indicate that\nwhen prompted, LLMs hardly think about intermediate steps, suggesting they may\njust rely on experience rather than strict step-by-step reasoning. But when\ntrained, they indeed calculate intermediate steps. Moreover, in both\nsituations, we find the effect of using implicit CoT is susceptible to the\nformat of the problem, reaffirming the current deficiency of implicit CoT."
                },
                "authors": [
                    {
                        "name": "Yijiong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Yijiong Yu"
                },
                "author": "Yijiong Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15862v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15862v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.08914v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.08914v3",
                "updated": "2024-12-27T06:56:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    6,
                    56,
                    18,
                    4,
                    362,
                    0
                ],
                "published": "2023-12-14T13:20:57Z",
                "published_parsed": [
                    2023,
                    12,
                    14,
                    13,
                    20,
                    57,
                    3,
                    348,
                    0
                ],
                "title": "CogAgent: A Visual Language Model for GUI Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CogAgent: A Visual Language Model for GUI Agents"
                },
                "summary": "People are spending an enormous amount of time on digital devices through\ngraphical user interfaces (GUIs), e.g., computer or smartphone screens. Large\nlanguage models (LLMs) such as ChatGPT can assist people in tasks like writing\nemails, but struggle to understand and interact with GUIs, thus limiting their\npotential to increase automation levels. In this paper, we introduce CogAgent,\nan 18-billion-parameter visual language model (VLM) specializing in GUI\nunderstanding and navigation. By utilizing both low-resolution and\nhigh-resolution image encoders, CogAgent supports input at a resolution of\n1120*1120, enabling it to recognize tiny page elements and text. As a\ngeneralist visual language model, CogAgent achieves the state of the art on\nfive text-rich and four general VQA benchmarks, including VQAv2, OK-VQA,\nText-VQA, ST-VQA, ChartQA, infoVQA, DocVQA, MM-Vet, and POPE. CogAgent, using\nonly screenshots as input, outperforms LLM-based methods that consume extracted\nHTML text on both PC and Android GUI navigation tasks -- Mind2Web and AITW,\nadvancing the state of the art. The model and codes are available at\nhttps://github.com/THUDM/CogVLM, with a new version of CogAgent-9B-20241220\navailable at https://github.com/THUDM/CogAgent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "People are spending an enormous amount of time on digital devices through\ngraphical user interfaces (GUIs), e.g., computer or smartphone screens. Large\nlanguage models (LLMs) such as ChatGPT can assist people in tasks like writing\nemails, but struggle to understand and interact with GUIs, thus limiting their\npotential to increase automation levels. In this paper, we introduce CogAgent,\nan 18-billion-parameter visual language model (VLM) specializing in GUI\nunderstanding and navigation. By utilizing both low-resolution and\nhigh-resolution image encoders, CogAgent supports input at a resolution of\n1120*1120, enabling it to recognize tiny page elements and text. As a\ngeneralist visual language model, CogAgent achieves the state of the art on\nfive text-rich and four general VQA benchmarks, including VQAv2, OK-VQA,\nText-VQA, ST-VQA, ChartQA, infoVQA, DocVQA, MM-Vet, and POPE. CogAgent, using\nonly screenshots as input, outperforms LLM-based methods that consume extracted\nHTML text on both PC and Android GUI navigation tasks -- Mind2Web and AITW,\nadvancing the state of the art. The model and codes are available at\nhttps://github.com/THUDM/CogVLM, with a new version of CogAgent-9B-20241220\navailable at https://github.com/THUDM/CogAgent."
                },
                "authors": [
                    {
                        "name": "Wenyi Hong"
                    },
                    {
                        "name": "Weihan Wang"
                    },
                    {
                        "name": "Qingsong Lv"
                    },
                    {
                        "name": "Jiazheng Xu"
                    },
                    {
                        "name": "Wenmeng Yu"
                    },
                    {
                        "name": "Junhui Ji"
                    },
                    {
                        "name": "Yan Wang"
                    },
                    {
                        "name": "Zihan Wang"
                    },
                    {
                        "name": "Yuxuan Zhang"
                    },
                    {
                        "name": "Juanzi Li"
                    },
                    {
                        "name": "Bin Xu"
                    },
                    {
                        "name": "Yuxiao Dong"
                    },
                    {
                        "name": "Ming Ding"
                    },
                    {
                        "name": "Jie Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Tang"
                },
                "author": "Jie Tang",
                "arxiv_comment": "CVPR 2024 (Highlight), 27 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.08914v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.08914v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17378v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17378v3",
                "updated": "2024-12-27T05:56:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    5,
                    56,
                    37,
                    4,
                    362,
                    0
                ],
                "published": "2024-06-25T08:55:12Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    8,
                    55,
                    12,
                    1,
                    177,
                    0
                ],
                "title": "A Text is Worth Several Tokens: Text Embedding from LLMs Secretly Aligns\n  Well with The Key Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Text is Worth Several Tokens: Text Embedding from LLMs Secretly Aligns\n  Well with The Key Tokens"
                },
                "summary": "Text embeddings from large language models (LLMs) have achieved excellent\nresults in tasks such as information retrieval, semantic textual similarity,\netc. In this work, we show an interesting finding: when feeding a text into the\nLLM-based embedder, the obtained text embedding will be able to be aligned with\nthe key tokens in the input text. We first fully analyze this phenomenon on\neight LLM-based embedders and show that this phenomenon is universal and is not\naffected by model architecture, training strategy, and embedding method. With a\ndeeper analysis, we find that the main change in embedding space between these\nembedders and their LLM backbones is in the first principal component. By\nadjusting the first principal component, we can align text embedding with the\nkey tokens. Finally, we give several examples to demonstrate the vast\napplication potential of this finding: (1) we propose a simple and practical\nsparse retrieval method based on the aligned tokens, which can achieve 80% of\nthe dense retrieval effect of the same model while reducing the computation\nsignificantly; (2) we show that our findings provide a novel perspective to\nhelp understand novel technologies (e.g., instruction-following embedding) and\nfuzzy concepts (e.g., semantic relatedness vs. similarity) in this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text embeddings from large language models (LLMs) have achieved excellent\nresults in tasks such as information retrieval, semantic textual similarity,\netc. In this work, we show an interesting finding: when feeding a text into the\nLLM-based embedder, the obtained text embedding will be able to be aligned with\nthe key tokens in the input text. We first fully analyze this phenomenon on\neight LLM-based embedders and show that this phenomenon is universal and is not\naffected by model architecture, training strategy, and embedding method. With a\ndeeper analysis, we find that the main change in embedding space between these\nembedders and their LLM backbones is in the first principal component. By\nadjusting the first principal component, we can align text embedding with the\nkey tokens. Finally, we give several examples to demonstrate the vast\napplication potential of this finding: (1) we propose a simple and practical\nsparse retrieval method based on the aligned tokens, which can achieve 80% of\nthe dense retrieval effect of the same model while reducing the computation\nsignificantly; (2) we show that our findings provide a novel perspective to\nhelp understand novel technologies (e.g., instruction-following embedding) and\nfuzzy concepts (e.g., semantic relatedness vs. similarity) in this field."
                },
                "authors": [
                    {
                        "name": "Zhijie Nie"
                    },
                    {
                        "name": "Richong Zhang"
                    },
                    {
                        "name": "Zhanyu Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhanyu Wu"
                },
                "author": "Zhanyu Wu",
                "arxiv_comment": "Work in Progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17378v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17378v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02115v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02115v2",
                "updated": "2024-12-27T05:48:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    5,
                    48,
                    14,
                    4,
                    362,
                    0
                ],
                "published": "2024-11-04T14:29:04Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    14,
                    29,
                    4,
                    0,
                    309,
                    0
                ],
                "title": "FedMoE-DA: Federated Mixture of Experts via Domain Aware Fine-grained\n  Aggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedMoE-DA: Federated Mixture of Experts via Domain Aware Fine-grained\n  Aggregation"
                },
                "summary": "Federated learning (FL) is a collaborative machine learning approach that\nenables multiple clients to train models without sharing their private data.\nWith the rise of deep learning, large-scale models have garnered significant\nattention due to their exceptional performance. However, a key challenge in FL\nis the limitation imposed by clients with constrained computational and\ncommunication resources, which hampers the deployment of these large models.\nThe Mixture of Experts (MoE) architecture addresses this challenge with its\nsparse activation property, which reduces computational workload and\ncommunication demands during inference and updates. Additionally, MoE\nfacilitates better personalization by allowing each expert to specialize in\ndifferent subsets of the data distribution. To alleviate the communication\nburdens between the server and clients, we propose FedMoE-DA, a new FL model\ntraining framework that leverages the MoE architecture and incorporates a novel\ndomain-aware, fine-grained aggregation strategy to enhance the robustness,\npersonalizability, and communication efficiency simultaneously. Specifically,\nthe correlation between both intra-client expert models and inter-client data\nheterogeneity is exploited. Moreover, we utilize peer-to-peer (P2P)\ncommunication between clients for selective expert model synchronization, thus\nsignificantly reducing the server-client transmissions. Experiments demonstrate\nthat our FedMoE-DA achieves excellent performance while reducing the\ncommunication pressure on the server.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated learning (FL) is a collaborative machine learning approach that\nenables multiple clients to train models without sharing their private data.\nWith the rise of deep learning, large-scale models have garnered significant\nattention due to their exceptional performance. However, a key challenge in FL\nis the limitation imposed by clients with constrained computational and\ncommunication resources, which hampers the deployment of these large models.\nThe Mixture of Experts (MoE) architecture addresses this challenge with its\nsparse activation property, which reduces computational workload and\ncommunication demands during inference and updates. Additionally, MoE\nfacilitates better personalization by allowing each expert to specialize in\ndifferent subsets of the data distribution. To alleviate the communication\nburdens between the server and clients, we propose FedMoE-DA, a new FL model\ntraining framework that leverages the MoE architecture and incorporates a novel\ndomain-aware, fine-grained aggregation strategy to enhance the robustness,\npersonalizability, and communication efficiency simultaneously. Specifically,\nthe correlation between both intra-client expert models and inter-client data\nheterogeneity is exploited. Moreover, we utilize peer-to-peer (P2P)\ncommunication between clients for selective expert model synchronization, thus\nsignificantly reducing the server-client transmissions. Experiments demonstrate\nthat our FedMoE-DA achieves excellent performance while reducing the\ncommunication pressure on the server."
                },
                "authors": [
                    {
                        "name": "Ziwei Zhan"
                    },
                    {
                        "name": "Wenkuan Zhao"
                    },
                    {
                        "name": "Yuanqing Li"
                    },
                    {
                        "name": "Weijie Liu"
                    },
                    {
                        "name": "Xiaoxi Zhang"
                    },
                    {
                        "name": "Chee Wei Tan"
                    },
                    {
                        "name": "Chuan Wu"
                    },
                    {
                        "name": "Deke Guo"
                    },
                    {
                        "name": "Xu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xu Chen"
                },
                "author": "Xu Chen",
                "arxiv_comment": "8 pages, 5 figures, accepted by The 20th International Conference on\n  Mobility, Sensing and Networking (MSN 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02115v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02115v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00652v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00652v2",
                "updated": "2024-12-27T05:32:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    5,
                    32,
                    11,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-01T03:12:26Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    3,
                    12,
                    26,
                    6,
                    336,
                    0
                ],
                "title": "Multi-Agent Collaboration in Incident Response with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Collaboration in Incident Response with Large Language\n  Models"
                },
                "summary": "Incident response (IR) is a critical aspect of cybersecurity, requiring rapid\ndecision-making and coordinated efforts to address cyberattacks effectively.\nLeveraging large language models (LLMs) as intelligent agents offers a novel\napproach to enhancing collaboration and efficiency in IR scenarios. This paper\nexplores the application of LLM-based multi-agent collaboration using the\nBackdoors & Breaches framework, a tabletop game designed for cybersecurity\ntraining. We simulate real-world IR dynamics through various team structures,\nincluding centralized, decentralized, and hybrid configurations. By analyzing\nagent interactions and performance across these setups, we provide insights\ninto optimizing multi-agent collaboration for incident response. Our findings\nhighlight the potential of LLMs to enhance decision-making, improve\nadaptability, and streamline IR processes, paving the way for more effective\nand coordinated responses to cyber threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incident response (IR) is a critical aspect of cybersecurity, requiring rapid\ndecision-making and coordinated efforts to address cyberattacks effectively.\nLeveraging large language models (LLMs) as intelligent agents offers a novel\napproach to enhancing collaboration and efficiency in IR scenarios. This paper\nexplores the application of LLM-based multi-agent collaboration using the\nBackdoors & Breaches framework, a tabletop game designed for cybersecurity\ntraining. We simulate real-world IR dynamics through various team structures,\nincluding centralized, decentralized, and hybrid configurations. By analyzing\nagent interactions and performance across these setups, we provide insights\ninto optimizing multi-agent collaboration for incident response. Our findings\nhighlight the potential of LLMs to enhance decision-making, improve\nadaptability, and streamline IR processes, paving the way for more effective\nand coordinated responses to cyber threats."
                },
                "authors": [
                    {
                        "name": "Zefang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zefang Liu"
                },
                "author": "Zefang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00652v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00652v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.10462v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.10462v3",
                "updated": "2024-12-27T05:30:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    5,
                    30,
                    0,
                    4,
                    362,
                    0
                ],
                "published": "2023-08-21T04:31:06Z",
                "published_parsed": [
                    2023,
                    8,
                    21,
                    4,
                    31,
                    6,
                    0,
                    233,
                    0
                ],
                "title": "Exploring Parameter-Efficient Fine-Tuning Techniques for Code Generation\n  with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Parameter-Efficient Fine-Tuning Techniques for Code Generation\n  with Large Language Models"
                },
                "summary": "Large language models (LLMs) demonstrate impressive capabilities to generate\naccurate code snippets given natural language intents in a zero-shot manner,\ni.e., without the need for specific fine-tuning. While prior studies have\nhighlighted the advantages of fine-tuning LLMs, this process incurs high\ncomputational costs, making it impractical in resource-scarce environments,\nparticularly for models with billions of parameters. To address these\nchallenges, previous research explored in-context learning (ICL) and\nretrieval-augmented generation (RAG) as strategies to guide the LLM generative\nprocess with task-specific prompt examples. However, ICL and RAG introduce\ninconveniences, such as the need for designing contextually relevant prompts\nand the absence of learning task-specific parameters, thereby limiting\ndownstream task performance. In this context, we foresee parameter-efficient\nfine-tuning (PEFT) as a promising approach to efficiently specialize LLMs to\ntask-specific data while maintaining reasonable resource consumption. In this\npaper, we deliver a comprehensive study of PEFT techniques for LLMs in the\ncontext of automated code generation. Our comprehensive investigation of PEFT\ntechniques for LLMs reveals their superiority and potential over ICL and RAG\nacross a diverse set of LLMs and three representative Python code generation\ndatasets: Conala, CodeAlpacaPy, and APPS. Furthermore, our study highlights the\npotential for tuning larger LLMs and significant reductions in memory usage by\ncombining PEFT with quantization. Therefore, this study opens opportunities for\nbroader applications of PEFT in software engineering scenarios. Our code is\navailable at https://github.com/martin-wey/peft-llm-code/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate impressive capabilities to generate\naccurate code snippets given natural language intents in a zero-shot manner,\ni.e., without the need for specific fine-tuning. While prior studies have\nhighlighted the advantages of fine-tuning LLMs, this process incurs high\ncomputational costs, making it impractical in resource-scarce environments,\nparticularly for models with billions of parameters. To address these\nchallenges, previous research explored in-context learning (ICL) and\nretrieval-augmented generation (RAG) as strategies to guide the LLM generative\nprocess with task-specific prompt examples. However, ICL and RAG introduce\ninconveniences, such as the need for designing contextually relevant prompts\nand the absence of learning task-specific parameters, thereby limiting\ndownstream task performance. In this context, we foresee parameter-efficient\nfine-tuning (PEFT) as a promising approach to efficiently specialize LLMs to\ntask-specific data while maintaining reasonable resource consumption. In this\npaper, we deliver a comprehensive study of PEFT techniques for LLMs in the\ncontext of automated code generation. Our comprehensive investigation of PEFT\ntechniques for LLMs reveals their superiority and potential over ICL and RAG\nacross a diverse set of LLMs and three representative Python code generation\ndatasets: Conala, CodeAlpacaPy, and APPS. Furthermore, our study highlights the\npotential for tuning larger LLMs and significant reductions in memory usage by\ncombining PEFT with quantization. Therefore, this study opens opportunities for\nbroader applications of PEFT in software engineering scenarios. Our code is\navailable at https://github.com/martin-wey/peft-llm-code/."
                },
                "authors": [
                    {
                        "name": "Martin Weyssow"
                    },
                    {
                        "name": "Xin Zhou"
                    },
                    {
                        "name": "Kisub Kim"
                    },
                    {
                        "name": "David Lo"
                    },
                    {
                        "name": "Houari Sahraoui"
                    }
                ],
                "author_detail": {
                    "name": "Houari Sahraoui"
                },
                "author": "Houari Sahraoui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.10462v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.10462v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.09032v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.09032v3",
                "updated": "2024-12-27T05:13:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    5,
                    13,
                    23,
                    4,
                    362,
                    0
                ],
                "published": "2024-03-14T01:51:35Z",
                "published_parsed": [
                    2024,
                    3,
                    14,
                    1,
                    51,
                    35,
                    3,
                    74,
                    0
                ],
                "title": "CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language\n  Models to Coding Preferences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language\n  Models to Coding Preferences"
                },
                "summary": "Evaluating the alignment of large language models (LLMs) with user-defined\ncoding preferences is a challenging endeavour that requires a deep assessment\nof LLMs' outputs. Existing methods and benchmarks rely primarily on automated\nmetrics and static analysis tools, which often fail to capture the nuances of\nuser instructions and LLM outputs. To address this gap, we propose using the\nLLM-as-a-Judge methodology to evaluate the alignment of LLMs with coding\npreferences. Based on this approach, we present CodeUltraFeedback, a\ncomprehensive dataset designed to facilitate the evaluation and improvement of\nLLM alignment. CodeUltraFeedback consists of 10,000 coding instructions, each\nannotated with four responses generated from a diverse pool of 14 LLMs. These\nresponses are ranked based on five distinct coding preferences using GPT-3.5 as\na judge, providing both numerical scores and detailed textual feedback. Our\nanalysis of CodeUltraFeedback reveals that responses from GPT-3.5 and GPT-4 are\ngenerally preferred over those from open-weight LLMs, highlighting significant\ndifferences in alignment between closed and open-weight models. In turn, we\nexplore the usage of CodeUltraFeedback as feedback data to fine-tune and align\nCodeLlama-7B-Instruct using supervised fine-tuning (SFT) and reinforcement\nlearning from AI feedback (RLAIF) with direct preference optimization (DPO).\nThe resulting aligned CodeLlama-7B-Instruct model outperforms larger LLMs in\nterms of alignment with coding preferences and shows improved functional\ncorrectness on the HumanEval+ benchmark compared to the original instruct\nmodel. Therefore, our contributions bridge the gap in preference tuning of LLMs\nfor code and set the stage for further advancements in model alignment and\nRLAIF in automated software engineering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the alignment of large language models (LLMs) with user-defined\ncoding preferences is a challenging endeavour that requires a deep assessment\nof LLMs' outputs. Existing methods and benchmarks rely primarily on automated\nmetrics and static analysis tools, which often fail to capture the nuances of\nuser instructions and LLM outputs. To address this gap, we propose using the\nLLM-as-a-Judge methodology to evaluate the alignment of LLMs with coding\npreferences. Based on this approach, we present CodeUltraFeedback, a\ncomprehensive dataset designed to facilitate the evaluation and improvement of\nLLM alignment. CodeUltraFeedback consists of 10,000 coding instructions, each\nannotated with four responses generated from a diverse pool of 14 LLMs. These\nresponses are ranked based on five distinct coding preferences using GPT-3.5 as\na judge, providing both numerical scores and detailed textual feedback. Our\nanalysis of CodeUltraFeedback reveals that responses from GPT-3.5 and GPT-4 are\ngenerally preferred over those from open-weight LLMs, highlighting significant\ndifferences in alignment between closed and open-weight models. In turn, we\nexplore the usage of CodeUltraFeedback as feedback data to fine-tune and align\nCodeLlama-7B-Instruct using supervised fine-tuning (SFT) and reinforcement\nlearning from AI feedback (RLAIF) with direct preference optimization (DPO).\nThe resulting aligned CodeLlama-7B-Instruct model outperforms larger LLMs in\nterms of alignment with coding preferences and shows improved functional\ncorrectness on the HumanEval+ benchmark compared to the original instruct\nmodel. Therefore, our contributions bridge the gap in preference tuning of LLMs\nfor code and set the stage for further advancements in model alignment and\nRLAIF in automated software engineering."
                },
                "authors": [
                    {
                        "name": "Martin Weyssow"
                    },
                    {
                        "name": "Aton Kamanda"
                    },
                    {
                        "name": "Xin Zhou"
                    },
                    {
                        "name": "Houari Sahraoui"
                    }
                ],
                "author_detail": {
                    "name": "Houari Sahraoui"
                },
                "author": "Houari Sahraoui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.09032v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.09032v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11071v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11071v2",
                "updated": "2024-12-27T04:54:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    4,
                    54,
                    2,
                    4,
                    362,
                    0
                ],
                "published": "2024-07-12T20:34:59Z",
                "published_parsed": [
                    2024,
                    7,
                    12,
                    20,
                    34,
                    59,
                    4,
                    194,
                    0
                ],
                "title": "MonoSparse-CAM: Efficient Tree Model Processing via Monotonicity and\n  Sparsity in CAMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MonoSparse-CAM: Efficient Tree Model Processing via Monotonicity and\n  Sparsity in CAMs"
                },
                "summary": "While the tree-based machine learning (TBML) models exhibit superior\nperformance compared to neural networks on tabular data and hold promise for\nenergy-efficient acceleration using aCAM arrays, their ideal deployment on\nhardware with explicit exploitation of TBML structure and aCAM circuitry\nremains a challenging task. In this work, we present MonoSparse-CAM, a new\nCAM-based optimization technique that exploits TBML sparsity and monotonicity\nin CAM circuitry to further advance processing performance. Our results\nindicate that MonoSparse-CAM reduces energy consumption by upto to 28.56x\ncompared to raw processing and by 18.51x compared to state-of-the-art\ntechniques, while improving the efficiency of computation by at least 1.68x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While the tree-based machine learning (TBML) models exhibit superior\nperformance compared to neural networks on tabular data and hold promise for\nenergy-efficient acceleration using aCAM arrays, their ideal deployment on\nhardware with explicit exploitation of TBML structure and aCAM circuitry\nremains a challenging task. In this work, we present MonoSparse-CAM, a new\nCAM-based optimization technique that exploits TBML sparsity and monotonicity\nin CAM circuitry to further advance processing performance. Our results\nindicate that MonoSparse-CAM reduces energy consumption by upto to 28.56x\ncompared to raw processing and by 18.51x compared to state-of-the-art\ntechniques, while improving the efficiency of computation by at least 1.68x."
                },
                "authors": [
                    {
                        "name": "Tergel Molom-Ochir"
                    },
                    {
                        "name": "Brady Taylor"
                    },
                    {
                        "name": "Hai Li"
                    },
                    {
                        "name": "Yiran Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yiran Chen"
                },
                "arxiv_affiliation": "Helen",
                "author": "Yiran Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11071v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11071v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19450v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19450v1",
                "updated": "2024-12-27T04:37:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    4,
                    37,
                    39,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T04:37:39Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    4,
                    37,
                    39,
                    4,
                    362,
                    0
                ],
                "title": "Find the Intention of Instruction: Comprehensive Evaluation of\n  Instruction Understanding for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Find the Intention of Instruction: Comprehensive Evaluation of\n  Instruction Understanding for Large Language Models"
                },
                "summary": "One of the key strengths of Large Language Models (LLMs) is their ability to\ninteract with humans by generating appropriate responses to given instructions.\nThis ability, known as instruction-following capability, has established a\nfoundation for the use of LLMs across various fields and serves as a crucial\nmetric for evaluating their performance. While numerous evaluation benchmarks\nhave been developed, most focus solely on clear and coherent instructions.\nHowever, we have noted that LLMs can become easily distracted by\ninstruction-formatted statements, which may lead to an oversight of their\ninstruction comprehension skills. To address this issue, we introduce the\nIntention of Instruction (IoInst) benchmark. This benchmark evaluates LLMs'\ncapacity to remain focused and understand instructions without being misled by\nextraneous instructions. The primary objective of this benchmark is to identify\nthe appropriate instruction that accurately guides the generation of a given\ncontext. Our findings suggest that even recently introduced state-of-the-art\nmodels still lack instruction understanding capability. Along with the\nproposition of IoInst in this study, we also present broad analyses of the\nseveral strategies potentially applicable to IoInst.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One of the key strengths of Large Language Models (LLMs) is their ability to\ninteract with humans by generating appropriate responses to given instructions.\nThis ability, known as instruction-following capability, has established a\nfoundation for the use of LLMs across various fields and serves as a crucial\nmetric for evaluating their performance. While numerous evaluation benchmarks\nhave been developed, most focus solely on clear and coherent instructions.\nHowever, we have noted that LLMs can become easily distracted by\ninstruction-formatted statements, which may lead to an oversight of their\ninstruction comprehension skills. To address this issue, we introduce the\nIntention of Instruction (IoInst) benchmark. This benchmark evaluates LLMs'\ncapacity to remain focused and understand instructions without being misled by\nextraneous instructions. The primary objective of this benchmark is to identify\nthe appropriate instruction that accurately guides the generation of a given\ncontext. Our findings suggest that even recently introduced state-of-the-art\nmodels still lack instruction understanding capability. Along with the\nproposition of IoInst in this study, we also present broad analyses of the\nseveral strategies potentially applicable to IoInst."
                },
                "authors": [
                    {
                        "name": "Hyeonseok Moon"
                    },
                    {
                        "name": "Jaehyung Seo"
                    },
                    {
                        "name": "Seungyoon Lee"
                    },
                    {
                        "name": "Chanjun Park"
                    },
                    {
                        "name": "Heuiseok Lim"
                    }
                ],
                "author_detail": {
                    "name": "Heuiseok Lim"
                },
                "author": "Heuiseok Lim",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19450v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19450v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19449v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19449v1",
                "updated": "2024-12-27T04:37:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    4,
                    37,
                    6,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T04:37:06Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    4,
                    37,
                    6,
                    4,
                    362,
                    0
                ],
                "title": "Feature Alignment-Based Knowledge Distillation for Efficient Compression\n  of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature Alignment-Based Knowledge Distillation for Efficient Compression\n  of Large Language Models"
                },
                "summary": "This study proposes a knowledge distillation algorithm based on large\nlanguage models and feature alignment, aiming to effectively transfer the\nknowledge of large pre-trained models into lightweight student models, thereby\nreducing computational costs while maintaining high model performance.\nDifferent from the traditional soft label distillation method, this method\nintroduces a multi-layer feature alignment strategy to deeply align the\nintermediate features and attention mechanisms of the teacher model and the\nstudent model, maximally retaining the semantic expression ability and context\nmodeling ability of the teacher model. In terms of method design, a multi-task\nloss function is constructed, including feature matching loss, attention\nalignment loss, and output distribution matching loss, to ensure multi-level\ninformation transfer through joint optimization. The experiments were\ncomprehensively evaluated on the GLUE data set and various natural language\nprocessing tasks. The results show that the proposed model performs very close\nto the state-of-the-art GPT-4 model in terms of evaluation indicators such as\nperplexity, BLEU, ROUGE, and CER. At the same time, it far exceeds baseline\nmodels such as DeBERTa, XLNet, and GPT-3, showing significant performance\nimprovements and computing efficiency advantages. Research results show that\nthe feature alignment distillation strategy is an effective model compression\nmethod that can significantly reduce computational overhead and storage\nrequirements while maintaining model capabilities. Future research can be\nfurther expanded in the directions of self-supervised learning, cross-modal\nfeature alignment, and multi-task transfer learning to provide more flexible\nand efficient solutions for the deployment and optimization of deep learning\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study proposes a knowledge distillation algorithm based on large\nlanguage models and feature alignment, aiming to effectively transfer the\nknowledge of large pre-trained models into lightweight student models, thereby\nreducing computational costs while maintaining high model performance.\nDifferent from the traditional soft label distillation method, this method\nintroduces a multi-layer feature alignment strategy to deeply align the\nintermediate features and attention mechanisms of the teacher model and the\nstudent model, maximally retaining the semantic expression ability and context\nmodeling ability of the teacher model. In terms of method design, a multi-task\nloss function is constructed, including feature matching loss, attention\nalignment loss, and output distribution matching loss, to ensure multi-level\ninformation transfer through joint optimization. The experiments were\ncomprehensively evaluated on the GLUE data set and various natural language\nprocessing tasks. The results show that the proposed model performs very close\nto the state-of-the-art GPT-4 model in terms of evaluation indicators such as\nperplexity, BLEU, ROUGE, and CER. At the same time, it far exceeds baseline\nmodels such as DeBERTa, XLNet, and GPT-3, showing significant performance\nimprovements and computing efficiency advantages. Research results show that\nthe feature alignment distillation strategy is an effective model compression\nmethod that can significantly reduce computational overhead and storage\nrequirements while maintaining model capabilities. Future research can be\nfurther expanded in the directions of self-supervised learning, cross-modal\nfeature alignment, and multi-task transfer learning to provide more flexible\nand efficient solutions for the deployment and optimization of deep learning\nmodels."
                },
                "authors": [
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Chihang Wang"
                    },
                    {
                        "name": "Jia Gao"
                    },
                    {
                        "name": "Zhen Qi"
                    },
                    {
                        "name": "Hongye Zheng"
                    },
                    {
                        "name": "Xiaoxuan Liao"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoxuan Liao"
                },
                "author": "Xiaoxuan Liao",
                "arxiv_comment": "4 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19449v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19449v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19442v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19442v1",
                "updated": "2024-12-27T04:17:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    4,
                    17,
                    57,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T04:17:57Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    4,
                    17,
                    57,
                    4,
                    362,
                    0
                ],
                "title": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management"
                },
                "summary": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}."
                },
                "authors": [
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Anxin Tian"
                    },
                    {
                        "name": "Tianhao Tang"
                    },
                    {
                        "name": "Zhanchao Xu"
                    },
                    {
                        "name": "Xuejia Chen"
                    },
                    {
                        "name": "Nicole Hu"
                    },
                    {
                        "name": "Wei Dong"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19442v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19442v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13949v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13949v2",
                "updated": "2024-12-27T03:00:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    3,
                    0,
                    19,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-18T15:29:30Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    15,
                    29,
                    30,
                    2,
                    353,
                    0
                ],
                "title": "Cracking the Code of Hallucination in LVLMs with Vision-aware Head\n  Divergence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cracking the Code of Hallucination in LVLMs with Vision-aware Head\n  Divergence"
                },
                "summary": "Large vision-language models (LVLMs) have made substantial progress in\nintegrating large language models (LLMs) with visual inputs, enabling advanced\nmultimodal reasoning. Despite their success, a persistent challenge is\nhallucination-where generated text fails to accurately reflect visual\ncontent-undermining both accuracy and reliability. Existing methods focus on\nalignment training or decoding refinements but primarily address symptoms at\nthe generation stage without probing the underlying causes. In this work, we\ninvestigate the internal mechanisms driving hallucination in LVLMs, with an\nemphasis on the multi-head attention module. Specifically, we introduce\nVision-aware Head Divergence (VHD), a metric that quantifies the sensitivity of\nattention head outputs to visual context. Based on this, our findings reveal\nthe presence of vision-aware attention heads that are more attuned to visual\ninformation; however, the model's overreliance on its prior language patterns\nis closely related to hallucinations. Building on these insights, we propose\nVision-aware Head Reinforcement (VHR), a training-free approach to mitigate\nhallucination by enhancing the role of vision-aware attention heads. Extensive\nexperiments demonstrate that our method achieves superior performance compared\nto state-of-the-art approaches in mitigating hallucinations, while maintaining\nhigh efficiency with negligible additional time overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large vision-language models (LVLMs) have made substantial progress in\nintegrating large language models (LLMs) with visual inputs, enabling advanced\nmultimodal reasoning. Despite their success, a persistent challenge is\nhallucination-where generated text fails to accurately reflect visual\ncontent-undermining both accuracy and reliability. Existing methods focus on\nalignment training or decoding refinements but primarily address symptoms at\nthe generation stage without probing the underlying causes. In this work, we\ninvestigate the internal mechanisms driving hallucination in LVLMs, with an\nemphasis on the multi-head attention module. Specifically, we introduce\nVision-aware Head Divergence (VHD), a metric that quantifies the sensitivity of\nattention head outputs to visual context. Based on this, our findings reveal\nthe presence of vision-aware attention heads that are more attuned to visual\ninformation; however, the model's overreliance on its prior language patterns\nis closely related to hallucinations. Building on these insights, we propose\nVision-aware Head Reinforcement (VHR), a training-free approach to mitigate\nhallucination by enhancing the role of vision-aware attention heads. Extensive\nexperiments demonstrate that our method achieves superior performance compared\nto state-of-the-art approaches in mitigating hallucinations, while maintaining\nhigh efficiency with negligible additional time overhead."
                },
                "authors": [
                    {
                        "name": "Jinghan He"
                    },
                    {
                        "name": "Kuan Zhu"
                    },
                    {
                        "name": "Haiyun Guo"
                    },
                    {
                        "name": "Junfeng Fang"
                    },
                    {
                        "name": "Zhenglin Hua"
                    },
                    {
                        "name": "Yuheng Jia"
                    },
                    {
                        "name": "Ming Tang"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    },
                    {
                        "name": "Jinqiao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jinqiao Wang"
                },
                "author": "Jinqiao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13949v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13949v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15519v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15519v2",
                "updated": "2024-12-27T02:48:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    2,
                    48,
                    4,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-20T03:15:02Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    3,
                    15,
                    2,
                    4,
                    355,
                    0
                ],
                "title": "PreNeT: Leveraging Computational Features to Predict Deep Neural Network\n  Training Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PreNeT: Leveraging Computational Features to Predict Deep Neural Network\n  Training Time"
                },
                "summary": "Training deep learning models, particularly Transformer-based architectures\nsuch as Large Language Models (LLMs), demands substantial computational\nresources and extended training periods. While optimal configuration and\ninfrastructure selection can significantly reduce associated costs, this\noptimization requires preliminary analysis tools. This paper introduces PreNeT,\na novel predictive framework designed to address this optimization challenge.\nPreNeT facilitates training optimization by integrating comprehensive\ncomputational metrics, including layer-specific parameters, arithmetic\noperations and memory utilization. A key feature of PreNeT is its capacity to\naccurately predict training duration on previously unexamined hardware\ninfrastructures, including novel accelerator architectures. This framework\nemploys a sophisticated approach to capture and analyze the distinct\ncharacteristics of various neural network layers, thereby enhancing existing\nprediction methodologies. Through proactive implementation of PreNeT,\nresearchers and practitioners can determine optimal configurations, parameter\nsettings, and hardware specifications to maximize cost-efficiency and minimize\ntraining duration. Experimental results demonstrate that PreNeT achieves up to\n72% improvement in prediction accuracy compared to contemporary\nstate-of-the-art frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training deep learning models, particularly Transformer-based architectures\nsuch as Large Language Models (LLMs), demands substantial computational\nresources and extended training periods. While optimal configuration and\ninfrastructure selection can significantly reduce associated costs, this\noptimization requires preliminary analysis tools. This paper introduces PreNeT,\na novel predictive framework designed to address this optimization challenge.\nPreNeT facilitates training optimization by integrating comprehensive\ncomputational metrics, including layer-specific parameters, arithmetic\noperations and memory utilization. A key feature of PreNeT is its capacity to\naccurately predict training duration on previously unexamined hardware\ninfrastructures, including novel accelerator architectures. This framework\nemploys a sophisticated approach to capture and analyze the distinct\ncharacteristics of various neural network layers, thereby enhancing existing\nprediction methodologies. Through proactive implementation of PreNeT,\nresearchers and practitioners can determine optimal configurations, parameter\nsettings, and hardware specifications to maximize cost-efficiency and minimize\ntraining duration. Experimental results demonstrate that PreNeT achieves up to\n72% improvement in prediction accuracy compared to contemporary\nstate-of-the-art frameworks."
                },
                "authors": [
                    {
                        "name": "Alireza Pourali"
                    },
                    {
                        "name": "Arian Boukani"
                    },
                    {
                        "name": "Hamzeh Khazaei"
                    }
                ],
                "author_detail": {
                    "name": "Hamzeh Khazaei"
                },
                "author": "Hamzeh Khazaei",
                "arxiv_comment": "11 pages, Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15519v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15519v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01207v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01207v2",
                "updated": "2024-12-27T01:23:05Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    1,
                    23,
                    5,
                    4,
                    362,
                    0
                ],
                "published": "2024-09-02T12:35:59Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    12,
                    35,
                    59,
                    0,
                    246,
                    0
                ],
                "title": "Towards General Industrial Intelligence: A Survey of Continual Large\n  Models in Industrial IoT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards General Industrial Intelligence: A Survey of Continual Large\n  Models in Industrial IoT"
                },
                "summary": "Industrial AI is transitioning from traditional deep learning models to\nlarge-scale transformer-based architectures, with the Industrial Internet of\nThings (IIoT) playing a pivotal role. IIoT evolves from a simple data pipeline\nto an intelligent infrastructure, enabling and enhancing these advanced AI\nsystems. This survey explores the integration of IIoT with large models (LMs)\nand their potential applications in industrial environments. We focus on four\nprimary types of industrial LMs: language-based, vision-based, time-series, and\nmultimodal models. The lifecycle of LMs is segmented into four critical phases:\ndata foundation, model training, model connectivity, and continuous evolution.\nFirst, we analyze how IIoT provides abundant and diverse data resources,\nsupporting the training and fine-tuning of LMs. Second, we discuss how IIoT\noffers an efficient training infrastructure in low-latency and\nbandwidth-optimized environments. Third, we highlight the deployment advantages\nof LMs within IIoT, emphasizing IIoT's role as a connectivity nexus fostering\nemergent intelligence through modular design, dynamic routing, and model\nmerging to enhance system scalability and adaptability. Finally, we demonstrate\nhow IIoT supports continual learning mechanisms, enabling LMs to adapt to\ndynamic industrial conditions and ensure long-term effectiveness. This paper\nunderscores IIoT's critical role in the evolution of industrial intelligence\nwith large models, offering a theoretical framework and actionable insights for\nfuture research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Industrial AI is transitioning from traditional deep learning models to\nlarge-scale transformer-based architectures, with the Industrial Internet of\nThings (IIoT) playing a pivotal role. IIoT evolves from a simple data pipeline\nto an intelligent infrastructure, enabling and enhancing these advanced AI\nsystems. This survey explores the integration of IIoT with large models (LMs)\nand their potential applications in industrial environments. We focus on four\nprimary types of industrial LMs: language-based, vision-based, time-series, and\nmultimodal models. The lifecycle of LMs is segmented into four critical phases:\ndata foundation, model training, model connectivity, and continuous evolution.\nFirst, we analyze how IIoT provides abundant and diverse data resources,\nsupporting the training and fine-tuning of LMs. Second, we discuss how IIoT\noffers an efficient training infrastructure in low-latency and\nbandwidth-optimized environments. Third, we highlight the deployment advantages\nof LMs within IIoT, emphasizing IIoT's role as a connectivity nexus fostering\nemergent intelligence through modular design, dynamic routing, and model\nmerging to enhance system scalability and adaptability. Finally, we demonstrate\nhow IIoT supports continual learning mechanisms, enabling LMs to adapt to\ndynamic industrial conditions and ensure long-term effectiveness. This paper\nunderscores IIoT's critical role in the evolution of industrial intelligence\nwith large models, offering a theoretical framework and actionable insights for\nfuture research."
                },
                "authors": [
                    {
                        "name": "Jiao Chen"
                    },
                    {
                        "name": "Jiayi He"
                    },
                    {
                        "name": "Fangfang Chen"
                    },
                    {
                        "name": "Zuohong Lv"
                    },
                    {
                        "name": "Jianhua Tang"
                    },
                    {
                        "name": "Weihua Li"
                    },
                    {
                        "name": "Zuozhu Liu"
                    },
                    {
                        "name": "Howard H. Yang"
                    },
                    {
                        "name": "Guangjie Han"
                    }
                ],
                "author_detail": {
                    "name": "Guangjie Han"
                },
                "author": "Guangjie Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01207v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01207v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12593v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12593v2",
                "updated": "2024-12-27T01:20:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    1,
                    20,
                    51,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-17T06:50:15Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    6,
                    50,
                    15,
                    1,
                    352,
                    0
                ],
                "title": "Asymmetric protocols for mode pairing quantum key distribution with\n  finite-key analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asymmetric protocols for mode pairing quantum key distribution with\n  finite-key analysis"
                },
                "summary": "The mode pairing quantum key distribution (MP-QKD) protocol has attracted\nconsiderable attention for its capability to ensure high secure key rates over\nlong distances without requiring global phase locking. However, ensuring\nsymmetric channels for the MP-QKD protocol is challenging in practical quantum\ncommunication networks. Previous studies on the asymmetric MP-QKD protocol have\nrelied on ideal decoy state assumptions and infinite-key analysis, which are\nunattainable for real-world deployment. In this paper, we conduct a security\nanalysis of asymmetric MP-QKD protocol with the finite-key analysis, where we\ndiscard the previously impractical assumptions made in the decoy-state method.\nCombined with statistical fluctuation analysis, we globally optimized the 12\nindependent parameters in the asymmetric MP-QKD protocol by employing our\nmodified particle swarm optimization. The simulation results demonstrate that\nour work can achieve significantly enhanced secure key rates and transmission\ndistances compared to the original strategy with adding extra attenuation. We\nfurther investigate the relationship between the intensities and probabilities\nof signal, decoy, and vacuum states with transmission distance, facilitating\nits more efficient deployment in future quantum networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The mode pairing quantum key distribution (MP-QKD) protocol has attracted\nconsiderable attention for its capability to ensure high secure key rates over\nlong distances without requiring global phase locking. However, ensuring\nsymmetric channels for the MP-QKD protocol is challenging in practical quantum\ncommunication networks. Previous studies on the asymmetric MP-QKD protocol have\nrelied on ideal decoy state assumptions and infinite-key analysis, which are\nunattainable for real-world deployment. In this paper, we conduct a security\nanalysis of asymmetric MP-QKD protocol with the finite-key analysis, where we\ndiscard the previously impractical assumptions made in the decoy-state method.\nCombined with statistical fluctuation analysis, we globally optimized the 12\nindependent parameters in the asymmetric MP-QKD protocol by employing our\nmodified particle swarm optimization. The simulation results demonstrate that\nour work can achieve significantly enhanced secure key rates and transmission\ndistances compared to the original strategy with adding extra attenuation. We\nfurther investigate the relationship between the intensities and probabilities\nof signal, decoy, and vacuum states with transmission distance, facilitating\nits more efficient deployment in future quantum networks."
                },
                "authors": [
                    {
                        "name": "Zhenhua Li"
                    },
                    {
                        "name": "Tianqi Dou"
                    },
                    {
                        "name": "Yuheng Xie"
                    },
                    {
                        "name": "Weiwen Kong"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Haiqiang Ma"
                    },
                    {
                        "name": "Jianjun Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jianjun Tang"
                },
                "author": "Jianjun Tang",
                "arxiv_comment": "9 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12593v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12593v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19394v1",
                "updated": "2024-12-27T01:00:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    1,
                    0,
                    23,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T01:00:23Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    1,
                    0,
                    23,
                    4,
                    362,
                    0
                ],
                "title": "An Engorgio Prompt Makes Large Language Model Babble on",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Engorgio Prompt Makes Large Language Model Babble on"
                },
                "summary": "Auto-regressive large language models (LLMs) have yielded impressive\nperformance in many real-world tasks. However, the new paradigm of these LLMs\nalso exposes novel threats. In this paper, we explore their vulnerability to\ninference cost attacks, where a malicious user crafts Engorgio prompts to\nintentionally increase the computation cost and latency of the inference\nprocess. We design Engorgio, a novel methodology, to efficiently generate\nadversarial Engorgio prompts to affect the target LLM's service availability.\nEngorgio has the following two technical contributions. (1) We employ a\nparameterized distribution to track LLMs' prediction trajectory. (2) Targeting\nthe auto-regressive nature of LLMs' inference process, we propose novel loss\nfunctions to stably suppress the appearance of the <EOS> token, whose\noccurrence will interrupt the LLM's generation process. We conduct extensive\nexperiments on 13 open-sourced LLMs with parameters ranging from 125M to 30B.\nThe results show that Engorgio prompts can successfully induce LLMs to generate\nabnormally long outputs (i.e., roughly 2-13$\\times$ longer to reach 90%+ of the\noutput length limit) in a white-box scenario and our real-world experiment\ndemonstrates Engergio's threat to LLM service with limited computing resources.\nThe code is accessible at https://github.com/jianshuod/Engorgio-prompt.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive large language models (LLMs) have yielded impressive\nperformance in many real-world tasks. However, the new paradigm of these LLMs\nalso exposes novel threats. In this paper, we explore their vulnerability to\ninference cost attacks, where a malicious user crafts Engorgio prompts to\nintentionally increase the computation cost and latency of the inference\nprocess. We design Engorgio, a novel methodology, to efficiently generate\nadversarial Engorgio prompts to affect the target LLM's service availability.\nEngorgio has the following two technical contributions. (1) We employ a\nparameterized distribution to track LLMs' prediction trajectory. (2) Targeting\nthe auto-regressive nature of LLMs' inference process, we propose novel loss\nfunctions to stably suppress the appearance of the <EOS> token, whose\noccurrence will interrupt the LLM's generation process. We conduct extensive\nexperiments on 13 open-sourced LLMs with parameters ranging from 125M to 30B.\nThe results show that Engorgio prompts can successfully induce LLMs to generate\nabnormally long outputs (i.e., roughly 2-13$\\times$ longer to reach 90%+ of the\noutput length limit) in a white-box scenario and our real-world experiment\ndemonstrates Engergio's threat to LLM service with limited computing resources.\nThe code is accessible at https://github.com/jianshuod/Engorgio-prompt."
                },
                "authors": [
                    {
                        "name": "Jianshuo Dong"
                    },
                    {
                        "name": "Ziyuan Zhang"
                    },
                    {
                        "name": "Qingjie Zhang"
                    },
                    {
                        "name": "Han Qiu"
                    },
                    {
                        "name": "Tianwei Zhang"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Hewu Li"
                    },
                    {
                        "name": "Qi Li"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Ke Xu"
                    }
                ],
                "author_detail": {
                    "name": "Ke Xu"
                },
                "author": "Ke Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01069v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01069v2",
                "updated": "2024-12-26T23:28:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    23,
                    28,
                    3,
                    3,
                    361,
                    0
                ],
                "published": "2024-09-02T08:48:16Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    8,
                    48,
                    16,
                    0,
                    246,
                    0
                ],
                "title": "A blueprint for large-scale quantum-network deployments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A blueprint for large-scale quantum-network deployments"
                },
                "summary": "Quantum Communications is a field that promises advances in cryptography,\nquantum computing and clock synchronisation, among other potential\napplications. However, communication based on quantum phenomena requires an\nextreme level of isolation from external disturbances, making the transmission\nof quantum signals together with classical ones difficult. A range of\ntechniques has been tested to introduce quantum communications in already\ndeployed optical networks which also carry legacy traffic. This comes with\nchallenges, not only at the physical layer but also at the operations and\nmanagement layer. To achieve a broad acceptance among network operators, the\njoint management and operation of quantum and classical resources, compliance\nwith standards, and quality and legal assurance need to be addressed. This\narticle presents a detailed account of solutions to the above issues, deployed\nand evaluated in the MadQCI (Madrid Quantum Communication Infrastructure)\ntestbed. This network is designed to integrate quantum communications in the\ntelecommunications ecosystem by installing quantum-key-distribution modules\nfrom multiple providers in production nodes of two different operators. The\nmodules were connected through an optical-switched network with more than 130\nkm of deployed optical fibre. The tests were done in compliance with strict\nservice level agreements that protected the legacy traffic of the pre-existing\nclassical network. The goal was to achieve full quantum-classical compatibility\nat all levels, while limiting the modifications of optical transport and\nencryption and complying with as many standards as possible. This effort was\nintended to serve as a blueprint, which can be used as the foundation of\nlarge-scale quantum network deployments. To demonstrate the capabilities of\nMadQCI, end-to-end encryption services were deployed and a variety of use-cases\nwere showcased.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Communications is a field that promises advances in cryptography,\nquantum computing and clock synchronisation, among other potential\napplications. However, communication based on quantum phenomena requires an\nextreme level of isolation from external disturbances, making the transmission\nof quantum signals together with classical ones difficult. A range of\ntechniques has been tested to introduce quantum communications in already\ndeployed optical networks which also carry legacy traffic. This comes with\nchallenges, not only at the physical layer but also at the operations and\nmanagement layer. To achieve a broad acceptance among network operators, the\njoint management and operation of quantum and classical resources, compliance\nwith standards, and quality and legal assurance need to be addressed. This\narticle presents a detailed account of solutions to the above issues, deployed\nand evaluated in the MadQCI (Madrid Quantum Communication Infrastructure)\ntestbed. This network is designed to integrate quantum communications in the\ntelecommunications ecosystem by installing quantum-key-distribution modules\nfrom multiple providers in production nodes of two different operators. The\nmodules were connected through an optical-switched network with more than 130\nkm of deployed optical fibre. The tests were done in compliance with strict\nservice level agreements that protected the legacy traffic of the pre-existing\nclassical network. The goal was to achieve full quantum-classical compatibility\nat all levels, while limiting the modifications of optical transport and\nencryption and complying with as many standards as possible. This effort was\nintended to serve as a blueprint, which can be used as the foundation of\nlarge-scale quantum network deployments. To demonstrate the capabilities of\nMadQCI, end-to-end encryption services were deployed and a variety of use-cases\nwere showcased."
                },
                "authors": [
                    {
                        "name": "Alberto Sebastián-Lombraña"
                    },
                    {
                        "name": "Hans H. Brunner"
                    },
                    {
                        "name": "Juan P. Brito"
                    },
                    {
                        "name": "Rubén B. Méndez"
                    },
                    {
                        "name": "Rafael J. Vicente"
                    },
                    {
                        "name": "Jaime S. Buruaga"
                    },
                    {
                        "name": "Laura Ortiz"
                    },
                    {
                        "name": "Chi-Hang Fred Fung"
                    },
                    {
                        "name": "Momtchil Peev"
                    },
                    {
                        "name": "José M. Rivas-Moscoso"
                    },
                    {
                        "name": "Felipe Jiménez"
                    },
                    {
                        "name": "Antonio Pastor"
                    },
                    {
                        "name": "Diego R. López"
                    },
                    {
                        "name": "Jesús Folgueira"
                    },
                    {
                        "name": "Vicente Martín"
                    }
                ],
                "author_detail": {
                    "name": "Vicente Martín"
                },
                "author": "Vicente Martín",
                "arxiv_comment": "18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01069v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01069v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19363v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19363v1",
                "updated": "2024-12-26T22:06:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    22,
                    6,
                    29,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T22:06:29Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    22,
                    6,
                    29,
                    3,
                    361,
                    0
                ],
                "title": "Large Language Models for Market Research: A Data-augmentation Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Market Research: A Data-augmentation Approach"
                },
                "summary": "Large Language Models (LLMs) have transformed artificial intelligence by\nexcelling in complex natural language processing tasks. Their ability to\ngenerate human-like text has opened new possibilities for market research,\nparticularly in conjoint analysis, where understanding consumer preferences is\nessential but often resource-intensive. Traditional survey-based methods face\nlimitations in scalability and cost, making LLM-generated data a promising\nalternative. However, while LLMs have the potential to simulate real consumer\nbehavior, recent studies highlight a significant gap between LLM-generated and\nhuman data, with biases introduced when substituting between the two. In this\npaper, we address this gap by proposing a novel statistical data augmentation\napproach that efficiently integrates LLM-generated data with real data in\nconjoint analysis. Our method leverages transfer learning principles to debias\nthe LLM-generated data using a small amount of human data. This results in\nstatistically robust estimators with consistent and asymptotically normal\nproperties, in contrast to naive approaches that simply substitute human data\nwith LLM-generated data, which can exacerbate bias. We validate our framework\nthrough an empirical study on COVID-19 vaccine preferences, demonstrating its\nsuperior ability to reduce estimation error and save data and costs by 24.9\\%\nto 79.8\\%. In contrast, naive approaches fail to save data due to the inherent\nbiases in LLM-generated data compared to human data. Another empirical study on\nsports car choices validates the robustness of our results. Our findings\nsuggest that while LLM-generated data is not a direct substitute for human\nresponses, it can serve as a valuable complement when used within a robust\nstatistical framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have transformed artificial intelligence by\nexcelling in complex natural language processing tasks. Their ability to\ngenerate human-like text has opened new possibilities for market research,\nparticularly in conjoint analysis, where understanding consumer preferences is\nessential but often resource-intensive. Traditional survey-based methods face\nlimitations in scalability and cost, making LLM-generated data a promising\nalternative. However, while LLMs have the potential to simulate real consumer\nbehavior, recent studies highlight a significant gap between LLM-generated and\nhuman data, with biases introduced when substituting between the two. In this\npaper, we address this gap by proposing a novel statistical data augmentation\napproach that efficiently integrates LLM-generated data with real data in\nconjoint analysis. Our method leverages transfer learning principles to debias\nthe LLM-generated data using a small amount of human data. This results in\nstatistically robust estimators with consistent and asymptotically normal\nproperties, in contrast to naive approaches that simply substitute human data\nwith LLM-generated data, which can exacerbate bias. We validate our framework\nthrough an empirical study on COVID-19 vaccine preferences, demonstrating its\nsuperior ability to reduce estimation error and save data and costs by 24.9\\%\nto 79.8\\%. In contrast, naive approaches fail to save data due to the inherent\nbiases in LLM-generated data compared to human data. Another empirical study on\nsports car choices validates the robustness of our results. Our findings\nsuggest that while LLM-generated data is not a direct substitute for human\nresponses, it can serve as a valuable complement when used within a robust\nstatistical framework."
                },
                "authors": [
                    {
                        "name": "Mengxin Wang"
                    },
                    {
                        "name": "Dennis J. Zhang"
                    },
                    {
                        "name": "Heng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Heng Zhang"
                },
                "arxiv_affiliation": "W. P. Carey School of Business, Arizona State University",
                "author": "Heng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19363v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19363v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50, 90B60, 62F12",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; J.4; G.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19361v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19361v1",
                "updated": "2024-12-26T22:04:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    22,
                    4,
                    23,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T22:04:23Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    22,
                    4,
                    23,
                    3,
                    361,
                    0
                ],
                "title": "Dynamic Skill Adaptation for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Skill Adaptation for Large Language Models"
                },
                "summary": "We present Dynamic Skill Adaptation (DSA), an adaptive and dynamic framework\nto adapt novel and complex skills to Large Language Models (LLMs). Compared\nwith previous work which learns from human-curated and static data in random\norders, we propose to first automatically generate and organize the training\ndata by mimicking the learning pathways of human and then dynamically tailor\nthe training data based on the training dynamics. Specifically, inspired by the\nlearning structures and teaching strategies in the human education system, we\nfirst construct a skill graph by decomposing complex skills into sub-skills and\narranging them based on their dependencies in human syllables. For every skill,\nwe utilize LLMs to generate both textbook-like data which contains detailed\ndescriptions of skills for pre-training and exercise-like data which targets at\nexplicitly utilizing the skills to solve problems for instruction-tuning.\nFurthermore, during the instruction-tuning, we dynamically update the training\ndata which down-weight easy-to-learn examples, generate more complex examples,\nand filter out data with errors. Experiments on large language models such as\nLLAMA and Mistral demonstrate the effectiveness of our proposed methods in\nadapting math reasoning skills and social study skills.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Dynamic Skill Adaptation (DSA), an adaptive and dynamic framework\nto adapt novel and complex skills to Large Language Models (LLMs). Compared\nwith previous work which learns from human-curated and static data in random\norders, we propose to first automatically generate and organize the training\ndata by mimicking the learning pathways of human and then dynamically tailor\nthe training data based on the training dynamics. Specifically, inspired by the\nlearning structures and teaching strategies in the human education system, we\nfirst construct a skill graph by decomposing complex skills into sub-skills and\narranging them based on their dependencies in human syllables. For every skill,\nwe utilize LLMs to generate both textbook-like data which contains detailed\ndescriptions of skills for pre-training and exercise-like data which targets at\nexplicitly utilizing the skills to solve problems for instruction-tuning.\nFurthermore, during the instruction-tuning, we dynamically update the training\ndata which down-weight easy-to-learn examples, generate more complex examples,\nand filter out data with errors. Experiments on large language models such as\nLLAMA and Mistral demonstrate the effectiveness of our proposed methods in\nadapting math reasoning skills and social study skills."
                },
                "authors": [
                    {
                        "name": "Jiaao Chen"
                    },
                    {
                        "name": "Diyi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Diyi Yang"
                },
                "author": "Diyi Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19361v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19354v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19354v1",
                "updated": "2024-12-26T21:32:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    21,
                    32,
                    8,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T21:32:08Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    21,
                    32,
                    8,
                    3,
                    361,
                    0
                ],
                "title": "Federated Hybrid Training and Self-Adversarial Distillation: Towards\n  Robust Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Hybrid Training and Self-Adversarial Distillation: Towards\n  Robust Edge Networks"
                },
                "summary": "Federated learning (FL) is a distributed training technology that enhances\ndata privacy in mobile edge networks by allowing data owners to collaborate\nwithout transmitting raw data to the edge server. However, data heterogeneity\nand adversarial attacks pose challenges to develop an unbiased and robust\nglobal model for edge deployment. To address this, we propose Federated hyBrid\nAdversarial training and self-adversarial disTillation (FedBAT), a new\nframework designed to improve both robustness and generalization of the global\nmodel. FedBAT seamlessly integrates hybrid adversarial training and\nself-adversarial distillation into the conventional FL framework from data\naugmentation and feature distillation perspectives. From a data augmentation\nperspective, we propose hybrid adversarial training to defend against\nadversarial attacks by balancing accuracy and robustness through a weighted\ncombination of standard and adversarial training. From a feature distillation\nperspective, we introduce a novel augmentation-invariant adversarial\ndistillation method that aligns local adversarial features of augmented images\nwith their corresponding unbiased global clean features. This alignment can\neffectively mitigate bias from data heterogeneity while enhancing both the\nrobustness and generalization of the global model. Extensive experimental\nresults across multiple datasets demonstrate that FedBAT yields comparable or\nsuperior performance gains in improving robustness while maintaining accuracy\ncompared to several baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated learning (FL) is a distributed training technology that enhances\ndata privacy in mobile edge networks by allowing data owners to collaborate\nwithout transmitting raw data to the edge server. However, data heterogeneity\nand adversarial attacks pose challenges to develop an unbiased and robust\nglobal model for edge deployment. To address this, we propose Federated hyBrid\nAdversarial training and self-adversarial disTillation (FedBAT), a new\nframework designed to improve both robustness and generalization of the global\nmodel. FedBAT seamlessly integrates hybrid adversarial training and\nself-adversarial distillation into the conventional FL framework from data\naugmentation and feature distillation perspectives. From a data augmentation\nperspective, we propose hybrid adversarial training to defend against\nadversarial attacks by balancing accuracy and robustness through a weighted\ncombination of standard and adversarial training. From a feature distillation\nperspective, we introduce a novel augmentation-invariant adversarial\ndistillation method that aligns local adversarial features of augmented images\nwith their corresponding unbiased global clean features. This alignment can\neffectively mitigate bias from data heterogeneity while enhancing both the\nrobustness and generalization of the global model. Extensive experimental\nresults across multiple datasets demonstrate that FedBAT yields comparable or\nsuperior performance gains in improving robustness while maintaining accuracy\ncompared to several baselines."
                },
                "authors": [
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Apurba Adhikary"
                    },
                    {
                        "name": "Kitae Kim"
                    },
                    {
                        "name": "Eui-Nam Huh"
                    },
                    {
                        "name": "Zhu Han"
                    },
                    {
                        "name": "Choong Seon Hong"
                    }
                ],
                "author_detail": {
                    "name": "Choong Seon Hong"
                },
                "author": "Choong Seon Hong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19354v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19354v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10935v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10935v2",
                "updated": "2024-12-26T21:12:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    21,
                    12,
                    31,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-14T19:06:01Z",
                "published_parsed": [
                    2024,
                    12,
                    14,
                    19,
                    6,
                    1,
                    5,
                    349,
                    0
                ],
                "title": "Progressive Compression with Universally Quantized Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Progressive Compression with Universally Quantized Diffusion Models"
                },
                "summary": "Diffusion probabilistic models have achieved mainstream success in many\ngenerative modeling tasks, from image generation to inverse problem solving. A\ndistinct feature of these models is that they correspond to deep hierarchical\nlatent variable models optimizing a variational evidence lower bound (ELBO) on\nthe data likelihood. Drawing on a basic connection between likelihood modeling\nand compression, we explore the potential of diffusion models for progressive\ncoding, resulting in a sequence of bits that can be incrementally transmitted\nand decoded with progressively improving reconstruction quality. Unlike prior\nwork based on Gaussian diffusion or conditional diffusion models, we propose a\nnew form of diffusion model with uniform noise in the forward process, whose\nnegative ELBO corresponds to the end-to-end compression cost using universal\nquantization. We obtain promising first results on image compression, achieving\ncompetitive rate-distortion and rate-realism results on a wide range of\nbit-rates with a single model, bringing neural codecs a step closer to\npractical deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion probabilistic models have achieved mainstream success in many\ngenerative modeling tasks, from image generation to inverse problem solving. A\ndistinct feature of these models is that they correspond to deep hierarchical\nlatent variable models optimizing a variational evidence lower bound (ELBO) on\nthe data likelihood. Drawing on a basic connection between likelihood modeling\nand compression, we explore the potential of diffusion models for progressive\ncoding, resulting in a sequence of bits that can be incrementally transmitted\nand decoded with progressively improving reconstruction quality. Unlike prior\nwork based on Gaussian diffusion or conditional diffusion models, we propose a\nnew form of diffusion model with uniform noise in the forward process, whose\nnegative ELBO corresponds to the end-to-end compression cost using universal\nquantization. We obtain promising first results on image compression, achieving\ncompetitive rate-distortion and rate-realism results on a wide range of\nbit-rates with a single model, bringing neural codecs a step closer to\npractical deployment."
                },
                "authors": [
                    {
                        "name": "Yibo Yang"
                    },
                    {
                        "name": "Justus C. Will"
                    },
                    {
                        "name": "Stephan Mandt"
                    }
                ],
                "author_detail": {
                    "name": "Stephan Mandt"
                },
                "author": "Stephan Mandt",
                "arxiv_comment": "20 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10935v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10935v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.19619v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.19619v2",
                "updated": "2024-12-26T20:04:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    20,
                    4,
                    21,
                    3,
                    361,
                    0
                ],
                "published": "2023-10-30T15:12:09Z",
                "published_parsed": [
                    2023,
                    10,
                    30,
                    15,
                    12,
                    9,
                    0,
                    303,
                    0
                ],
                "title": "Towards A Holistic Landscape of Situated Theory of Mind in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards A Holistic Landscape of Situated Theory of Mind in Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) have generated considerable interest and debate\nregarding their potential emergence of Theory of Mind (ToM). Several recent\ninquiries reveal a lack of robust ToM in these models and pose a pressing\ndemand to develop new benchmarks, as current ones primarily focus on different\naspects of ToM and are prone to shortcuts and data leakage. In this position\npaper, we seek to answer two road-blocking questions: (1) How can we taxonomize\na holistic landscape of machine ToM? (2) What is a more effective evaluation\nprotocol for machine ToM? Following psychological studies, we taxonomize\nmachine ToM into 7 mental state categories and delineate existing benchmarks to\nidentify under-explored aspects of ToM. We argue for a holistic and situated\nevaluation of ToM to break ToM into individual components and treat LLMs as an\nagent who is physically situated in environments and socially situated in\ninteractions with humans. Such situated evaluation provides a more\ncomprehensive assessment of mental states and potentially mitigates the risk of\nshortcuts and data leakage. We further present a pilot study in a grid world\nsetup as a proof of concept. We hope this position paper can facilitate future\nresearch to integrate ToM with LLMs and offer an intuitive means for\nresearchers to better position their work in the landscape of ToM. Project\npage: https://github.com/Mars-tin/awesome-theory-of-mind",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have generated considerable interest and debate\nregarding their potential emergence of Theory of Mind (ToM). Several recent\ninquiries reveal a lack of robust ToM in these models and pose a pressing\ndemand to develop new benchmarks, as current ones primarily focus on different\naspects of ToM and are prone to shortcuts and data leakage. In this position\npaper, we seek to answer two road-blocking questions: (1) How can we taxonomize\na holistic landscape of machine ToM? (2) What is a more effective evaluation\nprotocol for machine ToM? Following psychological studies, we taxonomize\nmachine ToM into 7 mental state categories and delineate existing benchmarks to\nidentify under-explored aspects of ToM. We argue for a holistic and situated\nevaluation of ToM to break ToM into individual components and treat LLMs as an\nagent who is physically situated in environments and socially situated in\ninteractions with humans. Such situated evaluation provides a more\ncomprehensive assessment of mental states and potentially mitigates the risk of\nshortcuts and data leakage. We further present a pilot study in a grid world\nsetup as a proof of concept. We hope this position paper can facilitate future\nresearch to integrate ToM with LLMs and offer an intuitive means for\nresearchers to better position their work in the landscape of ToM. Project\npage: https://github.com/Mars-tin/awesome-theory-of-mind"
                },
                "authors": [
                    {
                        "name": "Ziqiao Ma"
                    },
                    {
                        "name": "Jacob Sansom"
                    },
                    {
                        "name": "Run Peng"
                    },
                    {
                        "name": "Joyce Chai"
                    }
                ],
                "author_detail": {
                    "name": "Joyce Chai"
                },
                "author": "Joyce Chai",
                "arxiv_comment": "EMNLP 2023 (Findings)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.19619v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.19619v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15188v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15188v2",
                "updated": "2024-12-26T18:56:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    18,
                    56,
                    18,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-19T18:56:24Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    56,
                    24,
                    3,
                    354,
                    0
                ],
                "title": "LMFusion: Adapting Pretrained Language Models for Multimodal Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LMFusion: Adapting Pretrained Language Models for Multimodal Generation"
                },
                "summary": "We present LMFusion, a framework for empowering pretrained text-only large\nlanguage models (LLMs) with multimodal generative capabilities, enabling them\nto understand and generate both text and images in arbitrary sequences.\nLMFusion leverages existing Llama-3's weights for processing texts\nautoregressively while introducing additional and parallel transformer modules\nfor processing images with diffusion. During training, the data from each\nmodality is routed to its dedicated modules: modality-specific feedforward\nlayers, query-key-value projections, and normalization layers process each\nmodality independently, while the shared self-attention layers allow\ninteractions across text and image features. By freezing the text-specific\nmodules and only training the image-specific modules, LMFusion preserves the\nlanguage capabilities of text-only LLMs while developing strong visual\nunderstanding and generation abilities. Compared to methods that pretrain\nmultimodal generative models from scratch, our experiments demonstrate that,\nLMFusion improves image understanding by 20% and image generation by 3.6% using\nonly 50% of the FLOPs while maintaining Llama-3's language capabilities. We\nalso demonstrate that this framework can adapt existing vision-language models\nwith multimodal generation ability. Overall, this framework not only leverages\nexisting computational investments in text-only LLMs but also enables the\nparallel development of language and vision capabilities, presenting a\npromising direction for efficient multimodal model development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present LMFusion, a framework for empowering pretrained text-only large\nlanguage models (LLMs) with multimodal generative capabilities, enabling them\nto understand and generate both text and images in arbitrary sequences.\nLMFusion leverages existing Llama-3's weights for processing texts\nautoregressively while introducing additional and parallel transformer modules\nfor processing images with diffusion. During training, the data from each\nmodality is routed to its dedicated modules: modality-specific feedforward\nlayers, query-key-value projections, and normalization layers process each\nmodality independently, while the shared self-attention layers allow\ninteractions across text and image features. By freezing the text-specific\nmodules and only training the image-specific modules, LMFusion preserves the\nlanguage capabilities of text-only LLMs while developing strong visual\nunderstanding and generation abilities. Compared to methods that pretrain\nmultimodal generative models from scratch, our experiments demonstrate that,\nLMFusion improves image understanding by 20% and image generation by 3.6% using\nonly 50% of the FLOPs while maintaining Llama-3's language capabilities. We\nalso demonstrate that this framework can adapt existing vision-language models\nwith multimodal generation ability. Overall, this framework not only leverages\nexisting computational investments in text-only LLMs but also enables the\nparallel development of language and vision capabilities, presenting a\npromising direction for efficient multimodal model development."
                },
                "authors": [
                    {
                        "name": "Weijia Shi"
                    },
                    {
                        "name": "Xiaochuang Han"
                    },
                    {
                        "name": "Chunting Zhou"
                    },
                    {
                        "name": "Weixin Liang"
                    },
                    {
                        "name": "Xi Victoria Lin"
                    },
                    {
                        "name": "Luke Zettlemoyer"
                    },
                    {
                        "name": "Lili Yu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Yu"
                },
                "author": "Lili Yu",
                "arxiv_comment": "Name change: LlamaFusion to LMFusion",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15188v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15188v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.13168v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.13168v4",
                "updated": "2024-12-26T18:54:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    18,
                    54,
                    53,
                    3,
                    361,
                    0
                ],
                "published": "2023-05-22T15:56:44Z",
                "published_parsed": [
                    2023,
                    5,
                    22,
                    15,
                    56,
                    44,
                    0,
                    142,
                    0
                ],
                "title": "LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities\n  and Future Opportunities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities\n  and Future Opportunities"
                },
                "summary": "This paper presents an exhaustive quantitative and qualitative evaluation of\nLarge Language Models (LLMs) for Knowledge Graph (KG) construction and\nreasoning. We engage in experiments across eight diverse datasets, focusing on\nfour representative tasks encompassing entity and relation extraction, event\nextraction, link prediction, and question-answering, thereby thoroughly\nexploring LLMs' performance in the domain of construction and inference.\nEmpirically, our findings suggest that LLMs, represented by GPT-4, are more\nsuited as inference assistants rather than few-shot information extractors.\nSpecifically, while GPT-4 exhibits good performance in tasks related to KG\nconstruction, it excels further in reasoning tasks, surpassing fine-tuned\nmodels in certain cases. Moreover, our investigation extends to the potential\ngeneralization ability of LLMs for information extraction, leading to the\nproposition of a Virtual Knowledge Extraction task and the development of the\ncorresponding VINE dataset. Based on these empirical findings, we further\npropose AutoKG, a multi-agent-based approach employing LLMs and external\nsources for KG construction and reasoning. We anticipate that this research can\nprovide invaluable insights for future undertakings in the field of knowledge\ngraphs. The code and datasets are in https://github.com/zjunlp/AutoKG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an exhaustive quantitative and qualitative evaluation of\nLarge Language Models (LLMs) for Knowledge Graph (KG) construction and\nreasoning. We engage in experiments across eight diverse datasets, focusing on\nfour representative tasks encompassing entity and relation extraction, event\nextraction, link prediction, and question-answering, thereby thoroughly\nexploring LLMs' performance in the domain of construction and inference.\nEmpirically, our findings suggest that LLMs, represented by GPT-4, are more\nsuited as inference assistants rather than few-shot information extractors.\nSpecifically, while GPT-4 exhibits good performance in tasks related to KG\nconstruction, it excels further in reasoning tasks, surpassing fine-tuned\nmodels in certain cases. Moreover, our investigation extends to the potential\ngeneralization ability of LLMs for information extraction, leading to the\nproposition of a Virtual Knowledge Extraction task and the development of the\ncorresponding VINE dataset. Based on these empirical findings, we further\npropose AutoKG, a multi-agent-based approach employing LLMs and external\nsources for KG construction and reasoning. We anticipate that this research can\nprovide invaluable insights for future undertakings in the field of knowledge\ngraphs. The code and datasets are in https://github.com/zjunlp/AutoKG."
                },
                "authors": [
                    {
                        "name": "Yuqi Zhu"
                    },
                    {
                        "name": "Xiaohan Wang"
                    },
                    {
                        "name": "Jing Chen"
                    },
                    {
                        "name": "Shuofei Qiao"
                    },
                    {
                        "name": "Yixin Ou"
                    },
                    {
                        "name": "Yunzhi Yao"
                    },
                    {
                        "name": "Shumin Deng"
                    },
                    {
                        "name": "Huajun Chen"
                    },
                    {
                        "name": "Ningyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ningyu Zhang"
                },
                "author": "Ningyu Zhang",
                "arxiv_comment": "World Wide Web Journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.13168v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.13168v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19312v1",
                "updated": "2024-12-26T18:19:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    18,
                    19,
                    53,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T18:19:53Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    18,
                    19,
                    53,
                    3,
                    361,
                    0
                ],
                "title": "From Interets to Insights: An LLM Approach to Course Recommendations\n  Using Natural Language Queries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Interets to Insights: An LLM Approach to Course Recommendations\n  Using Natural Language Queries"
                },
                "summary": "Most universities in the United States encourage their students to explore\nacademic areas before declaring a major and to acquire academic breadth by\nsatisfying a variety of requirements. Each term, students must choose among\nmany thousands of offerings, spanning dozens of subject areas, a handful of\ncourses to take. The curricular environment is also dynamic, and poor\ncommunication and search functions on campus can limit a student's ability to\ndiscover new courses of interest. To support both students and their advisers\nin such a setting, we explore a novel Large Language Model (LLM) course\nrecommendation system that applies a Retrieval Augmented Generation (RAG)\nmethod to the corpus of course descriptions. The system first generates an\n'ideal' course description based on the user's query. This description is\nconverted into a search vector using embeddings, which is then used to find\nactual courses with similar content by comparing embedding similarities. We\ndescribe the method and assess the quality and fairness of some example\nprompts. Steps to deploy a pilot system on campus are discussed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most universities in the United States encourage their students to explore\nacademic areas before declaring a major and to acquire academic breadth by\nsatisfying a variety of requirements. Each term, students must choose among\nmany thousands of offerings, spanning dozens of subject areas, a handful of\ncourses to take. The curricular environment is also dynamic, and poor\ncommunication and search functions on campus can limit a student's ability to\ndiscover new courses of interest. To support both students and their advisers\nin such a setting, we explore a novel Large Language Model (LLM) course\nrecommendation system that applies a Retrieval Augmented Generation (RAG)\nmethod to the corpus of course descriptions. The system first generates an\n'ideal' course description based on the user's query. This description is\nconverted into a search vector using embeddings, which is then used to find\nactual courses with similar content by comparing embedding similarities. We\ndescribe the method and assess the quality and fairness of some example\nprompts. Steps to deploy a pilot system on campus are discussed."
                },
                "authors": [
                    {
                        "name": "Hugh Van Deventer"
                    },
                    {
                        "name": "Mark Mills"
                    },
                    {
                        "name": "August Evrard"
                    }
                ],
                "author_detail": {
                    "name": "August Evrard"
                },
                "author": "August Evrard",
                "arxiv_comment": "17 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19311v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19311v1",
                "updated": "2024-12-26T18:19:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    18,
                    19,
                    4,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T18:19:04Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    18,
                    19,
                    4,
                    3,
                    361,
                    0
                ],
                "title": "xSRL: Safety-Aware Explainable Reinforcement Learning -- Safety as a\n  Product of Explainability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xSRL: Safety-Aware Explainable Reinforcement Learning -- Safety as a\n  Product of Explainability"
                },
                "summary": "Reinforcement learning (RL) has shown great promise in simulated\nenvironments, such as games, where failures have minimal consequences. However,\nthe deployment of RL agents in real-world systems such as autonomous vehicles,\nrobotics, UAVs, and medical devices demands a higher level of safety and\ntransparency, particularly when facing adversarial threats. Safe RL algorithms\nhave been developed to address these concerns by optimizing both task\nperformance and safety constraints. However, errors are inevitable, and when\nthey occur, it is essential that the RL agents can also explain their actions\nto human operators. This makes trust in the safety mechanisms of RL systems\ncrucial for effective deployment. Explainability plays a key role in building\nthis trust by providing clear, actionable insights into the agent's\ndecision-making process, ensuring that safety-critical decisions are well\nunderstood. While machine learning (ML) has seen significant advances in\ninterpretability and visualization, explainability methods for RL remain\nlimited. Current tools fail to address the dynamic, sequential nature of RL and\nits needs to balance task performance with safety constraints over time. The\nre-purposing of traditional ML methods, such as saliency maps, is inadequate\nfor safety-critical RL applications where mistakes can result in severe\nconsequences. To bridge this gap, we propose xSRL, a framework that integrates\nboth local and global explanations to provide a comprehensive understanding of\nRL agents' behavior. xSRL also enables developers to identify policy\nvulnerabilities through adversarial attacks, offering tools to debug and patch\nagents without retraining. Our experiments and user studies demonstrate xSRL's\neffectiveness in increasing safety in RL systems, making them more reliable and\ntrustworthy for real-world deployment. Code is available at\nhttps://github.com/risal-shefin/xSRL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has shown great promise in simulated\nenvironments, such as games, where failures have minimal consequences. However,\nthe deployment of RL agents in real-world systems such as autonomous vehicles,\nrobotics, UAVs, and medical devices demands a higher level of safety and\ntransparency, particularly when facing adversarial threats. Safe RL algorithms\nhave been developed to address these concerns by optimizing both task\nperformance and safety constraints. However, errors are inevitable, and when\nthey occur, it is essential that the RL agents can also explain their actions\nto human operators. This makes trust in the safety mechanisms of RL systems\ncrucial for effective deployment. Explainability plays a key role in building\nthis trust by providing clear, actionable insights into the agent's\ndecision-making process, ensuring that safety-critical decisions are well\nunderstood. While machine learning (ML) has seen significant advances in\ninterpretability and visualization, explainability methods for RL remain\nlimited. Current tools fail to address the dynamic, sequential nature of RL and\nits needs to balance task performance with safety constraints over time. The\nre-purposing of traditional ML methods, such as saliency maps, is inadequate\nfor safety-critical RL applications where mistakes can result in severe\nconsequences. To bridge this gap, we propose xSRL, a framework that integrates\nboth local and global explanations to provide a comprehensive understanding of\nRL agents' behavior. xSRL also enables developers to identify policy\nvulnerabilities through adversarial attacks, offering tools to debug and patch\nagents without retraining. Our experiments and user studies demonstrate xSRL's\neffectiveness in increasing safety in RL systems, making them more reliable and\ntrustworthy for real-world deployment. Code is available at\nhttps://github.com/risal-shefin/xSRL."
                },
                "authors": [
                    {
                        "name": "Risal Shahriar Shefin"
                    },
                    {
                        "name": "Md Asifur Rahman"
                    },
                    {
                        "name": "Thai Le"
                    },
                    {
                        "name": "Sarra Alqahtani"
                    }
                ],
                "author_detail": {
                    "name": "Sarra Alqahtani"
                },
                "author": "Sarra Alqahtani",
                "arxiv_comment": "Accepted to 24th International Conference on Autonomous Agents and\n  Multiagent Systems (AAMAS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19311v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19311v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19304v1",
                "updated": "2024-12-26T17:53:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    17,
                    53,
                    14,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T17:53:14Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    17,
                    53,
                    14,
                    3,
                    361,
                    0
                ],
                "title": "Perceive, Query & Reason: Enhancing Video QA with Question-Guided\n  Temporal Queries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perceive, Query & Reason: Enhancing Video QA with Question-Guided\n  Temporal Queries"
                },
                "summary": "Video Question Answering (Video QA) is a challenging video understanding task\nthat requires models to comprehend entire videos, identify the most relevant\ninformation based on contextual cues from a given question, and reason\naccurately to provide answers. Recent advancements in Multimodal Large Language\nModels (MLLMs) have transformed video QA by leveraging their exceptional\ncommonsense reasoning capabilities. This progress is largely driven by the\neffective alignment between visual data and the language space of MLLMs.\nHowever, for video QA, an additional space-time alignment poses a considerable\nchallenge for extracting question-relevant information across frames. In this\nwork, we investigate diverse temporal modeling techniques to integrate with\nMLLMs, aiming to achieve question-guided temporal modeling that leverages\npre-trained visual and textual alignment in MLLMs. We propose T-Former, a novel\ntemporal modeling method that creates a question-guided temporal bridge between\nframe-wise visual perception and the reasoning capabilities of LLMs. Our\nevaluation across multiple video QA benchmarks demonstrates that T-Former\ncompetes favorably with existing temporal modeling approaches and aligns with\nrecent advancements in video QA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Question Answering (Video QA) is a challenging video understanding task\nthat requires models to comprehend entire videos, identify the most relevant\ninformation based on contextual cues from a given question, and reason\naccurately to provide answers. Recent advancements in Multimodal Large Language\nModels (MLLMs) have transformed video QA by leveraging their exceptional\ncommonsense reasoning capabilities. This progress is largely driven by the\neffective alignment between visual data and the language space of MLLMs.\nHowever, for video QA, an additional space-time alignment poses a considerable\nchallenge for extracting question-relevant information across frames. In this\nwork, we investigate diverse temporal modeling techniques to integrate with\nMLLMs, aiming to achieve question-guided temporal modeling that leverages\npre-trained visual and textual alignment in MLLMs. We propose T-Former, a novel\ntemporal modeling method that creates a question-guided temporal bridge between\nframe-wise visual perception and the reasoning capabilities of LLMs. Our\nevaluation across multiple video QA benchmarks demonstrates that T-Former\ncompetes favorably with existing temporal modeling approaches and aligns with\nrecent advancements in video QA."
                },
                "authors": [
                    {
                        "name": "Roberto Amoroso"
                    },
                    {
                        "name": "Gengyuan Zhang"
                    },
                    {
                        "name": "Rajat Koner"
                    },
                    {
                        "name": "Lorenzo Baraldi"
                    },
                    {
                        "name": "Rita Cucchiara"
                    },
                    {
                        "name": "Volker Tresp"
                    }
                ],
                "author_detail": {
                    "name": "Volker Tresp"
                },
                "author": "Volker Tresp",
                "arxiv_comment": "WACV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19291v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19291v1",
                "updated": "2024-12-26T17:34:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    17,
                    34,
                    26,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T17:34:26Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    17,
                    34,
                    26,
                    3,
                    361,
                    0
                ],
                "title": "RAG with Differential Privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAG with Differential Privacy"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has emerged as the dominant technique to\nprovide *Large Language Models* (LLM) with fresh and relevant context,\nmitigating the risk of hallucinations and improving the overall quality of\nresponses in environments with large and fast moving knowledge bases. However,\nthe integration of external documents into the generation process raises\nsignificant privacy concerns. Indeed, when added to a prompt, it is not\npossible to guarantee a response will not inadvertently expose confidential\ndata, leading to potential breaches of privacy and ethical dilemmas. This paper\nexplores a practical solution to this problem suitable to general knowledge\nextraction from personal data. It shows *differentially private token\ngeneration* is a viable approach to private RAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has emerged as the dominant technique to\nprovide *Large Language Models* (LLM) with fresh and relevant context,\nmitigating the risk of hallucinations and improving the overall quality of\nresponses in environments with large and fast moving knowledge bases. However,\nthe integration of external documents into the generation process raises\nsignificant privacy concerns. Indeed, when added to a prompt, it is not\npossible to guarantee a response will not inadvertently expose confidential\ndata, leading to potential breaches of privacy and ethical dilemmas. This paper\nexplores a practical solution to this problem suitable to general knowledge\nextraction from personal data. It shows *differentially private token\ngeneration* is a viable approach to private RAG."
                },
                "authors": [
                    {
                        "name": "Nicolas Grislain"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Grislain"
                },
                "author": "Nicolas Grislain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19291v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15648v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15648v2",
                "updated": "2024-12-26T17:00:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    17,
                    0,
                    51,
                    3,
                    361,
                    0
                ],
                "published": "2024-03-22T23:12:28Z",
                "published_parsed": [
                    2024,
                    3,
                    22,
                    23,
                    12,
                    28,
                    4,
                    82,
                    0
                ],
                "title": "SRLM: Human-in-Loop Interactive Social Robot Navigation with Large\n  Language Model and Deep Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SRLM: Human-in-Loop Interactive Social Robot Navigation with Large\n  Language Model and Deep Reinforcement Learning"
                },
                "summary": "An interactive social robotic assistant must provide services in complex and\ncrowded spaces while adapting its behavior based on real-time human language\ncommands or feedback. In this paper, we propose a novel hybrid approach called\nSocial Robot Planner (SRLM), which integrates Large Language Models (LLM) and\nDeep Reinforcement Learning (DRL) to navigate through human-filled public\nspaces and provide multiple social services. SRLM infers global planning from\nhuman-in-loop commands in real-time, and encodes social information into a\nLLM-based large navigation model (LNM) for low-level motion execution.\nMoreover, a DRL-based planner is designed to maintain benchmarking performance,\nwhich is blended with LNM by a large feedback model (LFM) to address the\ninstability of current text and LLM-driven LNM. Finally, SRLM demonstrates\noutstanding performance in extensive experiments. More details about this work\nare available at: https://sites.google.com/view/navi-srlm",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An interactive social robotic assistant must provide services in complex and\ncrowded spaces while adapting its behavior based on real-time human language\ncommands or feedback. In this paper, we propose a novel hybrid approach called\nSocial Robot Planner (SRLM), which integrates Large Language Models (LLM) and\nDeep Reinforcement Learning (DRL) to navigate through human-filled public\nspaces and provide multiple social services. SRLM infers global planning from\nhuman-in-loop commands in real-time, and encodes social information into a\nLLM-based large navigation model (LNM) for low-level motion execution.\nMoreover, a DRL-based planner is designed to maintain benchmarking performance,\nwhich is blended with LNM by a large feedback model (LFM) to address the\ninstability of current text and LLM-driven LNM. Finally, SRLM demonstrates\noutstanding performance in extensive experiments. More details about this work\nare available at: https://sites.google.com/view/navi-srlm"
                },
                "authors": [
                    {
                        "name": "Weizheng Wang"
                    },
                    {
                        "name": "Ike Obi"
                    },
                    {
                        "name": "Byung-Cheol Min"
                    }
                ],
                "author_detail": {
                    "name": "Byung-Cheol Min"
                },
                "author": "Byung-Cheol Min",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15648v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15648v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19260v1",
                "updated": "2024-12-26T15:54:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    15,
                    54,
                    10,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T15:54:10Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    15,
                    54,
                    10,
                    3,
                    361,
                    0
                ],
                "title": "MEDEC: A Benchmark for Medical Error Detection and Correction in\n  Clinical Notes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEDEC: A Benchmark for Medical Error Detection and Correction in\n  Clinical Notes"
                },
                "summary": "Several studies showed that Large Language Models (LLMs) can answer medical\nquestions correctly, even outperforming the average human score in some medical\nexams. However, to our knowledge, no study has been conducted to assess the\nability of language models to validate existing or generated medical text for\ncorrectness and consistency. In this paper, we introduce MEDEC\n(https://github.com/abachaa/MEDEC), the first publicly available benchmark for\nmedical error detection and correction in clinical notes, covering five types\nof errors (Diagnosis, Management, Treatment, Pharmacotherapy, and Causal\nOrganism). MEDEC consists of 3,848 clinical texts, including 488 clinical notes\nfrom three US hospital systems that were not previously seen by any LLM. The\ndataset has been used for the MEDIQA-CORR shared task to evaluate seventeen\nparticipating systems [Ben Abacha et al., 2024]. In this paper, we describe the\ndata creation methods and we evaluate recent LLMs (e.g., o1-preview, GPT-4,\nClaude 3.5 Sonnet, and Gemini 2.0 Flash) for the tasks of detecting and\ncorrecting medical errors requiring both medical knowledge and reasoning\ncapabilities. We also conducted a comparative study where two medical doctors\nperformed the same task on the MEDEC test set. The results showed that MEDEC is\na sufficiently challenging benchmark to assess the ability of models to\nvalidate existing or generated notes and to correct medical errors. We also\nfound that although recent LLMs have a good performance in error detection and\ncorrection, they are still outperformed by medical doctors in these tasks. We\ndiscuss the potential factors behind this gap, the insights from our\nexperiments, the limitations of current evaluation metrics, and share potential\npointers for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several studies showed that Large Language Models (LLMs) can answer medical\nquestions correctly, even outperforming the average human score in some medical\nexams. However, to our knowledge, no study has been conducted to assess the\nability of language models to validate existing or generated medical text for\ncorrectness and consistency. In this paper, we introduce MEDEC\n(https://github.com/abachaa/MEDEC), the first publicly available benchmark for\nmedical error detection and correction in clinical notes, covering five types\nof errors (Diagnosis, Management, Treatment, Pharmacotherapy, and Causal\nOrganism). MEDEC consists of 3,848 clinical texts, including 488 clinical notes\nfrom three US hospital systems that were not previously seen by any LLM. The\ndataset has been used for the MEDIQA-CORR shared task to evaluate seventeen\nparticipating systems [Ben Abacha et al., 2024]. In this paper, we describe the\ndata creation methods and we evaluate recent LLMs (e.g., o1-preview, GPT-4,\nClaude 3.5 Sonnet, and Gemini 2.0 Flash) for the tasks of detecting and\ncorrecting medical errors requiring both medical knowledge and reasoning\ncapabilities. We also conducted a comparative study where two medical doctors\nperformed the same task on the MEDEC test set. The results showed that MEDEC is\na sufficiently challenging benchmark to assess the ability of models to\nvalidate existing or generated notes and to correct medical errors. We also\nfound that although recent LLMs have a good performance in error detection and\ncorrection, they are still outperformed by medical doctors in these tasks. We\ndiscuss the potential factors behind this gap, the insights from our\nexperiments, the limitations of current evaluation metrics, and share potential\npointers for future research."
                },
                "authors": [
                    {
                        "name": "Asma Ben Abacha"
                    },
                    {
                        "name": "Wen-wai Yim"
                    },
                    {
                        "name": "Yujuan Fu"
                    },
                    {
                        "name": "Zhaoyi Sun"
                    },
                    {
                        "name": "Meliha Yetisgen"
                    },
                    {
                        "name": "Fei Xia"
                    },
                    {
                        "name": "Thomas Lin"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Lin"
                },
                "author": "Thomas Lin",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19245v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19245v1",
                "updated": "2024-12-26T15:01:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    15,
                    1,
                    24,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T15:01:24Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    15,
                    1,
                    24,
                    3,
                    361,
                    0
                ],
                "title": "Sentiment trading with large language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sentiment trading with large language models"
                },
                "summary": "We investigate the efficacy of large language models (LLMs) in sentiment\nanalysis of U.S. financial news and their potential in predicting stock market\nreturns. We analyze a dataset comprising 965,375 news articles that span from\nJanuary 1, 2010, to June 30, 2023; we focus on the performance of various LLMs,\nincluding BERT, OPT, FINBERT, and the traditional Loughran-McDonald dictionary\nmodel, which has been a dominant methodology in the finance literature. The\nstudy documents a significant association between LLM scores and subsequent\ndaily stock returns. Specifically, OPT, which is a GPT-3 based LLM, shows the\nhighest accuracy in sentiment prediction with an accuracy of 74.4%, slightly\nahead of BERT (72.5%) and FINBERT (72.2%). In contrast, the Loughran-McDonald\ndictionary model demonstrates considerably lower effectiveness with only 50.1%\naccuracy. Regression analyses highlight a robust positive impact of OPT model\nscores on next-day stock returns, with coefficients of 0.274 and 0.254 in\ndifferent model specifications. BERT and FINBERT also exhibit predictive\nrelevance, though to a lesser extent. Notably, we do not observe a significant\nrelationship between the Loughran-McDonald dictionary model scores and stock\nreturns, challenging the efficacy of this traditional method in the current\nfinancial context. In portfolio performance, the long-short OPT strategy excels\nwith a Sharpe ratio of 3.05, compared to 2.11 for BERT and 2.07 for FINBERT\nlong-short strategies. Strategies based on the Loughran-McDonald dictionary\nyield the lowest Sharpe ratio of 1.23. Our findings emphasize the superior\nperformance of advanced LLMs, especially OPT, in financial market prediction\nand portfolio management, marking a significant shift in the landscape of\nfinancial analysis tools with implications to financial regulation and policy\nanalysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the efficacy of large language models (LLMs) in sentiment\nanalysis of U.S. financial news and their potential in predicting stock market\nreturns. We analyze a dataset comprising 965,375 news articles that span from\nJanuary 1, 2010, to June 30, 2023; we focus on the performance of various LLMs,\nincluding BERT, OPT, FINBERT, and the traditional Loughran-McDonald dictionary\nmodel, which has been a dominant methodology in the finance literature. The\nstudy documents a significant association between LLM scores and subsequent\ndaily stock returns. Specifically, OPT, which is a GPT-3 based LLM, shows the\nhighest accuracy in sentiment prediction with an accuracy of 74.4%, slightly\nahead of BERT (72.5%) and FINBERT (72.2%). In contrast, the Loughran-McDonald\ndictionary model demonstrates considerably lower effectiveness with only 50.1%\naccuracy. Regression analyses highlight a robust positive impact of OPT model\nscores on next-day stock returns, with coefficients of 0.274 and 0.254 in\ndifferent model specifications. BERT and FINBERT also exhibit predictive\nrelevance, though to a lesser extent. Notably, we do not observe a significant\nrelationship between the Loughran-McDonald dictionary model scores and stock\nreturns, challenging the efficacy of this traditional method in the current\nfinancial context. In portfolio performance, the long-short OPT strategy excels\nwith a Sharpe ratio of 3.05, compared to 2.11 for BERT and 2.07 for FINBERT\nlong-short strategies. Strategies based on the Loughran-McDonald dictionary\nyield the lowest Sharpe ratio of 1.23. Our findings emphasize the superior\nperformance of advanced LLMs, especially OPT, in financial market prediction\nand portfolio management, marking a significant shift in the landscape of\nfinancial analysis tools with implications to financial regulation and policy\nanalysis."
                },
                "authors": [
                    {
                        "name": "Kemal Kirtac"
                    },
                    {
                        "name": "Guido Germano"
                    }
                ],
                "author_detail": {
                    "name": "Guido Germano"
                },
                "author": "Guido Germano",
                "arxiv_doi": "10.1016/j.frl.2024.105227",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.frl.2024.105227",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.19245v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19245v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Finance Research Letters, 62, p.105227 (2024)",
                "arxiv_primary_category": {
                    "term": "q-fin.CP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.CP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.PM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.TR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19237v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19237v1",
                "updated": "2024-12-26T14:40:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    14,
                    40,
                    38,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T14:40:38Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    14,
                    40,
                    38,
                    3,
                    361,
                    0
                ],
                "title": "SeaMo: A Multi-Seasonal and Multimodal Remote Sensing Foundation Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SeaMo: A Multi-Seasonal and Multimodal Remote Sensing Foundation Model"
                },
                "summary": "Remote Sensing (RS) data contains a wealth of multi-dimensional information\ncrucial for Earth observation. Owing to its vast volume, diverse sources, and\ntemporal properties, RS data is highly suitable for the development of large\nVisual Foundation Models (VFMs). VFMs act as robust feature extractors,\nlearning from extensive RS data, and are subsequently fine-tuned for deployment\nin various geoscientific tasks. However, current VFMs in the RS domain are\npredominantly pretrained and tailored exclusively for specific characteristics\nof RS imagery, neglecting the potential of utilizing the multi-dimensional\nproperties of RS data. Therefore, in this work, we propose SeaMo, a pioneering\nvisual foundation model that integrates multi-seasonal and multimodal\ninformation in the RS field. SeaMo is designed to harness multiple properties\nof RS data. Within the masked image modeling framework, we employ non-aligned\ncropping techniques to extract spatial properties, use multi-source inputs for\nmultimodal integration, and incorporate temporal-multimodal fusion blocks for\neffective assimilation of multi-seasonal data. SeaMo explicitly models the\nmulti-dimensional properties of RS data, making the model more comprehensive,\nrobust, and versatile. We applied SeaMo to several downstream geoscience tasks,\nwhich demonstrated exceptional performance. Extensive ablation studies were\nconducted to validate the model's superiority.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Remote Sensing (RS) data contains a wealth of multi-dimensional information\ncrucial for Earth observation. Owing to its vast volume, diverse sources, and\ntemporal properties, RS data is highly suitable for the development of large\nVisual Foundation Models (VFMs). VFMs act as robust feature extractors,\nlearning from extensive RS data, and are subsequently fine-tuned for deployment\nin various geoscientific tasks. However, current VFMs in the RS domain are\npredominantly pretrained and tailored exclusively for specific characteristics\nof RS imagery, neglecting the potential of utilizing the multi-dimensional\nproperties of RS data. Therefore, in this work, we propose SeaMo, a pioneering\nvisual foundation model that integrates multi-seasonal and multimodal\ninformation in the RS field. SeaMo is designed to harness multiple properties\nof RS data. Within the masked image modeling framework, we employ non-aligned\ncropping techniques to extract spatial properties, use multi-source inputs for\nmultimodal integration, and incorporate temporal-multimodal fusion blocks for\neffective assimilation of multi-seasonal data. SeaMo explicitly models the\nmulti-dimensional properties of RS data, making the model more comprehensive,\nrobust, and versatile. We applied SeaMo to several downstream geoscience tasks,\nwhich demonstrated exceptional performance. Extensive ablation studies were\nconducted to validate the model's superiority."
                },
                "authors": [
                    {
                        "name": "Xuyang Li"
                    },
                    {
                        "name": "Danfeng Hong"
                    },
                    {
                        "name": "Chenyu Li"
                    },
                    {
                        "name": "Jocelyn Chanussot"
                    }
                ],
                "author_detail": {
                    "name": "Jocelyn Chanussot"
                },
                "author": "Jocelyn Chanussot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19237v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19237v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18116v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18116v2",
                "updated": "2024-12-26T13:52:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    13,
                    52,
                    48,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-24T02:54:56Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    2,
                    54,
                    56,
                    1,
                    359,
                    0
                ],
                "title": "AutoDroid-V2: Boosting SLM-based GUI Agents via Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoDroid-V2: Boosting SLM-based GUI Agents via Code Generation"
                },
                "summary": "Large language models (LLMs) have brought exciting new advances to mobile UI\nagents, a long-standing research field that aims to complete arbitrary natural\nlanguage tasks through mobile UI interactions. However, existing UI agents\nusually demand high reasoning capabilities of powerful large models that are\ndifficult to be deployed locally on end-users' devices, which raises huge\nconcerns about user privacy and centralized serving cost. One way to reduce the\nrequired model size is to customize a smaller domain-specific model with\nhigh-quality training data, e.g. large-scale human demonstrations of diverse\ntypes of apps and tasks, while such datasets are extremely difficult to obtain.\nInspired by the remarkable coding abilities of recent small language models\n(SLMs), we propose to convert the UI task automation problem to a code\ngeneration problem, which can be effectively solved by an on-device SLM and\nefficiently executed with an on-device code interpreter. Unlike normal coding\ntasks that can be extensively pretrained with public datasets, generating UI\nautomation code is challenging due to the diversity, complexity, and\nvariability of target apps. Therefore, we adopt a document-centered approach\nthat automatically builds fine-grained API documentation for each app and\ngenerates diverse task samples based on this documentation. By guiding the\nagent with the synthetic documents and task samples, it learns to generate\nprecise and efficient scripts to complete unseen tasks. Based on detailed\ncomparisons with state-of-the-art mobile UI agents, our approach effectively\nimproves the mobile task automation with significantly higher success rates and\nlower latency/token consumption. Code will be open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have brought exciting new advances to mobile UI\nagents, a long-standing research field that aims to complete arbitrary natural\nlanguage tasks through mobile UI interactions. However, existing UI agents\nusually demand high reasoning capabilities of powerful large models that are\ndifficult to be deployed locally on end-users' devices, which raises huge\nconcerns about user privacy and centralized serving cost. One way to reduce the\nrequired model size is to customize a smaller domain-specific model with\nhigh-quality training data, e.g. large-scale human demonstrations of diverse\ntypes of apps and tasks, while such datasets are extremely difficult to obtain.\nInspired by the remarkable coding abilities of recent small language models\n(SLMs), we propose to convert the UI task automation problem to a code\ngeneration problem, which can be effectively solved by an on-device SLM and\nefficiently executed with an on-device code interpreter. Unlike normal coding\ntasks that can be extensively pretrained with public datasets, generating UI\nautomation code is challenging due to the diversity, complexity, and\nvariability of target apps. Therefore, we adopt a document-centered approach\nthat automatically builds fine-grained API documentation for each app and\ngenerates diverse task samples based on this documentation. By guiding the\nagent with the synthetic documents and task samples, it learns to generate\nprecise and efficient scripts to complete unseen tasks. Based on detailed\ncomparisons with state-of-the-art mobile UI agents, our approach effectively\nimproves the mobile task automation with significantly higher success rates and\nlower latency/token consumption. Code will be open-sourced."
                },
                "authors": [
                    {
                        "name": "Hao Wen"
                    },
                    {
                        "name": "Shizuo Tian"
                    },
                    {
                        "name": "Borislav Pavlov"
                    },
                    {
                        "name": "Wenjie Du"
                    },
                    {
                        "name": "Yixuan Li"
                    },
                    {
                        "name": "Ge Chang"
                    },
                    {
                        "name": "Shanhui Zhao"
                    },
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Yunxin Liu"
                    },
                    {
                        "name": "Ya-Qin Zhang"
                    },
                    {
                        "name": "Yuanchun Li"
                    }
                ],
                "author_detail": {
                    "name": "Yuanchun Li"
                },
                "author": "Yuanchun Li",
                "arxiv_comment": "15 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18116v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18116v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19211v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19211v1",
                "updated": "2024-12-26T13:21:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    13,
                    21,
                    9,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T13:21:09Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    13,
                    21,
                    9,
                    3,
                    361,
                    0
                ],
                "title": "Large Language Models Meet Graph Neural Networks: A Perspective of Graph\n  Mining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Meet Graph Neural Networks: A Perspective of Graph\n  Mining"
                },
                "summary": "Graph mining is an important area in data mining and machine learning that\ninvolves extracting valuable information from graph-structured data. In recent\nyears, significant progress has been made in this field through the development\nof graph neural networks (GNNs). However, GNNs are still deficient in\ngeneralizing to diverse graph data. Aiming to this issue, Large Language Models\n(LLMs) could provide new solutions for graph mining tasks with their superior\nsemantic understanding. In this review, we systematically review the\ncombination and application techniques of LLMs and GNNs and present a novel\ntaxonomy for research in this interdisciplinary field, which involves three\nmain categories: GNN-driving-LLM, LLM-driving-GNN, and GNN-LLM-co-driving.\nWithin this framework, we reveal the capabilities of LLMs in enhancing graph\nfeature extraction as well as improving the effectiveness of downstream tasks\nsuch as node classification, link prediction, and community detection. Although\nLLMs have demonstrated their great potential in handling graph-structured data,\ntheir high computational requirements and complexity remain challenges. Future\nresearch needs to continue to explore how to efficiently fuse LLMs and GNNs to\nachieve more powerful graph learning and reasoning capabilities and provide new\nimpetus for the development of graph mining techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph mining is an important area in data mining and machine learning that\ninvolves extracting valuable information from graph-structured data. In recent\nyears, significant progress has been made in this field through the development\nof graph neural networks (GNNs). However, GNNs are still deficient in\ngeneralizing to diverse graph data. Aiming to this issue, Large Language Models\n(LLMs) could provide new solutions for graph mining tasks with their superior\nsemantic understanding. In this review, we systematically review the\ncombination and application techniques of LLMs and GNNs and present a novel\ntaxonomy for research in this interdisciplinary field, which involves three\nmain categories: GNN-driving-LLM, LLM-driving-GNN, and GNN-LLM-co-driving.\nWithin this framework, we reveal the capabilities of LLMs in enhancing graph\nfeature extraction as well as improving the effectiveness of downstream tasks\nsuch as node classification, link prediction, and community detection. Although\nLLMs have demonstrated their great potential in handling graph-structured data,\ntheir high computational requirements and complexity remain challenges. Future\nresearch needs to continue to explore how to efficiently fuse LLMs and GNNs to\nachieve more powerful graph learning and reasoning capabilities and provide new\nimpetus for the development of graph mining techniques."
                },
                "authors": [
                    {
                        "name": "Yuxin You"
                    },
                    {
                        "name": "Zhen Liu"
                    },
                    {
                        "name": "Xiangchao Wen"
                    },
                    {
                        "name": "Yongtao Zhang"
                    },
                    {
                        "name": "Wei Ai"
                    }
                ],
                "author_detail": {
                    "name": "Wei Ai"
                },
                "author": "Wei Ai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19211v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19211v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.15351v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.15351v2",
                "updated": "2024-12-26T13:14:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    13,
                    14,
                    16,
                    3,
                    361,
                    0
                ],
                "published": "2024-02-23T14:38:19Z",
                "published_parsed": [
                    2024,
                    2,
                    23,
                    14,
                    38,
                    19,
                    4,
                    54,
                    0
                ],
                "title": "AutoMMLab: Automatically Generating Deployable Models from Language\n  Instructions for Computer Vision Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoMMLab: Automatically Generating Deployable Models from Language\n  Instructions for Computer Vision Tasks"
                },
                "summary": "Automated machine learning (AutoML) is a collection of techniques designed to\nautomate the machine learning development process. While traditional AutoML\napproaches have been successfully applied in several critical steps of model\ndevelopment (e.g. hyperparameter optimization), there lacks a AutoML system\nthat automates the entire end-to-end model production workflow for computer\nvision. To fill this blank, we propose a novel request-to-model task, which\ninvolves understanding the user's natural language request and execute the\nentire workflow to output production-ready models. This empowers non-expert\nindividuals to easily build task-specific models via a user-friendly language\ninterface. To facilitate development and evaluation, we develop a new\nexperimental platform called AutoMMLab and a new benchmark called LAMP for\nstudying key components in the end-to-end request-to-model pipeline.\nHyperparameter optimization (HPO) is one of the most important components for\nAutoML. Traditional approaches mostly rely on trial-and-error, leading to\ninefficient parameter search. To solve this problem, we propose a novel\nLLM-based HPO algorithm, called HPO-LLaMA. Equipped with extensive knowledge\nand experience in model hyperparameter tuning, HPO-LLaMA achieves significant\nimprovement of HPO efficiency. Dataset and code are available at\nhttps://github.com/yang-ze-kang/AutoMMLab.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated machine learning (AutoML) is a collection of techniques designed to\nautomate the machine learning development process. While traditional AutoML\napproaches have been successfully applied in several critical steps of model\ndevelopment (e.g. hyperparameter optimization), there lacks a AutoML system\nthat automates the entire end-to-end model production workflow for computer\nvision. To fill this blank, we propose a novel request-to-model task, which\ninvolves understanding the user's natural language request and execute the\nentire workflow to output production-ready models. This empowers non-expert\nindividuals to easily build task-specific models via a user-friendly language\ninterface. To facilitate development and evaluation, we develop a new\nexperimental platform called AutoMMLab and a new benchmark called LAMP for\nstudying key components in the end-to-end request-to-model pipeline.\nHyperparameter optimization (HPO) is one of the most important components for\nAutoML. Traditional approaches mostly rely on trial-and-error, leading to\ninefficient parameter search. To solve this problem, we propose a novel\nLLM-based HPO algorithm, called HPO-LLaMA. Equipped with extensive knowledge\nand experience in model hyperparameter tuning, HPO-LLaMA achieves significant\nimprovement of HPO efficiency. Dataset and code are available at\nhttps://github.com/yang-ze-kang/AutoMMLab."
                },
                "authors": [
                    {
                        "name": "Zekang Yang"
                    },
                    {
                        "name": "Wang Zeng"
                    },
                    {
                        "name": "Sheng Jin"
                    },
                    {
                        "name": "Chen Qian"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Wentao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Liu"
                },
                "author": "Wentao Liu",
                "arxiv_comment": "Accepted by AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.15351v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.15351v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19206v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19206v1",
                "updated": "2024-12-26T13:07:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    13,
                    7,
                    3,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T13:07:03Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    13,
                    7,
                    3,
                    3,
                    361,
                    0
                ],
                "title": "NADER: Neural Architecture Design via Multi-Agent Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NADER: Neural Architecture Design via Multi-Agent Collaboration"
                },
                "summary": "Designing effective neural architectures poses a significant challenge in\ndeep learning. While Neural Architecture Search (NAS) automates the search for\noptimal architectures, existing methods are often constrained by predetermined\nsearch spaces and may miss critical neural architectures. In this paper, we\nintroduce NADER (Neural Architecture Design via multi-agEnt collaboRation), a\nnovel framework that formulates neural architecture design (NAD) as a LLM-based\nmulti-agent collaboration problem. NADER employs a team of specialized agents\nto enhance a base architecture through iterative modification. Current\nLLM-based NAD methods typically operate independently, lacking the ability to\nlearn from past experiences, which results in repeated mistakes and inefficient\nexploration. To address this issue, we propose the Reflector, which effectively\nlearns from immediate feedback and long-term experiences. Additionally, unlike\nprevious LLM-based methods that use code to represent neural architectures, we\nutilize a graph-based representation. This approach allows agents to focus on\ndesign aspects without being distracted by coding. We demonstrate the\neffectiveness of NADER in discovering high-performing architectures beyond\npredetermined search spaces through extensive experiments on benchmark tasks,\nshowcasing its advantages over state-of-the-art methods. The codes will be\nreleased soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing effective neural architectures poses a significant challenge in\ndeep learning. While Neural Architecture Search (NAS) automates the search for\noptimal architectures, existing methods are often constrained by predetermined\nsearch spaces and may miss critical neural architectures. In this paper, we\nintroduce NADER (Neural Architecture Design via multi-agEnt collaboRation), a\nnovel framework that formulates neural architecture design (NAD) as a LLM-based\nmulti-agent collaboration problem. NADER employs a team of specialized agents\nto enhance a base architecture through iterative modification. Current\nLLM-based NAD methods typically operate independently, lacking the ability to\nlearn from past experiences, which results in repeated mistakes and inefficient\nexploration. To address this issue, we propose the Reflector, which effectively\nlearns from immediate feedback and long-term experiences. Additionally, unlike\nprevious LLM-based methods that use code to represent neural architectures, we\nutilize a graph-based representation. This approach allows agents to focus on\ndesign aspects without being distracted by coding. We demonstrate the\neffectiveness of NADER in discovering high-performing architectures beyond\npredetermined search spaces through extensive experiments on benchmark tasks,\nshowcasing its advantages over state-of-the-art methods. The codes will be\nreleased soon."
                },
                "authors": [
                    {
                        "name": "Zekang Yang"
                    },
                    {
                        "name": "Wang Zeng"
                    },
                    {
                        "name": "Sheng Jin"
                    },
                    {
                        "name": "Chen Qian"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Wentao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Liu"
                },
                "author": "Wentao Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19206v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19206v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19078v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19078v2",
                "updated": "2024-12-26T12:56:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    12,
                    56,
                    59,
                    3,
                    361,
                    0
                ],
                "published": "2024-09-27T18:25:54Z",
                "published_parsed": [
                    2024,
                    9,
                    27,
                    18,
                    25,
                    54,
                    4,
                    271,
                    0
                ],
                "title": "Differential privacy enables fair and accurate AI-based analysis of\n  speech disorders while protecting patient data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differential privacy enables fair and accurate AI-based analysis of\n  speech disorders while protecting patient data"
                },
                "summary": "Speech pathology has impacts on communication abilities and quality of life.\nWhile deep learning-based models have shown potential in diagnosing these\ndisorders, the use of sensitive data raises critical privacy concerns. Although\ndifferential privacy (DP) has been explored in the medical imaging domain, its\napplication in pathological speech analysis remains largely unexplored despite\nthe equally critical privacy concerns. This study is the first to investigate\nDP's impact on pathological speech data, focusing on the trade-offs between\nprivacy, diagnostic accuracy, and fairness. Using a large, real-world dataset\nof 200 hours of recordings from 2,839 German-speaking participants, we observed\na maximum accuracy reduction of 3.85% when training with DP with high privacy\nlevels. To highlight real-world privacy risks, we demonstrated the\nvulnerability of non-private models to explicit gradient inversion attacks,\nreconstructing identifiable speech samples and showcasing DP's effectiveness in\nmitigating these risks. To generalize our findings across languages and\ndisorders, we validated our approach on a dataset of Spanish-speaking\nParkinson's disease patients, leveraging pretrained models from healthy\nEnglish-speaking datasets, and demonstrated that careful pretraining on\nlarge-scale task-specific datasets can maintain favorable accuracy under DP\nconstraints. A comprehensive fairness analysis revealed minimal gender bias at\nreasonable privacy levels but underscored the need for addressing age-related\ndisparities. Our results establish that DP can balance privacy and utility in\nspeech disorder detection, while highlighting unique challenges in\nprivacy-fairness trade-offs for speech data. This provides a foundation for\nrefining DP methodologies and improving fairness across diverse patient groups\nin real-world deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speech pathology has impacts on communication abilities and quality of life.\nWhile deep learning-based models have shown potential in diagnosing these\ndisorders, the use of sensitive data raises critical privacy concerns. Although\ndifferential privacy (DP) has been explored in the medical imaging domain, its\napplication in pathological speech analysis remains largely unexplored despite\nthe equally critical privacy concerns. This study is the first to investigate\nDP's impact on pathological speech data, focusing on the trade-offs between\nprivacy, diagnostic accuracy, and fairness. Using a large, real-world dataset\nof 200 hours of recordings from 2,839 German-speaking participants, we observed\na maximum accuracy reduction of 3.85% when training with DP with high privacy\nlevels. To highlight real-world privacy risks, we demonstrated the\nvulnerability of non-private models to explicit gradient inversion attacks,\nreconstructing identifiable speech samples and showcasing DP's effectiveness in\nmitigating these risks. To generalize our findings across languages and\ndisorders, we validated our approach on a dataset of Spanish-speaking\nParkinson's disease patients, leveraging pretrained models from healthy\nEnglish-speaking datasets, and demonstrated that careful pretraining on\nlarge-scale task-specific datasets can maintain favorable accuracy under DP\nconstraints. A comprehensive fairness analysis revealed minimal gender bias at\nreasonable privacy levels but underscored the need for addressing age-related\ndisparities. Our results establish that DP can balance privacy and utility in\nspeech disorder detection, while highlighting unique challenges in\nprivacy-fairness trade-offs for speech data. This provides a foundation for\nrefining DP methodologies and improving fairness across diverse patient groups\nin real-world deployments."
                },
                "authors": [
                    {
                        "name": "Soroosh Tayebi Arasteh"
                    },
                    {
                        "name": "Mahshad Lotfinia"
                    },
                    {
                        "name": "Paula Andrea Perez-Toro"
                    },
                    {
                        "name": "Tomas Arias-Vergara"
                    },
                    {
                        "name": "Mahtab Ranji"
                    },
                    {
                        "name": "Juan Rafael Orozco-Arroyave"
                    },
                    {
                        "name": "Maria Schuster"
                    },
                    {
                        "name": "Andreas Maier"
                    },
                    {
                        "name": "Seung Hee Yang"
                    }
                ],
                "author_detail": {
                    "name": "Seung Hee Yang"
                },
                "author": "Seung Hee Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19078v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19078v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19191v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19191v1",
                "updated": "2024-12-26T12:12:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    12,
                    12,
                    23,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T12:12:23Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    12,
                    12,
                    23,
                    3,
                    361,
                    0
                ],
                "title": "Biology Instructions: A Dataset and Benchmark for Multi-Omics Sequence\n  Understanding Capability of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Biology Instructions: A Dataset and Benchmark for Multi-Omics Sequence\n  Understanding Capability of Large Language Models"
                },
                "summary": "Large language models have already demonstrated their formidable capabilities\nin general domains, ushering in a revolutionary transformation. However,\nexploring and exploiting the extensive knowledge of these models to comprehend\nmulti-omics biology remains underexplored. To fill this research gap, we first\nintroduce Biology-Instructions, the first large-scale multi-omics biological\nsequences-related instruction-tuning dataset including DNA, RNA, proteins, and\nmulti-molecules, designed to bridge the gap between large language models\n(LLMs) and complex biological sequences-related tasks. This dataset can enhance\nthe versatility of LLMs by integrating diverse biological sequenced-based\nprediction tasks with advanced reasoning capabilities, while maintaining\nconversational fluency. Additionally, we reveal significant performance\nlimitations in even state-of-the-art LLMs on biological sequence-related\nmulti-omics tasks without specialized pre-training and instruction-tuning. We\nfurther develop a strong baseline called ChatMultiOmics with a novel\nthree-stage training pipeline, demonstrating the powerful ability to understand\nbiology by using Biology-Instructions. Biology-Instructions and ChatMultiOmics\nare publicly available and crucial resources for enabling more effective\nintegration of LLMs with multi-omics sequence analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have already demonstrated their formidable capabilities\nin general domains, ushering in a revolutionary transformation. However,\nexploring and exploiting the extensive knowledge of these models to comprehend\nmulti-omics biology remains underexplored. To fill this research gap, we first\nintroduce Biology-Instructions, the first large-scale multi-omics biological\nsequences-related instruction-tuning dataset including DNA, RNA, proteins, and\nmulti-molecules, designed to bridge the gap between large language models\n(LLMs) and complex biological sequences-related tasks. This dataset can enhance\nthe versatility of LLMs by integrating diverse biological sequenced-based\nprediction tasks with advanced reasoning capabilities, while maintaining\nconversational fluency. Additionally, we reveal significant performance\nlimitations in even state-of-the-art LLMs on biological sequence-related\nmulti-omics tasks without specialized pre-training and instruction-tuning. We\nfurther develop a strong baseline called ChatMultiOmics with a novel\nthree-stage training pipeline, demonstrating the powerful ability to understand\nbiology by using Biology-Instructions. Biology-Instructions and ChatMultiOmics\nare publicly available and crucial resources for enabling more effective\nintegration of LLMs with multi-omics sequence analysis."
                },
                "authors": [
                    {
                        "name": "Haonan He"
                    },
                    {
                        "name": "Yuchen Ren"
                    },
                    {
                        "name": "Yining Tang"
                    },
                    {
                        "name": "Ziyang Xu"
                    },
                    {
                        "name": "Junxian Li"
                    },
                    {
                        "name": "Minghao Yang"
                    },
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Dong Yuan"
                    },
                    {
                        "name": "Tao Chen"
                    },
                    {
                        "name": "Shufei Zhang"
                    },
                    {
                        "name": "Yuqiang Li"
                    },
                    {
                        "name": "Nanqing Dong"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    },
                    {
                        "name": "Peng Ye"
                    }
                ],
                "author_detail": {
                    "name": "Peng Ye"
                },
                "author": "Peng Ye",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19191v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19191v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.BM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04135v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04135v3",
                "updated": "2024-12-26T11:55:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    11,
                    55,
                    16,
                    3,
                    361,
                    0
                ],
                "published": "2024-05-07T09:04:52Z",
                "published_parsed": [
                    2024,
                    5,
                    7,
                    9,
                    4,
                    52,
                    1,
                    128,
                    0
                ],
                "title": "Human-centric Reward Optimization for Reinforcement Learning-based\n  Automated Driving using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-centric Reward Optimization for Reinforcement Learning-based\n  Automated Driving using Large Language Models"
                },
                "summary": "One of the key challenges in current Reinforcement Learning (RL)-based\nAutomated Driving (AD) agents is achieving flexible, precise, and human-like\nbehavior cost-effectively. This paper introduces an innovative approach that\nuses large language models (LLMs) to intuitively and effectively optimize RL\nreward functions in a human-centric way. We developed a framework where\ninstructions and dynamic environment descriptions are input into the LLM. The\nLLM then utilizes this information to assist in generating rewards, thereby\nsteering the behavior of RL agents towards patterns that more closely resemble\nhuman driving. The experimental results demonstrate that this approach not only\nmakes RL agents more anthropomorphic but also achieves better performance.\nAdditionally, various strategies for reward-proxy and reward-shaping are\ninvestigated, revealing the significant impact of prompt design on shaping an\nAD vehicle's behavior. These findings offer a promising direction for the\ndevelopment of more advanced, human-like automated driving systems. Our\nexperimental data and source code can be found here",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One of the key challenges in current Reinforcement Learning (RL)-based\nAutomated Driving (AD) agents is achieving flexible, precise, and human-like\nbehavior cost-effectively. This paper introduces an innovative approach that\nuses large language models (LLMs) to intuitively and effectively optimize RL\nreward functions in a human-centric way. We developed a framework where\ninstructions and dynamic environment descriptions are input into the LLM. The\nLLM then utilizes this information to assist in generating rewards, thereby\nsteering the behavior of RL agents towards patterns that more closely resemble\nhuman driving. The experimental results demonstrate that this approach not only\nmakes RL agents more anthropomorphic but also achieves better performance.\nAdditionally, various strategies for reward-proxy and reward-shaping are\ninvestigated, revealing the significant impact of prompt design on shaping an\nAD vehicle's behavior. These findings offer a promising direction for the\ndevelopment of more advanced, human-like automated driving systems. Our\nexperimental data and source code can be found here"
                },
                "authors": [
                    {
                        "name": "Ziqi Zhou"
                    },
                    {
                        "name": "Jingyue Zhang"
                    },
                    {
                        "name": "Jingyuan Zhang"
                    },
                    {
                        "name": "Yangfan He"
                    },
                    {
                        "name": "Boyue Wang"
                    },
                    {
                        "name": "Tianyu Shi"
                    },
                    {
                        "name": "Alaa Khamis"
                    }
                ],
                "author_detail": {
                    "name": "Alaa Khamis"
                },
                "author": "Alaa Khamis",
                "arxiv_comment": "9 pages, 6 figures, 34 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.04135v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04135v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17131v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17131v2",
                "updated": "2024-12-26T11:43:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    11,
                    43,
                    37,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-22T18:38:24Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    18,
                    38,
                    24,
                    6,
                    357,
                    0
                ],
                "title": "LLMsAgainstHate @ NLU of Devanagari Script Languages 2025: Hate Speech\n  Detection and Target Identification in Devanagari Languages via Parameter\n  Efficient Fine-Tuning of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMsAgainstHate @ NLU of Devanagari Script Languages 2025: Hate Speech\n  Detection and Target Identification in Devanagari Languages via Parameter\n  Efficient Fine-Tuning of LLMs"
                },
                "summary": "The detection of hate speech has become increasingly important in combating\nonline hostility and its real-world consequences. Despite recent advancements,\nthere is limited research addressing hate speech detection in\nDevanagari-scripted languages, where resources and tools are scarce. While\nlarge language models (LLMs) have shown promise in language-related tasks,\ntraditional fine-tuning approaches are often infeasible given the size of the\nmodels. In this paper, we propose a Parameter Efficient Fine tuning (PEFT)\nbased solution for hate speech detection and target identification. We evaluate\nmultiple LLMs on the Devanagari dataset provided by (Thapa et al., 2025), which\ncontains annotated instances in 2 languages - Hindi and Nepali. The results\ndemonstrate the efficacy of our approach in handling Devanagari-scripted\ncontent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The detection of hate speech has become increasingly important in combating\nonline hostility and its real-world consequences. Despite recent advancements,\nthere is limited research addressing hate speech detection in\nDevanagari-scripted languages, where resources and tools are scarce. While\nlarge language models (LLMs) have shown promise in language-related tasks,\ntraditional fine-tuning approaches are often infeasible given the size of the\nmodels. In this paper, we propose a Parameter Efficient Fine tuning (PEFT)\nbased solution for hate speech detection and target identification. We evaluate\nmultiple LLMs on the Devanagari dataset provided by (Thapa et al., 2025), which\ncontains annotated instances in 2 languages - Hindi and Nepali. The results\ndemonstrate the efficacy of our approach in handling Devanagari-scripted\ncontent."
                },
                "authors": [
                    {
                        "name": "Rushendra Sidibomma"
                    },
                    {
                        "name": "Pransh Patwa"
                    },
                    {
                        "name": "Parth Patwa"
                    },
                    {
                        "name": "Aman Chadha"
                    },
                    {
                        "name": "Vinija Jain"
                    },
                    {
                        "name": "Amitava Das"
                    }
                ],
                "author_detail": {
                    "name": "Amitava Das"
                },
                "author": "Amitava Das",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17131v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17131v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06947v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06947v3",
                "updated": "2024-12-26T11:02:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    11,
                    2,
                    28,
                    3,
                    361,
                    0
                ],
                "published": "2024-06-11T05:21:20Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    5,
                    21,
                    20,
                    1,
                    163,
                    0
                ],
                "title": "CAAP: Context-Aware Action Planning Prompting to Solve Computer Tasks\n  with Front-End UI Only",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAAP: Context-Aware Action Planning Prompting to Solve Computer Tasks\n  with Front-End UI Only"
                },
                "summary": "Software robots have long been used in Robotic Process Automation (RPA) to\nautomate mundane and repetitive computer tasks. With the advent of Large\nLanguage Models (LLMs) and their advanced reasoning capabilities, these agents\nare now able to handle more complex or previously unseen tasks. However,\nLLM-based automation techniques in recent literature frequently rely on HTML\nsource code for input or application-specific API calls for actions, limiting\ntheir applicability to specific environments. We propose an LLM-based agent\nthat mimics human behavior in solving computer tasks. It perceives its\nenvironment solely through screenshot images, which are then converted into\ntext for an LLM to process. By leveraging the reasoning capability of the LLM,\nwe eliminate the need for large-scale human demonstration data typically\nrequired for model training. The agent only executes keyboard and mouse\noperations on Graphical User Interface (GUI), removing the need for\npre-provided APIs to function. To further enhance the agent's performance in\nthis setting, we propose a novel prompting strategy called Context-Aware Action\nPlanning (CAAP) prompting, which enables the agent to thoroughly examine the\ntask context from multiple perspectives. Our agent achieves an average success\nrate of 94.5% on MiniWoB++ and an average task score of 62.3 on WebShop,\noutperforming all previous studies of agents that rely solely on screen images.\nThis method demonstrates potential for broader applications, particularly for\ntasks requiring coordination across multiple applications on desktops or\nsmartphones, marking a significant advancement in the field of automation\nagents. Codes and models are accessible at\nhttps://github.com/caap-agent/caap-agent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software robots have long been used in Robotic Process Automation (RPA) to\nautomate mundane and repetitive computer tasks. With the advent of Large\nLanguage Models (LLMs) and their advanced reasoning capabilities, these agents\nare now able to handle more complex or previously unseen tasks. However,\nLLM-based automation techniques in recent literature frequently rely on HTML\nsource code for input or application-specific API calls for actions, limiting\ntheir applicability to specific environments. We propose an LLM-based agent\nthat mimics human behavior in solving computer tasks. It perceives its\nenvironment solely through screenshot images, which are then converted into\ntext for an LLM to process. By leveraging the reasoning capability of the LLM,\nwe eliminate the need for large-scale human demonstration data typically\nrequired for model training. The agent only executes keyboard and mouse\noperations on Graphical User Interface (GUI), removing the need for\npre-provided APIs to function. To further enhance the agent's performance in\nthis setting, we propose a novel prompting strategy called Context-Aware Action\nPlanning (CAAP) prompting, which enables the agent to thoroughly examine the\ntask context from multiple perspectives. Our agent achieves an average success\nrate of 94.5% on MiniWoB++ and an average task score of 62.3 on WebShop,\noutperforming all previous studies of agents that rely solely on screen images.\nThis method demonstrates potential for broader applications, particularly for\ntasks requiring coordination across multiple applications on desktops or\nsmartphones, marking a significant advancement in the field of automation\nagents. Codes and models are accessible at\nhttps://github.com/caap-agent/caap-agent."
                },
                "authors": [
                    {
                        "name": "Junhee Cho"
                    },
                    {
                        "name": "Jihoon Kim"
                    },
                    {
                        "name": "Daseul Bae"
                    },
                    {
                        "name": "Jinho Choo"
                    },
                    {
                        "name": "Youngjune Gwon"
                    },
                    {
                        "name": "Yeong-Dae Kwon"
                    }
                ],
                "author_detail": {
                    "name": "Yeong-Dae Kwon"
                },
                "author": "Yeong-Dae Kwon",
                "arxiv_comment": "11 pages, 7 figures; (20 pages and 16 figures more in appendix)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06947v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06947v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19139v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19139v1",
                "updated": "2024-12-26T09:51:05Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    9,
                    51,
                    5,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T09:51:05Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    9,
                    51,
                    5,
                    3,
                    361,
                    0
                ],
                "title": "PlanLLM: Video Procedure Planning with Refinable Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PlanLLM: Video Procedure Planning with Refinable Large Language Models"
                },
                "summary": "Video procedure planning, i.e., planning a sequence of action steps given the\nvideo frames of start and goal states, is an essential ability for embodied AI.\nRecent works utilize Large Language Models (LLMs) to generate enriched action\nstep description texts to guide action step decoding. Although LLMs are\nintroduced, these methods decode the action steps into a closed-set of one-hot\nvectors, limiting the model's capability of generalizing to new steps or tasks.\nAdditionally, fixed action step descriptions based on world-level commonsense\nmay contain noise in specific instances of visual states. In this paper, we\npropose PlanLLM, a cross-modal joint learning framework with LLMs for video\nprocedure planning. We propose an LLM-Enhanced Planning module which fully uses\nthe generalization ability of LLMs to produce free-form planning output and to\nenhance action step decoding. We also propose Mutual Information Maximization\nmodule to connect world-level commonsense of step descriptions and\nsample-specific information of visual states, enabling LLMs to employ the\nreasoning ability to generate step sequences. With the assistance of LLMs, our\nmethod can both closed-set and open vocabulary procedure planning tasks. Our\nPlanLLM achieves superior performance on three benchmarks, demonstrating the\neffectiveness of our designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video procedure planning, i.e., planning a sequence of action steps given the\nvideo frames of start and goal states, is an essential ability for embodied AI.\nRecent works utilize Large Language Models (LLMs) to generate enriched action\nstep description texts to guide action step decoding. Although LLMs are\nintroduced, these methods decode the action steps into a closed-set of one-hot\nvectors, limiting the model's capability of generalizing to new steps or tasks.\nAdditionally, fixed action step descriptions based on world-level commonsense\nmay contain noise in specific instances of visual states. In this paper, we\npropose PlanLLM, a cross-modal joint learning framework with LLMs for video\nprocedure planning. We propose an LLM-Enhanced Planning module which fully uses\nthe generalization ability of LLMs to produce free-form planning output and to\nenhance action step decoding. We also propose Mutual Information Maximization\nmodule to connect world-level commonsense of step descriptions and\nsample-specific information of visual states, enabling LLMs to employ the\nreasoning ability to generate step sequences. With the assistance of LLMs, our\nmethod can both closed-set and open vocabulary procedure planning tasks. Our\nPlanLLM achieves superior performance on three benchmarks, demonstrating the\neffectiveness of our designs."
                },
                "authors": [
                    {
                        "name": "Dejie Yang"
                    },
                    {
                        "name": "Zijing Zhao"
                    },
                    {
                        "name": "YangLiu"
                    }
                ],
                "author_detail": {
                    "name": "YangLiu"
                },
                "author": "YangLiu",
                "arxiv_comment": "accepted to AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19139v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19139v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19113v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19113v1",
                "updated": "2024-12-26T08:13:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    8,
                    13,
                    34,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T08:13:34Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    8,
                    13,
                    34,
                    3,
                    361,
                    0
                ],
                "title": "SketchFill: Sketch-Guided Code Generation for Imputing Derived Missing\n  Values",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SketchFill: Sketch-Guided Code Generation for Imputing Derived Missing\n  Values"
                },
                "summary": "Missing value is a critical issue in data science, significantly impacting\nthe reliability of analyses and predictions. Missing value imputation (MVI) is\na longstanding problem because it highly relies on domain knowledge. Large\nlanguage models (LLMs) have emerged as a promising tool for data cleaning,\nincluding MVI for tabular data, offering advanced capabilities for\nunderstanding and generating content. However, despite their promise, existing\nLLM techniques such as in-context learning and Chain-of-Thought (CoT) often\nfall short in guiding LLMs to perform complex reasoning for MVI, particularly\nwhen imputing derived missing values, which require mathematical formulas and\ndata relationships across rows and columns. This gap underscores the need for\nfurther advancements in LLM methodologies to enhance their reasoning\ncapabilities for more reliable imputation outcomes. To fill this gap, we\npropose SketchFill, a novel sketch-based method to guide LLMs in generating\naccurate formulas to impute missing numerical values. Our experimental results\ndemonstrate that SketchFill significantly outperforms state-of-the-art methods,\nachieving 56.2% higher accuracy than CoT-based methods and 78.8% higher\naccuracy than MetaGPT. This sets a new standard for automated data cleaning and\nadvances the field of MVI for numerical values.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Missing value is a critical issue in data science, significantly impacting\nthe reliability of analyses and predictions. Missing value imputation (MVI) is\na longstanding problem because it highly relies on domain knowledge. Large\nlanguage models (LLMs) have emerged as a promising tool for data cleaning,\nincluding MVI for tabular data, offering advanced capabilities for\nunderstanding and generating content. However, despite their promise, existing\nLLM techniques such as in-context learning and Chain-of-Thought (CoT) often\nfall short in guiding LLMs to perform complex reasoning for MVI, particularly\nwhen imputing derived missing values, which require mathematical formulas and\ndata relationships across rows and columns. This gap underscores the need for\nfurther advancements in LLM methodologies to enhance their reasoning\ncapabilities for more reliable imputation outcomes. To fill this gap, we\npropose SketchFill, a novel sketch-based method to guide LLMs in generating\naccurate formulas to impute missing numerical values. Our experimental results\ndemonstrate that SketchFill significantly outperforms state-of-the-art methods,\nachieving 56.2% higher accuracy than CoT-based methods and 78.8% higher\naccuracy than MetaGPT. This sets a new standard for automated data cleaning and\nadvances the field of MVI for numerical values."
                },
                "authors": [
                    {
                        "name": "Yunfan Zhang"
                    },
                    {
                        "name": "Changlun Li"
                    },
                    {
                        "name": "Yuyu Luo"
                    },
                    {
                        "name": "Nan Tang"
                    }
                ],
                "author_detail": {
                    "name": "Nan Tang"
                },
                "author": "Nan Tang",
                "arxiv_comment": "19 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19113v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19113v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19102v1",
                "updated": "2024-12-26T07:43:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    7,
                    43,
                    18,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T07:43:18Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    7,
                    43,
                    18,
                    3,
                    361,
                    0
                ],
                "title": "\"I've Heard of You!\": Generate Spoken Named Entity Recognition Data for\n  Unseen Entities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"I've Heard of You!\": Generate Spoken Named Entity Recognition Data for\n  Unseen Entities"
                },
                "summary": "Spoken named entity recognition (NER) aims to identify named entities from\nspeech, playing an important role in speech processing. New named entities\nappear every day, however, annotating their Spoken NER data is costly. In this\npaper, we demonstrate that existing Spoken NER systems perform poorly when\ndealing with previously unseen named entities. To tackle this challenge, we\npropose a method for generating Spoken NER data based on a named entity\ndictionary (NED) to reduce costs. Specifically, we first use a large language\nmodel (LLM) to generate sentences from the sampled named entities and then use\na text-to-speech (TTS) system to generate the speech. Furthermore, we introduce\na noise metric to filter out noisy data. To evaluate our approach, we release a\nnovel Spoken NER benchmark along with a corresponding NED containing 8,853\nentities. Experiment results show that our method achieves state-of-the-art\n(SOTA) performance in the in-domain, zero-shot domain adaptation, and fully\nzero-shot settings. Our data will be available at\nhttps://github.com/DeepLearnXMU/HeardU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spoken named entity recognition (NER) aims to identify named entities from\nspeech, playing an important role in speech processing. New named entities\nappear every day, however, annotating their Spoken NER data is costly. In this\npaper, we demonstrate that existing Spoken NER systems perform poorly when\ndealing with previously unseen named entities. To tackle this challenge, we\npropose a method for generating Spoken NER data based on a named entity\ndictionary (NED) to reduce costs. Specifically, we first use a large language\nmodel (LLM) to generate sentences from the sampled named entities and then use\na text-to-speech (TTS) system to generate the speech. Furthermore, we introduce\na noise metric to filter out noisy data. To evaluate our approach, we release a\nnovel Spoken NER benchmark along with a corresponding NED containing 8,853\nentities. Experiment results show that our method achieves state-of-the-art\n(SOTA) performance in the in-domain, zero-shot domain adaptation, and fully\nzero-shot settings. Our data will be available at\nhttps://github.com/DeepLearnXMU/HeardU."
                },
                "authors": [
                    {
                        "name": "Jiawei Yu"
                    },
                    {
                        "name": "Xiang Geng"
                    },
                    {
                        "name": "Yuang Li"
                    },
                    {
                        "name": "Mengxin Ren"
                    },
                    {
                        "name": "Wei Tang"
                    },
                    {
                        "name": "Jiahuan Li"
                    },
                    {
                        "name": "Zhibin Lan"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Hao Yang"
                    },
                    {
                        "name": "Shujian Huang"
                    },
                    {
                        "name": "Jinsong Su"
                    }
                ],
                "author_detail": {
                    "name": "Jinsong Su"
                },
                "author": "Jinsong Su",
                "arxiv_comment": "Accepted by ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2302.00842v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2302.00842v3",
                "updated": "2024-12-26T07:34:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    7,
                    34,
                    35,
                    3,
                    361,
                    0
                ],
                "published": "2023-02-02T03:00:36Z",
                "published_parsed": [
                    2023,
                    2,
                    2,
                    3,
                    0,
                    36,
                    3,
                    33,
                    0
                ],
                "title": "Effective Random Test Generation for Deep Learning Compilers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective Random Test Generation for Deep Learning Compilers"
                },
                "summary": "Deep learning compilers help address the difficulties of deploying deep\nlearning models on diverse types of hardware. Testing deep learning compilers\nis highly crucial, because they are impacting countless AI applications that\nuse them for model optimization and deployment. To test deep learning\ncompilers, random testing, the testing method popularly used for compiler\ntesting practices, faces the challenge of generating semantically valid test\ninputs, i.e., deep learning models that satisfy the semantic model\nspecifications (in short as semantic specifications). To tackle this challenge,\nin this paper, we propose a novel approach named Isra, including a\ndomain-specific constraint solver that resolves the constraints from the\nsemantic specifications without backtracking. We implement and apply our\napproach to three popular real-world deep learning compilers including TVM,\nGlow, and a commercial compiler named SophGo. The evaluation results show that\nIsra is more effective than the state-of-the-art approaches and the baseline\napproaches on constructing valid test inputs for compiler-bug detection, and\nIsra successfully finds 24 previously unknown bugs in released versions of the\nthree compilers. These results indicate Isra's effectiveness and practical\nvalue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning compilers help address the difficulties of deploying deep\nlearning models on diverse types of hardware. Testing deep learning compilers\nis highly crucial, because they are impacting countless AI applications that\nuse them for model optimization and deployment. To test deep learning\ncompilers, random testing, the testing method popularly used for compiler\ntesting practices, faces the challenge of generating semantically valid test\ninputs, i.e., deep learning models that satisfy the semantic model\nspecifications (in short as semantic specifications). To tackle this challenge,\nin this paper, we propose a novel approach named Isra, including a\ndomain-specific constraint solver that resolves the constraints from the\nsemantic specifications without backtracking. We implement and apply our\napproach to three popular real-world deep learning compilers including TVM,\nGlow, and a commercial compiler named SophGo. The evaluation results show that\nIsra is more effective than the state-of-the-art approaches and the baseline\napproaches on constructing valid test inputs for compiler-bug detection, and\nIsra successfully finds 24 previously unknown bugs in released versions of the\nthree compilers. These results indicate Isra's effectiveness and practical\nvalue."
                },
                "authors": [
                    {
                        "name": "Luyao Ren"
                    },
                    {
                        "name": "ZiHeng Wang"
                    },
                    {
                        "name": "Yingfei Xiong"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Guoyue Jiang"
                    },
                    {
                        "name": "Tao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Tao Xie"
                },
                "author": "Tao Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2302.00842v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2302.00842v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19088v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19088v1",
                "updated": "2024-12-26T07:03:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    7,
                    3,
                    55,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T07:03:55Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    7,
                    3,
                    55,
                    3,
                    361,
                    0
                ],
                "title": "Integrating Artificial Open Generative Artificial Intelligence into\n  Software Supply Chain Security",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Artificial Open Generative Artificial Intelligence into\n  Software Supply Chain Security"
                },
                "summary": "While new technologies emerge, human errors always looming. Software supply\nchain is increasingly complex and intertwined, the security of a service has\nbecome paramount to ensuring the integrity of products, safeguarding data\nprivacy, and maintaining operational continuity. In this work, we conducted\nexperiments on the promising open Large Language Models (LLMs) into two main\nsoftware security challenges: source code language errors and deprecated code,\nwith a focus on their potential to replace conventional static and dynamic\nsecurity scanners that rely on predefined rules and patterns. Our findings\nsuggest that while LLMs present some unexpected results, they also encounter\nsignificant limitations, particularly in memory complexity and the management\nof new and unfamiliar data patterns. Despite these challenges, the proactive\napplication of LLMs, coupled with extensive security databases and continuous\nupdates, holds the potential to fortify Software Supply Chain (SSC) processes\nagainst emerging threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While new technologies emerge, human errors always looming. Software supply\nchain is increasingly complex and intertwined, the security of a service has\nbecome paramount to ensuring the integrity of products, safeguarding data\nprivacy, and maintaining operational continuity. In this work, we conducted\nexperiments on the promising open Large Language Models (LLMs) into two main\nsoftware security challenges: source code language errors and deprecated code,\nwith a focus on their potential to replace conventional static and dynamic\nsecurity scanners that rely on predefined rules and patterns. Our findings\nsuggest that while LLMs present some unexpected results, they also encounter\nsignificant limitations, particularly in memory complexity and the management\nof new and unfamiliar data patterns. Despite these challenges, the proactive\napplication of LLMs, coupled with extensive security databases and continuous\nupdates, holds the potential to fortify Software Supply Chain (SSC) processes\nagainst emerging threats."
                },
                "authors": [
                    {
                        "name": "Vasileios Alevizos"
                    },
                    {
                        "name": "George A Papakostas"
                    },
                    {
                        "name": "Akebu Simasiku"
                    },
                    {
                        "name": "Dimitra Malliarou"
                    },
                    {
                        "name": "Antonis Messinis"
                    },
                    {
                        "name": "Sabrina Edralin"
                    },
                    {
                        "name": "Clark Xu"
                    },
                    {
                        "name": "Zongliang Yue"
                    }
                ],
                "author_detail": {
                    "name": "Zongliang Yue"
                },
                "author": "Zongliang Yue",
                "arxiv_doi": "10.1109/ICDABI63787.2024.10800301",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ICDABI63787.2024.10800301",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.19088v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19088v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "2024 5th International Conference on Data Analytics for Business\n  and Industry (ICDABI)",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12142v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12142v2",
                "updated": "2024-12-26T06:39:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    6,
                    39,
                    38,
                    3,
                    361,
                    0
                ],
                "published": "2024-08-22T05:59:47Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    5,
                    59,
                    47,
                    3,
                    235,
                    0
                ],
                "title": "MDD-5k: A New Diagnostic Conversation Dataset for Mental Disorders\n  Synthesized via Neuro-Symbolic LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MDD-5k: A New Diagnostic Conversation Dataset for Mental Disorders\n  Synthesized via Neuro-Symbolic LLM Agents"
                },
                "summary": "The clinical diagnosis of most mental disorders primarily relies on the\nconversations between psychiatrist and patient. The creation of such diagnostic\nconversation datasets is promising to boost the AI mental healthcare community.\nHowever, directly collecting the conversations in real diagnosis scenarios is\nnear impossible due to stringent privacy and ethical considerations. To address\nthis issue, we seek to synthesize diagnostic conversation by exploiting\nanonymized patient cases that are easier to access. Specifically, we design a\nneuro-symbolic multi-agent framework for synthesizing the diagnostic\nconversation of mental disorders with large language models. It takes patient\ncase as input and is capable of generating multiple diverse conversations with\none single patient case. The framework basically involves the interaction\nbetween a doctor agent and a patient agent, and generates conversations under\nsymbolic control via a dynamic diagnosis tree. By applying the proposed\nframework, we develop the largest Chinese mental disorders diagnosis dataset\nMDD-5k. This dataset is built upon 1000 real, anonymized patient cases by\ncooperating with Shanghai Mental Health Center and comprises 5000 high-quality\nlong conversations with diagnosis results and treatment opinions as labels. To\nthe best of our knowledge, it's also the first labeled dataset for Chinese\nmental disorders diagnosis. Human evaluation demonstrates the proposed MDD-5k\ndataset successfully simulates human-like diagnostic process of mental\ndisorders.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The clinical diagnosis of most mental disorders primarily relies on the\nconversations between psychiatrist and patient. The creation of such diagnostic\nconversation datasets is promising to boost the AI mental healthcare community.\nHowever, directly collecting the conversations in real diagnosis scenarios is\nnear impossible due to stringent privacy and ethical considerations. To address\nthis issue, we seek to synthesize diagnostic conversation by exploiting\nanonymized patient cases that are easier to access. Specifically, we design a\nneuro-symbolic multi-agent framework for synthesizing the diagnostic\nconversation of mental disorders with large language models. It takes patient\ncase as input and is capable of generating multiple diverse conversations with\none single patient case. The framework basically involves the interaction\nbetween a doctor agent and a patient agent, and generates conversations under\nsymbolic control via a dynamic diagnosis tree. By applying the proposed\nframework, we develop the largest Chinese mental disorders diagnosis dataset\nMDD-5k. This dataset is built upon 1000 real, anonymized patient cases by\ncooperating with Shanghai Mental Health Center and comprises 5000 high-quality\nlong conversations with diagnosis results and treatment opinions as labels. To\nthe best of our knowledge, it's also the first labeled dataset for Chinese\nmental disorders diagnosis. Human evaluation demonstrates the proposed MDD-5k\ndataset successfully simulates human-like diagnostic process of mental\ndisorders."
                },
                "authors": [
                    {
                        "name": "Congchi Yin"
                    },
                    {
                        "name": "Feng Li"
                    },
                    {
                        "name": "Shu Zhang"
                    },
                    {
                        "name": "Zike Wang"
                    },
                    {
                        "name": "Jun Shao"
                    },
                    {
                        "name": "Piji Li"
                    },
                    {
                        "name": "Jianhua Chen"
                    },
                    {
                        "name": "Xun Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Xun Jiang"
                },
                "author": "Xun Jiang",
                "arxiv_comment": "Accepted by the 39th Annual AAAI Conference on Artificial\n  Intelligence",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12142v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12142v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19076v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19076v1",
                "updated": "2024-12-26T06:23:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    6,
                    23,
                    53,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T06:23:53Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    6,
                    23,
                    53,
                    3,
                    361,
                    0
                ],
                "title": "Advancing LLM detection in the ALTA 2024 Shared Task: Techniques and\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing LLM detection in the ALTA 2024 Shared Task: Techniques and\n  Analysis"
                },
                "summary": "The recent proliferation of AI-generated content has prompted significant\ninterest in developing reliable detection methods. This study explores\ntechniques for identifying AI-generated text through sentence-level evaluation\nwithin hybrid articles. Our findings indicate that ChatGPT-3.5 Turbo exhibits\ndistinct, repetitive probability patterns that enable consistent in-domain\ndetection. Empirical tests show that minor textual modifications, such as\nrewording, have minimal impact on detection accuracy. These results provide\nvaluable insights for advancing AI detection methodologies, offering a pathway\ntoward robust solutions to address the complexities of synthetic text\nidentification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent proliferation of AI-generated content has prompted significant\ninterest in developing reliable detection methods. This study explores\ntechniques for identifying AI-generated text through sentence-level evaluation\nwithin hybrid articles. Our findings indicate that ChatGPT-3.5 Turbo exhibits\ndistinct, repetitive probability patterns that enable consistent in-domain\ndetection. Empirical tests show that minor textual modifications, such as\nrewording, have minimal impact on detection accuracy. These results provide\nvaluable insights for advancing AI detection methodologies, offering a pathway\ntoward robust solutions to address the complexities of synthetic text\nidentification."
                },
                "authors": [
                    {
                        "name": "Dima Galat"
                    }
                ],
                "author_detail": {
                    "name": "Dima Galat"
                },
                "author": "Dima Galat",
                "arxiv_journal_ref": "ALTA 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19076v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19076v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12639v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12639v2",
                "updated": "2024-12-26T06:20:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    6,
                    20,
                    21,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-17T08:02:08Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    8,
                    2,
                    8,
                    1,
                    352,
                    0
                ],
                "title": "Falcon: Faster and Parallel Inference of Large Language Models through\n  Enhanced Semi-Autoregressive Drafting and Custom-Designed Decoding Tree",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Falcon: Faster and Parallel Inference of Large Language Models through\n  Enhanced Semi-Autoregressive Drafting and Custom-Designed Decoding Tree"
                },
                "summary": "Striking an optimal balance between minimal drafting latency and high\nspeculation accuracy to enhance the inference speed of Large Language Models\nremains a significant challenge in speculative decoding. In this paper, we\nintroduce Falcon, an innovative semi-autoregressive speculative decoding\nframework fashioned to augment both the drafter's parallelism and output\nquality. Falcon incorporates the Coupled Sequential Glancing Distillation\ntechnique, which fortifies inter-token dependencies within the same block,\nleading to increased speculation accuracy. We offer a comprehensive theoretical\nanalysis to illuminate the underlying mechanisms. Additionally, we introduce a\nCustom-Designed Decoding Tree, which permits the drafter to generate multiple\ntokens in a single forward pass and accommodates multiple forward passes as\nneeded, thereby boosting the number of drafted tokens and significantly\nimproving the overall acceptance rate. Comprehensive evaluations on benchmark\ndatasets such as MT-Bench, HumanEval, and GSM8K demonstrate Falcon's superior\nacceleration capabilities. The framework achieves a lossless speedup ratio\nranging from 2.91x to 3.51x when tested on the Vicuna and LLaMA2-Chat model\nseries. These results outstrip existing speculative decoding methods for LLMs,\nincluding Eagle, Medusa, Lookahead, SPS, and PLD, while maintaining a compact\ndrafter architecture equivalent to merely two Transformer layers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Striking an optimal balance between minimal drafting latency and high\nspeculation accuracy to enhance the inference speed of Large Language Models\nremains a significant challenge in speculative decoding. In this paper, we\nintroduce Falcon, an innovative semi-autoregressive speculative decoding\nframework fashioned to augment both the drafter's parallelism and output\nquality. Falcon incorporates the Coupled Sequential Glancing Distillation\ntechnique, which fortifies inter-token dependencies within the same block,\nleading to increased speculation accuracy. We offer a comprehensive theoretical\nanalysis to illuminate the underlying mechanisms. Additionally, we introduce a\nCustom-Designed Decoding Tree, which permits the drafter to generate multiple\ntokens in a single forward pass and accommodates multiple forward passes as\nneeded, thereby boosting the number of drafted tokens and significantly\nimproving the overall acceptance rate. Comprehensive evaluations on benchmark\ndatasets such as MT-Bench, HumanEval, and GSM8K demonstrate Falcon's superior\nacceleration capabilities. The framework achieves a lossless speedup ratio\nranging from 2.91x to 3.51x when tested on the Vicuna and LLaMA2-Chat model\nseries. These results outstrip existing speculative decoding methods for LLMs,\nincluding Eagle, Medusa, Lookahead, SPS, and PLD, while maintaining a compact\ndrafter architecture equivalent to merely two Transformer layers."
                },
                "authors": [
                    {
                        "name": "Xiangxiang Gao"
                    },
                    {
                        "name": "Weisheng Xie"
                    },
                    {
                        "name": "Yiwei Xiang"
                    },
                    {
                        "name": "Feng Ji"
                    }
                ],
                "author_detail": {
                    "name": "Feng Ji"
                },
                "author": "Feng Ji",
                "arxiv_comment": "AAAI 2025 Accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12639v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12639v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.02957v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.02957v2",
                "updated": "2024-12-26T06:02:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    6,
                    2,
                    38,
                    3,
                    361,
                    0
                ],
                "published": "2024-05-05T14:53:51Z",
                "published_parsed": [
                    2024,
                    5,
                    5,
                    14,
                    53,
                    51,
                    6,
                    126,
                    0
                ],
                "title": "Agent Hospital: A Simulacrum of Hospital with Evolvable Medical Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent Hospital: A Simulacrum of Hospital with Evolvable Medical Agents"
                },
                "summary": "In this paper, we introduce a simulacrum of hospital called Agent Hospital\nthat simulates the entire process of treating illness. All patients, nurses,\nand doctors are autonomous agents powered by large language models (LLMs). Our\ncentral goal is to enable a doctor agent to learn how to treat illness within\nthe simulacrum. To do so, we propose a method called MedAgent-Zero. As the\nsimulacrum can simulate disease onset and progression based on knowledge bases\nand LLMs, doctor agents can keep accumulating experience from both successful\nand unsuccessful cases. Simulation experiments show that the treatment\nperformance of doctor agents consistently improves on various tasks. More\ninterestingly, the knowledge the doctor agents have acquired in Agent Hospital\nis applicable to real-world medicare benchmarks. After treating around ten\nthousand patients (real-world doctors may take over two years), the evolved\ndoctor agent achieves a state-of-the-art accuracy of 93.06% on a subset of the\nMedQA dataset that covers major respiratory diseases. This work paves the way\nfor advancing the applications of LLM-powered agent techniques in medical\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a simulacrum of hospital called Agent Hospital\nthat simulates the entire process of treating illness. All patients, nurses,\nand doctors are autonomous agents powered by large language models (LLMs). Our\ncentral goal is to enable a doctor agent to learn how to treat illness within\nthe simulacrum. To do so, we propose a method called MedAgent-Zero. As the\nsimulacrum can simulate disease onset and progression based on knowledge bases\nand LLMs, doctor agents can keep accumulating experience from both successful\nand unsuccessful cases. Simulation experiments show that the treatment\nperformance of doctor agents consistently improves on various tasks. More\ninterestingly, the knowledge the doctor agents have acquired in Agent Hospital\nis applicable to real-world medicare benchmarks. After treating around ten\nthousand patients (real-world doctors may take over two years), the evolved\ndoctor agent achieves a state-of-the-art accuracy of 93.06% on a subset of the\nMedQA dataset that covers major respiratory diseases. This work paves the way\nfor advancing the applications of LLM-powered agent techniques in medical\nscenarios."
                },
                "authors": [
                    {
                        "name": "Junkai Li"
                    },
                    {
                        "name": "Siyu Wang"
                    },
                    {
                        "name": "Meng Zhang"
                    },
                    {
                        "name": "Weitao Li"
                    },
                    {
                        "name": "Yunghwei Lai"
                    },
                    {
                        "name": "Xinhui Kang"
                    },
                    {
                        "name": "Weizhi Ma"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.02957v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.02957v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15237v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15237v2",
                "updated": "2024-12-26T05:27:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    5,
                    27,
                    51,
                    3,
                    361,
                    0
                ],
                "published": "2024-08-27T17:56:11Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    17,
                    56,
                    11,
                    1,
                    240,
                    0
                ],
                "title": "The Mamba in the Llama: Distilling and Accelerating Hybrid Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mamba in the Llama: Distilling and Accelerating Hybrid Models"
                },
                "summary": "Linear RNN architectures, like Mamba, can be competitive with Transformer\nmodels in language modeling while having advantageous deployment\ncharacteristics. Given the focus on training large-scale Transformer models, we\nconsider the challenge of converting these pretrained models for deployment. We\ndemonstrate that it is feasible to distill large Transformers into linear RNNs\nby reusing the linear projection weights from attention layers with academic\nGPU resources. The resulting hybrid model, which incorporates a quarter of the\nattention layers, achieves performance comparable to the original Transformer\nin chat benchmarks and outperforms open-source hybrid Mamba models trained from\nscratch with trillions of tokens in both chat benchmarks and general\nbenchmarks. Moreover, we introduce a hardware-aware speculative decoding\nalgorithm that accelerates the inference speed of Mamba and hybrid models.\nOverall we show how, with limited computation resources, we can remove many of\nthe original attention layers and generate from the resulting model more\nefficiently. Our top-performing model, distilled from Llama3-8B-Instruct,\nachieves a 29.61 length-controlled win rate on AlpacaEval 2 against GPT-4 and\n7.35 on MT-Bench, surpassing the best 8B scale instruction-tuned linear RNN\nmodel. We also find that the distilled model has natural length extrapolation,\nshowing almost perfect accuracy in the needle-in-a-haystack test at 20x the\ndistillation length. Code and pre-trained checkpoints are open-sourced at\nhttps://github.com/jxiw/MambaInLlama and\nhttps://github.com/itsdaniele/speculative_mamba.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear RNN architectures, like Mamba, can be competitive with Transformer\nmodels in language modeling while having advantageous deployment\ncharacteristics. Given the focus on training large-scale Transformer models, we\nconsider the challenge of converting these pretrained models for deployment. We\ndemonstrate that it is feasible to distill large Transformers into linear RNNs\nby reusing the linear projection weights from attention layers with academic\nGPU resources. The resulting hybrid model, which incorporates a quarter of the\nattention layers, achieves performance comparable to the original Transformer\nin chat benchmarks and outperforms open-source hybrid Mamba models trained from\nscratch with trillions of tokens in both chat benchmarks and general\nbenchmarks. Moreover, we introduce a hardware-aware speculative decoding\nalgorithm that accelerates the inference speed of Mamba and hybrid models.\nOverall we show how, with limited computation resources, we can remove many of\nthe original attention layers and generate from the resulting model more\nefficiently. Our top-performing model, distilled from Llama3-8B-Instruct,\nachieves a 29.61 length-controlled win rate on AlpacaEval 2 against GPT-4 and\n7.35 on MT-Bench, surpassing the best 8B scale instruction-tuned linear RNN\nmodel. We also find that the distilled model has natural length extrapolation,\nshowing almost perfect accuracy in the needle-in-a-haystack test at 20x the\ndistillation length. Code and pre-trained checkpoints are open-sourced at\nhttps://github.com/jxiw/MambaInLlama and\nhttps://github.com/itsdaniele/speculative_mamba."
                },
                "authors": [
                    {
                        "name": "Junxiong Wang"
                    },
                    {
                        "name": "Daniele Paliotta"
                    },
                    {
                        "name": "Avner May"
                    },
                    {
                        "name": "Alexander M. Rush"
                    },
                    {
                        "name": "Tri Dao"
                    }
                ],
                "author_detail": {
                    "name": "Tri Dao"
                },
                "author": "Tri Dao",
                "arxiv_comment": "NeurIPS 2024. v2 updates: 1. Improved distillation approach and new\n  results for Llama 3.1/3.2 distilled models. 2. Fixed math typos. 3. Added\n  needle in the haystack long-context experiments. 4. Mentioned Mamba-Zephyr as\n  subquadratic and added Mamba-Zephyr-8B lm_eval result",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15237v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15237v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16239v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16239v2",
                "updated": "2024-12-26T04:46:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    4,
                    46,
                    58,
                    3,
                    361,
                    0
                ],
                "published": "2024-11-25T09:54:42Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    9,
                    54,
                    42,
                    0,
                    330,
                    0
                ],
                "title": "CS-Eval: A Comprehensive Large Language Model Benchmark for\n  CyberSecurity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CS-Eval: A Comprehensive Large Language Model Benchmark for\n  CyberSecurity"
                },
                "summary": "Over the past year, there has been a notable rise in the use of large\nlanguage models (LLMs) for academic research and industrial practices within\nthe cybersecurity field. However, it remains a lack of comprehensive and\npublicly accessible benchmarks to evaluate the performance of LLMs on\ncybersecurity tasks. To address this gap, we introduce CS-Eval, a publicly\naccessible, comprehensive and bilingual LLM benchmark specifically designed for\ncybersecurity. CS-Eval synthesizes the research hotspots from academia and\npractical applications from industry, curating a diverse set of high-quality\nquestions across 42 categories within cybersecurity, systematically organized\ninto three cognitive levels: knowledge, ability, and application. Through an\nextensive evaluation of a wide range of LLMs using CS-Eval, we have uncovered\nvaluable insights. For instance, while GPT-4 generally excels overall, other\nmodels may outperform it in certain specific subcategories. Additionally, by\nconducting evaluations over several months, we observed significant\nimprovements in many LLMs' abilities to solve cybersecurity tasks. The\nbenchmarks are now publicly available at https://github.com/CS-EVAL/CS-Eval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over the past year, there has been a notable rise in the use of large\nlanguage models (LLMs) for academic research and industrial practices within\nthe cybersecurity field. However, it remains a lack of comprehensive and\npublicly accessible benchmarks to evaluate the performance of LLMs on\ncybersecurity tasks. To address this gap, we introduce CS-Eval, a publicly\naccessible, comprehensive and bilingual LLM benchmark specifically designed for\ncybersecurity. CS-Eval synthesizes the research hotspots from academia and\npractical applications from industry, curating a diverse set of high-quality\nquestions across 42 categories within cybersecurity, systematically organized\ninto three cognitive levels: knowledge, ability, and application. Through an\nextensive evaluation of a wide range of LLMs using CS-Eval, we have uncovered\nvaluable insights. For instance, while GPT-4 generally excels overall, other\nmodels may outperform it in certain specific subcategories. Additionally, by\nconducting evaluations over several months, we observed significant\nimprovements in many LLMs' abilities to solve cybersecurity tasks. The\nbenchmarks are now publicly available at https://github.com/CS-EVAL/CS-Eval."
                },
                "authors": [
                    {
                        "name": "Zhengmin Yu"
                    },
                    {
                        "name": "Jiutian Zeng"
                    },
                    {
                        "name": "Siyi Chen"
                    },
                    {
                        "name": "Wenhan Xu"
                    },
                    {
                        "name": "Dandan Xu"
                    },
                    {
                        "name": "Xiangyu Liu"
                    },
                    {
                        "name": "Zonghao Ying"
                    },
                    {
                        "name": "Nan Wang"
                    },
                    {
                        "name": "Yuan Zhang"
                    },
                    {
                        "name": "Min Yang"
                    }
                ],
                "author_detail": {
                    "name": "Min Yang"
                },
                "author": "Min Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16239v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16239v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16844v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16844v3",
                "updated": "2024-12-26T04:41:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    4,
                    41,
                    11,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-22T03:43:51Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    3,
                    43,
                    51,
                    6,
                    357,
                    0
                ],
                "title": "Sim911: Towards Effective and Equitable 9-1-1 Dispatcher Training with\n  an LLM-Enabled Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sim911: Towards Effective and Equitable 9-1-1 Dispatcher Training with\n  an LLM-Enabled Simulation"
                },
                "summary": "Emergency response services are vital for enhancing public safety by\nsafeguarding the environment, property, and human lives. As frontline members\nof these services, 9-1-1 dispatchers have a direct impact on response times and\nthe overall effectiveness of emergency operations. However, traditional\ndispatcher training methods, which rely on role-playing by experienced\npersonnel, are labor-intensive, time-consuming, and often neglect the specific\nneeds of underserved communities. To address these challenges, we introduce\nSim911, the first training simulation for 9-1-1 dispatchers powered by Large\nLanguage Models (LLMs). Sim911 enhances training through three key technical\ninnovations: (1) knowledge construction, which utilizes archived 9-1-1 call\ndata to generate simulations that closely mirror real-world scenarios; (2)\ncontext-aware controlled generation, which employs dynamic prompts and vector\nbases to ensure that LLM behavior aligns with training objectives; and (3)\nvalidation with looped correction, which filters out low-quality responses and\nrefines the system performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emergency response services are vital for enhancing public safety by\nsafeguarding the environment, property, and human lives. As frontline members\nof these services, 9-1-1 dispatchers have a direct impact on response times and\nthe overall effectiveness of emergency operations. However, traditional\ndispatcher training methods, which rely on role-playing by experienced\npersonnel, are labor-intensive, time-consuming, and often neglect the specific\nneeds of underserved communities. To address these challenges, we introduce\nSim911, the first training simulation for 9-1-1 dispatchers powered by Large\nLanguage Models (LLMs). Sim911 enhances training through three key technical\ninnovations: (1) knowledge construction, which utilizes archived 9-1-1 call\ndata to generate simulations that closely mirror real-world scenarios; (2)\ncontext-aware controlled generation, which employs dynamic prompts and vector\nbases to ensure that LLM behavior aligns with training objectives; and (3)\nvalidation with looped correction, which filters out low-quality responses and\nrefines the system performance."
                },
                "authors": [
                    {
                        "name": "Zirong Chen"
                    },
                    {
                        "name": "Elizabeth Chason"
                    },
                    {
                        "name": "Noah Mladenovski"
                    },
                    {
                        "name": "Erin Wilson"
                    },
                    {
                        "name": "Kristin Mullen"
                    },
                    {
                        "name": "Stephen Martini"
                    },
                    {
                        "name": "Meiyi Ma"
                    }
                ],
                "author_detail": {
                    "name": "Meiyi Ma"
                },
                "author": "Meiyi Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16844v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16844v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19048v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19048v1",
                "updated": "2024-12-26T04:05:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    4,
                    5,
                    28,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T04:05:28Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    4,
                    5,
                    28,
                    3,
                    361,
                    0
                ],
                "title": "Jasper and Stella: distillation of SOTA embedding models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jasper and Stella: distillation of SOTA embedding models"
                },
                "summary": "A crucial component of many deep learning applications (such as FAQ and RAG)\nis dense retrieval, in which embedding models are used to convert raw text to\nnumerical vectors and then get the most similar text by MIPS (Maximum Inner\nProduct Search). Some text embedding benchmarks (e.g. MTEB, BEIR, and\nAIR-Bench) have been established to evaluate embedding models accurately.\nThanks to these benchmarks, we can use SOTA models; however, the deployment and\napplication of these models in industry were hampered by their large vector\ndimensions and numerous parameters. To alleviate this problem, 1) we present a\ndistillation technique that can enable a smaller student model to achieve good\nperformance. 2) Inspired by MRL we present a training approach of reducing the\nvector dimensions based on its own vectors or its teacher vectors. 3) We do\nsimple yet effective alignment training between images and text to make our\nmodel a multimodal encoder. We trained Stella and Jasper models using the\ntechnologies above and achieved high scores on the MTEB leaderboard. We release\nthe model and data at Hugging Face Hub\n(https://huggingface.co/infgrad/jasper_en_vision_language_v1) and the training\nlogs are at https://api.wandb.ai/links/dunnzhang0/z8jqoqpb.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A crucial component of many deep learning applications (such as FAQ and RAG)\nis dense retrieval, in which embedding models are used to convert raw text to\nnumerical vectors and then get the most similar text by MIPS (Maximum Inner\nProduct Search). Some text embedding benchmarks (e.g. MTEB, BEIR, and\nAIR-Bench) have been established to evaluate embedding models accurately.\nThanks to these benchmarks, we can use SOTA models; however, the deployment and\napplication of these models in industry were hampered by their large vector\ndimensions and numerous parameters. To alleviate this problem, 1) we present a\ndistillation technique that can enable a smaller student model to achieve good\nperformance. 2) Inspired by MRL we present a training approach of reducing the\nvector dimensions based on its own vectors or its teacher vectors. 3) We do\nsimple yet effective alignment training between images and text to make our\nmodel a multimodal encoder. We trained Stella and Jasper models using the\ntechnologies above and achieved high scores on the MTEB leaderboard. We release\nthe model and data at Hugging Face Hub\n(https://huggingface.co/infgrad/jasper_en_vision_language_v1) and the training\nlogs are at https://api.wandb.ai/links/dunnzhang0/z8jqoqpb."
                },
                "authors": [
                    {
                        "name": "Dun Zhang"
                    },
                    {
                        "name": "FulongWang"
                    }
                ],
                "author_detail": {
                    "name": "FulongWang"
                },
                "author": "FulongWang",
                "arxiv_comment": "7 pages, 1 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19048v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19048v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01394v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01394v4",
                "updated": "2024-12-26T03:49:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    3,
                    49,
                    34,
                    3,
                    361,
                    0
                ],
                "published": "2024-06-03T14:57:39Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    14,
                    57,
                    39,
                    0,
                    155,
                    0
                ],
                "title": "PrivacyRestore: Privacy-Preserving Inference in Large Language Models\n  via Privacy Removal and Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrivacyRestore: Privacy-Preserving Inference in Large Language Models\n  via Privacy Removal and Restoration"
                },
                "summary": "The widespread usage of online Large Language Models (LLMs) inference\nservices has raised significant privacy concerns about the potential exposure\nof private information in user inputs to malicious eavesdroppers. Existing\nprivacy protection methods for LLMs suffer from either insufficient privacy\nprotection, performance degradation, or large inference time overhead. To\naddress these limitations, we propose PrivacyRestore, a plug-and-play method to\nprotect the privacy of user inputs during LLM inference. The server first\ntrains restoration vectors for each privacy span and then release to clients.\nPrivacy span is defined as a contiguous sequence of tokens within a text that\ncontain private information. The client then aggregate restoration vectors of\nall privacy spans in the input into a single meta restoration vector which is\nlater sent to the server side along with the input without privacy spans.The\nprivate information is restored via activation steering during inference.\nFurthermore, we prove that PrivacyRestore inherently prevents the linear growth\nof the privacy budget.We create three datasets, covering medical and legal\ndomains, to evaluate the effectiveness of privacy preserving methods. The\nexperimental results show that PrivacyRestore effectively protects private\ninformation and maintain acceptable levels of performance and inference\noverhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread usage of online Large Language Models (LLMs) inference\nservices has raised significant privacy concerns about the potential exposure\nof private information in user inputs to malicious eavesdroppers. Existing\nprivacy protection methods for LLMs suffer from either insufficient privacy\nprotection, performance degradation, or large inference time overhead. To\naddress these limitations, we propose PrivacyRestore, a plug-and-play method to\nprotect the privacy of user inputs during LLM inference. The server first\ntrains restoration vectors for each privacy span and then release to clients.\nPrivacy span is defined as a contiguous sequence of tokens within a text that\ncontain private information. The client then aggregate restoration vectors of\nall privacy spans in the input into a single meta restoration vector which is\nlater sent to the server side along with the input without privacy spans.The\nprivate information is restored via activation steering during inference.\nFurthermore, we prove that PrivacyRestore inherently prevents the linear growth\nof the privacy budget.We create three datasets, covering medical and legal\ndomains, to evaluate the effectiveness of privacy preserving methods. The\nexperimental results show that PrivacyRestore effectively protects private\ninformation and maintain acceptable levels of performance and inference\noverhead."
                },
                "authors": [
                    {
                        "name": "Ziqian Zeng"
                    },
                    {
                        "name": "Jianwei Wang"
                    },
                    {
                        "name": "Junyao Yang"
                    },
                    {
                        "name": "Zhengdong Lu"
                    },
                    {
                        "name": "Huiping Zhuang"
                    },
                    {
                        "name": "Cen Chen"
                    }
                ],
                "author_detail": {
                    "name": "Cen Chen"
                },
                "author": "Cen Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01394v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01394v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.11061v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.11061v4",
                "updated": "2024-12-26T03:38:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    3,
                    38,
                    10,
                    3,
                    361,
                    0
                ],
                "published": "2024-01-19T23:34:48Z",
                "published_parsed": [
                    2024,
                    1,
                    19,
                    23,
                    34,
                    48,
                    4,
                    19,
                    0
                ],
                "title": "PhotoBot: Reference-Guided Interactive Photography via Natural Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhotoBot: Reference-Guided Interactive Photography via Natural Language"
                },
                "summary": "We introduce PhotoBot, a framework for fully automated photo acquisition\nbased on an interplay between high-level human language guidance and a robot\nphotographer. We propose to communicate photography suggestions to the user via\nreference images that are selected from a curated gallery. We leverage a visual\nlanguage model (VLM) and an object detector to characterize the reference\nimages via textual descriptions and then use a large language model (LLM) to\nretrieve relevant reference images based on a user's language query through\ntext-based reasoning. To correspond the reference image and the observed scene,\nwe exploit pre-trained features from a vision transformer capable of capturing\nsemantic similarity across marked appearance variations. Using these features,\nwe compute suggested pose adjustments for an RGB-D camera by solving a\nperspective-n-point (PnP) problem. We demonstrate our approach using a\nmanipulator equipped with a wrist camera. Our user studies show that photos\ntaken by PhotoBot are often more aesthetically pleasing than those taken by\nusers themselves, as measured by human feedback. We also show that PhotoBot can\ngeneralize to other reference sources such as paintings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce PhotoBot, a framework for fully automated photo acquisition\nbased on an interplay between high-level human language guidance and a robot\nphotographer. We propose to communicate photography suggestions to the user via\nreference images that are selected from a curated gallery. We leverage a visual\nlanguage model (VLM) and an object detector to characterize the reference\nimages via textual descriptions and then use a large language model (LLM) to\nretrieve relevant reference images based on a user's language query through\ntext-based reasoning. To correspond the reference image and the observed scene,\nwe exploit pre-trained features from a vision transformer capable of capturing\nsemantic similarity across marked appearance variations. Using these features,\nwe compute suggested pose adjustments for an RGB-D camera by solving a\nperspective-n-point (PnP) problem. We demonstrate our approach using a\nmanipulator equipped with a wrist camera. Our user studies show that photos\ntaken by PhotoBot are often more aesthetically pleasing than those taken by\nusers themselves, as measured by human feedback. We also show that PhotoBot can\ngeneralize to other reference sources such as paintings."
                },
                "authors": [
                    {
                        "name": "Oliver Limoyo"
                    },
                    {
                        "name": "Jimmy Li"
                    },
                    {
                        "name": "Dmitriy Rivkin"
                    },
                    {
                        "name": "Jonathan Kelly"
                    },
                    {
                        "name": "Gregory Dudek"
                    }
                ],
                "author_detail": {
                    "name": "Gregory Dudek"
                },
                "author": "Gregory Dudek",
                "arxiv_doi": "10.1109/IROS58592.2024.10801790",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/IROS58592.2024.10801790",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2401.11061v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.11061v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings of the IEEE/RSJ International Conference on\n  Intelligent Robotics and Systems (IROS'24), Abu Dhabi, UAE, Oct. 14-18, 2024",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18169v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18169v2",
                "updated": "2024-12-26T03:28:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    3,
                    28,
                    3,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-24T05:07:46Z",
                "published_parsed": [
                    2024,
                    12,
                    24,
                    5,
                    7,
                    46,
                    1,
                    359,
                    0
                ],
                "title": "KunServe: Elastic and Efficient Large Language Model Serving with\n  Parameter-centric Memory Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KunServe: Elastic and Efficient Large Language Model Serving with\n  Parameter-centric Memory Management"
                },
                "summary": "The stateful nature of large language model (LLM) servingcan easily throttle\nprecious GPU memory under load burstor long-generation requests like\nchain-of-thought reasoning,causing latency spikes due to queuing incoming\nrequests. However, state-of-the-art KVCache centric approaches handleload\nspikes by dropping, migrating, or swapping KVCache,which faces an essential\ntradeoff between the performance ofongoing vs. incoming requests and thus still\nseverely violatesSLO.This paper makes a key observation such that model\nparam-eters are independent of the requests and are replicated acrossGPUs, and\nthus proposes a parameter-centric approach byselectively dropping replicated\nparameters to leave preciousmemory for requests. However, LLM requires KVCache\ntobe saved in bound with model parameters and thus droppingparameters can cause\neither huge computation waste or longnetwork delay, affecting all ongoing\nrequests. Based on the ob-servation that attention operators can be decoupled\nfrom otheroperators, this paper further proposes a novel remote\nattentionmechanism through pipeline parallelism so as to serve up-coming\nrequests with the additional memory borrowed fromparameters on remote GPUs.\nThis paper further addresses sev-eral other challenges including lively\nexchanging KVCachewith incomplete parameters, generating an appropriate\nplanthat balances memory requirements with cooperative exe-cution overhead, and\nseamlessly restoring parameters whenthe throttling has gone. Evaluations show\nthatKUNSERVEreduces the tail TTFT of requests under throttling by up to 27.3x\ncompared to the state-of-the-art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The stateful nature of large language model (LLM) servingcan easily throttle\nprecious GPU memory under load burstor long-generation requests like\nchain-of-thought reasoning,causing latency spikes due to queuing incoming\nrequests. However, state-of-the-art KVCache centric approaches handleload\nspikes by dropping, migrating, or swapping KVCache,which faces an essential\ntradeoff between the performance ofongoing vs. incoming requests and thus still\nseverely violatesSLO.This paper makes a key observation such that model\nparam-eters are independent of the requests and are replicated acrossGPUs, and\nthus proposes a parameter-centric approach byselectively dropping replicated\nparameters to leave preciousmemory for requests. However, LLM requires KVCache\ntobe saved in bound with model parameters and thus droppingparameters can cause\neither huge computation waste or longnetwork delay, affecting all ongoing\nrequests. Based on the ob-servation that attention operators can be decoupled\nfrom otheroperators, this paper further proposes a novel remote\nattentionmechanism through pipeline parallelism so as to serve up-coming\nrequests with the additional memory borrowed fromparameters on remote GPUs.\nThis paper further addresses sev-eral other challenges including lively\nexchanging KVCachewith incomplete parameters, generating an appropriate\nplanthat balances memory requirements with cooperative exe-cution overhead, and\nseamlessly restoring parameters whenthe throttling has gone. Evaluations show\nthatKUNSERVEreduces the tail TTFT of requests under throttling by up to 27.3x\ncompared to the state-of-the-art."
                },
                "authors": [
                    {
                        "name": "Rongxin Cheng"
                    },
                    {
                        "name": "Yifan Peng"
                    },
                    {
                        "name": "Yuxin Lai"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18169v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18169v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19037v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19037v1",
                "updated": "2024-12-26T03:13:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    3,
                    13,
                    3,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T03:13:03Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    3,
                    13,
                    3,
                    3,
                    361,
                    0
                ],
                "title": "CL-attack: Textual Backdoor Attacks via Cross-Lingual Triggers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CL-attack: Textual Backdoor Attacks via Cross-Lingual Triggers"
                },
                "summary": "Backdoor attacks significantly compromise the security of large language\nmodels by triggering them to output specific and controlled content. Currently,\ntriggers for textual backdoor attacks fall into two categories: fixed-token\ntriggers and sentence-pattern triggers. However, the former are typically easy\nto identify and filter, while the latter, such as syntax and style, do not\napply to all original samples and may lead to semantic shifts. In this paper,\ninspired by cross-lingual (CL) prompts of LLMs in real-world scenarios, we\npropose a higher-dimensional trigger method at the paragraph level, namely\nCL-attack. CL-attack injects the backdoor by using texts with specific\nstructures that incorporate multiple languages, thereby offering greater\nstealthiness and universality compared to existing backdoor attack techniques.\nExtensive experiments on different tasks and model architectures demonstrate\nthat CL-attack can achieve nearly 100% attack success rate with a low poisoning\nrate in both classification and generation tasks. We also empirically show that\nthe CL-attack is more robust against current major defense methods compared to\nbaseline backdoor attacks. Additionally, to mitigate CL-attack, we further\ndevelop a new defense called TranslateDefense, which can partially mitigate the\nimpact of CL-attack.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backdoor attacks significantly compromise the security of large language\nmodels by triggering them to output specific and controlled content. Currently,\ntriggers for textual backdoor attacks fall into two categories: fixed-token\ntriggers and sentence-pattern triggers. However, the former are typically easy\nto identify and filter, while the latter, such as syntax and style, do not\napply to all original samples and may lead to semantic shifts. In this paper,\ninspired by cross-lingual (CL) prompts of LLMs in real-world scenarios, we\npropose a higher-dimensional trigger method at the paragraph level, namely\nCL-attack. CL-attack injects the backdoor by using texts with specific\nstructures that incorporate multiple languages, thereby offering greater\nstealthiness and universality compared to existing backdoor attack techniques.\nExtensive experiments on different tasks and model architectures demonstrate\nthat CL-attack can achieve nearly 100% attack success rate with a low poisoning\nrate in both classification and generation tasks. We also empirically show that\nthe CL-attack is more robust against current major defense methods compared to\nbaseline backdoor attacks. Additionally, to mitigate CL-attack, we further\ndevelop a new defense called TranslateDefense, which can partially mitigate the\nimpact of CL-attack."
                },
                "authors": [
                    {
                        "name": "Jingyi Zheng"
                    },
                    {
                        "name": "Tianyi Hu"
                    },
                    {
                        "name": "Tianshuo Cong"
                    },
                    {
                        "name": "Xinlei He"
                    }
                ],
                "author_detail": {
                    "name": "Xinlei He"
                },
                "author": "Xinlei He",
                "arxiv_comment": "The paper has been accepted to AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19037v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19037v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03988v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03988v3",
                "updated": "2024-12-26T03:03:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    3,
                    3,
                    30,
                    3,
                    361,
                    0
                ],
                "published": "2024-05-07T04:00:30Z",
                "published_parsed": [
                    2024,
                    5,
                    7,
                    4,
                    0,
                    30,
                    1,
                    128,
                    0
                ],
                "title": "LEARN: Knowledge Adaptation from Large Language Model to Recommendation\n  for Practical Industrial Application",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LEARN: Knowledge Adaptation from Large Language Model to Recommendation\n  for Practical Industrial Application"
                },
                "summary": "Contemporary recommendation systems predominantly rely on ID embedding to\ncapture latent associations among users and items. However, this approach\noverlooks the wealth of semantic information embedded within textual\ndescriptions of items, leading to suboptimal performance and poor\ngeneralizations. Leveraging the capability of large language models to\ncomprehend and reason about textual content presents a promising avenue for\nadvancing recommendation systems. To achieve this, we propose an Llm-driven\nknowlEdge Adaptive RecommeNdation (LEARN) framework that synergizes open-world\nknowledge with collaborative knowledge. We address computational complexity\nconcerns by utilizing pretrained LLMs as item encoders and freezing LLM\nparameters to avoid catastrophic forgetting and preserve open-world knowledge.\nTo bridge the gap between the open-world and collaborative domains, we design a\ntwin-tower structure supervised by the recommendation task and tailored for\npractical industrial application. Through experiments on the real large-scale\nindustrial dataset and online A/B tests, we demonstrate the efficacy of our\napproach in industry application. We also achieve state-of-the-art performance\non six Amazon Review datasets to verify the superiority of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary recommendation systems predominantly rely on ID embedding to\ncapture latent associations among users and items. However, this approach\noverlooks the wealth of semantic information embedded within textual\ndescriptions of items, leading to suboptimal performance and poor\ngeneralizations. Leveraging the capability of large language models to\ncomprehend and reason about textual content presents a promising avenue for\nadvancing recommendation systems. To achieve this, we propose an Llm-driven\nknowlEdge Adaptive RecommeNdation (LEARN) framework that synergizes open-world\nknowledge with collaborative knowledge. We address computational complexity\nconcerns by utilizing pretrained LLMs as item encoders and freezing LLM\nparameters to avoid catastrophic forgetting and preserve open-world knowledge.\nTo bridge the gap between the open-world and collaborative domains, we design a\ntwin-tower structure supervised by the recommendation task and tailored for\npractical industrial application. Through experiments on the real large-scale\nindustrial dataset and online A/B tests, we demonstrate the efficacy of our\napproach in industry application. We also achieve state-of-the-art performance\non six Amazon Review datasets to verify the superiority of our method."
                },
                "authors": [
                    {
                        "name": "Jian Jia"
                    },
                    {
                        "name": "Yipei Wang"
                    },
                    {
                        "name": "Yan Li"
                    },
                    {
                        "name": "Honggang Chen"
                    },
                    {
                        "name": "Xuehan Bai"
                    },
                    {
                        "name": "Zhaocheng Liu"
                    },
                    {
                        "name": "Jian Liang"
                    },
                    {
                        "name": "Quan Chen"
                    },
                    {
                        "name": "Han Li"
                    },
                    {
                        "name": "Peng Jiang"
                    },
                    {
                        "name": "Kun Gai"
                    }
                ],
                "author_detail": {
                    "name": "Kun Gai"
                },
                "author": "Kun Gai",
                "arxiv_comment": "Accepted by AAAI 2025. Codes are released at\n  https://github.com/adxcreative/LEARN",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.03988v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03988v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19031v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19031v1",
                "updated": "2024-12-26T03:01:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    3,
                    1,
                    32,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T03:01:32Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    3,
                    1,
                    32,
                    3,
                    361,
                    0
                ],
                "title": "Repository Structure-Aware Training Makes SLMs Better Issue Resolver",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Repository Structure-Aware Training Makes SLMs Better Issue Resolver"
                },
                "summary": "Language models have been applied to various software development tasks, but\nthe performance varies according to the scale of the models. Large Language\nModels (LLMs) outperform Small Language Models (SLMs) in complex tasks like\nrepository-level issue resolving, but raise concerns about privacy and cost. In\ncontrast, SLMs are more accessible but under-perform in complex tasks. In this\npaper, we introduce ReSAT (Repository Structure-Aware Training), construct\ntraining data based on a large number of issues and corresponding pull requests\nfrom open-source communities to enhance the model's understanding of repository\nstructure and issue resolving ability. We construct two types of training data:\n(1) localization training data, a multi-level progressive localization data to\nimprove code understanding and localization capability; (2) code edit training\ndata, which improves context-based code editing capability. The evaluation\nresults on SWE-Bench-verified and RepoQA demonstrate that ReSAT effectively\nenhances SLMs' issue-resolving and repository-level long-context understanding\ncapabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models have been applied to various software development tasks, but\nthe performance varies according to the scale of the models. Large Language\nModels (LLMs) outperform Small Language Models (SLMs) in complex tasks like\nrepository-level issue resolving, but raise concerns about privacy and cost. In\ncontrast, SLMs are more accessible but under-perform in complex tasks. In this\npaper, we introduce ReSAT (Repository Structure-Aware Training), construct\ntraining data based on a large number of issues and corresponding pull requests\nfrom open-source communities to enhance the model's understanding of repository\nstructure and issue resolving ability. We construct two types of training data:\n(1) localization training data, a multi-level progressive localization data to\nimprove code understanding and localization capability; (2) code edit training\ndata, which improves context-based code editing capability. The evaluation\nresults on SWE-Bench-verified and RepoQA demonstrate that ReSAT effectively\nenhances SLMs' issue-resolving and repository-level long-context understanding\ncapabilities."
                },
                "authors": [
                    {
                        "name": "Zexiong Ma"
                    },
                    {
                        "name": "Shengnan An"
                    },
                    {
                        "name": "Zeqi Lin"
                    },
                    {
                        "name": "Yanzhen Zou"
                    },
                    {
                        "name": "Bing Xie"
                    }
                ],
                "author_detail": {
                    "name": "Bing Xie"
                },
                "author": "Bing Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19031v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19031v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06175v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06175v3",
                "updated": "2024-12-26T02:47:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    2,
                    47,
                    15,
                    3,
                    361,
                    0
                ],
                "published": "2024-11-09T13:17:39Z",
                "published_parsed": [
                    2024,
                    11,
                    9,
                    13,
                    17,
                    39,
                    5,
                    314,
                    0
                ],
                "title": "Clustering Algorithms and RAG Enhancing Semi-Supervised Text\n  Classification with Large LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clustering Algorithms and RAG Enhancing Semi-Supervised Text\n  Classification with Large LLMs"
                },
                "summary": "This paper proposes a Clustering, Labeling, then Augmenting framework that\nsignificantly enhances performance in Semi-Supervised Text Classification\n(SSTC) tasks, effectively addressing the challenge of vast datasets with\nlimited labeled examples. Unlike traditional SSTC approaches that rely on a\npredefined small set of labeled data to generate pseudo-labels for the\nunlabeled data, this framework innovatively employs clustering to select\nrepresentative \"landmarks\" for labeling. These landmarks subsequently act as\nintermediaries in an ensemble of augmentation techniques, including\nRetrieval-Augmented Generation (RAG), Large Language Model (LLMs)-based\nrewriting, and synonym substitution, to generate synthetic labeled data without\nmaking pseudo-labels for the unlabeled data. Empirical results show that even\nin complex text document classification scenarios involving over 100\ncategories, our method achieves state-of-the-art accuracies of 95.41% on the\nReuters dataset and 82.43% on the Web of Science dataset. Our approach\nsignificantly reduces the reliance on human labeling efforts and the associated\nexpenses, while simultaneously ensuring high data quality and minimizing\nprivacy risks. The finetuning results further show the efficiency of\nfine-tuning LLMs for text classification tasks, highlighting a robust solution\nfor leveraging limited labeled data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a Clustering, Labeling, then Augmenting framework that\nsignificantly enhances performance in Semi-Supervised Text Classification\n(SSTC) tasks, effectively addressing the challenge of vast datasets with\nlimited labeled examples. Unlike traditional SSTC approaches that rely on a\npredefined small set of labeled data to generate pseudo-labels for the\nunlabeled data, this framework innovatively employs clustering to select\nrepresentative \"landmarks\" for labeling. These landmarks subsequently act as\nintermediaries in an ensemble of augmentation techniques, including\nRetrieval-Augmented Generation (RAG), Large Language Model (LLMs)-based\nrewriting, and synonym substitution, to generate synthetic labeled data without\nmaking pseudo-labels for the unlabeled data. Empirical results show that even\nin complex text document classification scenarios involving over 100\ncategories, our method achieves state-of-the-art accuracies of 95.41% on the\nReuters dataset and 82.43% on the Web of Science dataset. Our approach\nsignificantly reduces the reliance on human labeling efforts and the associated\nexpenses, while simultaneously ensuring high data quality and minimizing\nprivacy risks. The finetuning results further show the efficiency of\nfine-tuning LLMs for text classification tasks, highlighting a robust solution\nfor leveraging limited labeled data."
                },
                "authors": [
                    {
                        "name": "Shan Zhong"
                    },
                    {
                        "name": "Jiahao Zeng"
                    },
                    {
                        "name": "Yongxin Yu"
                    },
                    {
                        "name": "Bohong Lin"
                    }
                ],
                "author_detail": {
                    "name": "Bohong Lin"
                },
                "author": "Bohong Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06175v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06175v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04272v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04272v2",
                "updated": "2024-12-26T02:24:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    2,
                    24,
                    52,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-05T15:54:16Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    15,
                    54,
                    16,
                    3,
                    340,
                    0
                ],
                "title": "PoTable: Programming Standardly on Table-based Reasoning Like a Human\n  Analyst",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PoTable: Programming Standardly on Table-based Reasoning Like a Human\n  Analyst"
                },
                "summary": "Table-based reasoning has garnered substantial research interest,\nparticularly in its integration with Large Language Model (LLM) which has\nrevolutionized the general reasoning paradigm. Numerous LLM-based studies\nintroduce symbolic tools (e.g., databases, Python) as assistants to extend\nhuman-like abilities in structured table understanding and complex arithmetic\ncomputations. However, these studies can be improved better in simulating human\ncognitive behavior when using symbolic tools, as they still suffer from\nlimitations of non-standard logical splits and constrained operation pools. In\nthis study, we propose PoTable as a novel table-based reasoning method that\nsimulates a human tabular analyst, which integrates a Python interpreter as the\nreal-time executor accompanied by an LLM-based operation planner and code\ngenerator. Specifically, PoTable follows a human-like logical stage split and\nextends the operation pool into an open-world space without any constraints.\nThrough planning and executing in each distinct stage, PoTable standardly\ncompletes the entire reasoning process and produces superior reasoning results\nalong with highly accurate, steply commented and completely executable\nprograms. Accordingly, the effectiveness and explainability of PoTable are\nfully demonstrated. Extensive experiments over three evaluation datasets from\ntwo public benchmarks on two backbones show the outstanding performance of our\napproach. In particular, GPT-based PoTable achieves over 4% higher absolute\naccuracy than runner-ups on all evaluation datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Table-based reasoning has garnered substantial research interest,\nparticularly in its integration with Large Language Model (LLM) which has\nrevolutionized the general reasoning paradigm. Numerous LLM-based studies\nintroduce symbolic tools (e.g., databases, Python) as assistants to extend\nhuman-like abilities in structured table understanding and complex arithmetic\ncomputations. However, these studies can be improved better in simulating human\ncognitive behavior when using symbolic tools, as they still suffer from\nlimitations of non-standard logical splits and constrained operation pools. In\nthis study, we propose PoTable as a novel table-based reasoning method that\nsimulates a human tabular analyst, which integrates a Python interpreter as the\nreal-time executor accompanied by an LLM-based operation planner and code\ngenerator. Specifically, PoTable follows a human-like logical stage split and\nextends the operation pool into an open-world space without any constraints.\nThrough planning and executing in each distinct stage, PoTable standardly\ncompletes the entire reasoning process and produces superior reasoning results\nalong with highly accurate, steply commented and completely executable\nprograms. Accordingly, the effectiveness and explainability of PoTable are\nfully demonstrated. Extensive experiments over three evaluation datasets from\ntwo public benchmarks on two backbones show the outstanding performance of our\napproach. In particular, GPT-based PoTable achieves over 4% higher absolute\naccuracy than runner-ups on all evaluation datasets."
                },
                "authors": [
                    {
                        "name": "Qingyang Mao"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Zhi Li"
                    },
                    {
                        "name": "Mingyue Cheng"
                    },
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Rui Li"
                    }
                ],
                "author_detail": {
                    "name": "Rui Li"
                },
                "author": "Rui Li",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04272v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04272v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03631v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03631v2",
                "updated": "2024-12-26T02:14:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    2,
                    14,
                    28,
                    3,
                    361,
                    0
                ],
                "published": "2024-08-07T08:43:32Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    8,
                    43,
                    32,
                    2,
                    220,
                    0
                ],
                "title": "Large Language Model as a Catalyst: A Paradigm Shift in Base Station\n  Siting Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model as a Catalyst: A Paradigm Shift in Base Station\n  Siting Optimization"
                },
                "summary": "Traditional base station siting (BSS) methods rely heavily on drive testing\nand user feedback, which are laborious and require extensive expertise in\ncommunication, networking, and optimization. As large language models (LLMs)\nand their associated technologies advance, particularly in the realms of prompt\nengineering and agent engineering, network optimization will witness a\nrevolutionary approach. This approach entails the strategic use of well-crafted\nprompts to infuse human experience and knowledge into these sophisticated LLMs,\nand the deployment of autonomous agents as a communication bridge to seamlessly\nconnect the machine language based LLMs with human users using natural\nlanguage. Furthermore, our proposed framework incorporates retrieval-augmented\ngeneration (RAG) to enhance the system's ability to acquire domain-specific\nknowledge and generate solutions, thereby enabling the customization and\noptimization of the BSS process. This integration represents the future\nparadigm of artificial intelligence (AI) as a service and AI for more ease.\nThis research first develops a novel LLM-empowered BSS optimization framework,\nand heuristically proposes three different potential implementations: the\nstrategies based on Prompt-optimized LLM (PoL), LLM-empowered autonomous BSS\nagent (LaBa), and Cooperative multiple LLM-based autonomous BSS agents (CLaBa).\nThrough evaluation on real-world data, the experiments demonstrate that\nprompt-assisted LLMs and LLM-based agents can generate more efficient and\nreliable network deployments, noticeably enhancing the efficiency of BSS\noptimization and reducing trivial manual participation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional base station siting (BSS) methods rely heavily on drive testing\nand user feedback, which are laborious and require extensive expertise in\ncommunication, networking, and optimization. As large language models (LLMs)\nand their associated technologies advance, particularly in the realms of prompt\nengineering and agent engineering, network optimization will witness a\nrevolutionary approach. This approach entails the strategic use of well-crafted\nprompts to infuse human experience and knowledge into these sophisticated LLMs,\nand the deployment of autonomous agents as a communication bridge to seamlessly\nconnect the machine language based LLMs with human users using natural\nlanguage. Furthermore, our proposed framework incorporates retrieval-augmented\ngeneration (RAG) to enhance the system's ability to acquire domain-specific\nknowledge and generate solutions, thereby enabling the customization and\noptimization of the BSS process. This integration represents the future\nparadigm of artificial intelligence (AI) as a service and AI for more ease.\nThis research first develops a novel LLM-empowered BSS optimization framework,\nand heuristically proposes three different potential implementations: the\nstrategies based on Prompt-optimized LLM (PoL), LLM-empowered autonomous BSS\nagent (LaBa), and Cooperative multiple LLM-based autonomous BSS agents (CLaBa).\nThrough evaluation on real-world data, the experiments demonstrate that\nprompt-assisted LLMs and LLM-based agents can generate more efficient and\nreliable network deployments, noticeably enhancing the efficiency of BSS\noptimization and reducing trivial manual participation."
                },
                "authors": [
                    {
                        "name": "Yanhu Wang"
                    },
                    {
                        "name": "Muhammad Muzammil Afzal"
                    },
                    {
                        "name": "Zhengyang Li"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Chenyuan Feng"
                    },
                    {
                        "name": "Shuaishuai Guo"
                    },
                    {
                        "name": "Tony Q. S. Quek"
                    }
                ],
                "author_detail": {
                    "name": "Tony Q. S. Quek"
                },
                "author": "Tony Q. S. Quek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03631v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03631v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]