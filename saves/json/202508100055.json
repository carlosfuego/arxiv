[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2508.05211v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05211v1",
                "updated": "2025-08-07T09:47:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    9,
                    47,
                    21,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T09:47:21Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    9,
                    47,
                    21,
                    3,
                    219,
                    0
                ],
                "title": "VFlowOpt: A Token Pruning Framework for LMMs with Visual Information\n  Flow-Guided Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VFlowOpt: A Token Pruning Framework for LMMs with Visual Information\n  Flow-Guided Optimization"
                },
                "summary": "Large Multimodal Models (LMMs) excel in visual-language tasks by leveraging\nnumerous visual tokens for fine-grained visual information, but this token\nredundancy results in significant computational costs. Previous research aimed\nat reducing visual tokens during inference typically leverages importance maps\nderived from attention scores among vision-only tokens or vision-language\ntokens to prune tokens across one or multiple pruning stages. Despite this\nprogress, pruning frameworks and strategies remain simplistic and\ninsufficiently explored, often resulting in substantial performance\ndegradation. In this paper, we propose VFlowOpt, a token pruning framework that\nintroduces an importance map derivation process and a progressive pruning\nmodule with a recycling mechanism. The hyperparameters of its pruning strategy\nare further optimized by a visual information flow-guided method. Specifically,\nwe compute an importance map for image tokens based on their attention-derived\ncontext relevance and patch-level information entropy. We then decide which\ntokens to retain or prune and aggregate the pruned ones as recycled tokens to\navoid potential information loss. Finally, we apply a visual information\nflow-guided method that regards the last token in the LMM as the most\nrepresentative signal of text-visual interactions. This method minimizes the\ndiscrepancy between token representations in LMMs with and without pruning,\nthereby enabling superior pruning strategies tailored to different LMMs.\nExperiments demonstrate that VFlowOpt can prune 90% of visual tokens while\nmaintaining comparable performance, leading to an 89% reduction in KV-Cache\nmemory and 3.8 times faster inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) excel in visual-language tasks by leveraging\nnumerous visual tokens for fine-grained visual information, but this token\nredundancy results in significant computational costs. Previous research aimed\nat reducing visual tokens during inference typically leverages importance maps\nderived from attention scores among vision-only tokens or vision-language\ntokens to prune tokens across one or multiple pruning stages. Despite this\nprogress, pruning frameworks and strategies remain simplistic and\ninsufficiently explored, often resulting in substantial performance\ndegradation. In this paper, we propose VFlowOpt, a token pruning framework that\nintroduces an importance map derivation process and a progressive pruning\nmodule with a recycling mechanism. The hyperparameters of its pruning strategy\nare further optimized by a visual information flow-guided method. Specifically,\nwe compute an importance map for image tokens based on their attention-derived\ncontext relevance and patch-level information entropy. We then decide which\ntokens to retain or prune and aggregate the pruned ones as recycled tokens to\navoid potential information loss. Finally, we apply a visual information\nflow-guided method that regards the last token in the LMM as the most\nrepresentative signal of text-visual interactions. This method minimizes the\ndiscrepancy between token representations in LMMs with and without pruning,\nthereby enabling superior pruning strategies tailored to different LMMs.\nExperiments demonstrate that VFlowOpt can prune 90% of visual tokens while\nmaintaining comparable performance, leading to an 89% reduction in KV-Cache\nmemory and 3.8 times faster inference."
                },
                "authors": [
                    {
                        "name": "Sihan Yang"
                    },
                    {
                        "name": "Runsen Xu"
                    },
                    {
                        "name": "Chenhang Cui"
                    },
                    {
                        "name": "Tai Wang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05211v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05211v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05091v1",
                "updated": "2025-08-07T07:19:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    7,
                    19,
                    2,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T07:19:02Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    7,
                    19,
                    2,
                    3,
                    219,
                    0
                ],
                "title": "PoseGen: In-Context LoRA Finetuning for Pose-Controllable Long Human\n  Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PoseGen: In-Context LoRA Finetuning for Pose-Controllable Long Human\n  Video Generation"
                },
                "summary": "Generating long, temporally coherent videos with precise control over subject\nidentity and motion is a formidable challenge for current diffusion models,\nwhich often suffer from identity drift and are limited to short clips. We\nintroduce PoseGen, a novel framework that generates arbitrarily long videos of\na specific subject from a single reference image and a driving pose sequence.\nOur core innovation is an in-context LoRA finetuning strategy that injects\nsubject appearance at the token level for identity preservation, while\nsimultaneously conditioning on pose information at the channel level for\nfine-grained motion control. To overcome duration limits, PoseGen pioneers an\ninterleaved segment generation method that seamlessly stitches video clips\ntogether, using a shared KV cache mechanism and a specialized transition\nprocess to ensure background consistency and temporal smoothness. Trained on a\nremarkably small 33-hour video dataset, extensive experiments show that PoseGen\nsignificantly outperforms state-of-the-art methods in identity fidelity, pose\naccuracy, and its unique ability to produce coherent, artifact-free videos of\nunlimited duration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating long, temporally coherent videos with precise control over subject\nidentity and motion is a formidable challenge for current diffusion models,\nwhich often suffer from identity drift and are limited to short clips. We\nintroduce PoseGen, a novel framework that generates arbitrarily long videos of\na specific subject from a single reference image and a driving pose sequence.\nOur core innovation is an in-context LoRA finetuning strategy that injects\nsubject appearance at the token level for identity preservation, while\nsimultaneously conditioning on pose information at the channel level for\nfine-grained motion control. To overcome duration limits, PoseGen pioneers an\ninterleaved segment generation method that seamlessly stitches video clips\ntogether, using a shared KV cache mechanism and a specialized transition\nprocess to ensure background consistency and temporal smoothness. Trained on a\nremarkably small 33-hour video dataset, extensive experiments show that PoseGen\nsignificantly outperforms state-of-the-art methods in identity fidelity, pose\naccuracy, and its unique ability to produce coherent, artifact-free videos of\nunlimited duration."
                },
                "authors": [
                    {
                        "name": "Jingxuan He"
                    },
                    {
                        "name": "Busheng Su"
                    },
                    {
                        "name": "Finn Wong"
                    }
                ],
                "author_detail": {
                    "name": "Finn Wong"
                },
                "author": "Finn Wong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05012v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05012v1",
                "updated": "2025-08-07T03:49:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    3,
                    49,
                    56,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T03:49:56Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    3,
                    49,
                    56,
                    3,
                    219,
                    0
                ],
                "title": "Making Prompts First-Class Citizens for Adaptive LLM Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making Prompts First-Class Citizens for Adaptive LLM Pipelines"
                },
                "summary": "Modern LLM pipelines increasingly resemble data-centric systems: they\nretrieve external context, compose intermediate outputs, validate results, and\nadapt based on runtime feedback. Yet, the central element guiding this process\n-- the prompt -- remains a brittle, opaque string, disconnected from the\nsurrounding dataflow. This disconnect limits reuse, optimization, and runtime\ncontrol.\n  In this paper, we describe our vision and an initial design for SPEAR, a\nlanguage and runtime that fills this prompt management gap by making prompts\nstructured, adaptive, and first-class components of the execution model. SPEAR\nenables (1) runtime prompt refinement -- modifying prompts dynamically in\nresponse to execution-time signals such as confidence, latency, or missing\ncontext; and (2) structured prompt management -- organizing prompt fragments\ninto versioned views with support for introspection and logging.\n  SPEAR defines a prompt algebra that governs how prompts are constructed and\nadapted within a pipeline. It supports multiple refinement modes (manual,\nassisted, and automatic), giving developers a balance between control and\nautomation. By treating prompt logic as structured data, SPEAR enables\noptimizations such as operator fusion, prefix caching, and view reuse.\nPreliminary experiments quantify the behavior of different refinement modes\ncompared to static prompts and agentic retries, as well as the impact of\nprompt-level optimizations such as operator fusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern LLM pipelines increasingly resemble data-centric systems: they\nretrieve external context, compose intermediate outputs, validate results, and\nadapt based on runtime feedback. Yet, the central element guiding this process\n-- the prompt -- remains a brittle, opaque string, disconnected from the\nsurrounding dataflow. This disconnect limits reuse, optimization, and runtime\ncontrol.\n  In this paper, we describe our vision and an initial design for SPEAR, a\nlanguage and runtime that fills this prompt management gap by making prompts\nstructured, adaptive, and first-class components of the execution model. SPEAR\nenables (1) runtime prompt refinement -- modifying prompts dynamically in\nresponse to execution-time signals such as confidence, latency, or missing\ncontext; and (2) structured prompt management -- organizing prompt fragments\ninto versioned views with support for introspection and logging.\n  SPEAR defines a prompt algebra that governs how prompts are constructed and\nadapted within a pipeline. It supports multiple refinement modes (manual,\nassisted, and automatic), giving developers a balance between control and\nautomation. By treating prompt logic as structured data, SPEAR enables\noptimizations such as operator fusion, prefix caching, and view reuse.\nPreliminary experiments quantify the behavior of different refinement modes\ncompared to static prompts and agentic retries, as well as the impact of\nprompt-level optimizations such as operator fusion."
                },
                "authors": [
                    {
                        "name": "Ugur Cetintemel"
                    },
                    {
                        "name": "Shu Chen"
                    },
                    {
                        "name": "Alexander W. Lee"
                    },
                    {
                        "name": "Deepti Raghavan"
                    }
                ],
                "author_detail": {
                    "name": "Deepti Raghavan"
                },
                "author": "Deepti Raghavan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05012v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05012v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04449v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04449v2",
                "updated": "2025-08-06T16:57:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    57,
                    39,
                    2,
                    218,
                    0
                ],
                "published": "2024-12-05T18:58:03Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    58,
                    3,
                    3,
                    340,
                    0
                ],
                "title": "p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay"
                },
                "summary": "Despite the remarkable performance of multimodal large language models\n(MLLMs) across diverse tasks, the substantial training and inference costs\nimpede their advancement. In this paper, we propose p-MoD, an efficient MLLM\narchitecture that significantly reduces training and inference costs while\nmaintaining model performance. The majority of computation in MLLMs stems from\nthe overwhelming volume of vision tokens processed by the transformer-based\nLLM. Accordingly, we leverage the Mixture-of-Depths (MoD) mechanism, where each\nLLM layer selects essential vision tokens to process while skipping redundant\nones. However, integrating MoD into MLLMs is non-trivial. To address the\nchallenges of training and inference stability as well as limited training\ndata, we adapt the MoD module with two novel designs: tanh-gated weight\nnormalization (TanhNorm) and symmetric token reweighting (STRing). Moreover, we\nobserve that vision tokens exhibit higher redundancy in deeper layers and thus\ndesign a progressive ratio decay (PRD) strategy, which gradually reduces the\ntoken retention ratio layer by layer, employing a shifted cosine schedule. This\ncrucial design fully unleashes the potential of MoD, significantly boosting the\nefficiency and performance of our models. Extensive experiments on two baseline\nmodels across 15 benchmarks show that our model matches or even surpasses the\nperformance of corresponding baselines, while requiring only 55.6% TFLOPs and\n53.7% KV cache storage during inference, and 77.7% GPU hours during training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable performance of multimodal large language models\n(MLLMs) across diverse tasks, the substantial training and inference costs\nimpede their advancement. In this paper, we propose p-MoD, an efficient MLLM\narchitecture that significantly reduces training and inference costs while\nmaintaining model performance. The majority of computation in MLLMs stems from\nthe overwhelming volume of vision tokens processed by the transformer-based\nLLM. Accordingly, we leverage the Mixture-of-Depths (MoD) mechanism, where each\nLLM layer selects essential vision tokens to process while skipping redundant\nones. However, integrating MoD into MLLMs is non-trivial. To address the\nchallenges of training and inference stability as well as limited training\ndata, we adapt the MoD module with two novel designs: tanh-gated weight\nnormalization (TanhNorm) and symmetric token reweighting (STRing). Moreover, we\nobserve that vision tokens exhibit higher redundancy in deeper layers and thus\ndesign a progressive ratio decay (PRD) strategy, which gradually reduces the\ntoken retention ratio layer by layer, employing a shifted cosine schedule. This\ncrucial design fully unleashes the potential of MoD, significantly boosting the\nefficiency and performance of our models. Extensive experiments on two baseline\nmodels across 15 benchmarks show that our model matches or even surpasses the\nperformance of corresponding baselines, while requiring only 55.6% TFLOPs and\n53.7% KV cache storage during inference, and 77.7% GPU hours during training."
                },
                "authors": [
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Desen Meng"
                    },
                    {
                        "name": "Zhengming Zhang"
                    },
                    {
                        "name": "Zhenpeng Huang"
                    },
                    {
                        "name": "Tao Wu"
                    },
                    {
                        "name": "Limin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Limin Wang"
                },
                "author": "Limin Wang",
                "arxiv_comment": "Accepted by ICCV 2025; Code released at\n  https://github.com/MCG-NJU/p-MoD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04449v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04449v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04581v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04581v1",
                "updated": "2025-08-06T16:06:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    6,
                    43,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T16:06:43Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    6,
                    43,
                    2,
                    218,
                    0
                ],
                "title": "Share Your Attention: Transformer Weight Sharing via Matrix-based\n  Dictionary Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Share Your Attention: Transformer Weight Sharing via Matrix-based\n  Dictionary Learning"
                },
                "summary": "Large language models (LLMs) have revolutionized AI applications, yet their\nhigh computational and memory demands hinder their widespread deployment.\nExisting compression techniques focus on intra-block optimizations (e.g.\nlow-rank approximation, attention head pruning), while the repetitive layered\nstructure of transformers implies significant inter-block redundancy - a\ndimension largely unexplored beyond key-value (KV) caching. Inspired by\ndictionary learning in CNNs, we propose a framework for structured weight\nsharing across transformer layers. Our approach decomposes attention projection\nmatrices into shared dictionary atoms, reducing the attention module's\nparameters by 66.7% while achieving on-par performance. Unlike complex methods\nrequiring distillation or architectural changes, MASA (Matrix Atom Sharing in\nAttention) operates as a drop-in replacement - trained with standard optimizers\n- and represents each layer's weights as linear combinations of shared matrix\natoms. Experiments across scales (100M-700M parameters) show that MASA achieves\nbetter benchmark accuracy and perplexity than grouped-query attention (GQA),\nlow-rank baselines and recently proposed Repeat-all-over/Sequential sharing at\ncomparable parameter budgets. Ablation studies confirm robustness to the\ndictionary size and the efficacy of shared representations in capturing\ncross-layer statistical regularities. Extending to Vision Transformers (ViT),\nMASA matches performance metrics on image classification and detection tasks\nwith 66.7% fewer attention parameters. By combining dictionary learning\nstrategies with transformer efficiency, MASA offers a scalable blueprint for\nparameter-efficient models without sacrificing performance. Finally, we\ninvestigate the possibility of employing MASA on pretrained LLMs to reduce\ntheir number of parameters without experiencing any significant drop in their\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have revolutionized AI applications, yet their\nhigh computational and memory demands hinder their widespread deployment.\nExisting compression techniques focus on intra-block optimizations (e.g.\nlow-rank approximation, attention head pruning), while the repetitive layered\nstructure of transformers implies significant inter-block redundancy - a\ndimension largely unexplored beyond key-value (KV) caching. Inspired by\ndictionary learning in CNNs, we propose a framework for structured weight\nsharing across transformer layers. Our approach decomposes attention projection\nmatrices into shared dictionary atoms, reducing the attention module's\nparameters by 66.7% while achieving on-par performance. Unlike complex methods\nrequiring distillation or architectural changes, MASA (Matrix Atom Sharing in\nAttention) operates as a drop-in replacement - trained with standard optimizers\n- and represents each layer's weights as linear combinations of shared matrix\natoms. Experiments across scales (100M-700M parameters) show that MASA achieves\nbetter benchmark accuracy and perplexity than grouped-query attention (GQA),\nlow-rank baselines and recently proposed Repeat-all-over/Sequential sharing at\ncomparable parameter budgets. Ablation studies confirm robustness to the\ndictionary size and the efficacy of shared representations in capturing\ncross-layer statistical regularities. Extending to Vision Transformers (ViT),\nMASA matches performance metrics on image classification and detection tasks\nwith 66.7% fewer attention parameters. By combining dictionary learning\nstrategies with transformer efficiency, MASA offers a scalable blueprint for\nparameter-efficient models without sacrificing performance. Finally, we\ninvestigate the possibility of employing MASA on pretrained LLMs to reduce\ntheir number of parameters without experiencing any significant drop in their\nperformance."
                },
                "authors": [
                    {
                        "name": "Magauiya Zhussip"
                    },
                    {
                        "name": "Dmitriy Shopkhoev"
                    },
                    {
                        "name": "Ammar Ali"
                    },
                    {
                        "name": "Stamatios Lefkimmiatis"
                    }
                ],
                "author_detail": {
                    "name": "Stamatios Lefkimmiatis"
                },
                "author": "Stamatios Lefkimmiatis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04581v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04581v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.01516v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.01516v3",
                "updated": "2025-08-06T15:38:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    15,
                    38,
                    6,
                    2,
                    218,
                    0
                ],
                "published": "2023-05-02T15:27:16Z",
                "published_parsed": [
                    2023,
                    5,
                    2,
                    15,
                    27,
                    16,
                    1,
                    122,
                    0
                ],
                "title": "From FASTER to F2: Evolving Concurrent Key-Value Store Designs for Large\n  Skewed Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From FASTER to F2: Evolving Concurrent Key-Value Store Designs for Large\n  Skewed Workloads"
                },
                "summary": "Modern large-scale services such as search engines, messaging platforms, and\nserverless functions, rely on key-value (KV) stores to maintain high\nperformance at scale. When such services are deployed in constrained memory\nenvironments, they present challenging requirements: point operations requiring\nhigh throughput, working sets much larger than main memory, and natural skew in\nkey access patterns. Traditional KV stores, based on LSM- and B-Trees, have\nbeen widely used to handle such use cases, but they often suffer from\nsuboptimal use of modern hardware resources. The FASTER project, developed as a\nhigh-performance open-source KV storage library, has demonstrated remarkable\nsuccess in both in-memory and hybrid storage environments. However, when tasked\nwith serving large skewed workloads, it faced challenges, including high\nindexing and compactions overheads, and inefficient management of\nnon-overlapping read-hot and write-hot working sets.\n  In this paper, we introduce F2 (for FASTER v2), an evolution of FASTER\ndesigned to meet the requirements of large skewed workloads common in industry\napplications. F2 adopts a two-tier record-oriented design to handle\nlarger-than-memory skewed workloads, along with new concurrent latch-free\nmechanisms and components to maximize performance on modern hardware. To\nrealize this design, F2 tackles key challenges and introduces several\ninnovations, including new latch-free algorithms for multi-threaded log\ncompaction, a two-level hash index to reduce indexing overhead for cold\nrecords, and a read-cache for serving read-hot records. Our evaluation shows\nthat F2 achieves 2-11.9x better throughput compared to existing KV stores,\neffectively serving the target workload. F2 is open-source and available as\npart of the FASTER project.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large-scale services such as search engines, messaging platforms, and\nserverless functions, rely on key-value (KV) stores to maintain high\nperformance at scale. When such services are deployed in constrained memory\nenvironments, they present challenging requirements: point operations requiring\nhigh throughput, working sets much larger than main memory, and natural skew in\nkey access patterns. Traditional KV stores, based on LSM- and B-Trees, have\nbeen widely used to handle such use cases, but they often suffer from\nsuboptimal use of modern hardware resources. The FASTER project, developed as a\nhigh-performance open-source KV storage library, has demonstrated remarkable\nsuccess in both in-memory and hybrid storage environments. However, when tasked\nwith serving large skewed workloads, it faced challenges, including high\nindexing and compactions overheads, and inefficient management of\nnon-overlapping read-hot and write-hot working sets.\n  In this paper, we introduce F2 (for FASTER v2), an evolution of FASTER\ndesigned to meet the requirements of large skewed workloads common in industry\napplications. F2 adopts a two-tier record-oriented design to handle\nlarger-than-memory skewed workloads, along with new concurrent latch-free\nmechanisms and components to maximize performance on modern hardware. To\nrealize this design, F2 tackles key challenges and introduces several\ninnovations, including new latch-free algorithms for multi-threaded log\ncompaction, a two-level hash index to reduce indexing overhead for cold\nrecords, and a read-cache for serving read-hot records. Our evaluation shows\nthat F2 achieves 2-11.9x better throughput compared to existing KV stores,\neffectively serving the target workload. F2 is open-source and available as\npart of the FASTER project."
                },
                "authors": [
                    {
                        "name": "Konstantinos Kanellis"
                    },
                    {
                        "name": "Badrish Chandramouli"
                    },
                    {
                        "name": "Ted Hart"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "arxiv_comment": "Proceedings of the VLDB Endowment 18 (VLDB'25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.01516v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.01516v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04462v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04462v1",
                "updated": "2025-08-06T14:02:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    14,
                    2,
                    10,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T14:02:10Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    14,
                    2,
                    10,
                    2,
                    218,
                    0
                ],
                "title": "CARD: Cache-Assisted Parallel Speculative Decoding for Efficient Large\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CARD: Cache-Assisted Parallel Speculative Decoding for Efficient Large\n  Language Model Inference"
                },
                "summary": "Speculative decoding (SD), where an extra draft model first provides multiple\ndraft tokens and the original target model then verifies these tokens in\nparallel, has shown great power for LLM inference acceleration. However,\nexisting SD methods must adhere to the 'draft-then-verify' paradigm, which\nforces drafting and verification processes to execute sequentially during SD,\nresulting in inefficient inference performance and limiting the size of the\ndraft model. Furthermore, once a single token in the candidate sequence is\nrejected during the drafting process, all subsequent candidate tokens must be\ndiscarded, leading to inefficient drafting. To address these challenges, we\npropose a cache-based parallel speculative decoding framework employing a\n'query-and-correct' paradigm. Specifically, CARD decouples drafting and\nverification: the draft model generates candidate tokens to populate a shared\ncache, while the target model concurrently rectifies the draft model's\ngeneration direction. This effectively enables the target model to perform\ninference at speed approaching that of the draft model. Our approach achieves\nup to 4.83 speedup over vanilla decoding without requiring fine-tuning of\neither the draft or target models. Our code is available at\nhttps://github.com/hunzhizi/CARD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding (SD), where an extra draft model first provides multiple\ndraft tokens and the original target model then verifies these tokens in\nparallel, has shown great power for LLM inference acceleration. However,\nexisting SD methods must adhere to the 'draft-then-verify' paradigm, which\nforces drafting and verification processes to execute sequentially during SD,\nresulting in inefficient inference performance and limiting the size of the\ndraft model. Furthermore, once a single token in the candidate sequence is\nrejected during the drafting process, all subsequent candidate tokens must be\ndiscarded, leading to inefficient drafting. To address these challenges, we\npropose a cache-based parallel speculative decoding framework employing a\n'query-and-correct' paradigm. Specifically, CARD decouples drafting and\nverification: the draft model generates candidate tokens to populate a shared\ncache, while the target model concurrently rectifies the draft model's\ngeneration direction. This effectively enables the target model to perform\ninference at speed approaching that of the draft model. Our approach achieves\nup to 4.83 speedup over vanilla decoding without requiring fine-tuning of\neither the draft or target models. Our code is available at\nhttps://github.com/hunzhizi/CARD."
                },
                "authors": [
                    {
                        "name": "Enyu Zhou"
                    },
                    {
                        "name": "Kai Sheng"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Xin He"
                    }
                ],
                "author_detail": {
                    "name": "Xin He"
                },
                "author": "Xin He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04462v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04462v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24007v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24007v3",
                "updated": "2025-08-06T11:46:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    11,
                    46,
                    11,
                    2,
                    218,
                    0
                ],
                "published": "2025-03-31T12:32:23Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    32,
                    23,
                    0,
                    90,
                    0
                ],
                "title": "CITRAS: Covariate-Informed Transformer for Time Series Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CITRAS: Covariate-Informed Transformer for Time Series Forecasting"
                },
                "summary": "In practical time series forecasting, covariates provide rich contextual\ninformation that can potentially enhance the forecast of target variables.\nAlthough some covariates extend into the future forecasting horizon (e.g.,\ncalendar events, discount schedules), most multivariate models fail to leverage\nthis pivotal insight due to the length discrepancy with target variables.\nAdditionally, capturing the dependency between target variables and covariates\nis non-trivial, as models must precisely reflect the local impact of covariates\nwhile also capturing global cross-variate dependencies. To overcome these\nchallenges, we propose CITRAS, a decoder-only Transformer that flexibly\nleverages multiple targets, past covariates, and future covariates. While\npreserving strong autoregressive capabilities, CITRAS introduces two novel\nmechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift and\nAttention Score Smoothing. KV Shift seamlessly incorporates future covariates\ninto the forecasting of target variables based on their concurrent\ndependencies. Additionally, Attention Score Smoothing refines locally accurate\npatch-wise cross-variate dependencies into global variate-level dependencies by\nsmoothing the past series of attention scores. Experimentally, CITRAS\noutperforms state-of-the-art models on thirteen real-world benchmarks from both\ncovariate-informed and multivariate settings, demonstrating its versatile\nability to leverage cross-variate and cross-time dependencies for improved\nforecasting accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In practical time series forecasting, covariates provide rich contextual\ninformation that can potentially enhance the forecast of target variables.\nAlthough some covariates extend into the future forecasting horizon (e.g.,\ncalendar events, discount schedules), most multivariate models fail to leverage\nthis pivotal insight due to the length discrepancy with target variables.\nAdditionally, capturing the dependency between target variables and covariates\nis non-trivial, as models must precisely reflect the local impact of covariates\nwhile also capturing global cross-variate dependencies. To overcome these\nchallenges, we propose CITRAS, a decoder-only Transformer that flexibly\nleverages multiple targets, past covariates, and future covariates. While\npreserving strong autoregressive capabilities, CITRAS introduces two novel\nmechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift and\nAttention Score Smoothing. KV Shift seamlessly incorporates future covariates\ninto the forecasting of target variables based on their concurrent\ndependencies. Additionally, Attention Score Smoothing refines locally accurate\npatch-wise cross-variate dependencies into global variate-level dependencies by\nsmoothing the past series of attention scores. Experimentally, CITRAS\noutperforms state-of-the-art models on thirteen real-world benchmarks from both\ncovariate-informed and multivariate settings, demonstrating its versatile\nability to leverage cross-variate and cross-time dependencies for improved\nforecasting accuracy."
                },
                "authors": [
                    {
                        "name": "Yosuke Yamaguchi"
                    },
                    {
                        "name": "Issei Suemitsu"
                    },
                    {
                        "name": "Wenpeng Wei"
                    }
                ],
                "author_detail": {
                    "name": "Wenpeng Wei"
                },
                "author": "Wenpeng Wei",
                "arxiv_comment": "Submission under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24007v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24007v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04257v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04257v1",
                "updated": "2025-08-06T09:40:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    9,
                    40,
                    9,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-06T09:40:09Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    9,
                    40,
                    9,
                    2,
                    218,
                    0
                ],
                "title": "KVSink: Understanding and Enhancing the Preservation of Attention Sinks\n  in KV Cache Quantization for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVSink: Understanding and Enhancing the Preservation of Attention Sinks\n  in KV Cache Quantization for LLMs"
                },
                "summary": "Key-Value (KV) cache quantization has become a widely adopted optimization\ntechnique for efficient large language models (LLMs) inference by reducing KV\ncache memory usage and mitigating memory-bound constraints. Recent studies have\nemphasized the importance of preserving the original precision of KVs for the\nfirst few tokens to ensure the protection of attention sinks. While this\napproach has proven effective in mitigating performance degradation, its\nunderlying principles remain insufficiently understood. Moreover, it fails to\naddress the recent discovery that attention sinks can emerge beyond the initial\ntoken positions. In this work, we elucidate the underlying mechanisms of\nattention sinks during inference by examining their role in the cross-layer\nevolution of extreme activation outliers. Additionally, we provide a\ncomprehensive analysis of the interplay between attention sinks and KV cache\nquantization. Based on our enhanced understanding, we introduce\n\\textit{\\textbf{KVSink}}, a plug-and-play method that effectively predicts sink\ntokens with negligible overhead, enabling more thorough preservation. Extensive\nexperiments demonstrate that KVSink outperforms the existing Preserve-First-N\n(PFN) strategy, offering more effective preservation of attention sinks during\nKV cache quantization. Moreover, when applied to the well-established KVQuant\nmethod, KVSink further improves perplexity (PPL) and reduces reliance on 16-bit\nnumerical outliers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) cache quantization has become a widely adopted optimization\ntechnique for efficient large language models (LLMs) inference by reducing KV\ncache memory usage and mitigating memory-bound constraints. Recent studies have\nemphasized the importance of preserving the original precision of KVs for the\nfirst few tokens to ensure the protection of attention sinks. While this\napproach has proven effective in mitigating performance degradation, its\nunderlying principles remain insufficiently understood. Moreover, it fails to\naddress the recent discovery that attention sinks can emerge beyond the initial\ntoken positions. In this work, we elucidate the underlying mechanisms of\nattention sinks during inference by examining their role in the cross-layer\nevolution of extreme activation outliers. Additionally, we provide a\ncomprehensive analysis of the interplay between attention sinks and KV cache\nquantization. Based on our enhanced understanding, we introduce\n\\textit{\\textbf{KVSink}}, a plug-and-play method that effectively predicts sink\ntokens with negligible overhead, enabling more thorough preservation. Extensive\nexperiments demonstrate that KVSink outperforms the existing Preserve-First-N\n(PFN) strategy, offering more effective preservation of attention sinks during\nKV cache quantization. Moreover, when applied to the well-established KVQuant\nmethod, KVSink further improves perplexity (PPL) and reduces reliance on 16-bit\nnumerical outliers."
                },
                "authors": [
                    {
                        "name": "Zunhai Su"
                    },
                    {
                        "name": "Kehong Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Kehong Yuan"
                },
                "author": "Kehong Yuan",
                "arxiv_comment": "Published as a conference paper at COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04257v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04257v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00370v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00370v2",
                "updated": "2025-08-06T08:32:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    6,
                    8,
                    32,
                    53,
                    2,
                    218,
                    0
                ],
                "published": "2025-08-01T07:03:16Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    7,
                    3,
                    16,
                    4,
                    213,
                    0
                ],
                "title": "EdgeInfinite-Instruct: Bridging SFT-Based Optimization and NPU-Level\n  Efficiency for Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeInfinite-Instruct: Bridging SFT-Based Optimization and NPU-Level\n  Efficiency for Edge Devices"
                },
                "summary": "Deploying Transformer-based large language models (LLMs) on\nresource-constrained edge devices for long-sequence tasks remains challenging\ndue to the quadratic time complexity of self-attention and growing Key-Value\n(KV) cache demands. While existing KV cache optimizations improve memory\nefficiency, they often fail to reduce time to first token (TTFT) and may\ndegrade performance through token pruning. Alternative sequence modeling\narchitectures address some of these limitations, but typically require full\nretraining and lack infrastructure support. EdgeInfinite offers an efficient\nsolution by fine-tuning only a small subset of parameters, maintaining quality\nwhile reducing both computational and memory costs, including improved TTFT.\nHowever, its instruction-following ability is limited, and it lacks\nmobile-specific optimizations. To address these issues, we propose\nEdgeInfinite-Instruct, which introduces a Segmented Supervised Fine-Tuning\n(S-SFT) strategy tailored to long-sequence tasks such as summarization and\nquestion answering. We further optimized EdgeInfinite-Instruct for efficient\ndeployment on edge NPUs by employing fine-grained post-training quantization\n(PTQ) to reduce computational demands while maintaining accuracy, and by\nimplementing a fixed-shape computation graph that balances memory usage and\non-device efficiency through scenario-specific customization of input token and\ncache sizes. Experiments on long-context benchmarks and real-world mobile tasks\nshow that our approach improves domain-specific performance while maintaining\nefficiency on NPU-accelerated edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying Transformer-based large language models (LLMs) on\nresource-constrained edge devices for long-sequence tasks remains challenging\ndue to the quadratic time complexity of self-attention and growing Key-Value\n(KV) cache demands. While existing KV cache optimizations improve memory\nefficiency, they often fail to reduce time to first token (TTFT) and may\ndegrade performance through token pruning. Alternative sequence modeling\narchitectures address some of these limitations, but typically require full\nretraining and lack infrastructure support. EdgeInfinite offers an efficient\nsolution by fine-tuning only a small subset of parameters, maintaining quality\nwhile reducing both computational and memory costs, including improved TTFT.\nHowever, its instruction-following ability is limited, and it lacks\nmobile-specific optimizations. To address these issues, we propose\nEdgeInfinite-Instruct, which introduces a Segmented Supervised Fine-Tuning\n(S-SFT) strategy tailored to long-sequence tasks such as summarization and\nquestion answering. We further optimized EdgeInfinite-Instruct for efficient\ndeployment on edge NPUs by employing fine-grained post-training quantization\n(PTQ) to reduce computational demands while maintaining accuracy, and by\nimplementing a fixed-shape computation graph that balances memory usage and\non-device efficiency through scenario-specific customization of input token and\ncache sizes. Experiments on long-context benchmarks and real-world mobile tasks\nshow that our approach improves domain-specific performance while maintaining\nefficiency on NPU-accelerated edge devices."
                },
                "authors": [
                    {
                        "name": "Jiyu Chen"
                    },
                    {
                        "name": "Poh Seng Lim"
                    },
                    {
                        "name": "Shuang Peng"
                    },
                    {
                        "name": "Daxiong Luo"
                    },
                    {
                        "name": "JungHau Foo"
                    },
                    {
                        "name": "Yap Deep"
                    },
                    {
                        "name": "Timothy Lee Jun Jie"
                    },
                    {
                        "name": "Kelvin Teh Kae Wen"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Danyu Feng"
                    },
                    {
                        "name": "Hao-Yun Chen"
                    },
                    {
                        "name": "Peng-Wen Chen"
                    },
                    {
                        "name": "Fangyuan Li"
                    },
                    {
                        "name": "Xiaoxin Chen"
                    },
                    {
                        "name": "Wong Wai Mun"
                    }
                ],
                "author_detail": {
                    "name": "Wong Wai Mun"
                },
                "author": "Wong Wai Mun",
                "arxiv_comment": "The data and method in the paper need to be re-audited",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00370v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00370v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03974v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03974v1",
                "updated": "2025-08-05T23:47:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    23,
                    47,
                    34,
                    1,
                    217,
                    0
                ],
                "published": "2025-08-05T23:47:34Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    23,
                    47,
                    34,
                    1,
                    217,
                    0
                ],
                "title": "Managing Data for Scalable and Interactive Event Sequence Visualization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Managing Data for Scalable and Interactive Event Sequence Visualization"
                },
                "summary": "Parallel event sequences, such as those collected in program execution traces\nand automated manufacturing pipelines, are typically visualized as interactive\nparallel timelines. As the dataset size grows, these charts frequently\nexperience lag during common interactions such as zooming, panning, and\nfiltering. Summarization approaches can improve interaction performance, but at\nthe cost of accuracy in representation. To address this challenge, we introduce\nESeMan (Event Sequence Manager), an event sequence management system designed\nto support interactive rendering of timeline visualizations with tunable\naccuracy. ESeMan employs hierarchical data structures and intelligent caching\nto provide visualizations with only the data necessary to generate accurate\nsummarizations with significantly reduced data fetch time. We evaluate ESeMan's\nquery times against summed area tables, M4 aggregation, and statistical\nsub-sampling on a variety of program execution traces. Our results demonstrate\nESeMan provides better performance, achieving sub-100ms fetch times while\nmaintaining visualization accuracy at the pixel level. We further present our\nbenchmarking harness, enabling future performance evaluations for event\nsequence visualization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel event sequences, such as those collected in program execution traces\nand automated manufacturing pipelines, are typically visualized as interactive\nparallel timelines. As the dataset size grows, these charts frequently\nexperience lag during common interactions such as zooming, panning, and\nfiltering. Summarization approaches can improve interaction performance, but at\nthe cost of accuracy in representation. To address this challenge, we introduce\nESeMan (Event Sequence Manager), an event sequence management system designed\nto support interactive rendering of timeline visualizations with tunable\naccuracy. ESeMan employs hierarchical data structures and intelligent caching\nto provide visualizations with only the data necessary to generate accurate\nsummarizations with significantly reduced data fetch time. We evaluate ESeMan's\nquery times against summed area tables, M4 aggregation, and statistical\nsub-sampling on a variety of program execution traces. Our results demonstrate\nESeMan provides better performance, achieving sub-100ms fetch times while\nmaintaining visualization accuracy at the pixel level. We further present our\nbenchmarking harness, enabling future performance evaluations for event\nsequence visualization."
                },
                "authors": [
                    {
                        "name": "Sayef Azad Sakin"
                    },
                    {
                        "name": "Katherine E. Isaacs"
                    }
                ],
                "author_detail": {
                    "name": "Katherine E. Isaacs"
                },
                "author": "Katherine E. Isaacs",
                "arxiv_comment": "The 15th IEEE Workshop on Large Data Analysis and Visualization",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03974v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03974v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03837v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03837v1",
                "updated": "2025-08-05T18:34:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    18,
                    34,
                    48,
                    1,
                    217,
                    0
                ],
                "published": "2025-08-05T18:34:48Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    18,
                    34,
                    48,
                    1,
                    217,
                    0
                ],
                "title": "Rhea: a Framework for Fast Design and Validation of RTL Cache-Coherent\n  Memory Subsystems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rhea: a Framework for Fast Design and Validation of RTL Cache-Coherent\n  Memory Subsystems"
                },
                "summary": "Designing and validating efficient cache-coherent memory subsystems is a\ncritical yet complex task in the development of modern multi-core\nsystem-on-chip architectures. Rhea is a unified framework that streamlines the\ndesign and system-level validation of RTL cache-coherent memory subsystems. On\nthe design side, Rhea generates synthesizable, highly configurable RTL\nsupporting various architectural parameters. On the validation side, Rhea\nintegrates Verilator's cycle-accurate RTL simulation with gem5's full-system\nsimulation, allowing realistic workloads and operating systems to run alongside\nthe actual RTL under test. We apply Rhea to design MSI-based RTL memory\nsubsystems with one and two levels of private caches and scaling up to sixteen\ncores. Their evaluation with 22 applications from state-of-the-art benchmark\nsuites shows intermediate performance relative to gem5 Ruby's MI and MOESI\nmodels. The hybrid gem5-Verilator co-simulation flow incurs a moderate\nsimulation overhead, up to 2.7 times compared to gem5 MI, but achieves higher\nfidelity by simulating real RTL hardware. This overhead decreases with scale,\ndown to 1.6 times in sixteen-core scenarios. These results demonstrate Rhea's\neffectiveness and scalability in enabling fast development of RTL\ncache-coherent memory subsystem designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing and validating efficient cache-coherent memory subsystems is a\ncritical yet complex task in the development of modern multi-core\nsystem-on-chip architectures. Rhea is a unified framework that streamlines the\ndesign and system-level validation of RTL cache-coherent memory subsystems. On\nthe design side, Rhea generates synthesizable, highly configurable RTL\nsupporting various architectural parameters. On the validation side, Rhea\nintegrates Verilator's cycle-accurate RTL simulation with gem5's full-system\nsimulation, allowing realistic workloads and operating systems to run alongside\nthe actual RTL under test. We apply Rhea to design MSI-based RTL memory\nsubsystems with one and two levels of private caches and scaling up to sixteen\ncores. Their evaluation with 22 applications from state-of-the-art benchmark\nsuites shows intermediate performance relative to gem5 Ruby's MI and MOESI\nmodels. The hybrid gem5-Verilator co-simulation flow incurs a moderate\nsimulation overhead, up to 2.7 times compared to gem5 MI, but achieves higher\nfidelity by simulating real RTL hardware. This overhead decreases with scale,\ndown to 1.6 times in sixteen-core scenarios. These results demonstrate Rhea's\neffectiveness and scalability in enabling fast development of RTL\ncache-coherent memory subsystem designs."
                },
                "authors": [
                    {
                        "name": "Davide Zoni"
                    },
                    {
                        "name": "Andrea Galimberti"
                    },
                    {
                        "name": "Adriano Guarisco"
                    }
                ],
                "author_detail": {
                    "name": "Adriano Guarisco"
                },
                "author": "Adriano Guarisco",
                "arxiv_comment": "9 pages, 13 figures, 1 table, accepted for presentation at 2025\n  International Conference on Computer-Aided Design (ICCAD), Munich, Germany,\n  October 26-30, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03837v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03837v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07120v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07120v2",
                "updated": "2025-08-05T16:17:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    16,
                    17,
                    1,
                    1,
                    217,
                    0
                ],
                "published": "2025-03-10T09:49:18Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    9,
                    49,
                    18,
                    0,
                    69,
                    0
                ],
                "title": "FEB-Cache: Frequency-Guided Exposure Bias Reduction for Enhancing\n  Diffusion Transformer Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FEB-Cache: Frequency-Guided Exposure Bias Reduction for Enhancing\n  Diffusion Transformer Caching"
                },
                "summary": "Diffusion Transformer (DiT) has exhibited impressive generation capabilities\nbut faces great challenges due to its high computational complexity. To address\nthis issue, various methods, notably feature caching, have been introduced.\nHowever, these approaches focus on aligning non-cache diffusion without\nanalyzing why caching damage the generation processes. In this paper, we first\nconfirm that the cache greatly amplifies the exposure bias, resulting in a\ndecline in the generation quality. However, directly applying noise scaling is\nchallenging for this issue due to the non-smoothness of exposure bias. We found\nthat this phenomenon stems from the mismatch between its frequency response\ncharacteristics and the simple cache of Attention and MLP. Since these two\ncomponents exhibit unique preferences for frequency signals, which provides us\nwith a caching strategy to separate Attention and MLP to achieve an enhanced\nfit of exposure bias and reduce it. Based on this, we introduced FEB-Cache, a\njoint caching strategy that aligns with the non-exposed bias diffusion process\n(which gives us a higher performance cap) of caching Attention and MLP based on\nthe frequency-guided cache table. Our approach combines a comprehensive\nunderstanding of the caching mechanism and offers a new perspective on\nleveraging caching to accelerate the diffusion process. Empirical results\nindicate that FEB-Cache optimizes model performance while concurrently\nfacilitating acceleration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT) has exhibited impressive generation capabilities\nbut faces great challenges due to its high computational complexity. To address\nthis issue, various methods, notably feature caching, have been introduced.\nHowever, these approaches focus on aligning non-cache diffusion without\nanalyzing why caching damage the generation processes. In this paper, we first\nconfirm that the cache greatly amplifies the exposure bias, resulting in a\ndecline in the generation quality. However, directly applying noise scaling is\nchallenging for this issue due to the non-smoothness of exposure bias. We found\nthat this phenomenon stems from the mismatch between its frequency response\ncharacteristics and the simple cache of Attention and MLP. Since these two\ncomponents exhibit unique preferences for frequency signals, which provides us\nwith a caching strategy to separate Attention and MLP to achieve an enhanced\nfit of exposure bias and reduce it. Based on this, we introduced FEB-Cache, a\njoint caching strategy that aligns with the non-exposed bias diffusion process\n(which gives us a higher performance cap) of caching Attention and MLP based on\nthe frequency-guided cache table. Our approach combines a comprehensive\nunderstanding of the caching mechanism and offers a new perspective on\nleveraging caching to accelerate the diffusion process. Empirical results\nindicate that FEB-Cache optimizes model performance while concurrently\nfacilitating acceleration."
                },
                "authors": [
                    {
                        "name": "Zhen Zou"
                    },
                    {
                        "name": "Feng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zhao"
                },
                "author": "Feng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07120v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07120v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03321v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03321v1",
                "updated": "2025-08-05T11:00:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    11,
                    0,
                    41,
                    1,
                    217,
                    0
                ],
                "published": "2025-08-05T11:00:41Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    11,
                    0,
                    41,
                    1,
                    217,
                    0
                ],
                "title": "Bidirectional TLS Handshake Caching for Constrained Industrial IoT\n  Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bidirectional TLS Handshake Caching for Constrained Industrial IoT\n  Scenarios"
                },
                "summary": "While TLS has become the de-facto standard for end-to-end security, its use\nto secure critical communication in evolving industrial IoT scenarios is\nseverely limited by prevalent resource constraints of devices and networks.\nMost notably, the TLS handshake to establish secure connections incurs\nsignificant bandwidth and processing overhead that often cannot be handled in\nconstrained environments. To alleviate this situation, we present BiTHaC which\nrealizes bidirectional TLS handshake caching by exploiting that significant\nparts of repeated TLS handshakes, especially certificates, are static. Thus,\nredundant information neither needs to be transmitted nor corresponding\ncomputations performed, saving valuable bandwidth and processing resources. By\nimplementing BiTHaC for wolfSSL, we show that we can reduce the bandwidth\nconsumption of TLS handshakes by up to 61.1% and the computational overhead by\nup to 8.5%, while incurring only well-manageable memory overhead and preserving\nthe strict security guarantees of TLS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While TLS has become the de-facto standard for end-to-end security, its use\nto secure critical communication in evolving industrial IoT scenarios is\nseverely limited by prevalent resource constraints of devices and networks.\nMost notably, the TLS handshake to establish secure connections incurs\nsignificant bandwidth and processing overhead that often cannot be handled in\nconstrained environments. To alleviate this situation, we present BiTHaC which\nrealizes bidirectional TLS handshake caching by exploiting that significant\nparts of repeated TLS handshakes, especially certificates, are static. Thus,\nredundant information neither needs to be transmitted nor corresponding\ncomputations performed, saving valuable bandwidth and processing resources. By\nimplementing BiTHaC for wolfSSL, we show that we can reduce the bandwidth\nconsumption of TLS handshakes by up to 61.1% and the computational overhead by\nup to 8.5%, while incurring only well-manageable memory overhead and preserving\nthe strict security guarantees of TLS."
                },
                "authors": [
                    {
                        "name": "Jrn Bodenhausen"
                    },
                    {
                        "name": "Simon Mangel"
                    },
                    {
                        "name": "Thomas Vogt"
                    },
                    {
                        "name": "Martin Henze"
                    }
                ],
                "author_detail": {
                    "name": "Martin Henze"
                },
                "author": "Martin Henze",
                "arxiv_comment": "Accepted for publication in Proceedings of the 2025 IEEE 50th\n  Conference on Local Computer Networks (LCN)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03321v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03321v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.03258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.03258v1",
                "updated": "2025-08-05T09:35:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    9,
                    35,
                    52,
                    1,
                    217,
                    0
                ],
                "published": "2025-08-05T09:35:52Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    9,
                    35,
                    52,
                    1,
                    217,
                    0
                ],
                "title": "SmartLLMs Scheduler: A Framework for Cost-Effective LLMs Utilization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmartLLMs Scheduler: A Framework for Cost-Effective LLMs Utilization"
                },
                "summary": "Large Language Models (LLMs) such as GPT-4 and Llama have shown remarkable\ncapabilities in a variety of software engineering tasks. Despite the\nadvancements, their practical deployment faces challenges, including high\nfinancial costs, long response time, and varying performance, especially when\nhandling a large number of queries (jobs). Existing optimization strategies for\ndeploying LLMs for diverse tasks focus on static scheduling, which requires\nextensive training data for performance prediction, increasing the\ncomputational costs and limiting the applicability and flexibility. In this\npaper, we propose the SmartLLMs Scheduler (SLS), a dynamic and cost-effective\nscheduling solution. The key idea is to learn LLMs' performance on diverse\ntasks and incorporate their real-time feedback to update strategies\nperiodically. Specifically, SLS incorporates three key components, including an\nAdaptive Cache Manager, a Performance-Cost Optimized Scheduler, and a Dynamic\nUpdate Manager. The Cache Manager stores the outputs of previously processed\nqueries and employs an adaptive strategy to reduce redundant computations and\nminimize response times. For queries not found in the cache, the Scheduler\ndynamically allocates them to the most suitable LLM based on the predicted\nperformance and cost from models that take both query-specific and LLM-specific\nfeatures as input. The Update Manager continuously refines the cache and\nscheduling strategies based on real-time feedback from the assigned queries to\nenhance decision-making and adapt to evolving task characteristics. To evaluate\nthe effectiveness of SLS, we conduct extensive experiments on two LLM-based\nsoftware engineering tasks, including log parsing and code generation. The\nresults show that SLS significantly outperforms the baseline methods, achieving\nan average performance improvement of 198.82% and an average processing time\nreduction of 63.28%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) such as GPT-4 and Llama have shown remarkable\ncapabilities in a variety of software engineering tasks. Despite the\nadvancements, their practical deployment faces challenges, including high\nfinancial costs, long response time, and varying performance, especially when\nhandling a large number of queries (jobs). Existing optimization strategies for\ndeploying LLMs for diverse tasks focus on static scheduling, which requires\nextensive training data for performance prediction, increasing the\ncomputational costs and limiting the applicability and flexibility. In this\npaper, we propose the SmartLLMs Scheduler (SLS), a dynamic and cost-effective\nscheduling solution. The key idea is to learn LLMs' performance on diverse\ntasks and incorporate their real-time feedback to update strategies\nperiodically. Specifically, SLS incorporates three key components, including an\nAdaptive Cache Manager, a Performance-Cost Optimized Scheduler, and a Dynamic\nUpdate Manager. The Cache Manager stores the outputs of previously processed\nqueries and employs an adaptive strategy to reduce redundant computations and\nminimize response times. For queries not found in the cache, the Scheduler\ndynamically allocates them to the most suitable LLM based on the predicted\nperformance and cost from models that take both query-specific and LLM-specific\nfeatures as input. The Update Manager continuously refines the cache and\nscheduling strategies based on real-time feedback from the assigned queries to\nenhance decision-making and adapt to evolving task characteristics. To evaluate\nthe effectiveness of SLS, we conduct extensive experiments on two LLM-based\nsoftware engineering tasks, including log parsing and code generation. The\nresults show that SLS significantly outperforms the baseline methods, achieving\nan average performance improvement of 198.82% and an average processing time\nreduction of 63.28%."
                },
                "authors": [
                    {
                        "name": "Yueyue Liu"
                    },
                    {
                        "name": "Hongyu Zhang"
                    },
                    {
                        "name": "Yuantian Miao"
                    }
                ],
                "author_detail": {
                    "name": "Yuantian Miao"
                },
                "author": "Yuantian Miao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.03258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.03258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02240v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02240v2",
                "updated": "2025-08-05T02:13:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    2,
                    13,
                    39,
                    1,
                    217,
                    0
                ],
                "published": "2025-08-04T09:39:31Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    9,
                    39,
                    31,
                    0,
                    216,
                    0
                ],
                "title": "Forecasting When to Forecast: Accelerating Diffusion Models with\n  Confidence-Gated Taylor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting When to Forecast: Accelerating Diffusion Models with\n  Confidence-Gated Taylor"
                },
                "summary": "Diffusion Transformers (DiTs) have demonstrated remarkable performance in\nvisual generation tasks. However, their low inference speed limits their\ndeployment in low-resource applications. Recent training-free approaches\nexploit the redundancy of features across timesteps by caching and reusing past\nrepresentations to accelerate inference. Building on this idea, TaylorSeer\ninstead uses cached features to predict future ones via Taylor expansion.\nHowever, its module-level prediction across all transformer blocks (e.g.,\nattention or feedforward modules) requires storing fine-grained intermediate\nfeatures, leading to notable memory and computation overhead. Moreover, it\nadopts a fixed caching schedule without considering the varying accuracy of\npredictions across timesteps, which can lead to degraded outputs when\nprediction fails. To address these limitations, we propose a novel approach to\nbetter leverage Taylor-based acceleration. First, we shift the Taylor\nprediction target from the module level to the last block level, significantly\nreducing the number of cached features. Furthermore, observing strong\nsequential dependencies among Transformer blocks, we propose to use the error\nbetween the Taylor-estimated and actual outputs of the first block as an\nindicator of prediction reliability. If the error is small, we trust the Taylor\nprediction for the last block; otherwise, we fall back to full computation,\nthereby enabling a dynamic caching mechanism. Empirical results show that our\nmethod achieves a better balance between speed and quality, achieving a 3.17x\nacceleration on FLUX, 2.36x on DiT, and 4.14x on Wan Video with negligible\nquality drop. The Project Page is\n\\href{https://cg-taylor-acce.github.io/CG-Taylor/}{here.}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have demonstrated remarkable performance in\nvisual generation tasks. However, their low inference speed limits their\ndeployment in low-resource applications. Recent training-free approaches\nexploit the redundancy of features across timesteps by caching and reusing past\nrepresentations to accelerate inference. Building on this idea, TaylorSeer\ninstead uses cached features to predict future ones via Taylor expansion.\nHowever, its module-level prediction across all transformer blocks (e.g.,\nattention or feedforward modules) requires storing fine-grained intermediate\nfeatures, leading to notable memory and computation overhead. Moreover, it\nadopts a fixed caching schedule without considering the varying accuracy of\npredictions across timesteps, which can lead to degraded outputs when\nprediction fails. To address these limitations, we propose a novel approach to\nbetter leverage Taylor-based acceleration. First, we shift the Taylor\nprediction target from the module level to the last block level, significantly\nreducing the number of cached features. Furthermore, observing strong\nsequential dependencies among Transformer blocks, we propose to use the error\nbetween the Taylor-estimated and actual outputs of the first block as an\nindicator of prediction reliability. If the error is small, we trust the Taylor\nprediction for the last block; otherwise, we fall back to full computation,\nthereby enabling a dynamic caching mechanism. Empirical results show that our\nmethod achieves a better balance between speed and quality, achieving a 3.17x\nacceleration on FLUX, 2.36x on DiT, and 4.14x on Wan Video with negligible\nquality drop. The Project Page is\n\\href{https://cg-taylor-acce.github.io/CG-Taylor/}{here.}"
                },
                "authors": [
                    {
                        "name": "Xiaoliu Guan"
                    },
                    {
                        "name": "Lielin Jiang"
                    },
                    {
                        "name": "Hanqi Chen"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Jiaxing Yan"
                    },
                    {
                        "name": "Guanzhong Wang"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Zetao Zhang"
                    },
                    {
                        "name": "Yu Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wu"
                },
                "author": "Yu Wu",
                "arxiv_comment": "15 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02240v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02240v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14866v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14866v5",
                "updated": "2025-08-05T00:25:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    0,
                    25,
                    53,
                    1,
                    217,
                    0
                ],
                "published": "2025-04-21T05:27:33Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    5,
                    27,
                    33,
                    0,
                    111,
                    0
                ],
                "title": "GainSight: A Unified Framework for Data Lifetime Profiling and\n  Heterogeneous Memory Composition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GainSight: A Unified Framework for Data Lifetime Profiling and\n  Heterogeneous Memory Composition"
                },
                "summary": "As AI workloads drive increasing memory requirements, domain-specific\naccelerators need higher-density on-chip memory beyond what current SRAM\nscaling trends can provide. Simultaneously, the vast amounts of short-lived\ndata in these workloads make SRAM overprovisioned in retention capability. To\naddress this mismatch, we propose a wholesale shift from uniform SRAM arrays to\nheterogeneous on-chip memory, incorporating denser short-term RAM (StRAM)\ndevices whose limited retention times align with transient data lifetimes. To\nfacilitate this shift, we introduce GainSight, the first comprehensive,\nopen-source framework that aligns dynamic, fine-grained workload lifetime\nprofiles with memory device characteristics to enable generation of optimal\nStRAM memory compositions. GainSight combines retargetable profiling backends\nwith an architecture-agnostic analytical frontend. The various backends capture\ncycle-accurate data lifetimes, while the frontend correlates workload patterns\nwith StRAM retention properties to generate optimal memory compositions and\nproject performance. GainSight elevates data lifetime to a first-class design\nconsideration for next-generation AI accelerators, enabling systematic\nexploitation of data transience for improved on-chip memory density and\nefficiency. Applying GainSight to MLPerf Inference and PolyBench workloads\nreveals that 64.3% of first-level GPU cache accesses and 79.01% of systolic\narray scratchpad accesses exhibit sub-microsecond lifetimes suitable for\nhigh-density StRAM, with optimal heterogeneous on-chip memory compositions\nachieving up to 3x active energy and 4x area reductions compared to uniform\nSRAM hierarchies. To facilitate adoption and further research, GainSight is\nopen-sourced at https://gainsight.stanford.edu/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI workloads drive increasing memory requirements, domain-specific\naccelerators need higher-density on-chip memory beyond what current SRAM\nscaling trends can provide. Simultaneously, the vast amounts of short-lived\ndata in these workloads make SRAM overprovisioned in retention capability. To\naddress this mismatch, we propose a wholesale shift from uniform SRAM arrays to\nheterogeneous on-chip memory, incorporating denser short-term RAM (StRAM)\ndevices whose limited retention times align with transient data lifetimes. To\nfacilitate this shift, we introduce GainSight, the first comprehensive,\nopen-source framework that aligns dynamic, fine-grained workload lifetime\nprofiles with memory device characteristics to enable generation of optimal\nStRAM memory compositions. GainSight combines retargetable profiling backends\nwith an architecture-agnostic analytical frontend. The various backends capture\ncycle-accurate data lifetimes, while the frontend correlates workload patterns\nwith StRAM retention properties to generate optimal memory compositions and\nproject performance. GainSight elevates data lifetime to a first-class design\nconsideration for next-generation AI accelerators, enabling systematic\nexploitation of data transience for improved on-chip memory density and\nefficiency. Applying GainSight to MLPerf Inference and PolyBench workloads\nreveals that 64.3% of first-level GPU cache accesses and 79.01% of systolic\narray scratchpad accesses exhibit sub-microsecond lifetimes suitable for\nhigh-density StRAM, with optimal heterogeneous on-chip memory compositions\nachieving up to 3x active energy and 4x area reductions compared to uniform\nSRAM hierarchies. To facilitate adoption and further research, GainSight is\nopen-sourced at https://gainsight.stanford.edu/."
                },
                "authors": [
                    {
                        "name": "Peijing Li"
                    },
                    {
                        "name": "Matthew Hung"
                    },
                    {
                        "name": "Yiming Tan"
                    },
                    {
                        "name": "Konstantin Hofeld"
                    },
                    {
                        "name": "Jake Cheng Jiajun"
                    },
                    {
                        "name": "Shuhan Liu"
                    },
                    {
                        "name": "Lixian Yan"
                    },
                    {
                        "name": "Xinxin Wang"
                    },
                    {
                        "name": "Philip Levis"
                    },
                    {
                        "name": "H. -S. Philip Wong"
                    },
                    {
                        "name": "Thierry Tambe"
                    }
                ],
                "author_detail": {
                    "name": "Thierry Tambe"
                },
                "author": "Thierry Tambe",
                "arxiv_comment": "14 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14866v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14866v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.7.1; B.3.1; C.3; I.6; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02558v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02558v1",
                "updated": "2025-08-04T16:14:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    14,
                    3,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T16:14:03Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    14,
                    3,
                    0,
                    216,
                    0
                ],
                "title": "Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction"
                },
                "summary": "Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and\nparallel decoding but suffer from prohibitive quadratic computational\ncomplexity and memory overhead during inference. Current caching techniques\naccelerate decoding by storing full-layer states, yet impose substantial memory\nusage that limit long-context applications. Our analysis of attention patterns\nin dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining\nsalient across decoding steps and low-relevance tokens staying unimportant,\nmotivating selective cache eviction. We propose Sparse-dLLM, the first\ntraining-free framework integrating dynamic cache eviction with sparse\nattention via delayed bidirectional sparse caching. By leveraging the stability\nof token saliency over steps, it retains critical tokens and dynamically evicts\nunimportant prefix/suffix entries using an attention-guided strategy. Extensive\nexperiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to\n10$\\times$ higher throughput than vanilla dLLMs, with comparable performance\nand similar peak memory costs, outperforming previous methods in efficiency and\neffectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and\nparallel decoding but suffer from prohibitive quadratic computational\ncomplexity and memory overhead during inference. Current caching techniques\naccelerate decoding by storing full-layer states, yet impose substantial memory\nusage that limit long-context applications. Our analysis of attention patterns\nin dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining\nsalient across decoding steps and low-relevance tokens staying unimportant,\nmotivating selective cache eviction. We propose Sparse-dLLM, the first\ntraining-free framework integrating dynamic cache eviction with sparse\nattention via delayed bidirectional sparse caching. By leveraging the stability\nof token saliency over steps, it retains critical tokens and dynamically evicts\nunimportant prefix/suffix entries using an attention-guided strategy. Extensive\nexperiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to\n10$\\times$ higher throughput than vanilla dLLMs, with comparable performance\nand similar peak memory costs, outperforming previous methods in efficiency and\neffectiveness."
                },
                "authors": [
                    {
                        "name": "Yuerong Song"
                    },
                    {
                        "name": "Xiaoran Liu"
                    },
                    {
                        "name": "Ruixiao Li"
                    },
                    {
                        "name": "Zhigeng Liu"
                    },
                    {
                        "name": "Zengfeng Huang"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02558v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02558v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02401v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02401v1",
                "updated": "2025-08-04T13:26:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    13,
                    26,
                    16,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T13:26:16Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    13,
                    26,
                    16,
                    0,
                    216,
                    0
                ],
                "title": "CompressKV: Semantic Retrieval Heads Know What Tokens are Not Important\n  Before Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CompressKV: Semantic Retrieval Heads Know What Tokens are Not Important\n  Before Generation"
                },
                "summary": "Recent advances in large language models (LLMs) have significantly boosted\nlong-context processing. However, the increasing key-value (KV) cache size\nposes critical challenges to memory and execution efficiency. Most KV cache\ncompression methods rely on heuristic token eviction using all attention heads\nin Grouped Query Attention (GQA)-based LLMs. This method ignores the different\nfunctionalities of attention heads, leading to the eviction of critical tokens\nand thus degrades the performance of LLMs.\n  To address the issue above, instead of using all the attention heads in\nGQA-based LLMs to determine important tokens as in the previous work, we first\nidentify the attention heads in each layer that are not only capable of\nretrieving the initial and final tokens of a prompt, but also capable of\nretrieving important tokens within the text and attending to their surrounding\nsemantic context. Afterwards, we exploit such heads to determine the important\ntokens and retain their corresponding KV cache pairs. Furthermore, we analyze\nthe cache eviction error of each layer individually and introduce a\nlayer-adaptive KV cache allocation strategy. Experimental results demonstrate\nthe proposed CompressKV consistently outperforms state-of-the-art approaches\nunder various memory budgets on LongBench and Needle-in-a-Haystack benchmarks.\nOur code is publicly available at: https://github.com/TUDa-HWAI/CompressKV.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have significantly boosted\nlong-context processing. However, the increasing key-value (KV) cache size\nposes critical challenges to memory and execution efficiency. Most KV cache\ncompression methods rely on heuristic token eviction using all attention heads\nin Grouped Query Attention (GQA)-based LLMs. This method ignores the different\nfunctionalities of attention heads, leading to the eviction of critical tokens\nand thus degrades the performance of LLMs.\n  To address the issue above, instead of using all the attention heads in\nGQA-based LLMs to determine important tokens as in the previous work, we first\nidentify the attention heads in each layer that are not only capable of\nretrieving the initial and final tokens of a prompt, but also capable of\nretrieving important tokens within the text and attending to their surrounding\nsemantic context. Afterwards, we exploit such heads to determine the important\ntokens and retain their corresponding KV cache pairs. Furthermore, we analyze\nthe cache eviction error of each layer individually and introduce a\nlayer-adaptive KV cache allocation strategy. Experimental results demonstrate\nthe proposed CompressKV consistently outperforms state-of-the-art approaches\nunder various memory budgets on LongBench and Needle-in-a-Haystack benchmarks.\nOur code is publicly available at: https://github.com/TUDa-HWAI/CompressKV.git."
                },
                "authors": [
                    {
                        "name": "Xiaolin Lin"
                    },
                    {
                        "name": "Jingcun Wang"
                    },
                    {
                        "name": "Olga Kondrateva"
                    },
                    {
                        "name": "Yiyu Shi"
                    },
                    {
                        "name": "Bing Li"
                    },
                    {
                        "name": "Grace Li Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Grace Li Zhang"
                },
                "author": "Grace Li Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02401v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02401v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02280v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02280v1",
                "updated": "2025-08-04T10:51:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    10,
                    51,
                    20,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T10:51:20Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    10,
                    51,
                    20,
                    0,
                    216,
                    0
                ],
                "title": "OnPair: Short Strings Compression for Fast Random Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OnPair: Short Strings Compression for Fast Random Access"
                },
                "summary": "We present OnPair, a dictionary-based compression algorithm designed to meet\nthe needs of in-memory database systems that require both high compression and\nfast random access. Existing methods either achieve strong compression ratios\nat significant computational and memory cost (e.g., BPE) or prioritize speed at\nthe expense of compression quality (e.g., FSST). OnPair bridges this gap by\nemploying a cache-friendly dictionary construction technique that incrementally\nmerges frequent adjacent substrings in a single sequential pass over a data\nsample. This enables fast, memory-efficient training without tracking global\npair positions, as required by traditional BPE. We also introduce OnPair16, a\nvariant that limits dictionary entries to 16 bytes, enabling faster parsing via\noptimized longest prefix matching. Both variants compress strings\nindependently, supporting fine-grained random access without block-level\noverhead. Experiments on real-world datasets show that OnPair and OnPair16\nachieve compression ratios comparable to BPE while significantly improving\ncompression speed and memory usage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present OnPair, a dictionary-based compression algorithm designed to meet\nthe needs of in-memory database systems that require both high compression and\nfast random access. Existing methods either achieve strong compression ratios\nat significant computational and memory cost (e.g., BPE) or prioritize speed at\nthe expense of compression quality (e.g., FSST). OnPair bridges this gap by\nemploying a cache-friendly dictionary construction technique that incrementally\nmerges frequent adjacent substrings in a single sequential pass over a data\nsample. This enables fast, memory-efficient training without tracking global\npair positions, as required by traditional BPE. We also introduce OnPair16, a\nvariant that limits dictionary entries to 16 bytes, enabling faster parsing via\noptimized longest prefix matching. Both variants compress strings\nindependently, supporting fine-grained random access without block-level\noverhead. Experiments on real-world datasets show that OnPair and OnPair16\nachieve compression ratios comparable to BPE while significantly improving\ncompression speed and memory usage."
                },
                "authors": [
                    {
                        "name": "Francesco Gargiulo"
                    },
                    {
                        "name": "Rossano Venturini"
                    }
                ],
                "author_detail": {
                    "name": "Rossano Venturini"
                },
                "author": "Rossano Venturini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02280v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02280v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.2.4; E.4; H.3.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02215v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02215v1",
                "updated": "2025-08-04T09:08:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    9,
                    8,
                    43,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T09:08:43Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    9,
                    8,
                    43,
                    0,
                    216,
                    0
                ],
                "title": "LeanK: Learnable K Cache Channel Pruning for Efficient Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LeanK: Learnable K Cache Channel Pruning for Efficient Decoding"
                },
                "summary": "Large language models (LLMs) enable long-context tasks but face efficiency\nchallenges due to the growing key-value (KV) cache. We propose LeanK, a\nlearning-based method that prunes unimportant key (K) cache channels by\nleveraging static channel sparsity. With a novel two-stage training process,\nLeanK learns channel-wise static mask that could satisfy specific sparsity\nratio and hardware alignment requirement. LeanK reduces GPU memory and\naccelerates decoding without sacrificing accuracy. Experiments demonstrate up\nto 70% K cache and 16%-18% V cache memory reduction. Custom decoding kernel\nenables 1.3x speedup for attention computation. We also provide insights into\nmodel channels and attention heads during long-context inference by analyzing\nthe learned importance distribution. Our code is available at\nhttps://aka.ms/LeanK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) enable long-context tasks but face efficiency\nchallenges due to the growing key-value (KV) cache. We propose LeanK, a\nlearning-based method that prunes unimportant key (K) cache channels by\nleveraging static channel sparsity. With a novel two-stage training process,\nLeanK learns channel-wise static mask that could satisfy specific sparsity\nratio and hardware alignment requirement. LeanK reduces GPU memory and\naccelerates decoding without sacrificing accuracy. Experiments demonstrate up\nto 70% K cache and 16%-18% V cache memory reduction. Custom decoding kernel\nenables 1.3x speedup for attention computation. We also provide insights into\nmodel channels and attention heads during long-context inference by analyzing\nthe learned importance distribution. Our code is available at\nhttps://aka.ms/LeanK."
                },
                "authors": [
                    {
                        "name": "Yike Zhang"
                    },
                    {
                        "name": "Zhiyuan He"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Jianyong Wang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02215v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02215v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19906v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19906v2",
                "updated": "2025-08-04T08:19:26Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    8,
                    19,
                    26,
                    0,
                    216,
                    0
                ],
                "published": "2025-07-26T10:34:53Z",
                "published_parsed": [
                    2025,
                    7,
                    26,
                    10,
                    34,
                    53,
                    5,
                    207,
                    0
                ],
                "title": "CaliDrop: KV Cache Compression with Calibration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CaliDrop: KV Cache Compression with Calibration"
                },
                "summary": "Large Language Models (LLMs) require substantial computational resources\nduring generation. While the Key-Value (KV) cache significantly accelerates\nthis process by storing attention intermediates, its memory footprint grows\nlinearly with sequence length, batch size, and model size, creating a\nbottleneck in long-context scenarios. Various KV cache compression techniques,\nincluding token eviction, quantization, and low-rank projection, have been\nproposed to mitigate this bottleneck, often complementing each other. This\npaper focuses on enhancing token eviction strategies. Token eviction leverages\nthe observation that the attention patterns are often sparse, allowing for the\nremoval of less critical KV entries to save memory. However, this reduction\nusually comes at the cost of notable accuracy degradation, particularly under\nhigh compression ratios. To address this issue, we propose \\textbf{CaliDrop}, a\nnovel strategy that enhances token eviction through calibration. Our\npreliminary experiments show that queries at nearby positions exhibit high\nsimilarity. Building on this observation, CaliDrop performs speculative\ncalibration on the discarded tokens to mitigate the accuracy loss caused by\ntoken eviction. Extensive experiments demonstrate that CaliDrop significantly\nimproves the accuracy of existing token eviction methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) require substantial computational resources\nduring generation. While the Key-Value (KV) cache significantly accelerates\nthis process by storing attention intermediates, its memory footprint grows\nlinearly with sequence length, batch size, and model size, creating a\nbottleneck in long-context scenarios. Various KV cache compression techniques,\nincluding token eviction, quantization, and low-rank projection, have been\nproposed to mitigate this bottleneck, often complementing each other. This\npaper focuses on enhancing token eviction strategies. Token eviction leverages\nthe observation that the attention patterns are often sparse, allowing for the\nremoval of less critical KV entries to save memory. However, this reduction\nusually comes at the cost of notable accuracy degradation, particularly under\nhigh compression ratios. To address this issue, we propose \\textbf{CaliDrop}, a\nnovel strategy that enhances token eviction through calibration. Our\npreliminary experiments show that queries at nearby positions exhibit high\nsimilarity. Building on this observation, CaliDrop performs speculative\ncalibration on the discarded tokens to mitigate the accuracy loss caused by\ntoken eviction. Extensive experiments demonstrate that CaliDrop significantly\nimproves the accuracy of existing token eviction methods."
                },
                "authors": [
                    {
                        "name": "Yi Su"
                    },
                    {
                        "name": "Quantong Qiu"
                    },
                    {
                        "name": "Yuechi Zhou"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Qingrong Xia"
                    },
                    {
                        "name": "Ping Li"
                    },
                    {
                        "name": "Xinyu Duan"
                    },
                    {
                        "name": "Zhefeng Wang"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19906v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19906v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21492v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21492v2",
                "updated": "2025-08-04T04:48:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    4,
                    48,
                    41,
                    0,
                    216,
                    0
                ],
                "published": "2025-07-29T04:21:11Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    4,
                    21,
                    11,
                    1,
                    210,
                    0
                ],
                "title": "Bridging Cache-Friendliness and Concurrency: A Locality-Optimized\n  In-Memory B-Skiplist",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Cache-Friendliness and Concurrency: A Locality-Optimized\n  In-Memory B-Skiplist"
                },
                "summary": "Skiplists are widely used for in-memory indexing in many key-value stores,\nsuch as RocksDB and LevelDB, due to their ease of implementation and simple\nconcurrency control mechanisms. However, traditional skiplists suffer from poor\ncache locality, as they store only a single element per node, leaving\nperformance on the table. Minimizing last-level cache misses is key to\nmaximizing in-memory index performance, making high cache locality essential.\nIn this paper, we present a practical concurrent B-skiplist that enhances cache\nlocality and performance while preserving the simplicity of traditional\nskiplist structures and concurrency control schemes. Our key contributions\ninclude a top-down, single-pass insertion algorithm for B-skiplists and a\ncorresponding simple and efficient top-down concurrency control scheme. On 128\nthreads, the proposed concurrent B-skiplist achieves between 2x-9x higher\nthroughput compared to state-of-the-art concurrent skiplist implementations,\nincluding Facebook's concurrent skiplist from Folly and the Java\nConcurrentSkipListMap. Furthermore, we find that the B-skiplist achieves\ncompetitive (0.9x-1.7x) throughput on point workloads compared to\nstate-of-the-art cache-optimized tree-based indices (e.g., Masstree). For a\nmore complete picture of the performance, we also measure the latency of\nskiplist and tree-based indices and find that the B-skiplist achieves between\n3.5x-103x lower 99% latency compared to other concurrent skiplists and between\n0.85x-64x lower 99% latency compared to tree-based indices on point workloads\nwith inserts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skiplists are widely used for in-memory indexing in many key-value stores,\nsuch as RocksDB and LevelDB, due to their ease of implementation and simple\nconcurrency control mechanisms. However, traditional skiplists suffer from poor\ncache locality, as they store only a single element per node, leaving\nperformance on the table. Minimizing last-level cache misses is key to\nmaximizing in-memory index performance, making high cache locality essential.\nIn this paper, we present a practical concurrent B-skiplist that enhances cache\nlocality and performance while preserving the simplicity of traditional\nskiplist structures and concurrency control schemes. Our key contributions\ninclude a top-down, single-pass insertion algorithm for B-skiplists and a\ncorresponding simple and efficient top-down concurrency control scheme. On 128\nthreads, the proposed concurrent B-skiplist achieves between 2x-9x higher\nthroughput compared to state-of-the-art concurrent skiplist implementations,\nincluding Facebook's concurrent skiplist from Folly and the Java\nConcurrentSkipListMap. Furthermore, we find that the B-skiplist achieves\ncompetitive (0.9x-1.7x) throughput on point workloads compared to\nstate-of-the-art cache-optimized tree-based indices (e.g., Masstree). For a\nmore complete picture of the performance, we also measure the latency of\nskiplist and tree-based indices and find that the B-skiplist achieves between\n3.5x-103x lower 99% latency compared to other concurrent skiplists and between\n0.85x-64x lower 99% latency compared to tree-based indices on point workloads\nwith inserts."
                },
                "authors": [
                    {
                        "name": "Yicong Luo"
                    },
                    {
                        "name": "Senhe Hao"
                    },
                    {
                        "name": "Brian Wheatman"
                    },
                    {
                        "name": "Prashant Pandey"
                    },
                    {
                        "name": "Helen Xu"
                    }
                ],
                "author_detail": {
                    "name": "Helen Xu"
                },
                "author": "Helen Xu",
                "arxiv_doi": "10.1145/3754598.3754655",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3754598.3754655",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.21492v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21492v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Original paper was accepted into ICPP 2025",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02930v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02930v4",
                "updated": "2025-08-04T02:47:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    2,
                    47,
                    35,
                    0,
                    216,
                    0
                ],
                "published": "2024-07-03T09:02:05Z",
                "published_parsed": [
                    2024,
                    7,
                    3,
                    9,
                    2,
                    5,
                    2,
                    185,
                    0
                ],
                "title": "Timely Requesting for Time-Critical Content Users in Decentralized\n  F-RANs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timely Requesting for Time-Critical Content Users in Decentralized\n  F-RANs"
                },
                "summary": "With the rising demand for high-rate and timely communications, fog radio\naccess networks (F-RANs) offer a promising solution. This work investigates age\nof information (AoI) performance in F-RANs, consisting of multiple content\nusers (CUs), enhanced remote radio heads (eRRHs), and content providers (CPs).\nTime-critical CUs need rapid content updates from CPs but cannot communicate\ndirectly with them; instead, eRRHs act as intermediaries. CUs decide whether to\nrequest content from a CP and which eRRH to send the request to, while eRRHs\ndecide whether to command CPs to update content or use cached content. We study\ntwo general classes of policies: (i) oblivious policies, where decision-making\nis independent of historical information, and (ii) non-oblivious policies,\nwhere decisions are influenced by historical information. First, we obtain\nclosed-form expressions for the average AoI of eRRHs under both policy types.\nDue to the complexity of calculating closed-form expressions for CUs, we then\nderive general upper bounds for their average AoI. Next, we identify optimal\npolicies for both types. Under both optimal policies, each CU requests content\nfrom each CP at an equal rate, consolidating all requests to a single eRRH when\ndemand is low or resources are limited, and distributing requests evenly among\neRRHs when demand is high and resources are ample. eRRHs command content from\neach CP at an equal rate under an optimal oblivious policy, while prioritize\nthe CP with the highest age under an optimal non-oblivious policy. Our\nnumerical results validate these theoretical findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rising demand for high-rate and timely communications, fog radio\naccess networks (F-RANs) offer a promising solution. This work investigates age\nof information (AoI) performance in F-RANs, consisting of multiple content\nusers (CUs), enhanced remote radio heads (eRRHs), and content providers (CPs).\nTime-critical CUs need rapid content updates from CPs but cannot communicate\ndirectly with them; instead, eRRHs act as intermediaries. CUs decide whether to\nrequest content from a CP and which eRRH to send the request to, while eRRHs\ndecide whether to command CPs to update content or use cached content. We study\ntwo general classes of policies: (i) oblivious policies, where decision-making\nis independent of historical information, and (ii) non-oblivious policies,\nwhere decisions are influenced by historical information. First, we obtain\nclosed-form expressions for the average AoI of eRRHs under both policy types.\nDue to the complexity of calculating closed-form expressions for CUs, we then\nderive general upper bounds for their average AoI. Next, we identify optimal\npolicies for both types. Under both optimal policies, each CU requests content\nfrom each CP at an equal rate, consolidating all requests to a single eRRH when\ndemand is low or resources are limited, and distributing requests evenly among\neRRHs when demand is high and resources are ample. eRRHs command content from\neach CP at an equal rate under an optimal oblivious policy, while prioritize\nthe CP with the highest age under an optimal non-oblivious policy. Our\nnumerical results validate these theoretical findings."
                },
                "authors": [
                    {
                        "name": "Xingran Chen"
                    },
                    {
                        "name": "Kai Li"
                    },
                    {
                        "name": "Kun Yang"
                    }
                ],
                "author_detail": {
                    "name": "Kun Yang"
                },
                "author": "Kun Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02930v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02930v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02252v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02252v2",
                "updated": "2025-08-04T02:17:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    2,
                    17,
                    56,
                    0,
                    216,
                    0
                ],
                "published": "2024-12-03T08:29:27Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    8,
                    29,
                    27,
                    1,
                    338,
                    0
                ],
                "title": "Compressing KV Cache for Long-Context LLM Inference with Inter-Layer\n  Attention Similarity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compressing KV Cache for Long-Context LLM Inference with Inter-Layer\n  Attention Similarity"
                },
                "summary": "The rapid expansion of context window sizes in Large Language Models~(LLMs)\nhas enabled them to tackle increasingly complex tasks involving lengthy\ndocuments. However, this progress comes at the cost of a substantial increase\nin memory usage during inference, primarily due to the linear growth of the\nkey-value~(KV) cache. Existing KV cache compression methods often discard less\nrelevant tokens, which can lead to significant performance degradation when\ncritical information is lost. In this paper, we propose \\textsc{PoD}~(Proximal\ntokens over Distant tokens), a novel KV cache compression framework that\nallocates memory according to token importance, retaining less important tokens\nin a more compact, shared form rather than discarding them entirely. Our\napproach is motivated by two key observations: (1) proximal tokens -- those at\nthe beginning and end of the context -- are significantly more important for\nnext-token prediction, and (2) attention scores for distant tokens are highly\nredundant across consecutive layers. Leveraging these insights, \\textsc{PoD}\npreserves the full KV cache for proximal tokens, while for distant tokens, it\nshares key states across layers. Since attention scores are determined by both\nqueries and keys, sharing key states enables multiple layers to reuse a single\nset of keys for distant tokens, substantially reducing KV cache memory without\ndiscarding essential context. We further introduce a lightweight post-training\nadaptation to enable the model to adjust to this new attention-sharing\nstructure. Extensive experiments on both synthetic~(Needle in a Haystack) and\nreal-world long-context benchmarks demonstrate that \\textsc{PoD} reduces KV\ncache memory usage by up to 35\\% without compromising performance. Our method\nis orthogonal to existing token-selection-based techniques and can be combined\nwith them for further KV cache compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid expansion of context window sizes in Large Language Models~(LLMs)\nhas enabled them to tackle increasingly complex tasks involving lengthy\ndocuments. However, this progress comes at the cost of a substantial increase\nin memory usage during inference, primarily due to the linear growth of the\nkey-value~(KV) cache. Existing KV cache compression methods often discard less\nrelevant tokens, which can lead to significant performance degradation when\ncritical information is lost. In this paper, we propose \\textsc{PoD}~(Proximal\ntokens over Distant tokens), a novel KV cache compression framework that\nallocates memory according to token importance, retaining less important tokens\nin a more compact, shared form rather than discarding them entirely. Our\napproach is motivated by two key observations: (1) proximal tokens -- those at\nthe beginning and end of the context -- are significantly more important for\nnext-token prediction, and (2) attention scores for distant tokens are highly\nredundant across consecutive layers. Leveraging these insights, \\textsc{PoD}\npreserves the full KV cache for proximal tokens, while for distant tokens, it\nshares key states across layers. Since attention scores are determined by both\nqueries and keys, sharing key states enables multiple layers to reuse a single\nset of keys for distant tokens, substantially reducing KV cache memory without\ndiscarding essential context. We further introduce a lightweight post-training\nadaptation to enable the model to adjust to this new attention-sharing\nstructure. Extensive experiments on both synthetic~(Needle in a Haystack) and\nreal-world long-context benchmarks demonstrate that \\textsc{PoD} reduces KV\ncache memory usage by up to 35\\% without compromising performance. Our method\nis orthogonal to existing token-selection-based techniques and can be combined\nwith them for further KV cache compression."
                },
                "authors": [
                    {
                        "name": "Da Ma"
                    },
                    {
                        "name": "Lu Chen"
                    },
                    {
                        "name": "Situo Zhang"
                    },
                    {
                        "name": "Yuxun Miao"
                    },
                    {
                        "name": "Su Zhu"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Hongshen Xu"
                    },
                    {
                        "name": "Hanqi Li"
                    },
                    {
                        "name": "Shuai Fan"
                    },
                    {
                        "name": "Lei Pan"
                    },
                    {
                        "name": "Kai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yu"
                },
                "author": "Kai Yu",
                "arxiv_comment": "14 pages, 7 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02252v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02252v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01898v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01898v1",
                "updated": "2025-08-03T19:16:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    3,
                    19,
                    16,
                    40,
                    6,
                    215,
                    0
                ],
                "published": "2025-08-03T19:16:40Z",
                "published_parsed": [
                    2025,
                    8,
                    3,
                    19,
                    16,
                    40,
                    6,
                    215,
                    0
                ],
                "title": "Revenue Optimization in Wireless Video Caching Networks: A\n  Privacy-Preserving Two-Stage Solution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revenue Optimization in Wireless Video Caching Networks: A\n  Privacy-Preserving Two-Stage Solution"
                },
                "summary": "Video caching can significantly improve delivery efficiency and enhance\nquality of video streaming, which constitutes the majority of wireless\ncommunication traffic. Due to limited cache size, caching strategies must be\ndesigned to adapt to and dynamic user demand in order to maximize system\nrevenue. The system revenue depends on the benefits of delivering the requested\nvideos and costs for (a) transporting the files to the users and (b) cache\nreplacement. Since the cache content at any point in time impacts the\nreplacement costs in the future, demand predictions over multiple cache\nplacement slots become an important prerequisite for efficient cache planning.\nMotivated by this, we introduce a novel two-stage privacy-preserving solution\nfor revenue optimization in wireless video caching networks. First, we train a\nTransformer using privacy-preserving federated learning (FL) to predict\nmulti-slot future demands. Given that prediction results are never entirely\naccurate, especially for longer horizons, we further combine global content\npopularity with per-user prediction results to estimate the content demand\ndistribution. Then, in the second stage, we leverage these estimation results\nto find caching strategies that maximize the long-term system revenue. This\nlatter problem takes on the form of a multi-stage knapsack problem, which we\nthen transform to a integer linear program. Our extensive simulation results\ndemonstrate that (i) our FL solution delivers nearly identical performance to\nthat of the ideal centralized solution and outperforms other existing caching\nmethods, and (ii) our novel revenue optimization approach provides deeper\nsystem performance insights than traditional cache hit ratio (CHR)-based\noptimization approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video caching can significantly improve delivery efficiency and enhance\nquality of video streaming, which constitutes the majority of wireless\ncommunication traffic. Due to limited cache size, caching strategies must be\ndesigned to adapt to and dynamic user demand in order to maximize system\nrevenue. The system revenue depends on the benefits of delivering the requested\nvideos and costs for (a) transporting the files to the users and (b) cache\nreplacement. Since the cache content at any point in time impacts the\nreplacement costs in the future, demand predictions over multiple cache\nplacement slots become an important prerequisite for efficient cache planning.\nMotivated by this, we introduce a novel two-stage privacy-preserving solution\nfor revenue optimization in wireless video caching networks. First, we train a\nTransformer using privacy-preserving federated learning (FL) to predict\nmulti-slot future demands. Given that prediction results are never entirely\naccurate, especially for longer horizons, we further combine global content\npopularity with per-user prediction results to estimate the content demand\ndistribution. Then, in the second stage, we leverage these estimation results\nto find caching strategies that maximize the long-term system revenue. This\nlatter problem takes on the form of a multi-stage knapsack problem, which we\nthen transform to a integer linear program. Our extensive simulation results\ndemonstrate that (i) our FL solution delivers nearly identical performance to\nthat of the ideal centralized solution and outperforms other existing caching\nmethods, and (ii) our novel revenue optimization approach provides deeper\nsystem performance insights than traditional cache hit ratio (CHR)-based\noptimization approaches."
                },
                "authors": [
                    {
                        "name": "Yijing Zhang"
                    },
                    {
                        "name": "Md-Ferdous Pervej"
                    },
                    {
                        "name": "Andreas F. Molisch"
                    }
                ],
                "author_detail": {
                    "name": "Andreas F. Molisch"
                },
                "author": "Andreas F. Molisch",
                "arxiv_comment": "Under review for possible publication in the IEEE Transactions on\n  Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01898v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01898v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01875v1",
                "updated": "2025-08-03T18:15:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    3,
                    18,
                    15,
                    42,
                    6,
                    215,
                    0
                ],
                "published": "2025-08-03T18:15:42Z",
                "published_parsed": [
                    2025,
                    8,
                    3,
                    18,
                    15,
                    42,
                    6,
                    215,
                    0
                ],
                "title": "StreamAgent: Towards Anticipatory Agents for Streaming Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamAgent: Towards Anticipatory Agents for Streaming Video\n  Understanding"
                },
                "summary": "Real-time streaming video understanding in domains such as autonomous driving\nand intelligent surveillance poses challenges beyond conventional offline video\nprocessing, requiring continuous perception, proactive decision making, and\nresponsive interaction based on dynamically evolving visual content. However,\nexisting methods rely on alternating perception-reaction or asynchronous\ntriggers, lacking task-driven planning and future anticipation, which limits\ntheir real-time responsiveness and proactive decision making in evolving video\nstreams. To this end, we propose a StreamAgent that anticipates the temporal\nintervals and spatial regions expected to contain future task-relevant\ninformation to enable proactive and goal-driven responses. Specifically, we\nintegrate question semantics and historical observations through prompting the\nanticipatory agent to anticipate the temporal progression of key events, align\ncurrent observations with the expected future evidence, and subsequently adjust\nthe perception action (e.g., attending to task-relevant regions or continuously\ntracking in subsequent frames). To enable efficient inference, we design a\nstreaming KV-cache memory mechanism that constructs a hierarchical memory\nstructure for selective recall of relevant tokens, enabling efficient semantic\nretrieval while reducing the overhead of storing all tokens in the traditional\nKV-cache. Extensive experiments on streaming and long video understanding tasks\ndemonstrate that our method outperforms existing methods in response accuracy\nand real-time efficiency, highlighting its practical value for real-world\nstreaming scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time streaming video understanding in domains such as autonomous driving\nand intelligent surveillance poses challenges beyond conventional offline video\nprocessing, requiring continuous perception, proactive decision making, and\nresponsive interaction based on dynamically evolving visual content. However,\nexisting methods rely on alternating perception-reaction or asynchronous\ntriggers, lacking task-driven planning and future anticipation, which limits\ntheir real-time responsiveness and proactive decision making in evolving video\nstreams. To this end, we propose a StreamAgent that anticipates the temporal\nintervals and spatial regions expected to contain future task-relevant\ninformation to enable proactive and goal-driven responses. Specifically, we\nintegrate question semantics and historical observations through prompting the\nanticipatory agent to anticipate the temporal progression of key events, align\ncurrent observations with the expected future evidence, and subsequently adjust\nthe perception action (e.g., attending to task-relevant regions or continuously\ntracking in subsequent frames). To enable efficient inference, we design a\nstreaming KV-cache memory mechanism that constructs a hierarchical memory\nstructure for selective recall of relevant tokens, enabling efficient semantic\nretrieval while reducing the overhead of storing all tokens in the traditional\nKV-cache. Extensive experiments on streaming and long video understanding tasks\ndemonstrate that our method outperforms existing methods in response accuracy\nand real-time efficiency, highlighting its practical value for real-world\nstreaming scenarios."
                },
                "authors": [
                    {
                        "name": "Haolin Yang"
                    },
                    {
                        "name": "Feilong Tang"
                    },
                    {
                        "name": "Linxiao Zhao"
                    },
                    {
                        "name": "Xiang An"
                    },
                    {
                        "name": "Ming Hu"
                    },
                    {
                        "name": "Huifa Li"
                    },
                    {
                        "name": "Xinlin Zhuang"
                    },
                    {
                        "name": "Boqian Wang"
                    },
                    {
                        "name": "Yifan Lu"
                    },
                    {
                        "name": "Xiaofeng Zhang"
                    },
                    {
                        "name": "Abdalla Swikir"
                    },
                    {
                        "name": "Junjun He"
                    },
                    {
                        "name": "Zongyuan Ge"
                    },
                    {
                        "name": "Imran Razzak"
                    }
                ],
                "author_detail": {
                    "name": "Imran Razzak"
                },
                "author": "Imran Razzak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16607v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16607v2",
                "updated": "2025-08-03T10:27:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    3,
                    10,
                    27,
                    19,
                    6,
                    215,
                    0
                ],
                "published": "2025-01-28T00:52:23Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    0,
                    52,
                    23,
                    1,
                    28,
                    0
                ],
                "title": "MCTS-SQL: Light-Weight LLMs can Master the Text-to-SQL through Monte\n  Carlo Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCTS-SQL: Light-Weight LLMs can Master the Text-to-SQL through Monte\n  Carlo Tree Search"
                },
                "summary": "Text-to-SQL is a fundamental yet challenging task in the NLP area, aiming at\ntranslating natural language questions into SQL queries. While recent advances\nin large language models have greatly improved performance, most existing\napproaches depend on models with tens of billions of parameters or costly APIs,\nlimiting their applicability in resource-constrained environments. For real\nworld, especially on edge devices, it is crucial for Text-to-SQL to ensure\ncost-effectiveness. Therefore, enabling the light-weight models for Text-to-SQL\nis of great practical significance. However, smaller LLMs often struggle with\ncomplicated user instruction, redundant schema linking or syntax correctness.\nTo address these challenges, we propose MCTS-SQL, a novel framework that uses\nMonte Carlo Tree Search to guide SQL generation through multi-step refinement.\nSince the light-weight models' weak performance of single-shot prediction, we\ngenerate better results through several trials with feedback. However, directly\napplying MCTS-based methods inevitably leads to significant time and\ncomputational overhead. Driven by this issue, we propose a token-level\nprefix-cache mechanism that stores prior information during iterations,\neffectively improved the execution speed. Experiments results on the SPIDER and\nBIRD benchmarks demonstrate the effectiveness of our approach. Using a small\nopen-source Qwen2.5-Coder-1.5B, our method outperforms ChatGPT-3.5. When\nleveraging a more powerful model Gemini 2.5 to explore the performance upper\nbound, we achieved results competitive with the SOTA. Our findings demonstrate\nthat even small models can be effectively deployed in practical Text-to-SQL\nsystems with the right strategy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-SQL is a fundamental yet challenging task in the NLP area, aiming at\ntranslating natural language questions into SQL queries. While recent advances\nin large language models have greatly improved performance, most existing\napproaches depend on models with tens of billions of parameters or costly APIs,\nlimiting their applicability in resource-constrained environments. For real\nworld, especially on edge devices, it is crucial for Text-to-SQL to ensure\ncost-effectiveness. Therefore, enabling the light-weight models for Text-to-SQL\nis of great practical significance. However, smaller LLMs often struggle with\ncomplicated user instruction, redundant schema linking or syntax correctness.\nTo address these challenges, we propose MCTS-SQL, a novel framework that uses\nMonte Carlo Tree Search to guide SQL generation through multi-step refinement.\nSince the light-weight models' weak performance of single-shot prediction, we\ngenerate better results through several trials with feedback. However, directly\napplying MCTS-based methods inevitably leads to significant time and\ncomputational overhead. Driven by this issue, we propose a token-level\nprefix-cache mechanism that stores prior information during iterations,\neffectively improved the execution speed. Experiments results on the SPIDER and\nBIRD benchmarks demonstrate the effectiveness of our approach. Using a small\nopen-source Qwen2.5-Coder-1.5B, our method outperforms ChatGPT-3.5. When\nleveraging a more powerful model Gemini 2.5 to explore the performance upper\nbound, we achieved results competitive with the SOTA. Our findings demonstrate\nthat even small models can be effectively deployed in practical Text-to-SQL\nsystems with the right strategy."
                },
                "authors": [
                    {
                        "name": "Shuozhi Yuan"
                    },
                    {
                        "name": "Limin Chen"
                    },
                    {
                        "name": "Miaomiao Yuan"
                    },
                    {
                        "name": "Jin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jin Zhao"
                },
                "author": "Jin Zhao",
                "arxiv_comment": "15 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16607v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16607v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02751v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02751v1",
                "updated": "2025-08-03T09:15:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    3,
                    9,
                    15,
                    36,
                    6,
                    215,
                    0
                ],
                "published": "2025-08-03T09:15:36Z",
                "published_parsed": [
                    2025,
                    8,
                    3,
                    9,
                    15,
                    36,
                    6,
                    215,
                    0
                ],
                "title": "SmallKV: Small Model Assisted Compensation of KV Cache Compression for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmallKV: Small Model Assisted Compensation of KV Cache Compression for\n  Efficient LLM Inference"
                },
                "summary": "KV cache eviction has emerged as an effective solution to alleviate resource\nconstraints faced by LLMs in long-context scenarios. However, existing\ntoken-level eviction methods often overlook two critical aspects: (1) their\nirreversible eviction strategy fails to adapt to dynamic attention patterns\nduring decoding (the saliency shift problem), and (2) they treat both\nmarginally important tokens and truly unimportant tokens equally, despite the\ncollective significance of marginal tokens to model performance (the marginal\ninformation over-compression problem). To address these issues, we design two\ncompensation mechanisms based on the high similarity of attention matrices\nbetween LLMs of different scales. We propose SmallKV, a small model assisted\ncompensation method for KV cache compression. SmallKV can maintain attention\nmatching between different-scale LLMs to: 1) assist the larger model in\nperceiving globally important information of attention; and 2) use the smaller\nmodel's attention scores to approximate those of marginal tokens in the larger\nmodel. Extensive experiments on benchmarks including GSM8K, BBH, MT-Bench, and\nLongBench demonstrate the effectiveness of SmallKV. Moreover, efficiency\nevaluations show that SmallKV achieves 1.75 - 2.56 times higher throughput than\nbaseline methods, highlighting its potential for efficient and performant LLM\ninference in resource constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache eviction has emerged as an effective solution to alleviate resource\nconstraints faced by LLMs in long-context scenarios. However, existing\ntoken-level eviction methods often overlook two critical aspects: (1) their\nirreversible eviction strategy fails to adapt to dynamic attention patterns\nduring decoding (the saliency shift problem), and (2) they treat both\nmarginally important tokens and truly unimportant tokens equally, despite the\ncollective significance of marginal tokens to model performance (the marginal\ninformation over-compression problem). To address these issues, we design two\ncompensation mechanisms based on the high similarity of attention matrices\nbetween LLMs of different scales. We propose SmallKV, a small model assisted\ncompensation method for KV cache compression. SmallKV can maintain attention\nmatching between different-scale LLMs to: 1) assist the larger model in\nperceiving globally important information of attention; and 2) use the smaller\nmodel's attention scores to approximate those of marginal tokens in the larger\nmodel. Extensive experiments on benchmarks including GSM8K, BBH, MT-Bench, and\nLongBench demonstrate the effectiveness of SmallKV. Moreover, efficiency\nevaluations show that SmallKV achieves 1.75 - 2.56 times higher throughput than\nbaseline methods, highlighting its potential for efficient and performant LLM\ninference in resource constrained environments."
                },
                "authors": [
                    {
                        "name": "Yi Zhao"
                    },
                    {
                        "name": "Yajuan Peng"
                    },
                    {
                        "name": "Cam-Tu Nguyen"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Xiaoliang Wang"
                    },
                    {
                        "name": "Hai Zhao"
                    },
                    {
                        "name": "Xiaoming Fu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoming Fu"
                },
                "author": "Xiaoming Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02751v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02751v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19718v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19718v2",
                "updated": "2025-08-02T23:59:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    23,
                    59,
                    11,
                    5,
                    214,
                    0
                ],
                "published": "2025-07-25T23:55:54Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    23,
                    55,
                    54,
                    4,
                    206,
                    0
                ],
                "title": "GSCache: Real-Time Radiance Caching for Volume Path Tracing using 3D\n  Gaussian Splatting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GSCache: Real-Time Radiance Caching for Volume Path Tracing using 3D\n  Gaussian Splatting"
                },
                "summary": "Real-time path tracing is rapidly becoming the standard for rendering in\nentertainment and professional applications. In scientific visualization,\nvolume rendering plays a crucial role in helping researchers analyze and\ninterpret complex 3D data. Recently, photorealistic rendering techniques have\ngained popularity in scientific visualization, yet they face significant\nchallenges. One of the most prominent issues is slow rendering performance and\nhigh pixel variance caused by Monte Carlo integration. In this work, we\nintroduce a novel radiance caching approach for path-traced volume rendering.\nOur method leverages advances in volumetric scene representation and adapts 3D\nGaussian splatting to function as a multi-level, path-space radiance cache.\nThis cache is designed to be trainable on the fly, dynamically adapting to\nchanges in scene parameters such as lighting configurations and transfer\nfunctions. By incorporating our cache, we achieve less noisy, higher-quality\nimages without increasing rendering costs. To evaluate our approach, we compare\nit against a baseline path tracer that supports uniform sampling and next-event\nestimation and the state-of-the-art for neural radiance caching. Through both\nquantitative and qualitative analyses, we demonstrate that our path-space\nradiance cache is a robust solution that is easy to integrate and significantly\nenhances the rendering quality of volumetric visualization applications while\nmaintaining comparable computational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time path tracing is rapidly becoming the standard for rendering in\nentertainment and professional applications. In scientific visualization,\nvolume rendering plays a crucial role in helping researchers analyze and\ninterpret complex 3D data. Recently, photorealistic rendering techniques have\ngained popularity in scientific visualization, yet they face significant\nchallenges. One of the most prominent issues is slow rendering performance and\nhigh pixel variance caused by Monte Carlo integration. In this work, we\nintroduce a novel radiance caching approach for path-traced volume rendering.\nOur method leverages advances in volumetric scene representation and adapts 3D\nGaussian splatting to function as a multi-level, path-space radiance cache.\nThis cache is designed to be trainable on the fly, dynamically adapting to\nchanges in scene parameters such as lighting configurations and transfer\nfunctions. By incorporating our cache, we achieve less noisy, higher-quality\nimages without increasing rendering costs. To evaluate our approach, we compare\nit against a baseline path tracer that supports uniform sampling and next-event\nestimation and the state-of-the-art for neural radiance caching. Through both\nquantitative and qualitative analyses, we demonstrate that our path-space\nradiance cache is a robust solution that is easy to integrate and significantly\nenhances the rendering quality of volumetric visualization applications while\nmaintaining comparable computational efficiency."
                },
                "authors": [
                    {
                        "name": "David Bauer"
                    },
                    {
                        "name": "Qi Wu"
                    },
                    {
                        "name": "Hamid Gadirov"
                    },
                    {
                        "name": "Kwan-Liu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Liu Ma"
                },
                "author": "Kwan-Liu Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19718v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19718v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01488v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01488v1",
                "updated": "2025-08-02T21:00:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    21,
                    0,
                    55,
                    5,
                    214,
                    0
                ],
                "published": "2025-08-02T21:00:55Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    21,
                    0,
                    55,
                    5,
                    214,
                    0
                ],
                "title": "PESTO: Real-Time Pitch Estimation with Self-supervised\n  Transposition-equivariant Objective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PESTO: Real-Time Pitch Estimation with Self-supervised\n  Transposition-equivariant Objective"
                },
                "summary": "In this paper, we introduce PESTO, a self-supervised learning approach for\nsingle-pitch estimation using a Siamese architecture. Our model processes\nindividual frames of a Variable-$Q$ Transform (VQT) and predicts pitch\ndistributions. The neural network is designed to be equivariant to\ntranslations, notably thanks to a Toeplitz fully-connected layer. In addition,\nwe construct pitch-shifted pairs by translating and cropping the VQT frames and\ntrain our model with a novel class-based transposition-equivariant objective,\neliminating the need for annotated data. Thanks to this architecture and\ntraining objective, our model achieves remarkable performances while being very\nlightweight ($130$k parameters). Evaluations on music and speech datasets\n(MIR-1K, MDB-stem-synth, and PTDB) demonstrate that PESTO not only outperforms\nself-supervised baselines but also competes with supervised methods, exhibiting\nsuperior cross-dataset generalization. Finally, we enhance PESTO's practical\nutility by developing a streamable VQT implementation using cached\nconvolutions. Combined with our model's low latency (less than 10 ms) and\nminimal parameter count, this makes PESTO particularly suitable for real-time\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce PESTO, a self-supervised learning approach for\nsingle-pitch estimation using a Siamese architecture. Our model processes\nindividual frames of a Variable-$Q$ Transform (VQT) and predicts pitch\ndistributions. The neural network is designed to be equivariant to\ntranslations, notably thanks to a Toeplitz fully-connected layer. In addition,\nwe construct pitch-shifted pairs by translating and cropping the VQT frames and\ntrain our model with a novel class-based transposition-equivariant objective,\neliminating the need for annotated data. Thanks to this architecture and\ntraining objective, our model achieves remarkable performances while being very\nlightweight ($130$k parameters). Evaluations on music and speech datasets\n(MIR-1K, MDB-stem-synth, and PTDB) demonstrate that PESTO not only outperforms\nself-supervised baselines but also competes with supervised methods, exhibiting\nsuperior cross-dataset generalization. Finally, we enhance PESTO's practical\nutility by developing a streamable VQT implementation using cached\nconvolutions. Combined with our model's low latency (less than 10 ms) and\nminimal parameter count, this makes PESTO particularly suitable for real-time\napplications."
                },
                "authors": [
                    {
                        "name": "Alain Riou"
                    },
                    {
                        "name": "Bernardo Torres"
                    },
                    {
                        "name": "Ben Hayes"
                    },
                    {
                        "name": "Stefan Lattner"
                    },
                    {
                        "name": "Gatan Hadjeres"
                    },
                    {
                        "name": "Gal Richard"
                    },
                    {
                        "name": "Geoffroy Peeters"
                    }
                ],
                "author_detail": {
                    "name": "Geoffroy Peeters"
                },
                "author": "Geoffroy Peeters",
                "arxiv_comment": "Accepted to the Transactions of the International Society for Music\n  Information Retrieval",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01488v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01298v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01298v1",
                "updated": "2025-08-02T10:12:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    10,
                    12,
                    45,
                    5,
                    214,
                    0
                ],
                "published": "2025-08-02T10:12:45Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    10,
                    12,
                    45,
                    5,
                    214,
                    0
                ],
                "title": "Improving performance of content-centric networks via decentralized\n  coded caching for multi-level popularity and access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving performance of content-centric networks via decentralized\n  coded caching for multi-level popularity and access"
                },
                "summary": "Content-Centric Networking (CCN) offers a novel architectural paradigm that\nseeks to address the inherent limitations of the prevailing Internet Protocol\n(IP)-based networking model. In contrast to the host-centric communication\napproach of IP networks, CCN prioritizes content by enabling direct addressing\nand routing based on content identifiers. The potential performance\nimprovements of CCN can be further amplified through optimized management of\ncoded data storage and transmission strategies. Decentralized Coded Caching\n(DCC) emerges as a promising technique that harnesses the collective caching\npower of distributed network elements. By strategically pre-positioning\nfrequently accessed content closer to potential consumers during periods of low\nnetwork utilization, DCC has the potential to mitigate content transfer rates\nduring peak traffic periods. This paper proposes a series of fundamental\nmodifications to the CCN architecture by integrating DCC. The proposed\nframework incorporates differentiated coding strategies tailored to user access\nprivileges, thereby eliminating the overhead associated with queue-based\nsearching. Additionally, the framework facilitates recoding of uncoded data\nencountered along the content delivery path. These combined methodologies\ndemonstrably enhance network throughput, elevate cache hit ratios, and\nconsequently, reduce content delivery latency compared to conventional CCN\nimplementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content-Centric Networking (CCN) offers a novel architectural paradigm that\nseeks to address the inherent limitations of the prevailing Internet Protocol\n(IP)-based networking model. In contrast to the host-centric communication\napproach of IP networks, CCN prioritizes content by enabling direct addressing\nand routing based on content identifiers. The potential performance\nimprovements of CCN can be further amplified through optimized management of\ncoded data storage and transmission strategies. Decentralized Coded Caching\n(DCC) emerges as a promising technique that harnesses the collective caching\npower of distributed network elements. By strategically pre-positioning\nfrequently accessed content closer to potential consumers during periods of low\nnetwork utilization, DCC has the potential to mitigate content transfer rates\nduring peak traffic periods. This paper proposes a series of fundamental\nmodifications to the CCN architecture by integrating DCC. The proposed\nframework incorporates differentiated coding strategies tailored to user access\nprivileges, thereby eliminating the overhead associated with queue-based\nsearching. Additionally, the framework facilitates recoding of uncoded data\nencountered along the content delivery path. These combined methodologies\ndemonstrably enhance network throughput, elevate cache hit ratios, and\nconsequently, reduce content delivery latency compared to conventional CCN\nimplementations."
                },
                "authors": [
                    {
                        "name": "Azadeh Sadat Miraftab"
                    },
                    {
                        "name": "Ahmadreza Montazerolghaem"
                    },
                    {
                        "name": "Behrad Mahboobi"
                    }
                ],
                "author_detail": {
                    "name": "Behrad Mahboobi"
                },
                "author": "Behrad Mahboobi",
                "arxiv_doi": "10.1007/s10586-025-05256-6",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s10586-025-05256-6",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.01298v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01298v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01261v1",
                "updated": "2025-08-02T08:33:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    8,
                    33,
                    30,
                    5,
                    214,
                    0
                ],
                "published": "2025-08-02T08:33:30Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    8,
                    33,
                    30,
                    5,
                    214,
                    0
                ],
                "title": "Unifying Mixture of Experts and Multi-Head Latent Attention for\n  Efficient Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying Mixture of Experts and Multi-Head Latent Attention for\n  Efficient Language Models"
                },
                "summary": "We present MoE-MLA-RoPE, a novel architecture combination that combines\nMixture of Experts (MoE) with Multi-head Latent Attention (MLA) and Rotary\nPosition Embeddings (RoPE) for efficient language modeling. Our approach\naddresses the fundamental trade-off between model capacity and computational\nefficiency through three key innovations: (1) fine-grained expert routing with\n64 micro-experts and top-$k$ selection, enabling flexible specialization\nthrough 3.6 * 10^7 possible expert combinations; (2) shared expert isolation\nthat dedicates 2 always active experts for common patterns while routing to 6\nof 62 specialized experts; and (3) gradient-conflict-free load balancing that\nmaintains expert utilization without interfering with primary loss\noptimization.\n  Extensive experiments on models ranging from 17M to 202M parameters\ndemonstrate that MoE-MLA-RoPE with compression ratio r=d/2 achieves 68% KV\ncache memory reduction and 3.2x inference speedup while maintaining competitive\nperplexity (0.8% degradation). Compared to the parameters with 53.9M\nparameters, MoE-MLA-RoPE improves the validation loss by 6.9% over the vanilla\ntransformers while using 42% fewer active parameters per forward pass.\nFLOP-matched experiments reveal even larger gains: 11.1% improvement with 3.2x\ninference acceleration. Automated evaluation using GPT-4 as a judge confirms\nquality improvements in generation, with higher scores on coherence (8.1/10),\ncreativity (7.9/10) and grammatical correctness (8.2/10). Our results establish\nthat architectural novelty, not parameter scaling, defines the efficiency\nfrontier for resource-constrained language model deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present MoE-MLA-RoPE, a novel architecture combination that combines\nMixture of Experts (MoE) with Multi-head Latent Attention (MLA) and Rotary\nPosition Embeddings (RoPE) for efficient language modeling. Our approach\naddresses the fundamental trade-off between model capacity and computational\nefficiency through three key innovations: (1) fine-grained expert routing with\n64 micro-experts and top-$k$ selection, enabling flexible specialization\nthrough 3.6 * 10^7 possible expert combinations; (2) shared expert isolation\nthat dedicates 2 always active experts for common patterns while routing to 6\nof 62 specialized experts; and (3) gradient-conflict-free load balancing that\nmaintains expert utilization without interfering with primary loss\noptimization.\n  Extensive experiments on models ranging from 17M to 202M parameters\ndemonstrate that MoE-MLA-RoPE with compression ratio r=d/2 achieves 68% KV\ncache memory reduction and 3.2x inference speedup while maintaining competitive\nperplexity (0.8% degradation). Compared to the parameters with 53.9M\nparameters, MoE-MLA-RoPE improves the validation loss by 6.9% over the vanilla\ntransformers while using 42% fewer active parameters per forward pass.\nFLOP-matched experiments reveal even larger gains: 11.1% improvement with 3.2x\ninference acceleration. Automated evaluation using GPT-4 as a judge confirms\nquality improvements in generation, with higher scores on coherence (8.1/10),\ncreativity (7.9/10) and grammatical correctness (8.2/10). Our results establish\nthat architectural novelty, not parameter scaling, defines the efficiency\nfrontier for resource-constrained language model deployment."
                },
                "authors": [
                    {
                        "name": "Sushant Mehta"
                    },
                    {
                        "name": "Raj Dandekar"
                    },
                    {
                        "name": "Rajat Dandekar"
                    },
                    {
                        "name": "Sreedath Panat"
                    }
                ],
                "author_detail": {
                    "name": "Sreedath Panat"
                },
                "author": "Sreedath Panat",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11972v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11972v2",
                "updated": "2025-08-02T06:50:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    6,
                    50,
                    59,
                    5,
                    214,
                    0
                ],
                "published": "2025-03-15T02:48:27Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    2,
                    48,
                    27,
                    5,
                    74,
                    0
                ],
                "title": "MoDM: Efficient Serving for Image Generation via Mixture-of-Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoDM: Efficient Serving for Image Generation via Mixture-of-Diffusion\n  Models"
                },
                "summary": "Diffusion-based text-to-image generation models trade latency for quality:\nsmall models are fast but generate lower-quality images, while large models\nproduce better images but are slow.\n  We present MoDM, a novel caching-based serving system for diffusion models\nthat dynamically balances latency and quality through a mixture of diffusion\nmodels. Unlike prior approaches that rely on model-specific internal features,\nMoDM caches final images, allowing seamless retrieval and reuse across multiple\ndiffusion model families.\n  This design enables adaptive serving by dynamically balancing latency and\nimage quality: using smaller models for cache-hit requests to reduce latency\nwhile reserving larger models for cache-miss requests to maintain quality.\nSmall model image quality is preserved using retrieved cached images.\n  We design a global monitor that optimally allocates GPU resources and\nbalances inference workload, ensuring high throughput while meeting\nservice-level objectives under varying request rates. Our evaluations show that\nMoDM significantly reduces average serving time by 2.5x while retaining image\nquality, making it a practical solution for scalable and resource-efficient\nmodel deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based text-to-image generation models trade latency for quality:\nsmall models are fast but generate lower-quality images, while large models\nproduce better images but are slow.\n  We present MoDM, a novel caching-based serving system for diffusion models\nthat dynamically balances latency and quality through a mixture of diffusion\nmodels. Unlike prior approaches that rely on model-specific internal features,\nMoDM caches final images, allowing seamless retrieval and reuse across multiple\ndiffusion model families.\n  This design enables adaptive serving by dynamically balancing latency and\nimage quality: using smaller models for cache-hit requests to reduce latency\nwhile reserving larger models for cache-miss requests to maintain quality.\nSmall model image quality is preserved using retrieved cached images.\n  We design a global monitor that optimally allocates GPU resources and\nbalances inference workload, ensuring high throughput while meeting\nservice-level objectives under varying request rates. Our evaluations show that\nMoDM significantly reduces average serving time by 2.5x while retaining image\nquality, making it a practical solution for scalable and resource-efficient\nmodel deployment."
                },
                "authors": [
                    {
                        "name": "Yuchen Xia"
                    },
                    {
                        "name": "Divyam Sharma"
                    },
                    {
                        "name": "Yichao Yuan"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Nishil Talati"
                    }
                ],
                "author_detail": {
                    "name": "Nishil Talati"
                },
                "author": "Nishil Talati",
                "arxiv_comment": "To appear in ASPLOS'26",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11972v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11972v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01225v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01225v1",
                "updated": "2025-08-02T06:43:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    6,
                    43,
                    43,
                    5,
                    214,
                    0
                ],
                "published": "2025-08-02T06:43:43Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    6,
                    43,
                    43,
                    5,
                    214,
                    0
                ],
                "title": "Multi-Cache Enhanced Prototype Learning for Test-Time Generalization of\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Cache Enhanced Prototype Learning for Test-Time Generalization of\n  Vision-Language Models"
                },
                "summary": "In zero-shot setting, test-time adaptation adjusts pre-trained models using\nunlabeled data from the test phase to enhance performance on unknown test\ndistributions. Existing cache-enhanced TTA methods rely on a low-entropy\ncriterion to select samples for prototype construction, assuming intra-class\ncompactness. However, low-entropy samples may be unreliable under distribution\nshifts, and the resulting prototypes may not ensure compact intra-class\ndistributions. This study identifies a positive correlation between\ncache-enhanced performance and intra-class compactness. Based on this\nobservation, we propose a Multi-Cache enhanced Prototype-based Test-Time\nAdaptation (MCP) featuring three caches: an entropy cache for initializing\nprototype representations with low-entropy samples, an align cache for\nintegrating visual and textual information to achieve compact intra-class\ndistributions, and a negative cache for prediction calibration using\nhigh-entropy samples. We further developed MCP++, a framework incorporating\ncross-modal prototype alignment and residual learning, introducing prototype\nresidual fine-tuning. Comparative and ablation experiments across 15 downstream\ntasks demonstrate that the proposed method and framework achieve\nstate-of-the-art generalization performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In zero-shot setting, test-time adaptation adjusts pre-trained models using\nunlabeled data from the test phase to enhance performance on unknown test\ndistributions. Existing cache-enhanced TTA methods rely on a low-entropy\ncriterion to select samples for prototype construction, assuming intra-class\ncompactness. However, low-entropy samples may be unreliable under distribution\nshifts, and the resulting prototypes may not ensure compact intra-class\ndistributions. This study identifies a positive correlation between\ncache-enhanced performance and intra-class compactness. Based on this\nobservation, we propose a Multi-Cache enhanced Prototype-based Test-Time\nAdaptation (MCP) featuring three caches: an entropy cache for initializing\nprototype representations with low-entropy samples, an align cache for\nintegrating visual and textual information to achieve compact intra-class\ndistributions, and a negative cache for prediction calibration using\nhigh-entropy samples. We further developed MCP++, a framework incorporating\ncross-modal prototype alignment and residual learning, introducing prototype\nresidual fine-tuning. Comparative and ablation experiments across 15 downstream\ntasks demonstrate that the proposed method and framework achieve\nstate-of-the-art generalization performance."
                },
                "authors": [
                    {
                        "name": "Xinyu Chen"
                    },
                    {
                        "name": "Haotian Zhai"
                    },
                    {
                        "name": "Can Zhang"
                    },
                    {
                        "name": "Xiupeng Shi"
                    },
                    {
                        "name": "Ruirui Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruirui Li"
                },
                "author": "Ruirui Li",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01225v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01225v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19849v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19849v2",
                "updated": "2025-08-02T00:31:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    0,
                    31,
                    18,
                    5,
                    214,
                    0
                ],
                "published": "2025-05-26T11:35:04Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    11,
                    35,
                    4,
                    0,
                    146,
                    0
                ],
                "title": "HIT Model: A Hierarchical Interaction-Enhanced Two-Tower Model for\n  Pre-Ranking Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HIT Model: A Hierarchical Interaction-Enhanced Two-Tower Model for\n  Pre-Ranking Systems"
                },
                "summary": "Online display advertising platforms rely on pre-ranking systems to\nefficiently filter and prioritize candidate ads from large corpora, balancing\nrelevance to users with strict computational constraints. The prevailing\ntwo-tower architecture, though highly efficient due to its decoupled design and\npre-caching, suffers from cross-domain interaction and coarse similarity\nmetrics, undermining its capacity to model complex user-ad relationships. In\nthis study, we propose the Hierarchical Interaction-Enhanced Two-Tower (HIT)\nmodel, a new architecture that augments the two-tower paradigm with two key\ncomponents: $\\textit{generators}$ that pre-generate holistic vectors\nincorporating coarse-grained user-ad interactions through a dual-generator\nframework with a cosine-similarity-based generation loss as the training\nobjective, and $\\textit{multi-head representers}$ that project embeddings into\nmultiple latent subspaces to capture fine-grained, multi-faceted user interests\nand multi-dimensional ad attributes. This design enhances modeling\neffectiveness without compromising inference efficiency. Extensive experiments\non public datasets and large-scale online A/B testing on Tencent's advertising\nplatform demonstrate that HIT significantly outperforms several baselines in\nrelevance metrics, yielding a $1.66\\%$ increase in Gross Merchandise Volume and\na $1.55\\%$ improvement in Return on Investment, alongside similar serving\nlatency to the vanilla two-tower models. The HIT model has been successfully\ndeployed in Tencent's online display advertising system, serving billions of\nimpressions daily. The code is available at\nhttps://github.com/HarveyYang123/HIT_model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online display advertising platforms rely on pre-ranking systems to\nefficiently filter and prioritize candidate ads from large corpora, balancing\nrelevance to users with strict computational constraints. The prevailing\ntwo-tower architecture, though highly efficient due to its decoupled design and\npre-caching, suffers from cross-domain interaction and coarse similarity\nmetrics, undermining its capacity to model complex user-ad relationships. In\nthis study, we propose the Hierarchical Interaction-Enhanced Two-Tower (HIT)\nmodel, a new architecture that augments the two-tower paradigm with two key\ncomponents: $\\textit{generators}$ that pre-generate holistic vectors\nincorporating coarse-grained user-ad interactions through a dual-generator\nframework with a cosine-similarity-based generation loss as the training\nobjective, and $\\textit{multi-head representers}$ that project embeddings into\nmultiple latent subspaces to capture fine-grained, multi-faceted user interests\nand multi-dimensional ad attributes. This design enhances modeling\neffectiveness without compromising inference efficiency. Extensive experiments\non public datasets and large-scale online A/B testing on Tencent's advertising\nplatform demonstrate that HIT significantly outperforms several baselines in\nrelevance metrics, yielding a $1.66\\%$ increase in Gross Merchandise Volume and\na $1.55\\%$ improvement in Return on Investment, alongside similar serving\nlatency to the vanilla two-tower models. The HIT model has been successfully\ndeployed in Tencent's online display advertising system, serving billions of\nimpressions daily. The code is available at\nhttps://github.com/HarveyYang123/HIT_model."
                },
                "authors": [
                    {
                        "name": "Haoqiang Yang"
                    },
                    {
                        "name": "Congde Yuan"
                    },
                    {
                        "name": "Kun Bai"
                    },
                    {
                        "name": "Mengzhuo Guo"
                    },
                    {
                        "name": "Wei Yang"
                    },
                    {
                        "name": "Chao Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Chao Zhou"
                },
                "author": "Chao Zhou",
                "arxiv_comment": "7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19849v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19849v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01051v1",
                "updated": "2025-08-01T20:08:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    20,
                    8,
                    52,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T20:08:52Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    20,
                    8,
                    52,
                    4,
                    213,
                    0
                ],
                "title": "QPP-RNG: A Conceptual Quantum System for True Randomness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QPP-RNG: A Conceptual Quantum System for True Randomness"
                },
                "summary": "We propose and experimentally demonstrate the \\emph{Quasi-Superposition\nQuantum-inspired System (QSQS)} -- a conceptual quantum system for randomness\ngeneration built on measuring two conjugate observables of a permutation\nsorting process: the deterministic permutation count $n_p$ and the\nfundamentally non-deterministic sorting time $t$. By analogy with quantum\nsystems, these observables are linked by an uncertainty-like constraint:\nalgorithmic determinism ensures structural uniformity, while system-level\nfluctuations introduce irreducible unpredictability. We realize this framework\nconcretely as \\emph{QPP-RNG}, a system-embedded, software-based true random\nnumber generator (TRNG). In QPP-RNG, real-time measurements of sorting time $t$\n-- shaped by CPU pipeline jitter, cache latency, and OS scheduling --\ndynamically reseed the PRNG driving the permutation sequence. Crucially, QSQS\ntransforms initially right-skewed raw distributions of $n_p$ and $t$ into\nnearly uniform outputs after modulo reduction, thanks to internal degeneracies\nthat collapse many distinct states into the same output symbol. Empirical\nresults show that as the repetition factor $m$ increases, output entropy\nconverges toward theoretical maxima: Shannon and min-entropy values approach 8\nbits, chi-squared statistics stabilize near ideal uniformity, and bell curves\nvisually confirm the flattening from skewed to uniform distributions. Beyond\npractical implications, QSQS unifies deterministic algorithmic processes with\nnon-deterministic physical fluctuations, offering a physics-based perspective\nfor engineering true randomness in post-quantum cryptographic systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose and experimentally demonstrate the \\emph{Quasi-Superposition\nQuantum-inspired System (QSQS)} -- a conceptual quantum system for randomness\ngeneration built on measuring two conjugate observables of a permutation\nsorting process: the deterministic permutation count $n_p$ and the\nfundamentally non-deterministic sorting time $t$. By analogy with quantum\nsystems, these observables are linked by an uncertainty-like constraint:\nalgorithmic determinism ensures structural uniformity, while system-level\nfluctuations introduce irreducible unpredictability. We realize this framework\nconcretely as \\emph{QPP-RNG}, a system-embedded, software-based true random\nnumber generator (TRNG). In QPP-RNG, real-time measurements of sorting time $t$\n-- shaped by CPU pipeline jitter, cache latency, and OS scheduling --\ndynamically reseed the PRNG driving the permutation sequence. Crucially, QSQS\ntransforms initially right-skewed raw distributions of $n_p$ and $t$ into\nnearly uniform outputs after modulo reduction, thanks to internal degeneracies\nthat collapse many distinct states into the same output symbol. Empirical\nresults show that as the repetition factor $m$ increases, output entropy\nconverges toward theoretical maxima: Shannon and min-entropy values approach 8\nbits, chi-squared statistics stabilize near ideal uniformity, and bell curves\nvisually confirm the flattening from skewed to uniform distributions. Beyond\npractical implications, QSQS unifies deterministic algorithmic processes with\nnon-deterministic physical fluctuations, offering a physics-based perspective\nfor engineering true randomness in post-quantum cryptographic systems."
                },
                "authors": [
                    {
                        "name": "Randy Kuang"
                    }
                ],
                "author_detail": {
                    "name": "Randy Kuang"
                },
                "author": "Randy Kuang",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00647v1",
                "updated": "2025-08-01T14:05:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    14,
                    5,
                    44,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T14:05:44Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    14,
                    5,
                    44,
                    4,
                    213,
                    0
                ],
                "title": "Study of the HV power supply modules for the CUbesat Solar Polarimeter\n  (CUSP)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Study of the HV power supply modules for the CUbesat Solar Polarimeter\n  (CUSP)"
                },
                "summary": "The CUbesat Solar Polarimeter (CUSP) project is a CubeSat mission orbiting\nthe Earth aimed to measure the linear polarization of solar flares in the hard\nX-ray band by means of a Compton scattering polarimeter. CUSP will allow to\nstudy the magnetic reconnection and particle acceleration in the flaring\nmagnetic structures of our star. CUSP is a project in the framework of the\nAlcor Program of the Italian Space Agency aimed to develop new CubeSat\nmissions. CUSP undergoing the Phase B started in December 2024 that will last\nfor 12 month. The Compton polarimeter of the CUSP payload performs coincidence\nmeasurements between plastic scintilaltors and GaGG(Ce) crystals to derive the\npolarization of X-rays. These sensors are readout by Multi Anode\nPhotomultiplier Tubes (MAPMTs) and Avalanche Photodiodes (APDs) respectively.\nBoth sensors need an HV power supply up to -1~kV (for the MAPMT) and +500~V\n(for the APD). We tested precision regulated High Voltage DC/DC Converters by\nHVM Technology Inc. with Sub-Miniature Case Size\n($0.85''\\times0.85''\\times0.60''$) of the SMHV series. These modules are\ncompact and suited for CubeSat missions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The CUbesat Solar Polarimeter (CUSP) project is a CubeSat mission orbiting\nthe Earth aimed to measure the linear polarization of solar flares in the hard\nX-ray band by means of a Compton scattering polarimeter. CUSP will allow to\nstudy the magnetic reconnection and particle acceleration in the flaring\nmagnetic structures of our star. CUSP is a project in the framework of the\nAlcor Program of the Italian Space Agency aimed to develop new CubeSat\nmissions. CUSP undergoing the Phase B started in December 2024 that will last\nfor 12 month. The Compton polarimeter of the CUSP payload performs coincidence\nmeasurements between plastic scintilaltors and GaGG(Ce) crystals to derive the\npolarization of X-rays. These sensors are readout by Multi Anode\nPhotomultiplier Tubes (MAPMTs) and Avalanche Photodiodes (APDs) respectively.\nBoth sensors need an HV power supply up to -1~kV (for the MAPMT) and +500~V\n(for the APD). We tested precision regulated High Voltage DC/DC Converters by\nHVM Technology Inc. with Sub-Miniature Case Size\n($0.85''\\times0.85''\\times0.60''$) of the SMHV series. These modules are\ncompact and suited for CubeSat missions."
                },
                "authors": [
                    {
                        "name": "Alessandro Lacerenza"
                    },
                    {
                        "name": "Alda Rubini"
                    },
                    {
                        "name": "Andrea Alimenti"
                    },
                    {
                        "name": "Sergio Fabiani"
                    },
                    {
                        "name": "Ettore Del Monte"
                    },
                    {
                        "name": "Riccardo Campana"
                    },
                    {
                        "name": "Mauro Centrone"
                    },
                    {
                        "name": "Enrico Costa"
                    },
                    {
                        "name": "Nicolas De Angelis"
                    },
                    {
                        "name": "Giovanni De Cesare"
                    },
                    {
                        "name": "Sergio Di Cosimo"
                    },
                    {
                        "name": "Giuseppe Di Persio"
                    },
                    {
                        "name": "Abhay Kumar"
                    },
                    {
                        "name": "Pasqualino Loffredo"
                    },
                    {
                        "name": "Giovanni Lombardi"
                    },
                    {
                        "name": "Gabriele Minervini"
                    },
                    {
                        "name": "Fabio Muleri"
                    },
                    {
                        "name": "Paolo Romano"
                    },
                    {
                        "name": "Emanuele Scalise"
                    },
                    {
                        "name": "Enrico Silva"
                    },
                    {
                        "name": "Paolo Soffitta"
                    },
                    {
                        "name": "Davide Albanesi"
                    },
                    {
                        "name": "Ilaria Baffo"
                    },
                    {
                        "name": "Daniele Brienza"
                    },
                    {
                        "name": "Valerio Campamaggiore"
                    },
                    {
                        "name": "Giovanni Cucinella"
                    },
                    {
                        "name": "Andrea Curatolo"
                    },
                    {
                        "name": "Giulia de Iulis"
                    },
                    {
                        "name": "Andrea Del Re"
                    },
                    {
                        "name": "Vito Di Bari"
                    },
                    {
                        "name": "Simone Di Filippo"
                    },
                    {
                        "name": "Immacolata Donnarumma"
                    },
                    {
                        "name": "Pierluigi Fanelli"
                    },
                    {
                        "name": "Nicolas Gagliardi"
                    },
                    {
                        "name": "Paolo Leonetti"
                    },
                    {
                        "name": "Matteo Merge"
                    },
                    {
                        "name": "Dario Modenini"
                    },
                    {
                        "name": "Andrea Negri"
                    },
                    {
                        "name": "Daniele Pecorella"
                    },
                    {
                        "name": "Massimo Perelli"
                    },
                    {
                        "name": "Alice Ponti"
                    },
                    {
                        "name": "Francesca Sbop"
                    },
                    {
                        "name": "Paolo Tortora"
                    },
                    {
                        "name": "Alessandro Turchi"
                    },
                    {
                        "name": "Valerio Vagelli"
                    },
                    {
                        "name": "Emanuele Zaccagnino"
                    },
                    {
                        "name": "Alessandro Zambardi"
                    },
                    {
                        "name": "Costantino Zazza"
                    }
                ],
                "author_detail": {
                    "name": "Costantino Zazza"
                },
                "author": "Costantino Zazza",
                "arxiv_comment": "6 pages, 2 figures, SPIE Optics+Photonics 2025 proceeding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.space-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00629v1",
                "updated": "2025-08-01T13:40:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    40,
                    52,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T13:40:52Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    40,
                    52,
                    4,
                    213,
                    0
                ],
                "title": "Energy-Aware CPU Orchestration in O-RAN: A dApp-Driven Lightweight\n  Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy-Aware CPU Orchestration in O-RAN: A dApp-Driven Lightweight\n  Approach"
                },
                "summary": "The transition toward softwarized Radio Access Networks (RANs), driven by the\nOpen RAN (O-RAN) paradigm, enables flexible, vendor-neutral deployments through\ndisaggregation and virtualization of base station functions. However, this\nshift introduces new challenges in managing CPU resources efficiently under\nstrict real-time constraints. In particular, the interplay between\nlatency-sensitive RAN workloads and general-purpose Operating System (OS)\nschedulers often leads to sub-optimal performance and unnecessary energy\nconsumption. This work proposes a lightweight, programmable distributed\napplication (dApp) deployed at the Distributed Unit (DU) level to dynamically\norchestrate CPU usage. The dApp operates in closed loop with the OS, leveraging\nthread-level telemetry like context switches, Instructions Per Cycle (IPC), and\ncache metrics, to adapt CPU thread affinity, core isolation, and frequency\nscaling in real time. Unlike existing solutions, it requires no access to\nproprietary RAN software, hardware-specific features, or kernel modifications.\nFully compliant with the O-RAN architecture and agnostic to the underlying RAN\nstack, the proposed solution introduces negligible overhead while improving\nenergy efficiency and CPU utilization. Experimental results using a\ncommercial-grade srsRAN deployment demonstrate consistent power savings without\ncompromising real-time processing performance, highlighting the potential of\nlow-latency dApps for fine-grained resource control in next-generation networks",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transition toward softwarized Radio Access Networks (RANs), driven by the\nOpen RAN (O-RAN) paradigm, enables flexible, vendor-neutral deployments through\ndisaggregation and virtualization of base station functions. However, this\nshift introduces new challenges in managing CPU resources efficiently under\nstrict real-time constraints. In particular, the interplay between\nlatency-sensitive RAN workloads and general-purpose Operating System (OS)\nschedulers often leads to sub-optimal performance and unnecessary energy\nconsumption. This work proposes a lightweight, programmable distributed\napplication (dApp) deployed at the Distributed Unit (DU) level to dynamically\norchestrate CPU usage. The dApp operates in closed loop with the OS, leveraging\nthread-level telemetry like context switches, Instructions Per Cycle (IPC), and\ncache metrics, to adapt CPU thread affinity, core isolation, and frequency\nscaling in real time. Unlike existing solutions, it requires no access to\nproprietary RAN software, hardware-specific features, or kernel modifications.\nFully compliant with the O-RAN architecture and agnostic to the underlying RAN\nstack, the proposed solution introduces negligible overhead while improving\nenergy efficiency and CPU utilization. Experimental results using a\ncommercial-grade srsRAN deployment demonstrate consistent power savings without\ncompromising real-time processing performance, highlighting the potential of\nlow-latency dApps for fine-grained resource control in next-generation networks"
                },
                "authors": [
                    {
                        "name": "Francisco Crespo"
                    },
                    {
                        "name": "Javier Villegas"
                    },
                    {
                        "name": "Carlos Baena"
                    },
                    {
                        "name": "Eduardo Baena"
                    },
                    {
                        "name": "Sergio Fortes"
                    },
                    {
                        "name": "Raquel Barco"
                    }
                ],
                "author_detail": {
                    "name": "Raquel Barco"
                },
                "author": "Raquel Barco",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00616v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00616v1",
                "updated": "2025-08-01T13:25:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    25,
                    28,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T13:25:28Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    25,
                    28,
                    4,
                    213,
                    0
                ],
                "title": "Joint Association and Phase Shifts Design for UAV-mounted Stacked\n  Intelligent Metasurfaces-assisted Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Association and Phase Shifts Design for UAV-mounted Stacked\n  Intelligent Metasurfaces-assisted Communications"
                },
                "summary": "Stacked intelligent metasurfaces (SIMs) have emerged as a promising\ntechnology for realizing wave-domain signal processing, while the fixed SIMs\nwill limit the communication performance of the system compared to the mobile\nSIMs. In this work, we consider a UAV-mounted SIMs (UAV-SIMs) assisted\ncommunication system, where UAVs as base stations (BSs) can cache the data\nprocessed by SIMs, and also as mobile vehicles flexibly deploy SIMs to enhance\nthe communication performance. To this end, we formulate a UAV-SIM-based joint\noptimization problem (USBJOP) to comprehensively consider the association\nbetween UAV-SIMs and users, the locations of UAV-SIMs, and the phase shifts of\nUAV-SIMs, aiming to maximize the network capacity. Due to the non-convexity and\nNP-hardness of USBJOP, we decompose it into three sub-optimization problems,\nwhich are the association between UAV-SIMs and users optimization problem\n(AUUOP), the UAV location optimization problem (ULOP), and the UAV-SIM phase\nshifts optimization problem (USPSOP). Then, these three sub-optimization\nproblems are solved by an alternating optimization (AO) strategy. Specifically,\nAUUOP and ULOP are transformed to a convex form and then solved by the CVX\ntool, while we employ a layer-by-layer iterative optimization method for\nUSPSOP. Simulation results verify the effectiveness of the proposed strategy\nunder different simulation setups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stacked intelligent metasurfaces (SIMs) have emerged as a promising\ntechnology for realizing wave-domain signal processing, while the fixed SIMs\nwill limit the communication performance of the system compared to the mobile\nSIMs. In this work, we consider a UAV-mounted SIMs (UAV-SIMs) assisted\ncommunication system, where UAVs as base stations (BSs) can cache the data\nprocessed by SIMs, and also as mobile vehicles flexibly deploy SIMs to enhance\nthe communication performance. To this end, we formulate a UAV-SIM-based joint\noptimization problem (USBJOP) to comprehensively consider the association\nbetween UAV-SIMs and users, the locations of UAV-SIMs, and the phase shifts of\nUAV-SIMs, aiming to maximize the network capacity. Due to the non-convexity and\nNP-hardness of USBJOP, we decompose it into three sub-optimization problems,\nwhich are the association between UAV-SIMs and users optimization problem\n(AUUOP), the UAV location optimization problem (ULOP), and the UAV-SIM phase\nshifts optimization problem (USPSOP). Then, these three sub-optimization\nproblems are solved by an alternating optimization (AO) strategy. Specifically,\nAUUOP and ULOP are transformed to a convex form and then solved by the CVX\ntool, while we employ a layer-by-layer iterative optimization method for\nUSPSOP. Simulation results verify the effectiveness of the proposed strategy\nunder different simulation setups."
                },
                "authors": [
                    {
                        "name": "Mingzhe Fan"
                    },
                    {
                        "name": "Geng Sun"
                    },
                    {
                        "name": "Hongyang Pan"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Jiancheng An"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Chau Yuen"
                    }
                ],
                "author_detail": {
                    "name": "Chau Yuen"
                },
                "author": "Chau Yuen",
                "arxiv_comment": "This papar has been submitted to the IEEE Global Communications\n  Conference. arXiv admin note: substantial text overlap with arXiv:2506.23488",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00616v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00616v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00412v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00412v1",
                "updated": "2025-08-01T08:10:54Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    8,
                    10,
                    54,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T08:10:54Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    8,
                    10,
                    54,
                    4,
                    213,
                    0
                ],
                "title": "Sortblock: Similarity-Aware Feature Reuse for Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sortblock: Similarity-Aware Feature Reuse for Diffusion Model"
                },
                "summary": "Diffusion Transformers (DiTs) have demonstrated remarkable generative\ncapabilities, particularly benefiting from Transformer architectures that\nenhance visual and artistic fidelity. However, their inherently sequential\ndenoising process results in high inference latency, limiting their deployment\nin real-time scenarios. Existing training-free acceleration approaches\ntypically reuse intermediate features at fixed timesteps or layers, overlooking\nthe evolving semantic focus across denoising stages and Transformer blocks.To\naddress this, we propose Sortblock, a training-free inference acceleration\nframework that dynamically caches block-wise features based on their similarity\nacross adjacent timesteps. By ranking the evolution of residuals, Sortblock\nadaptively determines a recomputation ratio, selectively skipping redundant\ncomputations while preserving generation quality. Furthermore, we incorporate a\nlightweight linear prediction mechanism to reduce accumulated errors in skipped\nblocks.Extensive experiments across various tasks and DiT architectures\ndemonstrate that Sortblock achieves over 2$\\times$ inference speedup with\nminimal degradation in output quality, offering an effective and generalizable\nsolution for accelerating diffusion-based generative models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have demonstrated remarkable generative\ncapabilities, particularly benefiting from Transformer architectures that\nenhance visual and artistic fidelity. However, their inherently sequential\ndenoising process results in high inference latency, limiting their deployment\nin real-time scenarios. Existing training-free acceleration approaches\ntypically reuse intermediate features at fixed timesteps or layers, overlooking\nthe evolving semantic focus across denoising stages and Transformer blocks.To\naddress this, we propose Sortblock, a training-free inference acceleration\nframework that dynamically caches block-wise features based on their similarity\nacross adjacent timesteps. By ranking the evolution of residuals, Sortblock\nadaptively determines a recomputation ratio, selectively skipping redundant\ncomputations while preserving generation quality. Furthermore, we incorporate a\nlightweight linear prediction mechanism to reduce accumulated errors in skipped\nblocks.Extensive experiments across various tasks and DiT architectures\ndemonstrate that Sortblock achieves over 2$\\times$ inference speedup with\nminimal degradation in output quality, offering an effective and generalizable\nsolution for accelerating diffusion-based generative models."
                },
                "authors": [
                    {
                        "name": "Hanqi Chen"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Xiaoliu Guan"
                    },
                    {
                        "name": "Lielin Jiang"
                    },
                    {
                        "name": "Guanzhong Wang"
                    },
                    {
                        "name": "Zeyu Chen"
                    },
                    {
                        "name": "Yi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yi Liu"
                },
                "author": "Yi Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00412v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00412v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23387v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23387v2",
                "updated": "2025-08-01T03:43:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    3,
                    43,
                    24,
                    4,
                    213,
                    0
                ],
                "published": "2025-07-31T10:02:26Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    10,
                    2,
                    26,
                    3,
                    212,
                    0
                ],
                "title": "SGEMM-cube: Emulating FP32 GEMM on Ascend NPUs Using FP16 Cube Units\n  with Precision Recovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SGEMM-cube: Emulating FP32 GEMM on Ascend NPUs Using FP16 Cube Units\n  with Precision Recovery"
                },
                "summary": "Low-precision matrix engines, such as FP16 cube, offer high throughput but\nlack support for full-precision computation. In this work, we propose\nSGEMM-cube, a high-performance algorithm for emulating FP32 general\nmatrix-matrix multiplication (GEMM) using only FP16 computation units on a\nrepresentative AI accelerator. The method decomposes each FP32 operand into two\nFP16 values and compensates for numerical errors through a tunable scaling\nstrategy. A detailed analysis of numerical errors, including underflow\nconditions and precision loss, guides the selection of scaling parameters to\npreserve up to 22 bits of mantissa accuracy. We further investigate the effect\nof computation order on accuracy and demonstrate that a term-wise accumulation\nscheme improves numerical stability over conventional FP32 GEMM in low-exponent\nregimes. Finally, a cache-aware blocking strategy and double-buffered pipeline\nare introduced to overlap memory transfers with computation, enabling\nSGEMM-cube to achieve up to 77\\% of the theoretical FP32-equivalent peak\nperformance on Ascend 910A NPU lacking native FP32 support. Extensive numerical\nexperiments confirm that our method not only recovers the accuracy of native\nFP32 GEMM but also exhibits superior numerical stability under certain\nconditions, due to its structured and error-aware computation order.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-precision matrix engines, such as FP16 cube, offer high throughput but\nlack support for full-precision computation. In this work, we propose\nSGEMM-cube, a high-performance algorithm for emulating FP32 general\nmatrix-matrix multiplication (GEMM) using only FP16 computation units on a\nrepresentative AI accelerator. The method decomposes each FP32 operand into two\nFP16 values and compensates for numerical errors through a tunable scaling\nstrategy. A detailed analysis of numerical errors, including underflow\nconditions and precision loss, guides the selection of scaling parameters to\npreserve up to 22 bits of mantissa accuracy. We further investigate the effect\nof computation order on accuracy and demonstrate that a term-wise accumulation\nscheme improves numerical stability over conventional FP32 GEMM in low-exponent\nregimes. Finally, a cache-aware blocking strategy and double-buffered pipeline\nare introduced to overlap memory transfers with computation, enabling\nSGEMM-cube to achieve up to 77\\% of the theoretical FP32-equivalent peak\nperformance on Ascend 910A NPU lacking native FP32 support. Extensive numerical\nexperiments confirm that our method not only recovers the accuracy of native\nFP32 GEMM but also exhibits superior numerical stability under certain\nconditions, due to its structured and error-aware computation order."
                },
                "authors": [
                    {
                        "name": "Weicheng Xue"
                    },
                    {
                        "name": "Baisong Xu"
                    },
                    {
                        "name": "Kai Yang"
                    },
                    {
                        "name": "Yongxiang Liu"
                    },
                    {
                        "name": "Dengdeng Fan"
                    },
                    {
                        "name": "Pengxiang Xu"
                    },
                    {
                        "name": "Yonghong Tian"
                    }
                ],
                "author_detail": {
                    "name": "Yonghong Tian"
                },
                "author": "Yonghong Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23387v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23387v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22746v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22746v2",
                "updated": "2025-08-01T03:37:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    3,
                    37,
                    42,
                    4,
                    213,
                    0
                ],
                "published": "2025-07-30T15:03:36Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    15,
                    3,
                    36,
                    2,
                    211,
                    0
                ],
                "title": "Next Tokens Denoising for Speech Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next Tokens Denoising for Speech Synthesis"
                },
                "summary": "While diffusion and autoregressive (AR) models have significantly advanced\ngenerative modeling, they each present distinct limitations. AR models, which\nrely on causal attention, cannot exploit future context and suffer from slow\ngeneration speeds. Conversely, diffusion models struggle with key-value (KV)\ncaching. To overcome these challenges, we introduce Dragon-FM, a novel\ntext-to-speech (TTS) design that unifies AR and flow-matching. This model\nprocesses 48 kHz audio codec tokens in chunks at a compact rate of 12.5 tokens\nper second. This design enables AR modeling across chunks, ensuring global\ncoherence, while parallel flow-matching within chunks facilitates fast\niterative denoising. Thus, the model leverages KV-cache across chunks and\nutilizes bidirectional context within each chunk. Furthermore, it bridges\ncontinuous and discrete feature modeling, demonstrating that continuous AR\nflow-matching can predict discrete tokens with finite scalar quantizers. This\nefficient codec and fast chunk-autoregressive architecture also make the model\nhighly effective for generating long-form content, such as podcasts.\nExperiments on podcast datasets demonstrate its capability to efficiently\ngenerate high-quality zero-shot podcasts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While diffusion and autoregressive (AR) models have significantly advanced\ngenerative modeling, they each present distinct limitations. AR models, which\nrely on causal attention, cannot exploit future context and suffer from slow\ngeneration speeds. Conversely, diffusion models struggle with key-value (KV)\ncaching. To overcome these challenges, we introduce Dragon-FM, a novel\ntext-to-speech (TTS) design that unifies AR and flow-matching. This model\nprocesses 48 kHz audio codec tokens in chunks at a compact rate of 12.5 tokens\nper second. This design enables AR modeling across chunks, ensuring global\ncoherence, while parallel flow-matching within chunks facilitates fast\niterative denoising. Thus, the model leverages KV-cache across chunks and\nutilizes bidirectional context within each chunk. Furthermore, it bridges\ncontinuous and discrete feature modeling, demonstrating that continuous AR\nflow-matching can predict discrete tokens with finite scalar quantizers. This\nefficient codec and fast chunk-autoregressive architecture also make the model\nhighly effective for generating long-form content, such as podcasts.\nExperiments on podcast datasets demonstrate its capability to efficiently\ngenerate high-quality zero-shot podcasts."
                },
                "authors": [
                    {
                        "name": "Yanqing Liu"
                    },
                    {
                        "name": "Ruiqing Xue"
                    },
                    {
                        "name": "Chong Zhang"
                    },
                    {
                        "name": "Yufei Liu"
                    },
                    {
                        "name": "Gang Wang"
                    },
                    {
                        "name": "Bohan Li"
                    },
                    {
                        "name": "Yao Qian"
                    },
                    {
                        "name": "Lei He"
                    },
                    {
                        "name": "Shujie Liu"
                    },
                    {
                        "name": "Sheng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Sheng Zhao"
                },
                "author": "Sheng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22746v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22746v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02659v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02659v2",
                "updated": "2025-07-31T21:00:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    21,
                    0,
                    28,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-03T14:20:41Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    20,
                    41,
                    3,
                    184,
                    0
                ],
                "title": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding"
                },
                "summary": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup."
                },
                "authors": [
                    {
                        "name": "Ramchalam Kinattinkara Ramakrishnan"
                    },
                    {
                        "name": "Zhaocong Yuan"
                    },
                    {
                        "name": "Shaojie Zhuo"
                    },
                    {
                        "name": "Chen Feng"
                    },
                    {
                        "name": "Yicheng Lin"
                    },
                    {
                        "name": "Chenzheng Su"
                    },
                    {
                        "name": "Xiaopeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaopeng Zhang"
                },
                "author": "Xiaopeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02659v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02659v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22701v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22701v2",
                "updated": "2025-07-31T16:21:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    16,
                    21,
                    3,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-30T14:10:16Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    14,
                    10,
                    16,
                    2,
                    211,
                    0
                ],
                "title": "SAM: A Stability-Aware Cache Manager for Multi-Tenant Embedded Databases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAM: A Stability-Aware Cache Manager for Multi-Tenant Embedded Databases"
                },
                "summary": "The co-location of multiple database instances on resource constrained edge\nnodes creates significant cache contention, where traditional schemes are\ninefficient and unstable under dynamic workloads. To address this, we present\nSAM(a Stability-Aware Manager), an autonomic cache manager that establishes\ndecision stability as a first-class design principle. It achieves this through\nits core control policy, AURA(Autonomic Utility-balancing Resource Allocator),\nwhich resolves the classic exploitation-exploration dilemma by synthesizing two\northogonal factors: the H-factor, representing proven historical efficiency\n(exploitation), and the V-factor, for estimated marginal gain (exploration).\nThrough this practical synthesis and adaptive control, SAM achieves sustained\nhigh performance with strategic stability and robustness in volatile\nconditions.\n  Extensive experiments against 14 diverse baselines demonstrate SAM's\nsuperiority. It achieves top-tier throughput while being uniquely resilient to\ncomplex workload shifts and adversarial workloads like cache pollution.\nFurthermore, its decision latency is highly scalable, remaining nearly constant\nas the system grows to 120 databases. Crucially, SAM achieves superior decision\nstability -- maintaining consistent optimization directions despite noise,\navoiding performance oscillations while ensuring predictable Quality of\nService. These results prove that a principled, stability-aware design is\nessential for sustained high performance in real-world, large-scale systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The co-location of multiple database instances on resource constrained edge\nnodes creates significant cache contention, where traditional schemes are\ninefficient and unstable under dynamic workloads. To address this, we present\nSAM(a Stability-Aware Manager), an autonomic cache manager that establishes\ndecision stability as a first-class design principle. It achieves this through\nits core control policy, AURA(Autonomic Utility-balancing Resource Allocator),\nwhich resolves the classic exploitation-exploration dilemma by synthesizing two\northogonal factors: the H-factor, representing proven historical efficiency\n(exploitation), and the V-factor, for estimated marginal gain (exploration).\nThrough this practical synthesis and adaptive control, SAM achieves sustained\nhigh performance with strategic stability and robustness in volatile\nconditions.\n  Extensive experiments against 14 diverse baselines demonstrate SAM's\nsuperiority. It achieves top-tier throughput while being uniquely resilient to\ncomplex workload shifts and adversarial workloads like cache pollution.\nFurthermore, its decision latency is highly scalable, remaining nearly constant\nas the system grows to 120 databases. Crucially, SAM achieves superior decision\nstability -- maintaining consistent optimization directions despite noise,\navoiding performance oscillations while ensuring predictable Quality of\nService. These results prove that a principled, stability-aware design is\nessential for sustained high performance in real-world, large-scale systems."
                },
                "authors": [
                    {
                        "name": "Haoran Zhang"
                    },
                    {
                        "name": "Decheng Zuo"
                    },
                    {
                        "name": "Yu Yan"
                    },
                    {
                        "name": "Zhiyu Liang"
                    },
                    {
                        "name": "Hongzhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hongzhi Wang"
                },
                "author": "Hongzhi Wang",
                "arxiv_comment": "17 pages, 10 figures. An extended version of a paper under review at\n  the VLDB 2026 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22701v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22701v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.2.4; H.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23674v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23674v1",
                "updated": "2025-07-31T15:50:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    15,
                    50,
                    57,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T15:50:57Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    15,
                    50,
                    57,
                    3,
                    212,
                    0
                ],
                "title": "TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached\n  Responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached\n  Responses"
                },
                "summary": "Large Language Models (LLMs) process millions of queries daily, making\nefficient response caching a compelling optimization for reducing cost and\nlatency. However, preserving relevance to user queries using this approach\nproves difficult due to the personalized nature of chatbot interactions and the\nlimited accuracy of semantic similarity search. To address this, we present\nTweakLLM, a novel routing architecture that employs a lightweight LLM to\ndynamically adapt cached responses to incoming prompts. Through comprehensive\nevaluation, including user studies with side-by-side comparisons, satisfaction\nvoting, as well as multi-agent LLM debates, we demonstrate that TweakLLM\nmaintains response quality comparable to frontier models while significantly\nimproving cache effectiveness. Our results across real-world datasets highlight\nTweakLLM as a scalable, resource-efficient caching solution for high-volume LLM\ndeployments without compromising user experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) process millions of queries daily, making\nefficient response caching a compelling optimization for reducing cost and\nlatency. However, preserving relevance to user queries using this approach\nproves difficult due to the personalized nature of chatbot interactions and the\nlimited accuracy of semantic similarity search. To address this, we present\nTweakLLM, a novel routing architecture that employs a lightweight LLM to\ndynamically adapt cached responses to incoming prompts. Through comprehensive\nevaluation, including user studies with side-by-side comparisons, satisfaction\nvoting, as well as multi-agent LLM debates, we demonstrate that TweakLLM\nmaintains response quality comparable to frontier models while significantly\nimproving cache effectiveness. Our results across real-world datasets highlight\nTweakLLM as a scalable, resource-efficient caching solution for high-volume LLM\ndeployments without compromising user experience."
                },
                "authors": [
                    {
                        "name": "Muhammad Taha Cheema"
                    },
                    {
                        "name": "Abeer Aamir"
                    },
                    {
                        "name": "Khawaja Gul Muhammad"
                    },
                    {
                        "name": "Naveed Anwar Bhatti"
                    },
                    {
                        "name": "Ihsan Ayyub Qazi"
                    },
                    {
                        "name": "Zafar Ayyub Qazi"
                    }
                ],
                "author_detail": {
                    "name": "Zafar Ayyub Qazi"
                },
                "author": "Zafar Ayyub Qazi",
                "arxiv_comment": "13 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23674v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23674v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21433v2",
                "updated": "2025-07-31T07:53:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    7,
                    53,
                    53,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-29T02:05:51Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    2,
                    5,
                    51,
                    1,
                    210,
                    0
                ],
                "title": "MemShare: Memory Efficient Inference for Large Reasoning Models through\n  KV Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemShare: Memory Efficient Inference for Large Reasoning Models through\n  KV Cache Reuse"
                },
                "summary": "Large Reasoning Models (LRMs) have achieved significant advances in\nmathematical reasoning and formal logic tasks. However, their tendency to\ngenerate lengthy chain-of-thought sequences leads to substantial memory\noverhead during inference. We observe that LRMs frequently produce highly\nsimilar intermediate reasoning steps, which correspond to similar KV cache\nstates across layers. Motivated by this observation, we propose MemShare, a\nnovel KV cache management approach that effectively reduces memory overhead.\nMemShare employs a collaborative filtering algorithm to efficiently identify\nreusable KV cache blocks and enables zero copy cache reuse to significantly\nreduce memory overhead, improve throughput while maintaining accuracy.\nExperimental results demonstrate that MemShare delivers up to 84.79\\%\nimprovement in throughput while maintaining better accuracy compared to\nexisting KV cache management methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Reasoning Models (LRMs) have achieved significant advances in\nmathematical reasoning and formal logic tasks. However, their tendency to\ngenerate lengthy chain-of-thought sequences leads to substantial memory\noverhead during inference. We observe that LRMs frequently produce highly\nsimilar intermediate reasoning steps, which correspond to similar KV cache\nstates across layers. Motivated by this observation, we propose MemShare, a\nnovel KV cache management approach that effectively reduces memory overhead.\nMemShare employs a collaborative filtering algorithm to efficiently identify\nreusable KV cache blocks and enables zero copy cache reuse to significantly\nreduce memory overhead, improve throughput while maintaining accuracy.\nExperimental results demonstrate that MemShare delivers up to 84.79\\%\nimprovement in throughput while maintaining better accuracy compared to\nexisting KV cache management methods."
                },
                "authors": [
                    {
                        "name": "Kaiwen Chen"
                    },
                    {
                        "name": "Xin Tan"
                    },
                    {
                        "name": "Minchen Yu"
                    },
                    {
                        "name": "Hong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Hong Xu"
                },
                "author": "Hong Xu",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.21433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01199v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01199v2",
                "updated": "2025-07-31T07:35:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    7,
                    35,
                    4,
                    3,
                    212,
                    0
                ],
                "published": "2025-03-03T05:52:02Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    5,
                    52,
                    2,
                    0,
                    62,
                    0
                ],
                "title": "LiteGS: A High-performance Framework to Train 3DGS in Subminutes via\n  System and Algorithm Codesign",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiteGS: A High-performance Framework to Train 3DGS in Subminutes via\n  System and Algorithm Codesign"
                },
                "summary": "3D Gaussian Splatting (3DGS) has emerged as promising alternative in 3D\nrepresentation. However, it still suffers from high training cost. This paper\nintroduces LiteGS, a high performance framework that systematically optimizes\nthe 3DGS training pipeline from multiple aspects. At the low-level computation\nlayer, we design a ``warp-based raster'' associated with two hardware-aware\noptimizations to significantly reduce gradient reduction overhead. At the\nmid-level data management layer, we introduce dynamic spatial sorting based on\nMorton coding to enable a performant ``Cluster-Cull-Compact'' pipeline and\nimprove data locality, therefore reducing cache misses. At the top-level\nalgorithm layer, we establish a new robust densification criterion based on the\nvariance of the opacity gradient, paired with a more stable opacity control\nmechanism, to achieve more precise parameter growth. Experimental results\ndemonstrate that LiteGS accelerates the original 3DGS training by up to 13.4x\nwith comparable or superior quality and surpasses the current SOTA in\nlightweight models by up to 1.4x speedup. For high-quality reconstruction\ntasks, LiteGS sets a new accuracy record and decreases the training time by an\norder of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Gaussian Splatting (3DGS) has emerged as promising alternative in 3D\nrepresentation. However, it still suffers from high training cost. This paper\nintroduces LiteGS, a high performance framework that systematically optimizes\nthe 3DGS training pipeline from multiple aspects. At the low-level computation\nlayer, we design a ``warp-based raster'' associated with two hardware-aware\noptimizations to significantly reduce gradient reduction overhead. At the\nmid-level data management layer, we introduce dynamic spatial sorting based on\nMorton coding to enable a performant ``Cluster-Cull-Compact'' pipeline and\nimprove data locality, therefore reducing cache misses. At the top-level\nalgorithm layer, we establish a new robust densification criterion based on the\nvariance of the opacity gradient, paired with a more stable opacity control\nmechanism, to achieve more precise parameter growth. Experimental results\ndemonstrate that LiteGS accelerates the original 3DGS training by up to 13.4x\nwith comparable or superior quality and surpasses the current SOTA in\nlightweight models by up to 1.4x speedup. For high-quality reconstruction\ntasks, LiteGS sets a new accuracy record and decreases the training time by an\norder of magnitude."
                },
                "authors": [
                    {
                        "name": "Kaimin Liao"
                    },
                    {
                        "name": "Hua Wang"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Luchao Wang"
                    },
                    {
                        "name": "Yaohua Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yaohua Tang"
                },
                "author": "Yaohua Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01199v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01199v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23292v1",
                "updated": "2025-07-31T07:10:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    7,
                    10,
                    39,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T07:10:39Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    7,
                    10,
                    39,
                    3,
                    212,
                    0
                ],
                "title": "SequenceLayers: Sequence Processing and Streaming Neural Networks Made\n  Easy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SequenceLayers: Sequence Processing and Streaming Neural Networks Made\n  Easy"
                },
                "summary": "We introduce a neural network layer API and library for sequence modeling,\ndesigned for easy creation of sequence models that can be executed both\nlayer-by-layer (e.g., teacher-forced training) and step-by-step (e.g.,\nautoregressive sampling). To achieve this, layers define an explicit\nrepresentation of their state over time (e.g., a Transformer KV cache, a\nconvolution buffer, an RNN hidden state), and a step method that evolves that\nstate, tested to give identical results to a stateless layer-wise invocation.\nThis and other aspects of the SequenceLayers contract enables complex models to\nbe immediately streamable, mitigates a wide range of common bugs arising in\nboth streaming and parallel sequence processing, and can be implemented in any\ndeep learning library. A composable and declarative API, along with a\ncomprehensive suite of layers and combinators, streamlines the construction of\nproduction-scale models from simple streamable components while preserving\nstrong correctness guarantees. Our current implementations of SequenceLayers\n(JAX, TensorFlow 2) are available at https://github.com/google/sequence-layers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a neural network layer API and library for sequence modeling,\ndesigned for easy creation of sequence models that can be executed both\nlayer-by-layer (e.g., teacher-forced training) and step-by-step (e.g.,\nautoregressive sampling). To achieve this, layers define an explicit\nrepresentation of their state over time (e.g., a Transformer KV cache, a\nconvolution buffer, an RNN hidden state), and a step method that evolves that\nstate, tested to give identical results to a stateless layer-wise invocation.\nThis and other aspects of the SequenceLayers contract enables complex models to\nbe immediately streamable, mitigates a wide range of common bugs arising in\nboth streaming and parallel sequence processing, and can be implemented in any\ndeep learning library. A composable and declarative API, along with a\ncomprehensive suite of layers and combinators, streamlines the construction of\nproduction-scale models from simple streamable components while preserving\nstrong correctness guarantees. Our current implementations of SequenceLayers\n(JAX, TensorFlow 2) are available at https://github.com/google/sequence-layers."
                },
                "authors": [
                    {
                        "name": "RJ Skerry-Ryan"
                    },
                    {
                        "name": "Julian Salazar"
                    },
                    {
                        "name": "Soroosh Mariooryad"
                    },
                    {
                        "name": "David Kao"
                    },
                    {
                        "name": "Daisy Stanton"
                    },
                    {
                        "name": "Eric Battenberg"
                    },
                    {
                        "name": "Matt Shannon"
                    },
                    {
                        "name": "Ron J. Weiss"
                    },
                    {
                        "name": "Robin Scheibler"
                    },
                    {
                        "name": "Jonas Rothfuss"
                    },
                    {
                        "name": "Tom Bagby"
                    }
                ],
                "author_detail": {
                    "name": "Tom Bagby"
                },
                "author": "Tom Bagby",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07966v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07966v3",
                "updated": "2025-07-30T16:55:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    16,
                    55,
                    33,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-10T17:47:40Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    47,
                    40,
                    3,
                    191,
                    0
                ],
                "title": "Scaling RL to Long Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling RL to Long Videos"
                },
                "summary": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 104K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves\nstrong performance on video benchmarks, reaching 65.1% and 71.1% accuracy on\nVideoMME without and with subtitles, respectively, and consistently\noutperforming LongVILA-7B across multiple benchmarks. Moreover, LongVILA-R1-7B\nsupports processing up to 8,192 video frames per video, and configurable FPS\nsettings. Notably, our MR-SP system achieves up to 2.1x speedup on long video\nRL training. In addition, we release our training system for public\navailability that supports RL training on various modalities (video, text, and\naudio), various models (VILA and Qwen series), and even image and video\ngeneration models. On a single A100 node (8 GPUs), it supports RL training on\nhour-long videos (e.g., 3,600 frames).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 104K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves\nstrong performance on video benchmarks, reaching 65.1% and 71.1% accuracy on\nVideoMME without and with subtitles, respectively, and consistently\noutperforming LongVILA-7B across multiple benchmarks. Moreover, LongVILA-R1-7B\nsupports processing up to 8,192 video frames per video, and configurable FPS\nsettings. Notably, our MR-SP system achieves up to 2.1x speedup on long video\nRL training. In addition, we release our training system for public\navailability that supports RL training on various modalities (video, text, and\naudio), various models (VILA and Qwen series), and even image and video\ngeneration models. On a single A100 node (8 GPUs), it supports RL training on\nhour-long videos (e.g., 3,600 frames)."
                },
                "authors": [
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Baifeng Shi"
                    },
                    {
                        "name": "Qinghao Hu"
                    },
                    {
                        "name": "Hanrong Ye"
                    },
                    {
                        "name": "Ligeng Zhu"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Xiaojuan Qi"
                    },
                    {
                        "name": "Sifei Liu"
                    },
                    {
                        "name": "Hongxu Yin"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Code at https://github.com/NVlabs/Long-RL and model at\n  https://huggingface.co/Efficient-Large-Model/LongVILA-R1-7B",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07966v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07966v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22801v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22801v1",
                "updated": "2025-07-30T16:04:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    16,
                    4,
                    1,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T16:04:01Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    16,
                    4,
                    1,
                    2,
                    211,
                    0
                ],
                "title": "DSPE: Profit Maximization in Edge-Cloud Storage System using Dynamic\n  Space Partitioning with Erasure Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DSPE: Profit Maximization in Edge-Cloud Storage System using Dynamic\n  Space Partitioning with Erasure Code"
                },
                "summary": "Edge Storage Systems have emerged as a critical enabler of low latency data\naccess in modern cloud networks by bringing storage and computation closer to\nend users. However, the limited storage capacity of edge servers poses\nsignificant challenges in handling high volume and latency sensitive data\naccess requests, particularly under dynamic workloads. In this work, we propose\na profit driven framework that integrates three key mechanisms which are\ncollaborative caching, erasure coding, and elastic storage partitioning. Unlike\ntraditional replication, erasure coding enables space efficient redundancy,\nallowing data to be reconstructed from any subset of K out of K plus M coded\nblocks. We dynamically partition each edge server s storage into private and\npublic regions. The private region is further subdivided among access points\nbased on their incoming request rates, enabling adaptive control over data\nlocality and ownership. We design a data placement and replacement policy that\ndetermines how and where to store or evict coded data blocks to maximize data\naccess within deadlines. While the private region serves requests from local\nAPs, the public region handles cooperative storage requests from neighboring\nservers. Our proposed Dynamic Space Partitioning and Elastic caching strategy\nis evaluated on both synthetic and real world traces from Netflix and Spotify.\nExperimental results show that our method improves overall system profitability\nby approximately 5 to 8% compared to state of the art approaches under varied\nworkload conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge Storage Systems have emerged as a critical enabler of low latency data\naccess in modern cloud networks by bringing storage and computation closer to\nend users. However, the limited storage capacity of edge servers poses\nsignificant challenges in handling high volume and latency sensitive data\naccess requests, particularly under dynamic workloads. In this work, we propose\na profit driven framework that integrates three key mechanisms which are\ncollaborative caching, erasure coding, and elastic storage partitioning. Unlike\ntraditional replication, erasure coding enables space efficient redundancy,\nallowing data to be reconstructed from any subset of K out of K plus M coded\nblocks. We dynamically partition each edge server s storage into private and\npublic regions. The private region is further subdivided among access points\nbased on their incoming request rates, enabling adaptive control over data\nlocality and ownership. We design a data placement and replacement policy that\ndetermines how and where to store or evict coded data blocks to maximize data\naccess within deadlines. While the private region serves requests from local\nAPs, the public region handles cooperative storage requests from neighboring\nservers. Our proposed Dynamic Space Partitioning and Elastic caching strategy\nis evaluated on both synthetic and real world traces from Netflix and Spotify.\nExperimental results show that our method improves overall system profitability\nby approximately 5 to 8% compared to state of the art approaches under varied\nworkload conditions."
                },
                "authors": [
                    {
                        "name": "Shubhradeep Roy"
                    },
                    {
                        "name": "Suvarthi Sarkar"
                    },
                    {
                        "name": "Vivek Verma"
                    },
                    {
                        "name": "Aryabartta Sahu"
                    }
                ],
                "author_detail": {
                    "name": "Aryabartta Sahu"
                },
                "author": "Aryabartta Sahu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22801v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22801v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22636v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22636v1",
                "updated": "2025-07-30T12:55:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    12,
                    55,
                    55,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T12:55:55Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    12,
                    55,
                    55,
                    2,
                    211,
                    0
                ],
                "title": "All-gluon amplitudes with off-shell recursion in multiplet bases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "All-gluon amplitudes with off-shell recursion in multiplet bases"
                },
                "summary": "The efficient computation of color-summed QCD amplitudes at high parton\nmultiplicities remains a central challenge for precision collider predictions.\nExisting approaches using trace, color-flow, or adjoint bases suffer from\nnon-orthogonality, which complicates the color algebra and scales poorly with\nmultiplicity. In this work, we present an off-shell recursive framework for\ncomputing all-gluon tree-level amplitudes directly in orthogonal multiplet\nbases. Utilizing Wigner $6j$ coefficients, we construct an algorithm that\nbuilds multiplet-projected off-shell currents from lower-point currents. By\noptimizing the recursion through partial summation and caching, we find that\nthe computational complexity of calculating $n$-gluon color-summed squared\namplitudes scales as $\\mathcal{O}(17^n)$. This demonstrates the potential\ncompetitiveness of multiplet bases for high-multiplicity processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficient computation of color-summed QCD amplitudes at high parton\nmultiplicities remains a central challenge for precision collider predictions.\nExisting approaches using trace, color-flow, or adjoint bases suffer from\nnon-orthogonality, which complicates the color algebra and scales poorly with\nmultiplicity. In this work, we present an off-shell recursive framework for\ncomputing all-gluon tree-level amplitudes directly in orthogonal multiplet\nbases. Utilizing Wigner $6j$ coefficients, we construct an algorithm that\nbuilds multiplet-projected off-shell currents from lower-point currents. By\noptimizing the recursion through partial summation and caching, we find that\nthe computational complexity of calculating $n$-gluon color-summed squared\namplitudes scales as $\\mathcal{O}(17^n)$. This demonstrates the potential\ncompetitiveness of multiplet bases for high-multiplicity processes."
                },
                "authors": [
                    {
                        "name": "Oskar Bolinder"
                    },
                    {
                        "name": "Rikkert Frederix"
                    },
                    {
                        "name": "Malin Sjodahl"
                    }
                ],
                "author_detail": {
                    "name": "Malin Sjodahl"
                },
                "author": "Malin Sjodahl",
                "arxiv_comment": "15 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22636v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22636v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20984v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20984v2",
                "updated": "2025-07-30T06:29:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    6,
                    29,
                    40,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-28T16:45:14Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    45,
                    14,
                    0,
                    209,
                    0
                ],
                "title": "SmallThinker: A Family of Efficient Large Language Models Natively\n  Trained for Local Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmallThinker: A Family of Efficient Large Language Models Natively\n  Trained for Local Deployment"
                },
                "summary": "While frontier large language models (LLMs) continue to push capability\nboundaries, their deployment remains confined to GPU-powered cloud\ninfrastructure. We challenge this paradigm with SmallThinker, a family of LLMs\nnatively designed - not adapted - for the unique constraints of local devices:\nweak computational power, limited memory, and slow storage. Unlike traditional\napproaches that mainly compress existing models built for clouds, we architect\nSmallThinker from the ground up to thrive within these limitations. Our\ninnovation lies in a deployment-aware architecture that transforms constraints\ninto design principles. First, We introduce a two-level sparse structure\ncombining fine-grained Mixture-of-Experts (MoE) with sparse feed-forward\nnetworks, drastically reducing computational demands without sacrificing model\ncapacity. Second, to conquer the I/O bottleneck of slow storage, we design a\npre-attention router that enables our co-designed inference engine to prefetch\nexpert parameters from storage while computing attention, effectively hiding\nstorage latency that would otherwise cripple on-device inference. Third, for\nmemory efficiency, we utilize NoPE-RoPE hybrid sparse attention mechanism to\nslash KV cache requirements. We release SmallThinker-4B-A0.6B and\nSmallThinker-21B-A3B, which achieve state-of-the-art performance scores and\neven outperform larger LLMs. Remarkably, our co-designed system mostly\neliminates the need for expensive GPU hardware: with Q4_0 quantization, both\nmodels exceed 20 tokens/s on ordinary consumer CPUs, while consuming only 1GB\nand 8GB of memory respectively. SmallThinker is publicly available at\nhf.co/PowerInfer/SmallThinker-4BA0.6B-Instruct and\nhf.co/PowerInfer/SmallThinker-21BA3B-Instruct.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While frontier large language models (LLMs) continue to push capability\nboundaries, their deployment remains confined to GPU-powered cloud\ninfrastructure. We challenge this paradigm with SmallThinker, a family of LLMs\nnatively designed - not adapted - for the unique constraints of local devices:\nweak computational power, limited memory, and slow storage. Unlike traditional\napproaches that mainly compress existing models built for clouds, we architect\nSmallThinker from the ground up to thrive within these limitations. Our\ninnovation lies in a deployment-aware architecture that transforms constraints\ninto design principles. First, We introduce a two-level sparse structure\ncombining fine-grained Mixture-of-Experts (MoE) with sparse feed-forward\nnetworks, drastically reducing computational demands without sacrificing model\ncapacity. Second, to conquer the I/O bottleneck of slow storage, we design a\npre-attention router that enables our co-designed inference engine to prefetch\nexpert parameters from storage while computing attention, effectively hiding\nstorage latency that would otherwise cripple on-device inference. Third, for\nmemory efficiency, we utilize NoPE-RoPE hybrid sparse attention mechanism to\nslash KV cache requirements. We release SmallThinker-4B-A0.6B and\nSmallThinker-21B-A3B, which achieve state-of-the-art performance scores and\neven outperform larger LLMs. Remarkably, our co-designed system mostly\neliminates the need for expensive GPU hardware: with Q4_0 quantization, both\nmodels exceed 20 tokens/s on ordinary consumer CPUs, while consuming only 1GB\nand 8GB of memory respectively. SmallThinker is publicly available at\nhf.co/PowerInfer/SmallThinker-4BA0.6B-Instruct and\nhf.co/PowerInfer/SmallThinker-21BA3B-Instruct."
                },
                "authors": [
                    {
                        "name": "Yixin Song"
                    },
                    {
                        "name": "Zhenliang Xue"
                    },
                    {
                        "name": "Dongliang Wei"
                    },
                    {
                        "name": "Feiyang Chen"
                    },
                    {
                        "name": "Jianxiang Gao"
                    },
                    {
                        "name": "Junchen Liu"
                    },
                    {
                        "name": "Hangyu Liang"
                    },
                    {
                        "name": "Guangshuo Qin"
                    },
                    {
                        "name": "Chengrong Tian"
                    },
                    {
                        "name": "Bo Wen"
                    },
                    {
                        "name": "Longyu Zhao"
                    },
                    {
                        "name": "Xinrui Zheng"
                    },
                    {
                        "name": "Zeyu Mi"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20984v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20984v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19442v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19442v3",
                "updated": "2025-07-30T05:24:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    5,
                    24,
                    46,
                    2,
                    211,
                    0
                ],
                "published": "2024-12-27T04:17:57Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    4,
                    17,
                    57,
                    4,
                    362,
                    0
                ],
                "title": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management"
                },
                "summary": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}."
                },
                "authors": [
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Anxin Tian"
                    },
                    {
                        "name": "Tianhao Tang"
                    },
                    {
                        "name": "Zhanchao Xu"
                    },
                    {
                        "name": "Xuejia Chen"
                    },
                    {
                        "name": "Nicole Hu"
                    },
                    {
                        "name": "Wei Dong"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "arxiv_comment": "Accepted to TMLR 2025. The revised version incorporates more papers\n  and has been further polished",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19442v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19442v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00904v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00904v1",
                "updated": "2025-07-29T03:08:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    29,
                    3,
                    8,
                    31,
                    1,
                    210,
                    0
                ],
                "published": "2025-07-29T03:08:31Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    3,
                    8,
                    31,
                    1,
                    210,
                    0
                ],
                "title": "Forecasting LLM Inference Performance via Hardware-Agnostic Analytical\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting LLM Inference Performance via Hardware-Agnostic Analytical\n  Modeling"
                },
                "summary": "Large language models (LLMs) have been increasingly deployed as local agents\non personal devices with CPUs, NPUs and integrated GPUs. However, forecasting\ninference performance on devices with such heterogeneity remains challenging\ndue to the dynamic compute and memory demands. Existing approaches rely on GPU\nbenchmarking or machine learning-based latency predictors, which are often\nhardware-specific and lack generalizability. To this end, we introduce LIFE, a\nlightweight and modular analytical framework that is comprised of modular\nanalytical model of operators, configurable to characterize LLM inference\nworkloads in a hardware and dataset-agnostic manner. LIFE characterizes the\ninfluence of software and model optimizations, such as quantization, KV cache\ncompression, LoRA adapters, chunked prefill, different attentions, and operator\nfusion, on performance metrics such as time-to-first-token (TTFT),\ntime-per-output-token (TPOT) and tokens-per-second (TPS). LIFE enables\nperformance forecasting using only hardware specifications, such as TOPS and\nmemory bandwidth, without requiring extensive dataset benchmarking. We validate\nLIFE's forecasting with inference on AMD Ryzen CPUs, NPUs, iGPUs and NVIDIA\nV100 GPUs, with Llama2-7B variants, demonstrating the utility of LIFE in\nforecasting LLM performance through lens of system efficiency to enable\nefficient LLM deployment across different hardware platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been increasingly deployed as local agents\non personal devices with CPUs, NPUs and integrated GPUs. However, forecasting\ninference performance on devices with such heterogeneity remains challenging\ndue to the dynamic compute and memory demands. Existing approaches rely on GPU\nbenchmarking or machine learning-based latency predictors, which are often\nhardware-specific and lack generalizability. To this end, we introduce LIFE, a\nlightweight and modular analytical framework that is comprised of modular\nanalytical model of operators, configurable to characterize LLM inference\nworkloads in a hardware and dataset-agnostic manner. LIFE characterizes the\ninfluence of software and model optimizations, such as quantization, KV cache\ncompression, LoRA adapters, chunked prefill, different attentions, and operator\nfusion, on performance metrics such as time-to-first-token (TTFT),\ntime-per-output-token (TPOT) and tokens-per-second (TPS). LIFE enables\nperformance forecasting using only hardware specifications, such as TOPS and\nmemory bandwidth, without requiring extensive dataset benchmarking. We validate\nLIFE's forecasting with inference on AMD Ryzen CPUs, NPUs, iGPUs and NVIDIA\nV100 GPUs, with Llama2-7B variants, demonstrating the utility of LIFE in\nforecasting LLM performance through lens of system efficiency to enable\nefficient LLM deployment across different hardware platforms."
                },
                "authors": [
                    {
                        "name": "Rajeev Patwari"
                    },
                    {
                        "name": "Ashish Sirasao"
                    },
                    {
                        "name": "Devleena Das"
                    }
                ],
                "author_detail": {
                    "name": "Devleena Das"
                },
                "author": "Devleena Das",
                "arxiv_comment": "10 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00904v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00904v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24358v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24358v2",
                "updated": "2025-07-28T20:44:23Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    20,
                    44,
                    23,
                    0,
                    209,
                    0
                ],
                "published": "2025-03-31T17:37:32Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    37,
                    32,
                    0,
                    90,
                    0
                ],
                "title": "SQuat: Subspace-orthogonal KV Cache Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SQuat: Subspace-orthogonal KV Cache Quantization"
                },
                "summary": "The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from\npreviously generated tokens. It reduces redundant computation at the cost of\nincreased memory usage. To mitigate this overhead, existing approaches compress\nKV tensors into lower-bit representations; however, quantization errors can\naccumulate as more tokens are generated, potentially resulting in undesired\noutputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache\nquantization). It first constructs a subspace spanned by query tensors to\ncapture the most critical task-related information. During key tensor\nquantization, it enforces that the difference between the (de)quantized and\noriginal keys remains orthogonal to this subspace, minimizing the impact of\nquantization errors on the attention mechanism's outputs. SQuat requires no\nmodel fine-tuning, no additional calibration dataset for offline learning, and\nis grounded in a theoretical framework we develop. Through numerical\nexperiments, we show that our method reduces peak memory by 2.17 to 2.82,\nimproves throughput by 2.45 to 3.60, and achieves more favorable benchmark\nscores than existing KV cache quantization algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from\npreviously generated tokens. It reduces redundant computation at the cost of\nincreased memory usage. To mitigate this overhead, existing approaches compress\nKV tensors into lower-bit representations; however, quantization errors can\naccumulate as more tokens are generated, potentially resulting in undesired\noutputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache\nquantization). It first constructs a subspace spanned by query tensors to\ncapture the most critical task-related information. During key tensor\nquantization, it enforces that the difference between the (de)quantized and\noriginal keys remains orthogonal to this subspace, minimizing the impact of\nquantization errors on the attention mechanism's outputs. SQuat requires no\nmodel fine-tuning, no additional calibration dataset for offline learning, and\nis grounded in a theoretical framework we develop. Through numerical\nexperiments, we show that our method reduces peak memory by 2.17 to 2.82,\nimproves throughput by 2.45 to 3.60, and achieves more favorable benchmark\nscores than existing KV cache quantization algorithms."
                },
                "authors": [
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Ligong Han"
                    },
                    {
                        "name": "Kai Xu"
                    },
                    {
                        "name": "Akash Srivastava"
                    }
                ],
                "author_detail": {
                    "name": "Akash Srivastava"
                },
                "author": "Akash Srivastava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24358v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24358v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.13349v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.13349v3",
                "updated": "2025-07-28T14:11:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    14,
                    11,
                    53,
                    0,
                    209,
                    0
                ],
                "published": "2023-11-22T12:34:51Z",
                "published_parsed": [
                    2023,
                    11,
                    22,
                    12,
                    34,
                    51,
                    2,
                    326,
                    0
                ],
                "title": "REDS: Resource-Efficient Deep Subnetworks for Dynamic Resource\n  Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REDS: Resource-Efficient Deep Subnetworks for Dynamic Resource\n  Constraints"
                },
                "summary": "Deep learning models deployed on edge devices frequently encounter resource\nvariability, which arises from fluctuating energy levels, timing constraints,\nor prioritization of other critical tasks within the system. State-of-the-art\nmachine learning pipelines generate resource-agnostic models that are not\ncapable to adapt at runtime. In this work, we introduce Resource-Efficient Deep\nSubnetworks (REDS) to tackle model adaptation to variable resources. In\ncontrast to the state-of-the-art, REDS leverages structured sparsity\nconstructively by exploiting permutation invariance of neurons, which allows\nfor hardware-specific optimizations. Specifically, REDS achieves computational\nefficiency by (1) skipping sequential computational blocks identified by a\nnovel iterative knapsack optimizer, and (2) taking advantage of data cache by\nre-arranging the order of operations in REDS computational graph. REDS supports\nconventional deep networks frequently deployed on the edge and provides\ncomputational benefits even for small and simple networks. We evaluate REDS on\neight benchmark architectures trained on the Visual Wake Words, Google Speech\nCommands, Fashion-MNIST, CIFAR-10 and ImageNet-1K datasets, and test on four\noff-the-shelf mobile and embedded hardware platforms. We provide a theoretical\nresult and empirical evidence demonstrating REDS' outstanding performance in\nterms of submodels' test set accuracy, and demonstrate an adaptation time in\nresponse to dynamic resource constraints of under 40$\\mu$s, utilizing a\nfully-connected network on Arduino Nano 33 BLE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning models deployed on edge devices frequently encounter resource\nvariability, which arises from fluctuating energy levels, timing constraints,\nor prioritization of other critical tasks within the system. State-of-the-art\nmachine learning pipelines generate resource-agnostic models that are not\ncapable to adapt at runtime. In this work, we introduce Resource-Efficient Deep\nSubnetworks (REDS) to tackle model adaptation to variable resources. In\ncontrast to the state-of-the-art, REDS leverages structured sparsity\nconstructively by exploiting permutation invariance of neurons, which allows\nfor hardware-specific optimizations. Specifically, REDS achieves computational\nefficiency by (1) skipping sequential computational blocks identified by a\nnovel iterative knapsack optimizer, and (2) taking advantage of data cache by\nre-arranging the order of operations in REDS computational graph. REDS supports\nconventional deep networks frequently deployed on the edge and provides\ncomputational benefits even for small and simple networks. We evaluate REDS on\neight benchmark architectures trained on the Visual Wake Words, Google Speech\nCommands, Fashion-MNIST, CIFAR-10 and ImageNet-1K datasets, and test on four\noff-the-shelf mobile and embedded hardware platforms. We provide a theoretical\nresult and empirical evidence demonstrating REDS' outstanding performance in\nterms of submodels' test set accuracy, and demonstrate an adaptation time in\nresponse to dynamic resource constraints of under 40$\\mu$s, utilizing a\nfully-connected network on Arduino Nano 33 BLE."
                },
                "authors": [
                    {
                        "name": "Francesco Corti"
                    },
                    {
                        "name": "Balz Maag"
                    },
                    {
                        "name": "Joachim Schauer"
                    },
                    {
                        "name": "Ulrich Pferschy"
                    },
                    {
                        "name": "Olga Saukh"
                    }
                ],
                "author_detail": {
                    "name": "Olga Saukh"
                },
                "author": "Olga Saukh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.13349v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.13349v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20677v1",
                "updated": "2025-07-28T09:59:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    9,
                    59,
                    22,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T09:59:22Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    9,
                    59,
                    22,
                    0,
                    209,
                    0
                ],
                "title": "Quantum Circuit Caches and Compressors for Low Latency, High Throughput\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Circuit Caches and Compressors for Low Latency, High Throughput\n  Computing"
                },
                "summary": "Utility-scale quantum programs contain operations on the order of $>10^{15}$\nwhich must be prepared and piped from a classical co-processor to the control\nunit of the quantum device. The latency of this process significantly increases\nwith the size of the program: existing high-level classical representations of\nquantum programs are typically memory intensive and do not na\\\"ively\nefficiently scale to the degree required to execute utility-scale programs in\nreal-time. To combat this limitation, we propose the utilization of high-level\nquantum circuit caches and compressors. The first save on the time associated\nwith repetitive tasks and sub-circuits, and the latter are useful for\nrepresenting the programs/circuits in memory-efficient formats. We present\nnumerical evidence that caches and compressors can offer five orders of\nmagnitude lower latencies during the automatic transpilation of extremely large\nquantum circuits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Utility-scale quantum programs contain operations on the order of $>10^{15}$\nwhich must be prepared and piped from a classical co-processor to the control\nunit of the quantum device. The latency of this process significantly increases\nwith the size of the program: existing high-level classical representations of\nquantum programs are typically memory intensive and do not na\\\"ively\nefficiently scale to the degree required to execute utility-scale programs in\nreal-time. To combat this limitation, we propose the utilization of high-level\nquantum circuit caches and compressors. The first save on the time associated\nwith repetitive tasks and sub-circuits, and the latter are useful for\nrepresenting the programs/circuits in memory-efficient formats. We present\nnumerical evidence that caches and compressors can offer five orders of\nmagnitude lower latencies during the automatic transpilation of extremely large\nquantum circuits."
                },
                "authors": [
                    {
                        "name": "Ioana Moflic"
                    },
                    {
                        "name": "Alan Robertson"
                    },
                    {
                        "name": "Simon J. Devitt"
                    },
                    {
                        "name": "Alexandru Paler"
                    }
                ],
                "author_detail": {
                    "name": "Alexandru Paler"
                },
                "author": "Alexandru Paler",
                "arxiv_comment": "accepted at Q-CORE workshop of the QCE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20613v1",
                "updated": "2025-07-28T08:27:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    8,
                    27,
                    40,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T08:27:40Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    8,
                    27,
                    40,
                    0,
                    209,
                    0
                ],
                "title": "Enhancing Large Multimodal Models with Adaptive Sparsity and KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Large Multimodal Models with Adaptive Sparsity and KV Cache\n  Compression"
                },
                "summary": "Large multimodal models (LMMs) have advanced significantly by integrating\nvisual encoders with extensive language models, enabling robust reasoning\ncapabilities. However, compressing LMMs for deployment on edge devices remains\na critical challenge. In this work, we propose an adaptive search algorithm\nthat optimizes sparsity and KV cache compression to enhance LMM efficiency.\nUtilizing the Tree-structured Parzen Estimator, our method dynamically adjusts\npruning ratios and KV cache quantization bandwidth across different LMM layers,\nusing model performance as the optimization objective. This approach uniquely\ncombines pruning with key-value cache quantization and incorporates a fast\npruning technique that eliminates the need for additional fine-tuning or weight\nadjustments, achieving efficient compression without compromising accuracy.\nComprehensive evaluations on benchmark datasets, including LLaVA-1.5 7B and\n13B, demonstrate our method superiority over state-of-the-art techniques such\nas SparseGPT and Wanda across various compression levels. Notably, our\nframework automatic allocation of KV cache compression resources sets a new\nstandard in LMM optimization, delivering memory efficiency without sacrificing\nmuch performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large multimodal models (LMMs) have advanced significantly by integrating\nvisual encoders with extensive language models, enabling robust reasoning\ncapabilities. However, compressing LMMs for deployment on edge devices remains\na critical challenge. In this work, we propose an adaptive search algorithm\nthat optimizes sparsity and KV cache compression to enhance LMM efficiency.\nUtilizing the Tree-structured Parzen Estimator, our method dynamically adjusts\npruning ratios and KV cache quantization bandwidth across different LMM layers,\nusing model performance as the optimization objective. This approach uniquely\ncombines pruning with key-value cache quantization and incorporates a fast\npruning technique that eliminates the need for additional fine-tuning or weight\nadjustments, achieving efficient compression without compromising accuracy.\nComprehensive evaluations on benchmark datasets, including LLaVA-1.5 7B and\n13B, demonstrate our method superiority over state-of-the-art techniques such\nas SparseGPT and Wanda across various compression levels. Notably, our\nframework automatic allocation of KV cache compression resources sets a new\nstandard in LMM optimization, delivering memory efficiency without sacrificing\nmuch performance."
                },
                "authors": [
                    {
                        "name": "Te Zhang"
                    },
                    {
                        "name": "Yuheng Li"
                    },
                    {
                        "name": "Junxiang Wang"
                    },
                    {
                        "name": "Lujun Li"
                    }
                ],
                "author_detail": {
                    "name": "Lujun Li"
                },
                "author": "Lujun Li",
                "arxiv_comment": "6 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01760v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01760v2",
                "updated": "2025-07-28T04:25:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    4,
                    25,
                    58,
                    0,
                    209,
                    0
                ],
                "published": "2024-10-02T17:14:47Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    17,
                    14,
                    47,
                    2,
                    276,
                    0
                ],
                "title": "Learning-Augmented Online Caching: New Upper Bounds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning-Augmented Online Caching: New Upper Bounds"
                },
                "summary": "We address the problem of learning-augmented online caching in the scenario\nwhen each request is accompanied by a prediction of the next occurrence of the\nrequested page. We improve currently known bounds on the competitive ratio of\nthe BlindOracle algorithm, which evicts a page predicted to be requested last.\nWe also prove a lower bound on the competitive ratio of any randomized\nalgorithm and show that a combination of the BlindOracle with the Marker\nalgorithm achieves a competitive ratio that is optimal up to some constant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the problem of learning-augmented online caching in the scenario\nwhen each request is accompanied by a prediction of the next occurrence of the\nrequested page. We improve currently known bounds on the competitive ratio of\nthe BlindOracle algorithm, which evicts a page predicted to be requested last.\nWe also prove a lower bound on the competitive ratio of any randomized\nalgorithm and show that a combination of the BlindOracle with the Marker\nalgorithm achieves a competitive ratio that is optimal up to some constant."
                },
                "authors": [
                    {
                        "name": "Daniel Skachkov"
                    },
                    {
                        "name": "Denis Ponomaryov"
                    },
                    {
                        "name": "Yuri Dorn"
                    },
                    {
                        "name": "Alexander Demin"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Demin"
                },
                "author": "Alexander Demin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01760v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01760v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08161v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08161v2",
                "updated": "2025-07-27T09:58:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    27,
                    9,
                    58,
                    25,
                    6,
                    208,
                    0
                ],
                "published": "2025-06-09T19:13:16Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    19,
                    13,
                    16,
                    0,
                    160,
                    0
                ],
                "title": "GATE: Geometry-Aware Trained Encoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GATE: Geometry-Aware Trained Encoding"
                },
                "summary": "The encoding of input parameters is one of the fundamental building blocks of\nneural network algorithms. Its goal is to map the input data to a\nhigher-dimensional space, typically supported by trained feature vectors. The\nmapping is crucial for the efficiency and approximation quality of neural\nnetworks. We propose a novel geometry-aware encoding called GATE that stores\nfeature vectors on the surface of triangular meshes. Our encoding is suitable\nfor neural rendering-related algorithms, for example, neural radiance caching.\nIt also avoids limitations of previous hash-based encoding schemes, such as\nhash collisions, selection of resolution versus scene size, and divergent\nmemory access. Our approach decouples feature vector density from geometry\ndensity using mesh colors, while allowing for finer control over neural network\ntraining and adaptive level-of-detail.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The encoding of input parameters is one of the fundamental building blocks of\nneural network algorithms. Its goal is to map the input data to a\nhigher-dimensional space, typically supported by trained feature vectors. The\nmapping is crucial for the efficiency and approximation quality of neural\nnetworks. We propose a novel geometry-aware encoding called GATE that stores\nfeature vectors on the surface of triangular meshes. Our encoding is suitable\nfor neural rendering-related algorithms, for example, neural radiance caching.\nIt also avoids limitations of previous hash-based encoding schemes, such as\nhash collisions, selection of resolution versus scene size, and divergent\nmemory access. Our approach decouples feature vector density from geometry\ndensity using mesh colors, while allowing for finer control over neural network\ntraining and adaptive level-of-detail."
                },
                "authors": [
                    {
                        "name": "Jakub Bokansk"
                    },
                    {
                        "name": "Daniel Meister"
                    },
                    {
                        "name": "Carsten Benthin"
                    }
                ],
                "author_detail": {
                    "name": "Carsten Benthin"
                },
                "author": "Carsten Benthin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08161v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08161v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20173v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20173v1",
                "updated": "2025-07-27T08:25:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    27,
                    8,
                    25,
                    8,
                    6,
                    208,
                    0
                ],
                "published": "2025-07-27T08:25:08Z",
                "published_parsed": [
                    2025,
                    7,
                    27,
                    8,
                    25,
                    8,
                    6,
                    208,
                    0
                ],
                "title": "High-Performance Parallel Optimization of the Fish School Behaviour on\n  the Setonix Platform Using OpenMP",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Performance Parallel Optimization of the Fish School Behaviour on\n  the Setonix Platform Using OpenMP"
                },
                "summary": "This paper presents an in-depth investigation into the high-performance\nparallel optimization of the Fish School Behaviour (FSB) algorithm on the\nSetonix supercomputing platform using the OpenMP framework. Given the\nincreasing demand for enhanced computational capabilities for complex,\nlarge-scale calculations across diverse domains, there's an imperative need for\noptimized parallel algorithms and computing structures. The FSB algorithm,\ninspired by nature's social behavior patterns, provides an ideal platform for\nparallelization due to its iterative and computationally intensive nature. This\nstudy leverages the capabilities of the Setonix platform and the OpenMP\nframework to analyze various aspects of multi-threading, such as thread counts,\nscheduling strategies, and OpenMP constructs, aiming to discern patterns and\nstrategies that can elevate program performance. Experiments were designed to\nrigorously test different configurations, and our results not only offer\ninsights for parallel optimization of FSB on Setonix but also provide valuable\nreferences for other parallel computational research using OpenMP. Looking\nforward, other factors, such as cache behavior and thread scheduling strategies\nat micro and macro levels, hold potential for further exploration and\noptimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an in-depth investigation into the high-performance\nparallel optimization of the Fish School Behaviour (FSB) algorithm on the\nSetonix supercomputing platform using the OpenMP framework. Given the\nincreasing demand for enhanced computational capabilities for complex,\nlarge-scale calculations across diverse domains, there's an imperative need for\noptimized parallel algorithms and computing structures. The FSB algorithm,\ninspired by nature's social behavior patterns, provides an ideal platform for\nparallelization due to its iterative and computationally intensive nature. This\nstudy leverages the capabilities of the Setonix platform and the OpenMP\nframework to analyze various aspects of multi-threading, such as thread counts,\nscheduling strategies, and OpenMP constructs, aiming to discern patterns and\nstrategies that can elevate program performance. Experiments were designed to\nrigorously test different configurations, and our results not only offer\ninsights for parallel optimization of FSB on Setonix but also provide valuable\nreferences for other parallel computational research using OpenMP. Looking\nforward, other factors, such as cache behavior and thread scheduling strategies\nat micro and macro levels, hold potential for further exploration and\noptimization."
                },
                "authors": [
                    {
                        "name": "Haitian Wang"
                    },
                    {
                        "name": "Long Qin"
                    }
                ],
                "author_detail": {
                    "name": "Long Qin"
                },
                "author": "Long Qin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20173v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20173v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20116v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20116v1",
                "updated": "2025-07-27T03:45:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    27,
                    3,
                    45,
                    7,
                    6,
                    208,
                    0
                ],
                "published": "2025-07-27T03:45:07Z",
                "published_parsed": [
                    2025,
                    7,
                    27,
                    3,
                    45,
                    7,
                    6,
                    208,
                    0
                ],
                "title": "Accelerating Containerized Service Delivery at the Network Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Containerized Service Delivery at the Network Edge"
                },
                "summary": "Efficient container image distribution is crucial for enabling machine\nlearning inference at the network edge, where resource limitations and dynamic\nnetwork conditions create significant challenges. In this paper, we present\nPeerSync, a decentralized P2P-based system designed to optimize image\ndistribution in edge environments. PeerSync employs a popularity- and\nnetwork-aware download engine that dynamically adapts to content popularity and\nreal-time network conditions using a sliding window mechanism. PeerSync further\nintegrates automated tracker election for rapid peer discovery and dynamic\ncache management for efficient storage utilization. We implement PeerSync with\n8000+ lines of Rust code and test its performance extensively on both physical\nedge devices and Docker-based emulations. Experimental results show that\nPeerSync delivers a remarkable speed increase of 2.72$\\times$, 1.79$\\times$,\nand 1.28$\\times$ compared to the Baseline, Dragonfly, and Kraken, respectively,\nwhile significantly reducing peak cross-network traffic by 90.72\\% under\ncongested and varying network conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient container image distribution is crucial for enabling machine\nlearning inference at the network edge, where resource limitations and dynamic\nnetwork conditions create significant challenges. In this paper, we present\nPeerSync, a decentralized P2P-based system designed to optimize image\ndistribution in edge environments. PeerSync employs a popularity- and\nnetwork-aware download engine that dynamically adapts to content popularity and\nreal-time network conditions using a sliding window mechanism. PeerSync further\nintegrates automated tracker election for rapid peer discovery and dynamic\ncache management for efficient storage utilization. We implement PeerSync with\n8000+ lines of Rust code and test its performance extensively on both physical\nedge devices and Docker-based emulations. Experimental results show that\nPeerSync delivers a remarkable speed increase of 2.72$\\times$, 1.79$\\times$,\nand 1.28$\\times$ compared to the Baseline, Dragonfly, and Kraken, respectively,\nwhile significantly reducing peak cross-network traffic by 90.72\\% under\ncongested and varying network conditions."
                },
                "authors": [
                    {
                        "name": "Yinuo Deng"
                    },
                    {
                        "name": "Hailiang Zhao"
                    },
                    {
                        "name": "Dongjing Wang"
                    },
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Wenzhuo Qian"
                    },
                    {
                        "name": "Jianwei Yin"
                    },
                    {
                        "name": "Schahram Dustdar"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20116v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20116v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18300v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18300v3",
                "updated": "2025-07-27T00:40:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    27,
                    0,
                    40,
                    47,
                    6,
                    208,
                    0
                ],
                "published": "2025-05-23T18:46:10Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    18,
                    46,
                    10,
                    4,
                    143,
                    0
                ],
                "title": "Beyond Self-Repellent Kernels: History-Driven Target Towards Efficient\n  Nonlinear MCMC on General Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Self-Repellent Kernels: History-Driven Target Towards Efficient\n  Nonlinear MCMC on General Graphs"
                },
                "summary": "We propose a history-driven target (HDT) framework in Markov Chain Monte\nCarlo (MCMC) to improve any random walk algorithm on discrete state spaces,\nsuch as general undirected graphs, for efficient sampling from target\ndistribution $\\boldsymbol{\\mu}$. With broad applications in network science and\ndistributed optimization, recent innovations like the self-repellent random\nwalk (SRRW) achieve near-zero variance by prioritizing under-sampled states\nthrough transition kernel modifications based on past visit frequencies.\nHowever, SRRW's reliance on explicit computation of transition probabilities\nfor all neighbors at each step introduces substantial computational overhead,\nwhile its strict dependence on time-reversible Markov chains excludes advanced\nnon-reversible MCMC methods. To overcome these limitations, instead of direct\nmodification of transition kernel, HDT introduces a history-dependent target\ndistribution $\\boldsymbol{\\pi}[\\mathbf{x}]$ to replace the original target\n$\\boldsymbol{\\mu}$ in any graph sampler, where $\\mathbf{x}$ represents the\nempirical measure of past visits. This design preserves lightweight\nimplementation by requiring only local information between the current and\nproposed states and achieves compatibility with both reversible and\nnon-reversible MCMC samplers, while retaining unbiased samples with target\ndistribution $\\boldsymbol{\\mu}$ and near-zero variance performance. Extensive\nexperiments in graph sampling demonstrate consistent performance gains, and a\nmemory-efficient Least Recently Used (LRU) cache ensures scalability to large\ngeneral graphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a history-driven target (HDT) framework in Markov Chain Monte\nCarlo (MCMC) to improve any random walk algorithm on discrete state spaces,\nsuch as general undirected graphs, for efficient sampling from target\ndistribution $\\boldsymbol{\\mu}$. With broad applications in network science and\ndistributed optimization, recent innovations like the self-repellent random\nwalk (SRRW) achieve near-zero variance by prioritizing under-sampled states\nthrough transition kernel modifications based on past visit frequencies.\nHowever, SRRW's reliance on explicit computation of transition probabilities\nfor all neighbors at each step introduces substantial computational overhead,\nwhile its strict dependence on time-reversible Markov chains excludes advanced\nnon-reversible MCMC methods. To overcome these limitations, instead of direct\nmodification of transition kernel, HDT introduces a history-dependent target\ndistribution $\\boldsymbol{\\pi}[\\mathbf{x}]$ to replace the original target\n$\\boldsymbol{\\mu}$ in any graph sampler, where $\\mathbf{x}$ represents the\nempirical measure of past visits. This design preserves lightweight\nimplementation by requiring only local information between the current and\nproposed states and achieves compatibility with both reversible and\nnon-reversible MCMC samplers, while retaining unbiased samples with target\ndistribution $\\boldsymbol{\\mu}$ and near-zero variance performance. Extensive\nexperiments in graph sampling demonstrate consistent performance gains, and a\nmemory-efficient Least Recently Used (LRU) cache ensures scalability to large\ngeneral graphs."
                },
                "authors": [
                    {
                        "name": "Jie Hu"
                    },
                    {
                        "name": "Yi-Ting Ma"
                    },
                    {
                        "name": "Do Young Eun"
                    }
                ],
                "author_detail": {
                    "name": "Do Young Eun"
                },
                "author": "Do Young Eun",
                "arxiv_comment": "Accepted at ICML 2025 (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18300v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18300v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20030v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20030v1",
                "updated": "2025-07-26T18:20:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    26,
                    18,
                    20,
                    25,
                    5,
                    207,
                    0
                ],
                "published": "2025-07-26T18:20:25Z",
                "published_parsed": [
                    2025,
                    7,
                    26,
                    18,
                    20,
                    25,
                    5,
                    207,
                    0
                ],
                "title": "FAEDKV: Infinite-Window Fourier Transform for Unbiased KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAEDKV: Infinite-Window Fourier Transform for Unbiased KV Cache\n  Compression"
                },
                "summary": "The efficacy of Large Language Models (LLMs) in long-context tasks is often\nhampered by the substantial memory footprint and computational demands of the\nKey-Value (KV) cache. Current compression strategies, including token eviction\nand learned projections, frequently lead to biased representations -- either by\noveremphasizing recent/high-attention tokens or by repeatedly degrading\ninformation from earlier context -- and may require costly model retraining. We\npresent FAEDKV (Frequency-Adaptive Infinite-Window for KV cache), a novel,\ntraining-free KV cache compression framework that ensures unbiased information\nretention. FAEDKV operates by transforming the KV cache into the frequency\ndomain using a proposed Infinite-Window Fourier Transform (IWDFT). This\napproach allows for the equalized contribution of all tokens to the compressed\nrepresentation, effectively preserving both early and recent contextual\ninformation. A preliminary frequency ablation study identifies critical\nspectral components for layer-wise, targeted compression. Experiments on\nLongBench benchmark demonstrate FAEDKV's superiority over existing methods by\nup to 22\\%. In addition, our method shows superior, position-agnostic retrieval\naccuracy on the Needle-In-A-Haystack task compared to compression based\napproaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficacy of Large Language Models (LLMs) in long-context tasks is often\nhampered by the substantial memory footprint and computational demands of the\nKey-Value (KV) cache. Current compression strategies, including token eviction\nand learned projections, frequently lead to biased representations -- either by\noveremphasizing recent/high-attention tokens or by repeatedly degrading\ninformation from earlier context -- and may require costly model retraining. We\npresent FAEDKV (Frequency-Adaptive Infinite-Window for KV cache), a novel,\ntraining-free KV cache compression framework that ensures unbiased information\nretention. FAEDKV operates by transforming the KV cache into the frequency\ndomain using a proposed Infinite-Window Fourier Transform (IWDFT). This\napproach allows for the equalized contribution of all tokens to the compressed\nrepresentation, effectively preserving both early and recent contextual\ninformation. A preliminary frequency ablation study identifies critical\nspectral components for layer-wise, targeted compression. Experiments on\nLongBench benchmark demonstrate FAEDKV's superiority over existing methods by\nup to 22\\%. In addition, our method shows superior, position-agnostic retrieval\naccuracy on the Needle-In-A-Haystack task compared to compression based\napproaches."
                },
                "authors": [
                    {
                        "name": "Runchao Li"
                    },
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Mu Sheng"
                    },
                    {
                        "name": "Xianxuan Long"
                    },
                    {
                        "name": "Haotian Yu"
                    },
                    {
                        "name": "Pan Li"
                    }
                ],
                "author_detail": {
                    "name": "Pan Li"
                },
                "author": "Pan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20030v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20030v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03140v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03140v2",
                "updated": "2025-07-26T15:25:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    26,
                    15,
                    25,
                    22,
                    5,
                    207,
                    0
                ],
                "published": "2025-04-04T03:30:15Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    3,
                    30,
                    15,
                    4,
                    94,
                    0
                ],
                "title": "Model Reveals What to Cache: Profiling-Based Feature Reuse for Video\n  Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Reveals What to Cache: Profiling-Based Feature Reuse for Video\n  Diffusion Models"
                },
                "summary": "Recent advances in diffusion models have demonstrated remarkable capabilities\nin video generation. However, the computational intensity remains a significant\nchallenge for practical applications. While feature caching has been proposed\nto reduce the computational burden of diffusion models, existing methods\ntypically overlook the heterogeneous significance of individual blocks,\nresulting in suboptimal reuse and degraded output quality. To this end, we\naddress this gap by introducing ProfilingDiT, a novel adaptive caching strategy\nthat explicitly disentangles foreground and background-focused blocks. Through\na systematic analysis of attention distributions in diffusion models, we reveal\na key observation: 1) Most layers exhibit a consistent preference for either\nforeground or background regions. 2) Predicted noise shows low inter-step\nsimilarity initially, which stabilizes as denoising progresses. This finding\ninspires us to formulate a selective caching strategy that preserves full\ncomputation for dynamic foreground elements while efficiently caching static\nbackground features. Our approach substantially reduces computational overhead\nwhile preserving visual fidelity. Extensive experiments demonstrate that our\nframework achieves significant acceleration (e.g., 2.01 times speedup for\nWan2.1) while maintaining visual fidelity across comprehensive quality metrics,\nestablishing a viable method for efficient video generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in diffusion models have demonstrated remarkable capabilities\nin video generation. However, the computational intensity remains a significant\nchallenge for practical applications. While feature caching has been proposed\nto reduce the computational burden of diffusion models, existing methods\ntypically overlook the heterogeneous significance of individual blocks,\nresulting in suboptimal reuse and degraded output quality. To this end, we\naddress this gap by introducing ProfilingDiT, a novel adaptive caching strategy\nthat explicitly disentangles foreground and background-focused blocks. Through\na systematic analysis of attention distributions in diffusion models, we reveal\na key observation: 1) Most layers exhibit a consistent preference for either\nforeground or background regions. 2) Predicted noise shows low inter-step\nsimilarity initially, which stabilizes as denoising progresses. This finding\ninspires us to formulate a selective caching strategy that preserves full\ncomputation for dynamic foreground elements while efficiently caching static\nbackground features. Our approach substantially reduces computational overhead\nwhile preserving visual fidelity. Extensive experiments demonstrate that our\nframework achieves significant acceleration (e.g., 2.01 times speedup for\nWan2.1) while maintaining visual fidelity across comprehensive quality metrics,\nestablishing a viable method for efficient video generation."
                },
                "authors": [
                    {
                        "name": "Xuran Ma"
                    },
                    {
                        "name": "Yexin Liu"
                    },
                    {
                        "name": "Yaofu Liu"
                    },
                    {
                        "name": "Xianfeng Wu"
                    },
                    {
                        "name": "Mingzhe Zheng"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Ser-Nam Lim"
                    },
                    {
                        "name": "Harry Yang"
                    }
                ],
                "author_detail": {
                    "name": "Harry Yang"
                },
                "author": "Harry Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03140v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03140v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09720v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09720v3",
                "updated": "2025-07-26T15:13:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    26,
                    15,
                    13,
                    56,
                    5,
                    207,
                    0
                ],
                "published": "2025-02-13T19:11:40Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    19,
                    11,
                    40,
                    3,
                    44,
                    0
                ],
                "title": "NestQuant: Nested Lattice Quantization for Matrix Products and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NestQuant: Nested Lattice Quantization for Matrix Products and LLMs"
                },
                "summary": "Post-training quantization (PTQ) has emerged as a critical technique for\nefficient deployment of large language models (LLMs). This work proposes\nNestQuant, a novel PTQ scheme for weights and activations that is based on\nself-similar nested lattices. Recent works have mathematically shown such\nquantizers to be information-theoretically optimal for low-precision matrix\nmultiplication. We implement a practical low-complexity version of NestQuant\nbased on Gosset lattice, making it a drop-in quantizer for any matrix\nmultiplication step (e.g., in self-attention, MLP etc). For example, NestQuant\nquantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving\nperplexity of 6.6 on wikitext2. This represents more than 55% reduction in\nperplexity gap with respect to unquantized model (perplexity of 6.14) compared\nto state-of-the-art Metas SpinQuant (perplexity 7.3), OstQuant (7.3) and QuaRot\n(8.2). Comparisons on bigger models (up to 70B) and on various LLM evaluation\nbenchmarks confirm uniform superiority of NestQuant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) has emerged as a critical technique for\nefficient deployment of large language models (LLMs). This work proposes\nNestQuant, a novel PTQ scheme for weights and activations that is based on\nself-similar nested lattices. Recent works have mathematically shown such\nquantizers to be information-theoretically optimal for low-precision matrix\nmultiplication. We implement a practical low-complexity version of NestQuant\nbased on Gosset lattice, making it a drop-in quantizer for any matrix\nmultiplication step (e.g., in self-attention, MLP etc). For example, NestQuant\nquantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving\nperplexity of 6.6 on wikitext2. This represents more than 55% reduction in\nperplexity gap with respect to unquantized model (perplexity of 6.14) compared\nto state-of-the-art Metas SpinQuant (perplexity 7.3), OstQuant (7.3) and QuaRot\n(8.2). Comparisons on bigger models (up to 70B) and on various LLM evaluation\nbenchmarks confirm uniform superiority of NestQuant."
                },
                "authors": [
                    {
                        "name": "Semyon Savkin"
                    },
                    {
                        "name": "Eitan Porat"
                    },
                    {
                        "name": "Or Ordentlich"
                    },
                    {
                        "name": "Yury Polyanskiy"
                    }
                ],
                "author_detail": {
                    "name": "Yury Polyanskiy"
                },
                "author": "Yury Polyanskiy",
                "arxiv_comment": "23 pages; Accepted at the 42nd International Conference on Machine\n  Learning (ICML 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09720v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09720v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20677v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20677v2",
                "updated": "2025-07-26T13:33:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    26,
                    13,
                    33,
                    6,
                    5,
                    207,
                    0
                ],
                "published": "2024-12-30T03:05:45Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    3,
                    5,
                    45,
                    0,
                    365,
                    0
                ],
                "title": "Align Attention Heads Before Merging Them: An Effective Way for\n  Converting MHA to GQA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Align Attention Heads Before Merging Them: An Effective Way for\n  Converting MHA to GQA"
                },
                "summary": "Large language models (LLMs) have demonstrated exceptional performance across\ndiverse natural language processing tasks. However, as the model size and the\ninput sequence's length increase, the linearly increasing key-value (KV) cache\nsignificantly degrades inference throughput. Therefore, grouped-query attention\n(GQA), as an alternative to multi-head attention (MHA), has been widely\nintroduced into LLMs. In this work, we propose a cost-effective method for\nconverting MHA into GQA with any compression ratio of KV heads. The key point\nof our method lies in the application of Procrustes analysis to the attention\nheads, which enhances the similarity among attention heads while preserving\ncomputational invariance, thereby improving the model's post-training\nperformance. Subsequently, we employ $\\mathit{L_0}$ regularization to prune\nredundant parameters. The model after pruning can be adapted to the standard\nGQA framework. Experimental results show that our strategy can compress up to\n87.5\\% KV heads of LLaMA2-7B model and 75\\% KV heads of Sheared-LLaMA-1.3B with\nacceptable performance degradation. Our code is released at\nhttps://github.com/fpcsong/mha2gqa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated exceptional performance across\ndiverse natural language processing tasks. However, as the model size and the\ninput sequence's length increase, the linearly increasing key-value (KV) cache\nsignificantly degrades inference throughput. Therefore, grouped-query attention\n(GQA), as an alternative to multi-head attention (MHA), has been widely\nintroduced into LLMs. In this work, we propose a cost-effective method for\nconverting MHA into GQA with any compression ratio of KV heads. The key point\nof our method lies in the application of Procrustes analysis to the attention\nheads, which enhances the similarity among attention heads while preserving\ncomputational invariance, thereby improving the model's post-training\nperformance. Subsequently, we employ $\\mathit{L_0}$ regularization to prune\nredundant parameters. The model after pruning can be adapted to the standard\nGQA framework. Experimental results show that our strategy can compress up to\n87.5\\% KV heads of LLaMA2-7B model and 75\\% KV heads of Sheared-LLaMA-1.3B with\nacceptable performance degradation. Our code is released at\nhttps://github.com/fpcsong/mha2gqa."
                },
                "authors": [
                    {
                        "name": "Qingyun Jin"
                    },
                    {
                        "name": "Xiaohui Song"
                    },
                    {
                        "name": "Feng Zhou"
                    },
                    {
                        "name": "Zengchang Qin"
                    }
                ],
                "author_detail": {
                    "name": "Zengchang Qin"
                },
                "author": "Zengchang Qin",
                "arxiv_comment": "13 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20677v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20677v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19823v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19823v1",
                "updated": "2025-07-26T06:43:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    26,
                    6,
                    43,
                    14,
                    5,
                    207,
                    0
                ],
                "published": "2025-07-26T06:43:14Z",
                "published_parsed": [
                    2025,
                    7,
                    26,
                    6,
                    43,
                    14,
                    5,
                    207,
                    0
                ],
                "title": "HCAttention: Extreme KV Cache Compression via Heterogeneous Attention\n  Computing for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HCAttention: Extreme KV Cache Compression via Heterogeneous Attention\n  Computing for LLMs"
                },
                "summary": "Processing long-context inputs with large language models presents a\nsignificant challenge due to the enormous memory requirements of the Key-Value\n(KV) cache during inference. Existing KV cache compression methods exhibit\nnoticeable performance degradation when memory is reduced by more than 85%.\nAdditionally, strategies that leverage GPU-CPU collaboration for approximate\nattention remain underexplored in this setting. We propose HCAttention, a\nheterogeneous attention computation framework that integrates key quantization,\nvalue offloading, and dynamic KV eviction to enable efficient inference under\nextreme memory constraints. The method is compatible with existing transformer\narchitectures and does not require model fine-tuning. Experimental results on\nthe LongBench benchmark demonstrate that our approach preserves the accuracy of\nfull-attention model while shrinking the KV cache memory footprint to 25% of\nits original size. Remarkably, it stays competitive with only 12.5% of the\ncache, setting a new state-of-the-art in LLM KV cache compression. To the best\nof our knowledge, HCAttention is the first to extend the Llama-3-8B model to\nprocess 4 million tokens on a single A100 GPU with 80GB memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long-context inputs with large language models presents a\nsignificant challenge due to the enormous memory requirements of the Key-Value\n(KV) cache during inference. Existing KV cache compression methods exhibit\nnoticeable performance degradation when memory is reduced by more than 85%.\nAdditionally, strategies that leverage GPU-CPU collaboration for approximate\nattention remain underexplored in this setting. We propose HCAttention, a\nheterogeneous attention computation framework that integrates key quantization,\nvalue offloading, and dynamic KV eviction to enable efficient inference under\nextreme memory constraints. The method is compatible with existing transformer\narchitectures and does not require model fine-tuning. Experimental results on\nthe LongBench benchmark demonstrate that our approach preserves the accuracy of\nfull-attention model while shrinking the KV cache memory footprint to 25% of\nits original size. Remarkably, it stays competitive with only 12.5% of the\ncache, setting a new state-of-the-art in LLM KV cache compression. To the best\nof our knowledge, HCAttention is the first to extend the Llama-3-8B model to\nprocess 4 million tokens on a single A100 GPU with 80GB memory."
                },
                "authors": [
                    {
                        "name": "Dongquan Yang"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Xiaotian Yu"
                    },
                    {
                        "name": "Xianbiao Qi"
                    },
                    {
                        "name": "Rong Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Rong Xiao"
                },
                "author": "Rong Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19823v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19823v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19427v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19427v1",
                "updated": "2025-07-25T16:53:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    16,
                    53,
                    13,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T16:53:13Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    16,
                    53,
                    13,
                    4,
                    206,
                    0
                ],
                "title": "Step-3 is Large yet Affordable: Model-system Co-design for\n  Cost-effective Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Step-3 is Large yet Affordable: Model-system Co-design for\n  Cost-effective Decoding"
                },
                "summary": "Large language models (LLMs) face low hardware efficiency during decoding,\nespecially for long-context reasoning tasks. This paper introduces Step-3, a\n321B-parameter VLM with hardware-aware model-system co-design optimized for\nminimizing decoding costs. Step-3 innovates in two key dimensions: (1) A novel\nMulti-Matrix Factorization Attention (MFA) mechanism that significantly reduces\nboth KV cache size and computation while maintaining high attention\nexpressiveness, and (2) Attention-FFN Disaggregation (AFD), a distributed\ninference system that decouples attention and Feed-Forward Network (FFN) layers\ninto specialized subsystems. This co-design achieves unprecedented cost\nefficiency: Step-3 significantly reduces theoretical decoding costs compared\nwith models like DeepSeek-V3 and Qwen3 MoE 235B, with the gains widening at\nlonger context. Step-3 achieves low cost while activating 38B parameters per\ntoken (more than DeepSeek-V3 and Qwen3 MoE 235B), demonstrating that\nhardware-aligned attention arithmetic intensity, MoE sparsity, and AFD are\ncritical to cost-effectiveness. We perform a head-to-head comparison with\nDeepSeek-V3 in its favorable scenarios. Our implementation on Hopper GPUs\nachieves a decoding throughput of up to 4,039 tokens per second per GPU under\n50ms TPOT SLA (4K context, FP8, no MTP). It is higher than DeepSeek-V3's 2,324\nin the same setup and sets a new Pareto frontier for LLM decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) face low hardware efficiency during decoding,\nespecially for long-context reasoning tasks. This paper introduces Step-3, a\n321B-parameter VLM with hardware-aware model-system co-design optimized for\nminimizing decoding costs. Step-3 innovates in two key dimensions: (1) A novel\nMulti-Matrix Factorization Attention (MFA) mechanism that significantly reduces\nboth KV cache size and computation while maintaining high attention\nexpressiveness, and (2) Attention-FFN Disaggregation (AFD), a distributed\ninference system that decouples attention and Feed-Forward Network (FFN) layers\ninto specialized subsystems. This co-design achieves unprecedented cost\nefficiency: Step-3 significantly reduces theoretical decoding costs compared\nwith models like DeepSeek-V3 and Qwen3 MoE 235B, with the gains widening at\nlonger context. Step-3 achieves low cost while activating 38B parameters per\ntoken (more than DeepSeek-V3 and Qwen3 MoE 235B), demonstrating that\nhardware-aligned attention arithmetic intensity, MoE sparsity, and AFD are\ncritical to cost-effectiveness. We perform a head-to-head comparison with\nDeepSeek-V3 in its favorable scenarios. Our implementation on Hopper GPUs\nachieves a decoding throughput of up to 4,039 tokens per second per GPU under\n50ms TPOT SLA (4K context, FP8, no MTP). It is higher than DeepSeek-V3's 2,324\nin the same setup and sets a new Pareto frontier for LLM decoding."
                },
                "authors": [
                    {
                        "name": "StepFun"
                    },
                    {
                        "name": ":"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Bojun Wang"
                    },
                    {
                        "name": "Changyi Wan"
                    },
                    {
                        "name": "Guanzhe Huang"
                    },
                    {
                        "name": "Hanpeng Hu"
                    },
                    {
                        "name": "Haonan Jia"
                    },
                    {
                        "name": "Hao Nie"
                    },
                    {
                        "name": "Mingliang Li"
                    },
                    {
                        "name": "Nuo Chen"
                    },
                    {
                        "name": "Siyu Chen"
                    },
                    {
                        "name": "Song Yuan"
                    },
                    {
                        "name": "Wuxun Xie"
                    },
                    {
                        "name": "Xiaoniu Song"
                    },
                    {
                        "name": "Xing Chen"
                    },
                    {
                        "name": "Xingping Yang"
                    },
                    {
                        "name": "Xuelin Zhang"
                    },
                    {
                        "name": "Yanbo Yu"
                    },
                    {
                        "name": "Yaoyu Wang"
                    },
                    {
                        "name": "Yibo Zhu"
                    },
                    {
                        "name": "Yimin Jiang"
                    },
                    {
                        "name": "Yu Zhou"
                    },
                    {
                        "name": "Yuanwei Lu"
                    },
                    {
                        "name": "Houyi Li"
                    },
                    {
                        "name": "Jingcheng Hu"
                    },
                    {
                        "name": "Ka Man Lo"
                    },
                    {
                        "name": "Ailin Huang"
                    },
                    {
                        "name": "Binxing Jiao"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Boyu Chen"
                    },
                    {
                        "name": "Changxin Miao"
                    },
                    {
                        "name": "Chang Lou"
                    },
                    {
                        "name": "Chen Hu"
                    },
                    {
                        "name": "Chen Xu"
                    },
                    {
                        "name": "Chenfeng Yu"
                    },
                    {
                        "name": "Chengyuan Yao"
                    },
                    {
                        "name": "Daokuan Lv"
                    },
                    {
                        "name": "Dapeng Shi"
                    },
                    {
                        "name": "Deshan Sun"
                    },
                    {
                        "name": "Ding Huang"
                    },
                    {
                        "name": "Dingyuan Hu"
                    },
                    {
                        "name": "Dongqing Pang"
                    },
                    {
                        "name": "Enle Liu"
                    },
                    {
                        "name": "Fajie Zhang"
                    },
                    {
                        "name": "Fanqi Wan"
                    },
                    {
                        "name": "Gulin Yan"
                    },
                    {
                        "name": "Han Zhang"
                    },
                    {
                        "name": "Han Zhou"
                    },
                    {
                        "name": "Hanghao Wu"
                    },
                    {
                        "name": "Hangyu Guo"
                    },
                    {
                        "name": "Hanqi Chen"
                    },
                    {
                        "name": "Hanshan Zhang"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Haocheng Zhang"
                    },
                    {
                        "name": "Haolong Yan"
                    },
                    {
                        "name": "Haoran Lv"
                    },
                    {
                        "name": "Haoran Wei"
                    },
                    {
                        "name": "Hebin Zhou"
                    },
                    {
                        "name": "Heng Wang"
                    },
                    {
                        "name": "Heng Wang"
                    },
                    {
                        "name": "Hongxin Li"
                    },
                    {
                        "name": "Hongyu Zhou"
                    },
                    {
                        "name": "Hongyuan Wang"
                    },
                    {
                        "name": "Huiyong Guo"
                    },
                    {
                        "name": "Jia Wang"
                    },
                    {
                        "name": "Jiahao Gong"
                    },
                    {
                        "name": "Jialing Xie"
                    },
                    {
                        "name": "Jian Zhou"
                    },
                    {
                        "name": "Jianjian Sun"
                    },
                    {
                        "name": "Jiaoren Wu"
                    },
                    {
                        "name": "Jiaran Zhang"
                    },
                    {
                        "name": "Jiayu Liu"
                    },
                    {
                        "name": "Jie Cheng"
                    },
                    {
                        "name": "Jie Luo"
                    },
                    {
                        "name": "Jie Yan"
                    },
                    {
                        "name": "Jie Yang"
                    },
                    {
                        "name": "Jieyi Hou"
                    },
                    {
                        "name": "Jinguang Zhang"
                    },
                    {
                        "name": "Jinlan Cao"
                    },
                    {
                        "name": "Jisheng Yin"
                    },
                    {
                        "name": "Junfeng Liu"
                    },
                    {
                        "name": "Junhao Huang"
                    },
                    {
                        "name": "Junzhe Lin"
                    },
                    {
                        "name": "Kaijun Tan"
                    },
                    {
                        "name": "Kaixiang Li"
                    },
                    {
                        "name": "Kang An"
                    },
                    {
                        "name": "Kangheng Lin"
                    },
                    {
                        "name": "Kenkun Liu"
                    },
                    {
                        "name": "Lei Yang"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Liangyu Chen"
                    },
                    {
                        "name": "Lieyu Shi"
                    },
                    {
                        "name": "Liguo Tan"
                    },
                    {
                        "name": "Lin Lin"
                    },
                    {
                        "name": "Lin Zhang"
                    },
                    {
                        "name": "Lina Chen"
                    },
                    {
                        "name": "Liwen Huang"
                    },
                    {
                        "name": "Liying Shi"
                    },
                    {
                        "name": "Longlong Gu"
                    },
                    {
                        "name": "Mei Chen"
                    },
                    {
                        "name": "Mengqiang Ren"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Mingzhe Chen"
                    },
                    {
                        "name": "Na Wang"
                    },
                    {
                        "name": "Nan Wu"
                    },
                    {
                        "name": "Qi Han"
                    },
                    {
                        "name": "Qian Zhao"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Qianni Liu"
                    },
                    {
                        "name": "Qiaohui Chen"
                    },
                    {
                        "name": "Qiling Wu"
                    },
                    {
                        "name": "Qinglin He"
                    },
                    {
                        "name": "Qinyuan Tan"
                    },
                    {
                        "name": "Qiufeng Wang"
                    },
                    {
                        "name": "Qiuping Wu"
                    },
                    {
                        "name": "Qiuyan Liang"
                    },
                    {
                        "name": "Quan Sun"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Ruihang Miao"
                    },
                    {
                        "name": "Ruosi Wan"
                    },
                    {
                        "name": "Ruyan Guo"
                    },
                    {
                        "name": "Shangwu Zhong"
                    },
                    {
                        "name": "Shaoliang Pang"
                    },
                    {
                        "name": "Shengjie Fan"
                    },
                    {
                        "name": "Shijie Shang"
                    },
                    {
                        "name": "Shilei Jiang"
                    },
                    {
                        "name": "Shiliang Yang"
                    },
                    {
                        "name": "Shiming Hao"
                    },
                    {
                        "name": "Shuli Gao"
                    },
                    {
                        "name": "Siming Huang"
                    },
                    {
                        "name": "Siqi Liu"
                    },
                    {
                        "name": "Tiancheng Cao"
                    },
                    {
                        "name": "Tianhao Cheng"
                    },
                    {
                        "name": "Tianhao Peng"
                    },
                    {
                        "name": "Wang You"
                    },
                    {
                        "name": "Wei Ji"
                    },
                    {
                        "name": "Wen Sun"
                    },
                    {
                        "name": "Wenjin Deng"
                    },
                    {
                        "name": "Wenqing He"
                    },
                    {
                        "name": "Wenzhen Zheng"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Xiangwen Kong"
                    },
                    {
                        "name": "Xianzhen Luo"
                    },
                    {
                        "name": "Xiaobo Yang"
                    },
                    {
                        "name": "Xiaojia Liu"
                    },
                    {
                        "name": "Xiaoxiao Ren"
                    },
                    {
                        "name": "Xin Han"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Xin Wu"
                    },
                    {
                        "name": "Xu Zhao"
                    },
                    {
                        "name": "Yanan Wei"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Yangguang Li"
                    },
                    {
                        "name": "Yangshijie Xu"
                    },
                    {
                        "name": "Yanming Xu"
                    },
                    {
                        "name": "Yaqiang Shi"
                    },
                    {
                        "name": "Yeqing Shen"
                    },
                    {
                        "name": "Yi Yang"
                    },
                    {
                        "name": "Yifei Yang"
                    },
                    {
                        "name": "Yifeng Gong"
                    },
                    {
                        "name": "Yihan Chen"
                    },
                    {
                        "name": "Yijing Yang"
                    },
                    {
                        "name": "Yinmin Zhang"
                    },
                    {
                        "name": "Yizhuang Zhou"
                    },
                    {
                        "name": "Yuanhao Ding"
                    },
                    {
                        "name": "Yuantao Fan"
                    },
                    {
                        "name": "Yuanzhen Yang"
                    },
                    {
                        "name": "Yuchu Luo"
                    },
                    {
                        "name": "Yue Peng"
                    },
                    {
                        "name": "Yufan Lu"
                    },
                    {
                        "name": "Yuhang Deng"
                    },
                    {
                        "name": "Yuhe Yin"
                    },
                    {
                        "name": "Yujie Liu"
                    },
                    {
                        "name": "Yukun Chen"
                    },
                    {
                        "name": "Yuling Zhao"
                    },
                    {
                        "name": "Yun Mou"
                    },
                    {
                        "name": "Yunlong Li"
                    },
                    {
                        "name": "Yunzhou Ju"
                    },
                    {
                        "name": "Yusheng Li"
                    },
                    {
                        "name": "Yuxiang Yang"
                    },
                    {
                        "name": "Yuxiang Zhang"
                    },
                    {
                        "name": "Yuyang Chen"
                    },
                    {
                        "name": "Zejia Weng"
                    },
                    {
                        "name": "Zhe Xie"
                    },
                    {
                        "name": "Zheng Ge"
                    },
                    {
                        "name": "Zheng Gong"
                    },
                    {
                        "name": "Zhenyi Lu"
                    },
                    {
                        "name": "Zhewei Huang"
                    },
                    {
                        "name": "Zhichao Chang"
                    },
                    {
                        "name": "Zhiguo Huang"
                    },
                    {
                        "name": "Zhirui Wang"
                    },
                    {
                        "name": "Zidong Yang"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Ziqi Wang"
                    },
                    {
                        "name": "Zixin Zhang"
                    },
                    {
                        "name": "Binxing Jiao"
                    },
                    {
                        "name": "Daxin Jiang"
                    },
                    {
                        "name": "Heung-Yeung Shum"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Zhang"
                },
                "author": "Xiangyu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19427v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19427v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19367v1",
                "updated": "2025-07-25T15:17:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    15,
                    17,
                    29,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T15:17:29Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    15,
                    17,
                    29,
                    4,
                    206,
                    0
                ],
                "title": "Empowering IoT Firmware Secure Update with Customization Rights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empowering IoT Firmware Secure Update with Customization Rights"
                },
                "summary": "Firmware updates remain the primary line of defense for IoT devices; however,\nthe update channel itself has become a well-established attack vector. Existing\ndefenses mainly focus on securing monolithic firmware images, leaving\nmodule-level customization -a growing user demand-largely unprotected and\ninsufficiently explored. To address this gap, we conduct a pilot study on the\nupdate workflows of 200 Linux-based IoT devices across 23 vendors, uncovering\nfive previously undocumented vulnerabilities caused by customization practices.\nA broader analysis of update-related CVEs from 2020 to 2024 reveals that over\nhalf originate from customization-induced issues. These findings highlight a\ncritical yet underexamined reality: as customization increases, so does the\nattack surface, while current defenses fail to keep pace. We propose IMUP\n(Integrity-Centric Modular Update Platform), the first framework to address two\nkey challenges: constructing a trustworthy cross-module integrity chain and\nscaling update performance under mass customization. IMUP combines three\ntechniques: per-module chameleon hashing for integrity, server-side\nproof-of-work offloading to reduce device overhead, and server-side caching to\nreuse module combinations, minimizing rebuild costs. Security analysis shows\nthat even when 95 percent of secret keys are exposed, forging a valid image\nincurs over 300 times the cost of the legitimate server. Experiments on\nheterogeneous IoT devices demonstrate that IMUP reduces server-side generation\ntime by 2.9 times and device downtime by 5.9 times compared to a\npackage-manager baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Firmware updates remain the primary line of defense for IoT devices; however,\nthe update channel itself has become a well-established attack vector. Existing\ndefenses mainly focus on securing monolithic firmware images, leaving\nmodule-level customization -a growing user demand-largely unprotected and\ninsufficiently explored. To address this gap, we conduct a pilot study on the\nupdate workflows of 200 Linux-based IoT devices across 23 vendors, uncovering\nfive previously undocumented vulnerabilities caused by customization practices.\nA broader analysis of update-related CVEs from 2020 to 2024 reveals that over\nhalf originate from customization-induced issues. These findings highlight a\ncritical yet underexamined reality: as customization increases, so does the\nattack surface, while current defenses fail to keep pace. We propose IMUP\n(Integrity-Centric Modular Update Platform), the first framework to address two\nkey challenges: constructing a trustworthy cross-module integrity chain and\nscaling update performance under mass customization. IMUP combines three\ntechniques: per-module chameleon hashing for integrity, server-side\nproof-of-work offloading to reduce device overhead, and server-side caching to\nreuse module combinations, minimizing rebuild costs. Security analysis shows\nthat even when 95 percent of secret keys are exposed, forging a valid image\nincurs over 300 times the cost of the legitimate server. Experiments on\nheterogeneous IoT devices demonstrate that IMUP reduces server-side generation\ntime by 2.9 times and device downtime by 5.9 times compared to a\npackage-manager baseline."
                },
                "authors": [
                    {
                        "name": "Weihao Chen"
                    },
                    {
                        "name": "Yansong Gao"
                    },
                    {
                        "name": "Boyu Kuang"
                    },
                    {
                        "name": "Jin B. Hong"
                    },
                    {
                        "name": "Yuqing Zhang"
                    },
                    {
                        "name": "Anmin Fu"
                    }
                ],
                "author_detail": {
                    "name": "Anmin Fu"
                },
                "author": "Anmin Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01802v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01802v2",
                "updated": "2025-07-24T19:44:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    19,
                    44,
                    36,
                    3,
                    205,
                    0
                ],
                "published": "2025-02-03T20:30:25Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    20,
                    30,
                    25,
                    0,
                    34,
                    0
                ],
                "title": "General kinetic ion induced electron emission model for metallic walls\n  applied to biased Z-pinch electrodes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General kinetic ion induced electron emission model for metallic walls\n  applied to biased Z-pinch electrodes"
                },
                "summary": "A kinetic ion induced electron emission (IIEE) model for general applications\nis developed to obtain the emitted electron energy spectrum for a distribution\nof ion impacts on a metallic surface. We assume an ionization cascade mechanism\nand use empirical models for the ion and electron stopping powers. The emission\nspectrum and the secondary electron yield (SEY) are validated for a variety of\nmaterials. The IIEE model is used to study the effect of IIEE on the\nplasma-material interactions of Z-pinch electrodes. Un-magnetized\nBoltzmann-Poisson simulations are performed for a Z-pinch plasma doubly bounded\nby two biased copper electrodes with and without IIEE at bias potentials from 0\nto 9 kV. At the anode, the SEY decreases from 0 to 1 kV, but then increases at\nhigher bias potentials. At the cathode, the SEY is much larger due to higher\nenergy ion bombardment and grows with bias potential. As the bias potential\nincreases, the emitted cathode electrons are accelerated to higher energies\ninto the domain collisionally heating the plasma. Above 1 kV, the heating is\nstrong enough to increase the plasma potential. Despite SEY greater than 1,\nonly a classical sheath forms as opposed to a space-charge limited or inverse\nsheath due to the emitted electron flux not reaching the space charge current\nsaturation limits. Furthermore, the current in the emissionless cases saturates\nto a value lower than experiment. With IIEE, the current does not saturate and\ncontinues to increase with the 4 kV case matching most closely with experiment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A kinetic ion induced electron emission (IIEE) model for general applications\nis developed to obtain the emitted electron energy spectrum for a distribution\nof ion impacts on a metallic surface. We assume an ionization cascade mechanism\nand use empirical models for the ion and electron stopping powers. The emission\nspectrum and the secondary electron yield (SEY) are validated for a variety of\nmaterials. The IIEE model is used to study the effect of IIEE on the\nplasma-material interactions of Z-pinch electrodes. Un-magnetized\nBoltzmann-Poisson simulations are performed for a Z-pinch plasma doubly bounded\nby two biased copper electrodes with and without IIEE at bias potentials from 0\nto 9 kV. At the anode, the SEY decreases from 0 to 1 kV, but then increases at\nhigher bias potentials. At the cathode, the SEY is much larger due to higher\nenergy ion bombardment and grows with bias potential. As the bias potential\nincreases, the emitted cathode electrons are accelerated to higher energies\ninto the domain collisionally heating the plasma. Above 1 kV, the heating is\nstrong enough to increase the plasma potential. Despite SEY greater than 1,\nonly a classical sheath forms as opposed to a space-charge limited or inverse\nsheath due to the emitted electron flux not reaching the space charge current\nsaturation limits. Furthermore, the current in the emissionless cases saturates\nto a value lower than experiment. With IIEE, the current does not saturate and\ncontinues to increase with the 4 kV case matching most closely with experiment."
                },
                "authors": [
                    {
                        "name": "Chirag R. Skolar"
                    },
                    {
                        "name": "Kolter Bradshaw"
                    },
                    {
                        "name": "Manaure Francisquez"
                    },
                    {
                        "name": "Lucio Murillo"
                    },
                    {
                        "name": "Vignesh Krishna Kumar"
                    },
                    {
                        "name": "Bhuvana Srinivasan"
                    }
                ],
                "author_detail": {
                    "name": "Bhuvana Srinivasan"
                },
                "author": "Bhuvana Srinivasan",
                "arxiv_comment": "21 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01802v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01802v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16870v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16870v2",
                "updated": "2025-07-24T17:30:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    17,
                    30,
                    12,
                    3,
                    205,
                    0
                ],
                "published": "2025-03-21T05:58:18Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    5,
                    58,
                    18,
                    4,
                    80,
                    0
                ],
                "title": "Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs"
                },
                "summary": "Knowledge distillation can be a cost-effective technique to distill knowledge\nin Large Language Models, if the teacher output logits can be pre-computed and\ncached. However, successfully applying this to pre-training remains largely\nunexplored. In this work, we prove that naive approaches for sparse knowledge\ndistillation such as caching Top-K probabilities, while intuitive, provide\nbiased estimates of teacher probability distribution to the student, resulting\nin suboptimal performance and calibration. We propose an\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\nwhich provides unbiased estimates, preserves the gradient in expectation, and\nrequires storing significantly sparser logits. Our method enables faster\ntraining of student models with marginal overhead (<10%) compared to\ncross-entropy based training, while maintaining competitive performance\ncompared to full distillation, across a range of model sizes from 300M to 3B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge distillation can be a cost-effective technique to distill knowledge\nin Large Language Models, if the teacher output logits can be pre-computed and\ncached. However, successfully applying this to pre-training remains largely\nunexplored. In this work, we prove that naive approaches for sparse knowledge\ndistillation such as caching Top-K probabilities, while intuitive, provide\nbiased estimates of teacher probability distribution to the student, resulting\nin suboptimal performance and calibration. We propose an\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\nwhich provides unbiased estimates, preserves the gradient in expectation, and\nrequires storing significantly sparser logits. Our method enables faster\ntraining of student models with marginal overhead (<10%) compared to\ncross-entropy based training, while maintaining competitive performance\ncompared to full distillation, across a range of model sizes from 300M to 3B."
                },
                "authors": [
                    {
                        "name": "Anshumann"
                    },
                    {
                        "name": "Mohd Abbas Zaidi"
                    },
                    {
                        "name": "Akhil Kedia"
                    },
                    {
                        "name": "Jinwoo Ahn"
                    },
                    {
                        "name": "Taehwak Kwon"
                    },
                    {
                        "name": "Kangwook Lee"
                    },
                    {
                        "name": "Haejun Lee"
                    },
                    {
                        "name": "Joohyung Lee"
                    }
                ],
                "author_detail": {
                    "name": "Joohyung Lee"
                },
                "author": "Joohyung Lee",
                "arxiv_comment": "Accepted as Oral paper at ACL 2025. Source code is available at\n  https://github.com/akhilkedia/RandomSamplingKD . Anshumann, Mohd Abbas Zaidi\n  and Akhil Kedia have Equal Contribution",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16870v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16870v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04704v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04704v2",
                "updated": "2025-07-24T16:25:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    25,
                    51,
                    3,
                    205,
                    0
                ],
                "published": "2025-04-07T03:22:15Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    3,
                    22,
                    15,
                    0,
                    97,
                    0
                ],
                "title": "LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are\n  Important",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are\n  Important"
                },
                "summary": "The increasing size of the Key-Value (KV) cache during the Large Language\nModels long-context inference is the main obstacle for its balance between the\ndeployment cost and task accuracy. To reduce the KV cache size in such\nscenarios, most previous efforts leveraged on the attention weight to evict\nnon-critical cache tokens. But there is a trade-off in those methods, they\nusually require major modification of the inference infrastructure and\nsignificant computation overhead. Based on the fact that the Large Language\nmodels are autoregressive models, we propose LagKV, a KV compression strategy\nonly relying on straight forward comparison among KV themselves. It is a\ntotally attention free method which offers easy integration to the main stream\ninference platform and comparable performance comparing to other complicated KV\ncompression methods. Results on RULER benchmark show that, our approach\noutperforms SnapKV and StreamingLLM in different compression ratios. Especially\nin the 64-digit passkey retrieval task, our method outperforms the attention\nweight based method $H_2O$ over $50\\%$ with same compression ratios. Our code\nis available at https://github.com/AI-Lab-China-Merchants-Bank/LagKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing size of the Key-Value (KV) cache during the Large Language\nModels long-context inference is the main obstacle for its balance between the\ndeployment cost and task accuracy. To reduce the KV cache size in such\nscenarios, most previous efforts leveraged on the attention weight to evict\nnon-critical cache tokens. But there is a trade-off in those methods, they\nusually require major modification of the inference infrastructure and\nsignificant computation overhead. Based on the fact that the Large Language\nmodels are autoregressive models, we propose LagKV, a KV compression strategy\nonly relying on straight forward comparison among KV themselves. It is a\ntotally attention free method which offers easy integration to the main stream\ninference platform and comparable performance comparing to other complicated KV\ncompression methods. Results on RULER benchmark show that, our approach\noutperforms SnapKV and StreamingLLM in different compression ratios. Especially\nin the 64-digit passkey retrieval task, our method outperforms the attention\nweight based method $H_2O$ over $50\\%$ with same compression ratios. Our code\nis available at https://github.com/AI-Lab-China-Merchants-Bank/LagKV."
                },
                "authors": [
                    {
                        "name": "Manlai Liang"
                    },
                    {
                        "name": "JiaMing Zhang"
                    },
                    {
                        "name": "Xiong Li"
                    },
                    {
                        "name": "Jinlong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinlong Li"
                },
                "author": "Jinlong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04704v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04704v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18446v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18446v1",
                "updated": "2025-07-24T14:30:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    14,
                    30,
                    48,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T14:30:48Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    14,
                    30,
                    48,
                    3,
                    205,
                    0
                ],
                "title": "Streaming Sortformer: Speaker Cache-Based Online Speaker Diarization\n  with Arrival-Time Ordering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming Sortformer: Speaker Cache-Based Online Speaker Diarization\n  with Arrival-Time Ordering"
                },
                "summary": "This paper presents a streaming extension for the Sortformer speaker\ndiarization framework, whose key property is the arrival-time ordering of\noutput speakers. The proposed approach employs an Arrival-Order Speaker Cache\n(AOSC) to store frame-level acoustic embeddings of previously observed\nspeakers. Unlike conventional speaker-tracing buffers, AOSC orders embeddings\nby speaker index corresponding to their arrival time order, and is dynamically\nupdated by selecting frames with the highest scores based on the model's past\npredictions. Notably, the number of stored embeddings per speaker is determined\ndynamically by the update mechanism, ensuring efficient cache utilization and\nprecise speaker tracking. Experiments on benchmark datasets confirm the\neffectiveness and flexibility of our approach, even in low-latency setups.\nThese results establish Streaming Sortformer as a robust solution for real-time\nmulti-speaker tracking and a foundation for streaming multi-talker speech\nprocessing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a streaming extension for the Sortformer speaker\ndiarization framework, whose key property is the arrival-time ordering of\noutput speakers. The proposed approach employs an Arrival-Order Speaker Cache\n(AOSC) to store frame-level acoustic embeddings of previously observed\nspeakers. Unlike conventional speaker-tracing buffers, AOSC orders embeddings\nby speaker index corresponding to their arrival time order, and is dynamically\nupdated by selecting frames with the highest scores based on the model's past\npredictions. Notably, the number of stored embeddings per speaker is determined\ndynamically by the update mechanism, ensuring efficient cache utilization and\nprecise speaker tracking. Experiments on benchmark datasets confirm the\neffectiveness and flexibility of our approach, even in low-latency setups.\nThese results establish Streaming Sortformer as a robust solution for real-time\nmulti-speaker tracking and a foundation for streaming multi-talker speech\nprocessing."
                },
                "authors": [
                    {
                        "name": "Ivan Medennikov"
                    },
                    {
                        "name": "Taejin Park"
                    },
                    {
                        "name": "Weiqing Wang"
                    },
                    {
                        "name": "He Huang"
                    },
                    {
                        "name": "Kunal Dhawan"
                    },
                    {
                        "name": "Jinhan Wang"
                    },
                    {
                        "name": "Jagadeesh Balam"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "arxiv_comment": "Accepted to Interspeech 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18446v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18446v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18028v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18028v1",
                "updated": "2025-07-24T02:00:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    2,
                    0,
                    9,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T02:00:09Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    2,
                    0,
                    9,
                    3,
                    205,
                    0
                ],
                "title": "NeuralDB: Scaling Knowledge Editing in LLMs to 100,000 Facts with Neural\n  KV Database",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeuralDB: Scaling Knowledge Editing in LLMs to 100,000 Facts with Neural\n  KV Database"
                },
                "summary": "Efficiently editing knowledge stored in large language models (LLMs) enables\nmodel updates without large-scale training. One possible solution is\nLocate-and-Edit (L\\&E), allowing simultaneous modifications of a massive number\nof facts. However, such editing may compromise the general abilities of LLMs\nand even result in forgetting edited facts when scaling up to thousands of\nedits. In this paper, we model existing linear L\\&E methods as querying a\nKey-Value (KV) database. From this perspective, we then propose NeuralDB, an\nediting framework that explicitly represents the edited facts as a neural KV\ndatabase equipped with a non-linear gated retrieval module, % In particular,\nour gated module only operates when inference involves the edited facts,\neffectively preserving the general abilities of LLMs. Comprehensive experiments\ninvolving the editing of 10,000 facts were conducted on the ZsRE and\nCounterFacts datasets, using GPT2-XL, GPT-J (6B) and Llama-3 (8B). The results\ndemonstrate that NeuralDB not only excels in editing efficacy, generalization,\nspecificity, fluency, and consistency, but also preserves overall performance\nacross six representative text understanding and generation tasks. Further\nexperiments indicate that NeuralDB maintains its effectiveness even when scaled\nto 100,000 facts (\\textbf{50x} more than in prior work).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently editing knowledge stored in large language models (LLMs) enables\nmodel updates without large-scale training. One possible solution is\nLocate-and-Edit (L\\&E), allowing simultaneous modifications of a massive number\nof facts. However, such editing may compromise the general abilities of LLMs\nand even result in forgetting edited facts when scaling up to thousands of\nedits. In this paper, we model existing linear L\\&E methods as querying a\nKey-Value (KV) database. From this perspective, we then propose NeuralDB, an\nediting framework that explicitly represents the edited facts as a neural KV\ndatabase equipped with a non-linear gated retrieval module, % In particular,\nour gated module only operates when inference involves the edited facts,\neffectively preserving the general abilities of LLMs. Comprehensive experiments\ninvolving the editing of 10,000 facts were conducted on the ZsRE and\nCounterFacts datasets, using GPT2-XL, GPT-J (6B) and Llama-3 (8B). The results\ndemonstrate that NeuralDB not only excels in editing efficacy, generalization,\nspecificity, fluency, and consistency, but also preserves overall performance\nacross six representative text understanding and generation tasks. Further\nexperiments indicate that NeuralDB maintains its effectiveness even when scaled\nto 100,000 facts (\\textbf{50x} more than in prior work)."
                },
                "authors": [
                    {
                        "name": "Weizhi Fei"
                    },
                    {
                        "name": "Hao Shi"
                    },
                    {
                        "name": "Jing Xu"
                    },
                    {
                        "name": "Jingchen Peng"
                    },
                    {
                        "name": "Jiazheng Li"
                    },
                    {
                        "name": "Jingzhao Zhang"
                    },
                    {
                        "name": "Bo Bai"
                    },
                    {
                        "name": "Wei Han"
                    },
                    {
                        "name": "Zhenyuan Chen"
                    },
                    {
                        "name": "Xueyan Niu"
                    }
                ],
                "author_detail": {
                    "name": "Xueyan Niu"
                },
                "author": "Xueyan Niu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18028v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18028v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17744v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17744v1",
                "updated": "2025-07-23T17:57:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    17,
                    57,
                    9,
                    2,
                    204,
                    0
                ],
                "published": "2025-07-23T17:57:09Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    17,
                    57,
                    9,
                    2,
                    204,
                    0
                ],
                "title": "Yume: An Interactive World Generation Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yume: An Interactive World Generation Model"
                },
                "summary": "Yume aims to use images, text, or videos to create an interactive, realistic,\nand dynamic world, which allows exploration and control using peripheral\ndevices or neural signals. In this report, we present a preview version of\n\\method, which creates a dynamic world from an input image and allows\nexploration of the world using keyboard actions. To achieve this high-fidelity\nand interactive video world generation, we introduce a well-designed framework,\nwhich consists of four main components, including camera motion quantization,\nvideo generation architecture, advanced sampler, and model acceleration. First,\nwe quantize camera motions for stable training and user-friendly interaction\nusing keyboard inputs. Then, we introduce the Masked Video Diffusion\nTransformer~(MVDT) with a memory module for infinite video generation in an\nautoregressive manner. After that, training-free Anti-Artifact Mechanism (AAM)\nand Time Travel Sampling based on Stochastic Differential Equations (TTS-SDE)\nare introduced to the sampler for better visual quality and more precise\ncontrol. Moreover, we investigate model acceleration by synergistic\noptimization of adversarial distillation and caching mechanisms. We use the\nhigh-quality world exploration dataset \\sekai to train \\method, and it achieves\nremarkable results in diverse scenes and applications. All data, codebase, and\nmodel weights are available on https://github.com/stdstu12/YUME. Yume will\nupdate monthly to achieve its original goal. Project page:\nhttps://stdstu12.github.io/YUME-Project/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yume aims to use images, text, or videos to create an interactive, realistic,\nand dynamic world, which allows exploration and control using peripheral\ndevices or neural signals. In this report, we present a preview version of\n\\method, which creates a dynamic world from an input image and allows\nexploration of the world using keyboard actions. To achieve this high-fidelity\nand interactive video world generation, we introduce a well-designed framework,\nwhich consists of four main components, including camera motion quantization,\nvideo generation architecture, advanced sampler, and model acceleration. First,\nwe quantize camera motions for stable training and user-friendly interaction\nusing keyboard inputs. Then, we introduce the Masked Video Diffusion\nTransformer~(MVDT) with a memory module for infinite video generation in an\nautoregressive manner. After that, training-free Anti-Artifact Mechanism (AAM)\nand Time Travel Sampling based on Stochastic Differential Equations (TTS-SDE)\nare introduced to the sampler for better visual quality and more precise\ncontrol. Moreover, we investigate model acceleration by synergistic\noptimization of adversarial distillation and caching mechanisms. We use the\nhigh-quality world exploration dataset \\sekai to train \\method, and it achieves\nremarkable results in diverse scenes and applications. All data, codebase, and\nmodel weights are available on https://github.com/stdstu12/YUME. Yume will\nupdate monthly to achieve its original goal. Project page:\nhttps://stdstu12.github.io/YUME-Project/."
                },
                "authors": [
                    {
                        "name": "Xiaofeng Mao"
                    },
                    {
                        "name": "Shaoheng Lin"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Chuanhao Li"
                    },
                    {
                        "name": "Wenshuo Peng"
                    },
                    {
                        "name": "Tong He"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    },
                    {
                        "name": "Mingmin Chi"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Kaipeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Kaipeng Zhang"
                },
                "author": "Kaipeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17744v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17744v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17647v1",
                "updated": "2025-07-23T16:09:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    16,
                    9,
                    10,
                    2,
                    204,
                    0
                ],
                "published": "2025-07-23T16:09:10Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    16,
                    9,
                    10,
                    2,
                    204,
                    0
                ],
                "title": "SHINE: A Scalable HNSW Index in Disaggregated Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SHINE: A Scalable HNSW Index in Disaggregated Memory"
                },
                "summary": "Approximate nearest neighbor (ANN) search is a fundamental problem in\ncomputer science for which in-memory graph-based methods, such as Hierarchical\nNavigable Small World (HNSW), perform exceptionally well. To scale beyond\nbillions of high-dimensional vectors, the index must be distributed. The\ndisaggregated memory architecture physically separates compute and memory into\ntwo distinct hardware units and has become popular in modern data centers. Both\nunits are connected via RDMA networks that allow compute nodes to directly\naccess remote memory and perform all the computations, posing unique challenges\nfor disaggregated indexes.\n  In this work, we propose a scalable HNSW index for ANN search in\ndisaggregated memory. In contrast to existing distributed approaches, which\npartition the graph at the cost of accuracy, our method builds a\ngraph-preserving index that reaches the same accuracy as a single-machine HNSW.\nContinuously fetching high-dimensional vector data from remote memory leads to\nsevere network bandwidth limitations, which we overcome by employing an\nefficient caching mechanism. Since answering a single query involves processing\nnumerous unique graph nodes, caching alone is not sufficient to achieve high\nscalability. We logically combine the caches of the compute nodes to increase\nthe overall cache effectiveness and confirm the efficiency and scalability of\nour method in our evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate nearest neighbor (ANN) search is a fundamental problem in\ncomputer science for which in-memory graph-based methods, such as Hierarchical\nNavigable Small World (HNSW), perform exceptionally well. To scale beyond\nbillions of high-dimensional vectors, the index must be distributed. The\ndisaggregated memory architecture physically separates compute and memory into\ntwo distinct hardware units and has become popular in modern data centers. Both\nunits are connected via RDMA networks that allow compute nodes to directly\naccess remote memory and perform all the computations, posing unique challenges\nfor disaggregated indexes.\n  In this work, we propose a scalable HNSW index for ANN search in\ndisaggregated memory. In contrast to existing distributed approaches, which\npartition the graph at the cost of accuracy, our method builds a\ngraph-preserving index that reaches the same accuracy as a single-machine HNSW.\nContinuously fetching high-dimensional vector data from remote memory leads to\nsevere network bandwidth limitations, which we overcome by employing an\nefficient caching mechanism. Since answering a single query involves processing\nnumerous unique graph nodes, caching alone is not sufficient to achieve high\nscalability. We logically combine the caches of the compute nodes to increase\nthe overall cache effectiveness and confirm the efficiency and scalability of\nour method in our evaluation."
                },
                "authors": [
                    {
                        "name": "Manuel Widmoser"
                    },
                    {
                        "name": "Daniel Kocher"
                    },
                    {
                        "name": "Nikolaus Augsten"
                    }
                ],
                "author_detail": {
                    "name": "Nikolaus Augsten"
                },
                "author": "Nikolaus Augsten",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16242v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16242v2",
                "updated": "2025-07-23T15:59:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    15,
                    59,
                    38,
                    2,
                    204,
                    0
                ],
                "published": "2025-07-22T05:26:28Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    5,
                    26,
                    28,
                    1,
                    203,
                    0
                ],
                "title": "Toward a Lightweight and Robust Design for Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward a Lightweight and Robust Design for Caching"
                },
                "summary": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce significant computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_k + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only $O(1)$ additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce significant computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_k + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only $O(1)$ additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice."
                },
                "authors": [
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Hailiang Zhao"
                    },
                    {
                        "name": "Jiaji Zhang"
                    },
                    {
                        "name": "Xueyan Tang"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16242v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16242v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17554v1",
                "updated": "2025-07-23T14:43:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    14,
                    43,
                    22,
                    2,
                    204,
                    0
                ],
                "published": "2025-07-23T14:43:22Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    14,
                    43,
                    22,
                    2,
                    204,
                    0
                ],
                "title": "An h-space Based Adversarial Attack for Protection Against Few-shot\n  Personalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An h-space Based Adversarial Attack for Protection Against Few-shot\n  Personalization"
                },
                "summary": "The versatility of diffusion models in generating customized images from few\nsamples raises significant privacy concerns, particularly regarding\nunauthorized modifications of private content. This concerning issue has\nrenewed the efforts in developing protection mechanisms based on adversarial\nattacks, which generate effective perturbations to poison diffusion models. Our\nwork is motivated by the observation that these models exhibit a high degree of\nabstraction within their semantic latent space (`h-space'), which encodes\ncritical high-level features for generating coherent and meaningful content. In\nthis paper, we propose a novel anti-customization approach, called HAAD\n(h-space based Adversarial Attack for Diffusion models), that leverages\nadversarial attacks to craft perturbations based on the h-space that can\nefficiently degrade the image generation process. Building upon HAAD, we\nfurther introduce a more efficient variant, HAAD-KV, that constructs\nperturbations solely based on the KV parameters of the h-space. This strategy\noffers a stronger protection, that is computationally less expensive. Despite\ntheir simplicity, our methods outperform state-of-the-art adversarial attacks,\nhighlighting their effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The versatility of diffusion models in generating customized images from few\nsamples raises significant privacy concerns, particularly regarding\nunauthorized modifications of private content. This concerning issue has\nrenewed the efforts in developing protection mechanisms based on adversarial\nattacks, which generate effective perturbations to poison diffusion models. Our\nwork is motivated by the observation that these models exhibit a high degree of\nabstraction within their semantic latent space (`h-space'), which encodes\ncritical high-level features for generating coherent and meaningful content. In\nthis paper, we propose a novel anti-customization approach, called HAAD\n(h-space based Adversarial Attack for Diffusion models), that leverages\nadversarial attacks to craft perturbations based on the h-space that can\nefficiently degrade the image generation process. Building upon HAAD, we\nfurther introduce a more efficient variant, HAAD-KV, that constructs\nperturbations solely based on the KV parameters of the h-space. This strategy\noffers a stronger protection, that is computationally less expensive. Despite\ntheir simplicity, our methods outperform state-of-the-art adversarial attacks,\nhighlighting their effectiveness."
                },
                "authors": [
                    {
                        "name": "Xide Xu"
                    },
                    {
                        "name": "Sandesh Kamath"
                    },
                    {
                        "name": "Muhammad Atif Butt"
                    },
                    {
                        "name": "Bogdan Raducanu"
                    }
                ],
                "author_detail": {
                    "name": "Bogdan Raducanu"
                },
                "author": "Bogdan Raducanu",
                "arxiv_comment": "32 pages, 15 figures. Accepted by ACM Multimedia 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23956v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23956v3",
                "updated": "2025-07-23T11:42:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    11,
                    42,
                    3,
                    2,
                    204,
                    0
                ],
                "published": "2025-03-31T11:13:18Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    13,
                    18,
                    0,
                    90,
                    0
                ],
                "title": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference"
                },
                "summary": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches."
                },
                "authors": [
                    {
                        "name": "Kai Huang"
                    },
                    {
                        "name": "Hao Zou"
                    },
                    {
                        "name": "Bochen Wang"
                    },
                    {
                        "name": "Ye Xi"
                    },
                    {
                        "name": "Zhen Xie"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23956v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23956v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17411v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17411v1",
                "updated": "2025-07-23T11:12:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    11,
                    12,
                    8,
                    2,
                    204,
                    0
                ],
                "published": "2025-07-23T11:12:08Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    11,
                    12,
                    8,
                    2,
                    204,
                    0
                ],
                "title": "Multiprocessor Scheduling with Memory Constraints: Fundamental\n  Properties and Finding Optimal Solutions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiprocessor Scheduling with Memory Constraints: Fundamental\n  Properties and Finding Optimal Solutions"
                },
                "summary": "We study the problem of scheduling a general computational DAG on multiple\nprocessors in a 2-level memory hierarchy. This setting is a natural\ngeneralization of several prominent models in the literature, and it\nsimultaneously captures workload balancing, communication, and data movement\ndue to cache size limitations. We first analyze the fundamental properties of\nthis problem from a theoretical perspective, such as its computational\ncomplexity. We also prove that optimizing parallelization and memory management\nseparately, as done in many applications, can result in a solution that is a\nlinear factor away from the optimum.\n  On the algorithmic side, we discuss a natural technique to represent and\nsolve the problem as an Integer Linear Program (ILP). We develop a holistic\nscheduling algorithm based on this approach, and we experimentally study its\nperformance and properties on a small benchmark of computational tasks. Our\nresults confirm that the ILP-based method can indeed find considerably better\nsolutions than a baseline which combines classical scheduling algorithms and\nmemory management policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the problem of scheduling a general computational DAG on multiple\nprocessors in a 2-level memory hierarchy. This setting is a natural\ngeneralization of several prominent models in the literature, and it\nsimultaneously captures workload balancing, communication, and data movement\ndue to cache size limitations. We first analyze the fundamental properties of\nthis problem from a theoretical perspective, such as its computational\ncomplexity. We also prove that optimizing parallelization and memory management\nseparately, as done in many applications, can result in a solution that is a\nlinear factor away from the optimum.\n  On the algorithmic side, we discuss a natural technique to represent and\nsolve the problem as an Integer Linear Program (ILP). We develop a holistic\nscheduling algorithm based on this approach, and we experimentally study its\nperformance and properties on a small benchmark of computational tasks. Our\nresults confirm that the ILP-based method can indeed find considerably better\nsolutions than a baseline which combines classical scheduling algorithms and\nmemory management policies."
                },
                "authors": [
                    {
                        "name": "Pl Andrs Papp"
                    },
                    {
                        "name": "Toni Bhnlein"
                    },
                    {
                        "name": "A. N. Yzelman"
                    }
                ],
                "author_detail": {
                    "name": "A. N. Yzelman"
                },
                "author": "A. N. Yzelman",
                "arxiv_doi": "10.1145/3754598.3754676",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3754598.3754676",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.17411v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17411v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in the 54th International Conference on Parallel Processing\n  (ICPP 2025)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "90B35, 90C10, 68Q10, 68W10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13373v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13373v2",
                "updated": "2025-07-23T10:10:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    10,
                    10,
                    53,
                    2,
                    204,
                    0
                ],
                "published": "2024-11-20T14:52:36Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    14,
                    52,
                    36,
                    2,
                    325,
                    0
                ],
                "title": "Upgrade of the Diagnostic Neutral Beam Injector for the RFX-mod2\n  experiment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Upgrade of the Diagnostic Neutral Beam Injector for the RFX-mod2\n  experiment"
                },
                "summary": "Diagnostic Neutral Beam Injectors (DNBI), through the combined use of Charge\nExchange Recombination Spectroscopy (CHERS) and Motional Stark effect\ndiagnostics (MSE), are a well-known tool to access important information about\nmagnetically confined plasmas, such as radial profiles of ion temperature, ion\nflow, impurity content and intensity and direction of the magnetic field. For\nthis purpose, a DNBI was installed and operated in the RFX-mod experiment,\nwhich was designed to confine plasma mainly through the Reversed Field Pinch\nconfiguration. The DNBI, designed and built by the Budker Institute of Nuclear\nPhysics (BINP), was based on a source of positive hydrogen ions, accelerated to\n50 keV and for a maximum ion current of 5 A. The beam could be modulated and\nthe maximum overall duration was 50 ms. With the upgrade of RFX-mod to the\npresent RFX-mod2 machine, the DNBI is being renovated to solve several power\nunits faults and improve the overall reliability of the system. The 50 kV power\nsupply is being improved, as well as the power supplies in the high voltage\ndeck and its insulation transformer. Magnetic field survival tests were\nperformed on the toroidal-core-based DC-DC converters that should power the\nelectronic boards in a reliable way. The control system, originally based on\nCAMAC technology, was redesigned to be fully replaced. This contribution\nreviews the technical criticalities emerged in the DNBI check-up and the new\nsolutions adopted to make the DNBI operative and more reliable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diagnostic Neutral Beam Injectors (DNBI), through the combined use of Charge\nExchange Recombination Spectroscopy (CHERS) and Motional Stark effect\ndiagnostics (MSE), are a well-known tool to access important information about\nmagnetically confined plasmas, such as radial profiles of ion temperature, ion\nflow, impurity content and intensity and direction of the magnetic field. For\nthis purpose, a DNBI was installed and operated in the RFX-mod experiment,\nwhich was designed to confine plasma mainly through the Reversed Field Pinch\nconfiguration. The DNBI, designed and built by the Budker Institute of Nuclear\nPhysics (BINP), was based on a source of positive hydrogen ions, accelerated to\n50 keV and for a maximum ion current of 5 A. The beam could be modulated and\nthe maximum overall duration was 50 ms. With the upgrade of RFX-mod to the\npresent RFX-mod2 machine, the DNBI is being renovated to solve several power\nunits faults and improve the overall reliability of the system. The 50 kV power\nsupply is being improved, as well as the power supplies in the high voltage\ndeck and its insulation transformer. Magnetic field survival tests were\nperformed on the toroidal-core-based DC-DC converters that should power the\nelectronic boards in a reliable way. The control system, originally based on\nCAMAC technology, was redesigned to be fully replaced. This contribution\nreviews the technical criticalities emerged in the DNBI check-up and the new\nsolutions adopted to make the DNBI operative and more reliable."
                },
                "authors": [
                    {
                        "name": "Marco Barbisan"
                    },
                    {
                        "name": "Marco Boldrin"
                    },
                    {
                        "name": "Luca Cinnirella"
                    },
                    {
                        "name": "Bruno Laterza"
                    },
                    {
                        "name": "Alberto Maistrello"
                    },
                    {
                        "name": "Lionello Marrelli"
                    },
                    {
                        "name": "Federico Molon"
                    },
                    {
                        "name": "Simone Peruzzo"
                    },
                    {
                        "name": "Cesare Taliercio"
                    },
                    {
                        "name": "Marco Valisa"
                    },
                    {
                        "name": "Enrico Zampiva"
                    }
                ],
                "author_detail": {
                    "name": "Enrico Zampiva"
                },
                "author": "Enrico Zampiva",
                "arxiv_doi": "10.1016/j.fusengdes.2025.115320",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.fusengdes.2025.115320",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.13373v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13373v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages (excl. highlights), 8 figures. Contribution to the 33rd\n  Symposium on Fusion Technology (SOFT), 22-27 September 2024. This is the\n  accepted manuscript for the \"Fusion Engineering and Design\" journal",
                "arxiv_journal_ref": "Fusion Engineering and Design, Volume 220, November 2025, 115320",
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16391v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16391v2",
                "updated": "2025-07-23T09:31:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    9,
                    31,
                    1,
                    2,
                    204,
                    0
                ],
                "published": "2025-07-22T09:35:59Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    9,
                    35,
                    59,
                    1,
                    203,
                    0
                ],
                "title": "Ironman: Accelerating Oblivious Transfer Extension for\n  Privacy-Preserving AI with Near-Memory Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ironman: Accelerating Oblivious Transfer Extension for\n  Privacy-Preserving AI with Near-Memory Processing"
                },
                "summary": "With the wide application of machine learning (ML), privacy concerns arise\nwith user data as they may contain sensitive information. Privacy-preserving ML\n(PPML) based on cryptographic primitives has emerged as a promising solution in\nwhich an ML model is directly computed on the encrypted data to provide a\nformal privacy guarantee. However, PPML frameworks heavily rely on the\noblivious transfer (OT) primitive to compute nonlinear functions. OT mainly\ninvolves the computation of single-point correlated OT (SPCOT) and learning\nparity with noise (LPN) operations. As OT is still computed extensively on\ngeneral-purpose CPUs, it becomes the latency bottleneck of modern PPML\nframeworks.\n  In this paper, we propose a novel OT accelerator, dubbed Ironman, to\nsignificantly increase the efficiency of OT and the overall PPML framework. We\nobserve that SPCOT is computation-bounded, and thus propose a hardware-friendly\nSPCOT algorithm with a customized accelerator to improve SPCOT computation\nthroughput. In contrast, LPN is memory-bandwidth-bounded due to irregular\nmemory access patterns. Hence, we further leverage the near-memory processing\n(NMP) architecture equipped with memory-side cache and index sorting to improve\neffective memory bandwidth. With extensive experiments, we demonstrate Ironman\nachieves a 39.2-237.4 times improvement in OT throughput across different NMP\nconfigurations compared to the full-thread CPU implementation. For different\nPPML frameworks, Ironman demonstrates a 2.1-3.4 times reduction in end-to-end\nlatency for both CNN and Transformer models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the wide application of machine learning (ML), privacy concerns arise\nwith user data as they may contain sensitive information. Privacy-preserving ML\n(PPML) based on cryptographic primitives has emerged as a promising solution in\nwhich an ML model is directly computed on the encrypted data to provide a\nformal privacy guarantee. However, PPML frameworks heavily rely on the\noblivious transfer (OT) primitive to compute nonlinear functions. OT mainly\ninvolves the computation of single-point correlated OT (SPCOT) and learning\nparity with noise (LPN) operations. As OT is still computed extensively on\ngeneral-purpose CPUs, it becomes the latency bottleneck of modern PPML\nframeworks.\n  In this paper, we propose a novel OT accelerator, dubbed Ironman, to\nsignificantly increase the efficiency of OT and the overall PPML framework. We\nobserve that SPCOT is computation-bounded, and thus propose a hardware-friendly\nSPCOT algorithm with a customized accelerator to improve SPCOT computation\nthroughput. In contrast, LPN is memory-bandwidth-bounded due to irregular\nmemory access patterns. Hence, we further leverage the near-memory processing\n(NMP) architecture equipped with memory-side cache and index sorting to improve\neffective memory bandwidth. With extensive experiments, we demonstrate Ironman\nachieves a 39.2-237.4 times improvement in OT throughput across different NMP\nconfigurations compared to the full-thread CPU implementation. For different\nPPML frameworks, Ironman demonstrates a 2.1-3.4 times reduction in end-to-end\nlatency for both CNN and Transformer models."
                },
                "authors": [
                    {
                        "name": "Chenqi Lin"
                    },
                    {
                        "name": "Kang Yang"
                    },
                    {
                        "name": "Tianshi Xu"
                    },
                    {
                        "name": "Ling Liang"
                    },
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Zhaohui Chen"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Mingyu Gao"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16391v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16391v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02634v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02634v4",
                "updated": "2025-07-23T08:07:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    8,
                    7,
                    19,
                    2,
                    204,
                    0
                ],
                "published": "2025-06-03T08:51:38Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    8,
                    51,
                    38,
                    1,
                    154,
                    0
                ],
                "title": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache\n  at a Large Cloud Provider",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache\n  at a Large Cloud Provider"
                },
                "summary": "Serving large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity."
                },
                "authors": [
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Jinbo Han"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Sijie Shen"
                    },
                    {
                        "name": "Dingyan Zhang"
                    },
                    {
                        "name": "Chenguang Fang"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "Accepted by USENIX ATC'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02634v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02634v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17286v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17286v2",
                "updated": "2025-07-23T05:57:32Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    5,
                    57,
                    32,
                    2,
                    204,
                    0
                ],
                "published": "2025-06-15T07:19:33Z",
                "published_parsed": [
                    2025,
                    6,
                    15,
                    7,
                    19,
                    33,
                    6,
                    166,
                    0
                ],
                "title": "GTA: Grouped-head latenT Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GTA: Grouped-head latenT Attention"
                },
                "summary": "Attention mechanisms underpin the success of large language models (LLMs),\nyet their substantial computational and memory overhead poses challenges for\noptimizing efficiency and performance. A critical bottleneck arises as KV cache\nand attention computations scale rapidly with text length, challenging\ndeployment on hardware with limited computational and memory resources. We\nobserve that attention mechanisms exhibit substantial redundancy, since the KV\ncache can be significantly compressed and attention maps across heads display\nhigh similarity, revealing that much of the computation and storage is\nunnecessary. Leveraging these insights, we propose \\textbf{G}rouped-Head\nLaten\\textbf{T} \\textbf{A}ttention (GTA), a novel attention mechanism that\nreduces memory usage and computational complexity while maintaining\nperformance. GTA comprises two components: (1) a shared attention map mechanism\nthat reuses attention scores across multiple heads, decreasing the key cache\nsize; and (2) a nonlinear value decoder with learned projections that\ncompresses the value cache into a latent space, further cutting memory needs.\nGTA cuts attention computation FLOPs by up to \\emph{62.5\\%} versus\nGrouped-Query Attention and shrink the KV cache by up to \\emph{70\\%}, all while\navoiding the extra overhead of Multi-Head Latent Attention to improve LLM\ndeployment efficiency. Consequently, GTA models achieve a \\emph{2x} increase in\nend-to-end inference speed, with prefill benefiting from reduced computational\ncost and decoding benefiting from the smaller cache footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention mechanisms underpin the success of large language models (LLMs),\nyet their substantial computational and memory overhead poses challenges for\noptimizing efficiency and performance. A critical bottleneck arises as KV cache\nand attention computations scale rapidly with text length, challenging\ndeployment on hardware with limited computational and memory resources. We\nobserve that attention mechanisms exhibit substantial redundancy, since the KV\ncache can be significantly compressed and attention maps across heads display\nhigh similarity, revealing that much of the computation and storage is\nunnecessary. Leveraging these insights, we propose \\textbf{G}rouped-Head\nLaten\\textbf{T} \\textbf{A}ttention (GTA), a novel attention mechanism that\nreduces memory usage and computational complexity while maintaining\nperformance. GTA comprises two components: (1) a shared attention map mechanism\nthat reuses attention scores across multiple heads, decreasing the key cache\nsize; and (2) a nonlinear value decoder with learned projections that\ncompresses the value cache into a latent space, further cutting memory needs.\nGTA cuts attention computation FLOPs by up to \\emph{62.5\\%} versus\nGrouped-Query Attention and shrink the KV cache by up to \\emph{70\\%}, all while\navoiding the extra overhead of Multi-Head Latent Attention to improve LLM\ndeployment efficiency. Consequently, GTA models achieve a \\emph{2x} increase in\nend-to-end inference speed, with prefill benefiting from reduced computational\ncost and decoding benefiting from the smaller cache footprint."
                },
                "authors": [
                    {
                        "name": "Luoyang Sun"
                    },
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Jiwen Jiang"
                    },
                    {
                        "name": "Xinjian Wu"
                    },
                    {
                        "name": "Haifeng Zhang"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Lionel Ni"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17286v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17286v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11046v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11046v2",
                "updated": "2025-07-23T01:42:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    1,
                    42,
                    19,
                    2,
                    204,
                    0
                ],
                "published": "2025-02-16T09:08:36Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    9,
                    8,
                    36,
                    6,
                    47,
                    0
                ],
                "title": "Enabling Efficient Transaction Processing on CXL-Based Memory Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Efficient Transaction Processing on CXL-Based Memory Sharing"
                },
                "summary": "Transaction processing systems are the crux for modern data-center\napplications, yet current multi-node systems are slow due to network overheads.\nThis paper advocates for Compute Express Link (CXL) as a network alternative,\nwhich enables low-latency and cache-coherent shared memory accesses. However,\ndirectly adopting standard CXL primitives leads to performance degradation due\nto the high cost of maintaining cross-node cache coherence. To address the CXL\nchallenges, this paper introduces CtXnL, a software-hardware co-designed system\nthat implements a novel hybrid coherence primitive tailored to the loosely\ncoherent nature of transactional data. The core innovation of CtXnL is\nempowering transaction system developers with the ability to selectively\nachieve data coherence. Our evaluations on OLTP workloads demonstrate that\nCtXnL enhances performance, outperforming current network-based systems and\nachieves with up to 2.08x greater throughput than vanilla CXL memory sharing\narchitectures across universal transaction processing policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transaction processing systems are the crux for modern data-center\napplications, yet current multi-node systems are slow due to network overheads.\nThis paper advocates for Compute Express Link (CXL) as a network alternative,\nwhich enables low-latency and cache-coherent shared memory accesses. However,\ndirectly adopting standard CXL primitives leads to performance degradation due\nto the high cost of maintaining cross-node cache coherence. To address the CXL\nchallenges, this paper introduces CtXnL, a software-hardware co-designed system\nthat implements a novel hybrid coherence primitive tailored to the loosely\ncoherent nature of transactional data. The core innovation of CtXnL is\nempowering transaction system developers with the ability to selectively\nachieve data coherence. Our evaluations on OLTP workloads demonstrate that\nCtXnL enhances performance, outperforming current network-based systems and\nachieves with up to 2.08x greater throughput than vanilla CXL memory sharing\narchitectures across universal transaction processing policies."
                },
                "authors": [
                    {
                        "name": "Zhao Wang"
                    },
                    {
                        "name": "Yiqi Chen"
                    },
                    {
                        "name": "Cong Li"
                    },
                    {
                        "name": "Dimin Niu"
                    },
                    {
                        "name": "Tianchan Guan"
                    },
                    {
                        "name": "Zhaoyang Du"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11046v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11046v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17033v1",
                "updated": "2025-07-22T21:41:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    21,
                    41,
                    43,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T21:41:43Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    21,
                    41,
                    43,
                    1,
                    203,
                    0
                ],
                "title": "GATEBLEED: Exploiting On-Core Accelerator Power Gating for High\n  Performance & Stealthy Attacks on AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GATEBLEED: Exploiting On-Core Accelerator Power Gating for High\n  Performance & Stealthy Attacks on AI"
                },
                "summary": "As power consumption from AI training and inference continues to increase, AI\naccelerators are being integrated directly into the CPU. Intel's Advanced\nMatrix Extensions (AMX) is one such example, debuting on the 4th generation\nIntel Xeon Scalable CPU. We discover a timing side and covert channel,\nGATEBLEED, caused by the aggressive power gating utilized to keep the CPU\nwithin operating limits. We show that the GATEBLEED side channel is a threat to\nAI privacy as many ML models such as transformers and CNNs make critical\ncomputationally-heavy decisions based on private values like confidence\nthresholds and routing logits. Timing delays from selective powering down of\nAMX components mean that each matrix multiplication is a potential leakage\npoint when executed on the AMX accelerator. Our research identifies over a\ndozen potential gadgets across popular ML libraries (HuggingFace, PyTorch,\nTensorFlow, etc.), revealing that they can leak sensitive and private\ninformation. GATEBLEED poses a risk for local and remote timing inference, even\nunder previous protective measures. GATEBLEED can be used as a high\nperformance, stealthy remote covert channel and a generic magnifier for timing\ntransmission channels, capable of bypassing traditional cache defenses to leak\narbitrary memory addresses and evading state of the art microarchitectural\nattack detectors under realistic network conditions and system configurations\nin which previous attacks fail. We implement an end-to-end microarchitectural\ninference attack on a transformer model optimized with Intel AMX, achieving a\nmembership inference accuracy of 81% and a precision of 0.89. In a CNN-based or\ntransformer-based mixture-of-experts model optimized with Intel AMX, we leak\nexpert choice with 100% accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As power consumption from AI training and inference continues to increase, AI\naccelerators are being integrated directly into the CPU. Intel's Advanced\nMatrix Extensions (AMX) is one such example, debuting on the 4th generation\nIntel Xeon Scalable CPU. We discover a timing side and covert channel,\nGATEBLEED, caused by the aggressive power gating utilized to keep the CPU\nwithin operating limits. We show that the GATEBLEED side channel is a threat to\nAI privacy as many ML models such as transformers and CNNs make critical\ncomputationally-heavy decisions based on private values like confidence\nthresholds and routing logits. Timing delays from selective powering down of\nAMX components mean that each matrix multiplication is a potential leakage\npoint when executed on the AMX accelerator. Our research identifies over a\ndozen potential gadgets across popular ML libraries (HuggingFace, PyTorch,\nTensorFlow, etc.), revealing that they can leak sensitive and private\ninformation. GATEBLEED poses a risk for local and remote timing inference, even\nunder previous protective measures. GATEBLEED can be used as a high\nperformance, stealthy remote covert channel and a generic magnifier for timing\ntransmission channels, capable of bypassing traditional cache defenses to leak\narbitrary memory addresses and evading state of the art microarchitectural\nattack detectors under realistic network conditions and system configurations\nin which previous attacks fail. We implement an end-to-end microarchitectural\ninference attack on a transformer model optimized with Intel AMX, achieving a\nmembership inference accuracy of 81% and a precision of 0.89. In a CNN-based or\ntransformer-based mixture-of-experts model optimized with Intel AMX, we leak\nexpert choice with 100% accuracy."
                },
                "authors": [
                    {
                        "name": "Joshua Kalyanapu"
                    },
                    {
                        "name": "Farshad Dizani"
                    },
                    {
                        "name": "Darsh Asher"
                    },
                    {
                        "name": "Azam Ghanbari"
                    },
                    {
                        "name": "Rosario Cammarota"
                    },
                    {
                        "name": "Aydin Aysu"
                    },
                    {
                        "name": "Samira Mirbagher Ajorpaz"
                    }
                ],
                "author_detail": {
                    "name": "Samira Mirbagher Ajorpaz"
                },
                "author": "Samira Mirbagher Ajorpaz",
                "arxiv_comment": "Accepted at MICRO 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17029v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17029v1",
                "updated": "2025-07-22T21:33:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    21,
                    33,
                    30,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T21:33:30Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    21,
                    33,
                    30,
                    1,
                    203,
                    0
                ],
                "title": "StreamME: Simplify 3D Gaussian Avatar within Live Stream",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamME: Simplify 3D Gaussian Avatar within Live Stream"
                },
                "summary": "We propose StreamME, a method focuses on fast 3D avatar reconstruction. The\nStreamME synchronously records and reconstructs a head avatar from live video\nstreams without any pre-cached data, enabling seamless integration of the\nreconstructed appearance into downstream applications. This exceptionally fast\ntraining strategy, which we refer to as on-the-fly training, is central to our\napproach. Our method is built upon 3D Gaussian Splatting (3DGS), eliminating\nthe reliance on MLPs in deformable 3DGS and relying solely on geometry, which\nsignificantly improves the adaptation speed to facial expression. To further\nensure high efficiency in on-the-fly training, we introduced a simplification\nstrategy based on primary points, which distributes the point clouds more\nsparsely across the facial surface, optimizing points number while maintaining\nrendering quality. Leveraging the on-the-fly training capabilities, our method\nprotects the facial privacy and reduces communication bandwidth in VR system or\nonline conference. Additionally, it can be directly applied to downstream\napplication such as animation, toonify, and relighting. Please refer to our\nproject page for more details: https://songluchuan.github.io/StreamME/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose StreamME, a method focuses on fast 3D avatar reconstruction. The\nStreamME synchronously records and reconstructs a head avatar from live video\nstreams without any pre-cached data, enabling seamless integration of the\nreconstructed appearance into downstream applications. This exceptionally fast\ntraining strategy, which we refer to as on-the-fly training, is central to our\napproach. Our method is built upon 3D Gaussian Splatting (3DGS), eliminating\nthe reliance on MLPs in deformable 3DGS and relying solely on geometry, which\nsignificantly improves the adaptation speed to facial expression. To further\nensure high efficiency in on-the-fly training, we introduced a simplification\nstrategy based on primary points, which distributes the point clouds more\nsparsely across the facial surface, optimizing points number while maintaining\nrendering quality. Leveraging the on-the-fly training capabilities, our method\nprotects the facial privacy and reduces communication bandwidth in VR system or\nonline conference. Additionally, it can be directly applied to downstream\napplication such as animation, toonify, and relighting. Please refer to our\nproject page for more details: https://songluchuan.github.io/StreamME/."
                },
                "authors": [
                    {
                        "name": "Luchuan Song"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Zhan Xu"
                    },
                    {
                        "name": "Yi Zhou"
                    },
                    {
                        "name": "Deepali Aneja"
                    },
                    {
                        "name": "Chenliang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chenliang Xu"
                },
                "author": "Chenliang Xu",
                "arxiv_comment": "12 pages, 15 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17029v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17029v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16933v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16933v1",
                "updated": "2025-07-22T18:17:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    18,
                    17,
                    53,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T18:17:53Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    18,
                    17,
                    53,
                    1,
                    203,
                    0
                ],
                "title": "SiLQ: Simple Large Language Model Quantization-Aware Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SiLQ: Simple Large Language Model Quantization-Aware Training"
                },
                "summary": "Large language models can be quantized to reduce inference time latency,\nmodel size, and energy consumption, thereby delivering a better user experience\nat lower cost. A challenge exists to deliver quantized models with minimal loss\nof accuracy in reasonable time, and in particular to do so without requiring\nmechanisms incompatible with specialized inference accelerators. Here, we\ndemonstrate a simple, end-to-end quantization-aware training approach that,\nwith an increase in total model training budget of less than 0.1%, outperforms\nthe leading published quantization methods by large margins on several modern\nbenchmarks, with both base and instruct model variants. The approach easily\ngeneralizes across different model architectures, can be applied to\nactivations, cache, and weights, and requires the introduction of no additional\noperations to the model other than the quantization itself.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models can be quantized to reduce inference time latency,\nmodel size, and energy consumption, thereby delivering a better user experience\nat lower cost. A challenge exists to deliver quantized models with minimal loss\nof accuracy in reasonable time, and in particular to do so without requiring\nmechanisms incompatible with specialized inference accelerators. Here, we\ndemonstrate a simple, end-to-end quantization-aware training approach that,\nwith an increase in total model training budget of less than 0.1%, outperforms\nthe leading published quantization methods by large margins on several modern\nbenchmarks, with both base and instruct model variants. The approach easily\ngeneralizes across different model architectures, can be applied to\nactivations, cache, and weights, and requires the introduction of no additional\noperations to the model other than the quantization itself."
                },
                "authors": [
                    {
                        "name": "Steven K. Esser"
                    },
                    {
                        "name": "Jeffrey L. McKinstry"
                    },
                    {
                        "name": "Deepika Bablani"
                    },
                    {
                        "name": "Rathinakumar Appuswamy"
                    },
                    {
                        "name": "Dharmendra S. Modha"
                    }
                ],
                "author_detail": {
                    "name": "Dharmendra S. Modha"
                },
                "author": "Dharmendra S. Modha",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16933v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16933v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16784v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16784v1",
                "updated": "2025-07-22T17:30:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    17,
                    30,
                    4,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T17:30:04Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    17,
                    30,
                    4,
                    1,
                    203,
                    0
                ],
                "title": "Beyond Context Limits: Subconscious Threads for Long-Horizon Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Context Limits: Subconscious Threads for Long-Horizon Reasoning"
                },
                "summary": "To break the context limits of large language models (LLMs) that bottleneck\nreasoning accuracy and efficiency, we propose the Thread Inference Model (TIM),\na family of LLMs trained for recursive and decompositional problem solving, and\nTIMRUN, an inference runtime enabling long-horizon structured reasoning beyond\ncontext limits. Together, TIM hosted on TIMRUN supports virtually unlimited\nworking memory and multi-hop tool calls within a single language model\ninference, overcoming output limits, positional-embedding constraints, and\nGPU-memory bottlenecks. Performance is achieved by modeling natural language as\nreasoning trees measured by both length and depth instead of linear sequences.\nThe reasoning trees consist of tasks with thoughts, recursive subtasks, and\nconclusions based on the concept we proposed in Schroeder et al, 2025. During\ngeneration, we maintain a working memory that retains only the key-value states\nof the most relevant context tokens, selected by a rule-based subtask-pruning\nmechanism, enabling reuse of positional embeddings and GPU memory pages\nthroughout reasoning. Experimental results show that our system sustains high\ninference throughput, even when manipulating up to 90% of the KV cache in GPU\nmemory. It also delivers accurate reasoning on mathematical tasks and handles\ninformation retrieval challenges that require long-horizon reasoning and\nmulti-hop tool use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To break the context limits of large language models (LLMs) that bottleneck\nreasoning accuracy and efficiency, we propose the Thread Inference Model (TIM),\na family of LLMs trained for recursive and decompositional problem solving, and\nTIMRUN, an inference runtime enabling long-horizon structured reasoning beyond\ncontext limits. Together, TIM hosted on TIMRUN supports virtually unlimited\nworking memory and multi-hop tool calls within a single language model\ninference, overcoming output limits, positional-embedding constraints, and\nGPU-memory bottlenecks. Performance is achieved by modeling natural language as\nreasoning trees measured by both length and depth instead of linear sequences.\nThe reasoning trees consist of tasks with thoughts, recursive subtasks, and\nconclusions based on the concept we proposed in Schroeder et al, 2025. During\ngeneration, we maintain a working memory that retains only the key-value states\nof the most relevant context tokens, selected by a rule-based subtask-pruning\nmechanism, enabling reuse of positional embeddings and GPU memory pages\nthroughout reasoning. Experimental results show that our system sustains high\ninference throughput, even when manipulating up to 90% of the KV cache in GPU\nmemory. It also delivers accurate reasoning on mathematical tasks and handles\ninformation retrieval challenges that require long-horizon reasoning and\nmulti-hop tool use."
                },
                "authors": [
                    {
                        "name": "Hongyin Luo"
                    },
                    {
                        "name": "Nathaniel Morgan"
                    },
                    {
                        "name": "Tina Li"
                    },
                    {
                        "name": "Derek Zhao"
                    },
                    {
                        "name": "Ai Vy Ngo"
                    },
                    {
                        "name": "Philip Schroeder"
                    },
                    {
                        "name": "Lijie Yang"
                    },
                    {
                        "name": "Assaf Ben-Kish"
                    },
                    {
                        "name": "Jack O'Brien"
                    },
                    {
                        "name": "James Glass"
                    }
                ],
                "author_detail": {
                    "name": "James Glass"
                },
                "author": "James Glass",
                "arxiv_comment": "Research preview",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16784v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16784v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16768v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16768v1",
                "updated": "2025-07-22T17:13:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    17,
                    13,
                    47,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T17:13:47Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    17,
                    13,
                    47,
                    1,
                    203,
                    0
                ],
                "title": "WGRAMMAR: Leverage Prior Knowledge to Accelerate Structured Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WGRAMMAR: Leverage Prior Knowledge to Accelerate Structured Decoding"
                },
                "summary": "Structured decoding enables large language models (LLMs) to generate outputs\nin formats required by downstream systems, such as HTML or JSON. However,\nexisting methods suffer from efficiency bottlenecks due to grammar compilation,\nstate tracking, and mask creation. We observe that many real-world tasks embed\nstrong prior knowledge about output structure. Leveraging this, we propose a\ndecomposition of constraints into static and dynamic components -- precompiling\nstatic structures offline and instantiating dynamic arguments at runtime using\ngrammar snippets. Instead of relying on pushdown automata, we employ a\ncompositional set of operators to model regular formats, achieving lower\ntransition latency. We introduce wgrammar, a lightweight decoding engine that\nintegrates domain-aware simplification, constraint decomposition, and mask\ncaching, achieving up to 250x speedup over existing systems. wgrammar's source\ncode is publicly available at https://github.com/wrran/wgrammar.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured decoding enables large language models (LLMs) to generate outputs\nin formats required by downstream systems, such as HTML or JSON. However,\nexisting methods suffer from efficiency bottlenecks due to grammar compilation,\nstate tracking, and mask creation. We observe that many real-world tasks embed\nstrong prior knowledge about output structure. Leveraging this, we propose a\ndecomposition of constraints into static and dynamic components -- precompiling\nstatic structures offline and instantiating dynamic arguments at runtime using\ngrammar snippets. Instead of relying on pushdown automata, we employ a\ncompositional set of operators to model regular formats, achieving lower\ntransition latency. We introduce wgrammar, a lightweight decoding engine that\nintegrates domain-aware simplification, constraint decomposition, and mask\ncaching, achieving up to 250x speedup over existing systems. wgrammar's source\ncode is publicly available at https://github.com/wrran/wgrammar."
                },
                "authors": [
                    {
                        "name": "Ran Wang"
                    },
                    {
                        "name": "Xiaoxuan Liu"
                    },
                    {
                        "name": "Hao Ren"
                    },
                    {
                        "name": "Gang Chen"
                    },
                    {
                        "name": "Fanchao Qi"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16768v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16768v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2212.10131v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2212.10131v3",
                "updated": "2025-07-22T16:49:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    16,
                    49,
                    24,
                    1,
                    203,
                    0
                ],
                "published": "2022-12-20T09:58:39Z",
                "published_parsed": [
                    2022,
                    12,
                    20,
                    9,
                    58,
                    39,
                    1,
                    354,
                    0
                ],
                "title": "Hydra: Virtualized Multi-Language Runtime for High-Density Serverless\n  Platforms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hydra: Virtualized Multi-Language Runtime for High-Density Serverless\n  Platforms"
                },
                "summary": "Serverless is an attractive computing model that offers seamless scalability\nand elasticity; it takes the infrastructure management burden away from users\nand enables a pay-as-you-use billing model. As a result, serverless is becoming\nincreasingly popular to support highly elastic and bursty workloads. However,\nexisting platforms are supported by bloated virtualization stacks, which,\ncombined with bursty and irregular invocations, lead to high memory and latency\noverheads.\n  To reduce the virtualization stack bloat, we propose Hydra, a virtualized\nmulti-language runtime and platform capable of hosting multiple sandboxes\nrunning concurrently. To fully leverage Hydra's virtualized runtime, we revisit\nthe existing serverless platform design to make it colocation-aware across\nowners and functions, and to feature a caching layer of pre-allocated Hydra\ninstances that can be used by different functions written in different\nlanguages to reduce cold starts. We also propose a snapshotting mechanism to\ncheckpoint and restore individual sandboxes.\n  By consolidating multiple serverless function invocations through Hydra, we\nimprove the overall function density (ops/GB-sec) by 2.41x on average compared\nto OpenWhisk runtimes, the state-of-the-art single-language runtimes used in\nmost serverless platforms, and by 1.43x on average compared to Knative runtimes\nsupporting invocation colocation within the same function. When reproducing the\nAzure Functions trace, our serverless platform operating Hydra instances\nreduces the overall memory footprint by 21.3-43.9% compared to operating\nOpenWhisk instances and by 14.5-30% compared to operating Knative instances.\nHydra eliminates cold starts thanks to the pool of pre-warmed runtime\ninstances, reducing p99 latency by 45.3-375.5x compared to OpenWhisk and by\n1.9-51.4x compared to Knative.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless is an attractive computing model that offers seamless scalability\nand elasticity; it takes the infrastructure management burden away from users\nand enables a pay-as-you-use billing model. As a result, serverless is becoming\nincreasingly popular to support highly elastic and bursty workloads. However,\nexisting platforms are supported by bloated virtualization stacks, which,\ncombined with bursty and irregular invocations, lead to high memory and latency\noverheads.\n  To reduce the virtualization stack bloat, we propose Hydra, a virtualized\nmulti-language runtime and platform capable of hosting multiple sandboxes\nrunning concurrently. To fully leverage Hydra's virtualized runtime, we revisit\nthe existing serverless platform design to make it colocation-aware across\nowners and functions, and to feature a caching layer of pre-allocated Hydra\ninstances that can be used by different functions written in different\nlanguages to reduce cold starts. We also propose a snapshotting mechanism to\ncheckpoint and restore individual sandboxes.\n  By consolidating multiple serverless function invocations through Hydra, we\nimprove the overall function density (ops/GB-sec) by 2.41x on average compared\nto OpenWhisk runtimes, the state-of-the-art single-language runtimes used in\nmost serverless platforms, and by 1.43x on average compared to Knative runtimes\nsupporting invocation colocation within the same function. When reproducing the\nAzure Functions trace, our serverless platform operating Hydra instances\nreduces the overall memory footprint by 21.3-43.9% compared to operating\nOpenWhisk instances and by 14.5-30% compared to operating Knative instances.\nHydra eliminates cold starts thanks to the pool of pre-warmed runtime\ninstances, reducing p99 latency by 45.3-375.5x compared to OpenWhisk and by\n1.9-51.4x compared to Knative."
                },
                "authors": [
                    {
                        "name": "Serhii Ivanenko"
                    },
                    {
                        "name": "Vasyl Lanko"
                    },
                    {
                        "name": "Rudi Horn"
                    },
                    {
                        "name": "Vojin Jovanovic"
                    },
                    {
                        "name": "Rodrigo Bruno"
                    }
                ],
                "author_detail": {
                    "name": "Rodrigo Bruno"
                },
                "author": "Rodrigo Bruno",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2212.10131v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2212.10131v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16243v1",
                "updated": "2025-07-22T05:34:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    5,
                    34,
                    3,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T05:34:03Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    5,
                    34,
                    3,
                    1,
                    203,
                    0
                ],
                "title": "Genus Zero Kashiwara-Vergne Solutions from Braids",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Genus Zero Kashiwara-Vergne Solutions from Braids"
                },
                "summary": "Using the language of moperads-monoids in the category of right modules over\nan operad-we reinterpret the Alekseev-Enriquez-Torossian construction of\nKashiwara-Vergne (KV) solutions from associators. We show that any isomorphism\nbetween the moperad of parenthesized braids with a frozen strand and the\nmoperad of chord diagrams gives rise to a family of genus zero KV solutions\noperadically generated by a single classical KV solution. We show that the\nGrothendieck-Teichm\\\"uller module groups act on the latter, intertwining the\nactions of the KV symmetry groups. In the other direction, we show that any\nsymmetric KV solution gives rise to a morphism from the moperad of\nparenthesized braids with a frozen strand to the moperad of tangential\nautomorphisms of free Lie algebras. This morphism factors through the moperad\nof chord diagrams if and only if the associated KV associator is a Drinfeld\nassociator.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using the language of moperads-monoids in the category of right modules over\nan operad-we reinterpret the Alekseev-Enriquez-Torossian construction of\nKashiwara-Vergne (KV) solutions from associators. We show that any isomorphism\nbetween the moperad of parenthesized braids with a frozen strand and the\nmoperad of chord diagrams gives rise to a family of genus zero KV solutions\noperadically generated by a single classical KV solution. We show that the\nGrothendieck-Teichm\\\"uller module groups act on the latter, intertwining the\nactions of the KV symmetry groups. In the other direction, we show that any\nsymmetric KV solution gives rise to a morphism from the moperad of\nparenthesized braids with a frozen strand to the moperad of tangential\nautomorphisms of free Lie algebras. This morphism factors through the moperad\nof chord diagrams if and only if the associated KV associator is a Drinfeld\nassociator."
                },
                "authors": [
                    {
                        "name": "Zsuzsanna Dancso"
                    },
                    {
                        "name": "Iva Halacheva"
                    },
                    {
                        "name": "Guillaume Laplante-Anfossi"
                    },
                    {
                        "name": "Marcy Robertson"
                    },
                    {
                        "name": "Chandan Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandan Singh"
                },
                "author": "Chandan Singh",
                "arxiv_comment": "comments welcome!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.AT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.AT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.CT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.QA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "18M60, 17B, 55",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16217v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16217v1",
                "updated": "2025-07-22T04:21:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    4,
                    21,
                    3,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T04:21:03Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    4,
                    21,
                    3,
                    1,
                    203,
                    0
                ],
                "title": "Towards Compute-Optimal Many-Shot In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Compute-Optimal Many-Shot In-Context Learning"
                },
                "summary": "Long-context large language models (LLMs) are able to process inputs\ncontaining up to several million tokens. In the scope of in-context learning\n(ICL), this translates into using hundreds/thousands of demonstrations in the\ninput prompt, enabling many-shot ICL. In practice, a fixed set of\ndemonstrations is often selected at random in many-shot settings due to (1)\nhigh inference costs, (2) the benefits of caching and reusing computations, and\n(3) the similar performance offered by this strategy compared to others when\nscaled. In this work, we propose two straightforward strategies for\ndemonstration selection in many-shot ICL that improve performance with minimal\ncomputational overhead. Our first method combines a small number of\ndemonstrations, selected based on their similarity to each test sample, with a\ndisproportionately larger set of random demonstrations that are cached. The\nsecond strategy improves the first by replacing random demonstrations with\nthose selected using centroids derived from test sample representations via\nk-means clustering. Our experiments with Gemini Pro and Flash across several\ndatasets indicate that our strategies consistently outperform random selection\nand surpass or match the most performant selection approach while supporting\ncaching and reducing inference cost by up to an order of magnitude. We also\nshow that adjusting the proportion of demonstrations selected based on\ndifferent criteria can balance performance and inference cost in many-shot ICL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context large language models (LLMs) are able to process inputs\ncontaining up to several million tokens. In the scope of in-context learning\n(ICL), this translates into using hundreds/thousands of demonstrations in the\ninput prompt, enabling many-shot ICL. In practice, a fixed set of\ndemonstrations is often selected at random in many-shot settings due to (1)\nhigh inference costs, (2) the benefits of caching and reusing computations, and\n(3) the similar performance offered by this strategy compared to others when\nscaled. In this work, we propose two straightforward strategies for\ndemonstration selection in many-shot ICL that improve performance with minimal\ncomputational overhead. Our first method combines a small number of\ndemonstrations, selected based on their similarity to each test sample, with a\ndisproportionately larger set of random demonstrations that are cached. The\nsecond strategy improves the first by replacing random demonstrations with\nthose selected using centroids derived from test sample representations via\nk-means clustering. Our experiments with Gemini Pro and Flash across several\ndatasets indicate that our strategies consistently outperform random selection\nand surpass or match the most performant selection approach while supporting\ncaching and reducing inference cost by up to an order of magnitude. We also\nshow that adjusting the proportion of demonstrations selected based on\ndifferent criteria can balance performance and inference cost in many-shot ICL."
                },
                "authors": [
                    {
                        "name": "Shahriar Golchin"
                    },
                    {
                        "name": "Yanfei Chen"
                    },
                    {
                        "name": "Rujun Han"
                    },
                    {
                        "name": "Manan Gandhi"
                    },
                    {
                        "name": "Tianli Yu"
                    },
                    {
                        "name": "Swaroop Mishra"
                    },
                    {
                        "name": "Mihai Surdeanu"
                    },
                    {
                        "name": "Rishabh Agarwal"
                    },
                    {
                        "name": "Chen-Yu Lee"
                    },
                    {
                        "name": "Tomas Pfister"
                    }
                ],
                "author_detail": {
                    "name": "Tomas Pfister"
                },
                "author": "Tomas Pfister",
                "arxiv_comment": "Final version; accepted at COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16217v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16217v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10789v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10789v2",
                "updated": "2025-07-21T19:31:37Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    19,
                    31,
                    37,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-14T20:38:09Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    20,
                    38,
                    9,
                    0,
                    195,
                    0
                ],
                "title": "Dissecting the NVIDIA Blackwell Architecture with Microbenchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting the NVIDIA Blackwell Architecture with Microbenchmarks"
                },
                "summary": "The rapid development in scientific research provides a need for more compute\npower, which is partly being solved by GPUs. This paper presents a\nmicroarchitectural analysis of the modern NVIDIA Blackwell architecture by\nstudying GPU performance\n  features with thought through microbenchmarks. We unveil key subsystems,\nincluding the memory hierarchy, SM execution\n  pipelines, and the SM sub-core units, including the 5th generation tensor\ncores supporting FP4 and FP6 precisions.\n  To understand the different key features of the NVIDIA GPU, we study latency,\nthroughput, cache behavior, and scheduling\n  details, revealing subtle tuning metrics in the design of Blackwell. To\ndevelop a comprehensive analysis, we compare the\n  Blackwell architecture with the previous Hopper architecture by using the\nGeForce RTX 5080 and H100 PCIe, respectively. We\n  evaluate and compare results, presenting both generational improvements and\nperformance regressions. Additionally, we\n  investigate the role of power efficiency and energy consumption under varied\nworkloads. Our findings provide actionable insights\n  for application developers, compiler writers, and performance engineers to\noptimize workloads on Blackwell-based platforms,\n  and contribute new data to the growing research on GPU architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development in scientific research provides a need for more compute\npower, which is partly being solved by GPUs. This paper presents a\nmicroarchitectural analysis of the modern NVIDIA Blackwell architecture by\nstudying GPU performance\n  features with thought through microbenchmarks. We unveil key subsystems,\nincluding the memory hierarchy, SM execution\n  pipelines, and the SM sub-core units, including the 5th generation tensor\ncores supporting FP4 and FP6 precisions.\n  To understand the different key features of the NVIDIA GPU, we study latency,\nthroughput, cache behavior, and scheduling\n  details, revealing subtle tuning metrics in the design of Blackwell. To\ndevelop a comprehensive analysis, we compare the\n  Blackwell architecture with the previous Hopper architecture by using the\nGeForce RTX 5080 and H100 PCIe, respectively. We\n  evaluate and compare results, presenting both generational improvements and\nperformance regressions. Additionally, we\n  investigate the role of power efficiency and energy consumption under varied\nworkloads. Our findings provide actionable insights\n  for application developers, compiler writers, and performance engineers to\noptimize workloads on Blackwell-based platforms,\n  and contribute new data to the growing research on GPU architectures."
                },
                "authors": [
                    {
                        "name": "Aaron Jarmusch"
                    },
                    {
                        "name": "Nathan Graddon"
                    },
                    {
                        "name": "Sunita Chandrasekaran"
                    }
                ],
                "author_detail": {
                    "name": "Sunita Chandrasekaran"
                },
                "author": "Sunita Chandrasekaran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10789v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10789v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14003v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14003v2",
                "updated": "2025-07-21T19:05:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    19,
                    5,
                    1,
                    0,
                    202,
                    0
                ],
                "published": "2024-10-17T20:11:34Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    20,
                    11,
                    34,
                    3,
                    291,
                    0
                ],
                "title": "Per-Bank Bandwidth Regulation of Shared Last-Level Cache for Real-Time\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Per-Bank Bandwidth Regulation of Shared Last-Level Cache for Real-Time\n  Systems"
                },
                "summary": "Modern commercial-off-the-shelf (COTS) multicore processors have advanced\nmemory hierarchies that enhance memory-level parallelism (MLP), which is\ncrucial for high performance. To support high MLP, shared last-level caches\n(LLCs) are divided into multiple banks, allowing parallel access. However,\nuneven distribution of cache requests from the cores, especially when requests\nfrom multiple cores are concentrated on a single bank, can result in\nsignificant contention affecting all cores that access the cache. Such cache\nbank contention can even be maliciously induced -- known as cache bank-aware\ndenial-of-service (DoS) attacks -- in order to jeopardize the system's timing\npredictability.\n  In this paper, we propose a per-bank bandwidth regulation approach for\nmulti-banked shared LLC based multicore real-time systems. By regulating\nbandwidth on a per-bank basis, the approach aims to prevent unnecessary\nthrottling of cache accesses to non-contended banks, thus improving overall\nperformance (throughput) without compromising isolation benefits of throttling.\nWe implement our approach on a RISC-V system-on-chip (SoC) platform using\nFireSim and evaluate extensively using both synthetic and real-world workloads.\nOur evaluation results show that the proposed per-bank regulation approach\neffectively protects real-time tasks from co-running cache bank-aware DoS\nattacks, and offers up to a 3.66$\\times$ performance improvement for the\nthrottled benign best-effort tasks compared to prior bank-oblivious bandwidth\nthrottling approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern commercial-off-the-shelf (COTS) multicore processors have advanced\nmemory hierarchies that enhance memory-level parallelism (MLP), which is\ncrucial for high performance. To support high MLP, shared last-level caches\n(LLCs) are divided into multiple banks, allowing parallel access. However,\nuneven distribution of cache requests from the cores, especially when requests\nfrom multiple cores are concentrated on a single bank, can result in\nsignificant contention affecting all cores that access the cache. Such cache\nbank contention can even be maliciously induced -- known as cache bank-aware\ndenial-of-service (DoS) attacks -- in order to jeopardize the system's timing\npredictability.\n  In this paper, we propose a per-bank bandwidth regulation approach for\nmulti-banked shared LLC based multicore real-time systems. By regulating\nbandwidth on a per-bank basis, the approach aims to prevent unnecessary\nthrottling of cache accesses to non-contended banks, thus improving overall\nperformance (throughput) without compromising isolation benefits of throttling.\nWe implement our approach on a RISC-V system-on-chip (SoC) platform using\nFireSim and evaluate extensively using both synthetic and real-world workloads.\nOur evaluation results show that the proposed per-bank regulation approach\neffectively protects real-time tasks from co-running cache bank-aware DoS\nattacks, and offers up to a 3.66$\\times$ performance improvement for the\nthrottled benign best-effort tasks compared to prior bank-oblivious bandwidth\nthrottling approaches."
                },
                "authors": [
                    {
                        "name": "Connor Sullivan"
                    },
                    {
                        "name": "Alex Manley"
                    },
                    {
                        "name": "Mohammad Alian"
                    },
                    {
                        "name": "Heechul Yun"
                    }
                ],
                "author_detail": {
                    "name": "Heechul Yun"
                },
                "author": "Heechul Yun",
                "arxiv_doi": "10.1109/RTSS62706.2024.00036",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/RTSS62706.2024.00036",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.14003v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14003v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Update to Fig. 11: The previous version used mismatched cache\n  capacities between the 2-bank and 4-bank configurations in the simulation\n  setup. This has been corrected to ensure both configurations have equal total\n  cache capacity. As a result, the specific numerical results in Fig. 11 have\n  changed. However, the overall trend shown in Fig. 11 and key findings of the\n  paper remain consistent",
                "arxiv_journal_ref": "IEEE Real-Time Systems Symposium (RTSS), 2024, pp. 336-348",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18974v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18974v3",
                "updated": "2025-07-21T14:50:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    14,
                    50,
                    41,
                    0,
                    202,
                    0
                ],
                "published": "2025-03-22T06:14:33Z",
                "published_parsed": [
                    2025,
                    3,
                    22,
                    6,
                    14,
                    33,
                    5,
                    81,
                    0
                ],
                "title": "An Efficient Frequency-Based Approach for Maximal Square Detection in\n  Binary Matrices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Frequency-Based Approach for Maximal Square Detection in\n  Binary Matrices"
                },
                "summary": "Detecting maximal square submatrices of ones in binary matrices is a\nfundamental problem with applications in computer vision and pattern\nrecognition. While the standard dynamic programming (DP) solution achieves\noptimal asymptotic complexity, its practical performance suffers from repeated\nminimum operations and inefficient memory access patterns that degrade cache\nutilization. To address these limitations, we introduce a novel frequency-based\nalgorithm that employs a greedy approach to track the columnar continuity of\nones through an adaptive frequency array and a dynamic thresholding mechanism.\nExtensive benchmarking demonstrates that the frequency-based algorithm achieves\nfaster performance than the standard DP in 100% of test cases with an average\nspeedup of 3.32x, a maximum speedup of 4.60x, and a minimum speedup of 2.31x\nacross matrices up to 5000x5000 with densities from 0.1 to 0.9. The algorithm's\naverage speedup exceeds 2.5x for all densities and rises to over 3.5x for\ndensities of 0.7 and higher across all matrix sizes. These results demonstrate\nthat the frequency-based approach is a superior alternative to standard DP and\nopens new possibilities for efficient matrix analysis in performance-critical\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting maximal square submatrices of ones in binary matrices is a\nfundamental problem with applications in computer vision and pattern\nrecognition. While the standard dynamic programming (DP) solution achieves\noptimal asymptotic complexity, its practical performance suffers from repeated\nminimum operations and inefficient memory access patterns that degrade cache\nutilization. To address these limitations, we introduce a novel frequency-based\nalgorithm that employs a greedy approach to track the columnar continuity of\nones through an adaptive frequency array and a dynamic thresholding mechanism.\nExtensive benchmarking demonstrates that the frequency-based algorithm achieves\nfaster performance than the standard DP in 100% of test cases with an average\nspeedup of 3.32x, a maximum speedup of 4.60x, and a minimum speedup of 2.31x\nacross matrices up to 5000x5000 with densities from 0.1 to 0.9. The algorithm's\naverage speedup exceeds 2.5x for all densities and rises to over 3.5x for\ndensities of 0.7 and higher across all matrix sizes. These results demonstrate\nthat the frequency-based approach is a superior alternative to standard DP and\nopens new possibilities for efficient matrix analysis in performance-critical\napplications."
                },
                "authors": [
                    {
                        "name": "Swastik Bhandari"
                    }
                ],
                "author_detail": {
                    "name": "Swastik Bhandari"
                },
                "author": "Swastik Bhandari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18974v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18974v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10524v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10524v2",
                "updated": "2025-07-21T07:45:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    7,
                    45,
                    14,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-14T17:49:00Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    49,
                    0,
                    0,
                    195,
                    0
                ],
                "title": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive\n  Token-Level Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive\n  Token-Level Computation"
                },
                "summary": "Scaling language models unlocks impressive capabilities, but the accompanying\ncomputational and memory demands make both training and deployment expensive.\nExisting efficiency efforts typically target either parameter sharing or\nadaptive computation, leaving open the question of how to attain both\nsimultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework\nthat combines the two axes of efficiency inside a single Recursive Transformer.\nMoR reuses a shared stack of layers across recursion steps to achieve parameter\nefficiency, while lightweight routers enable adaptive token-level thinking by\ndynamically assigning different recursion depths to individual tokens. This\nallows MoR to focus quadratic attention computation only among tokens still\nactive at a given recursion depth, further improving memory access efficiency\nby selectively caching only their key-value pairs. Beyond these core\nmechanisms, we also propose a KV sharing variant that reuses KV pairs from the\nfirst recursion, specifically designed to decrease prefill latency and memory\nfootprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms\na new Pareto frontier: at equal training FLOPs and smaller model sizes, it\nsignificantly lowers validation perplexity and improves few-shot accuracy,\nwhile delivering higher throughput compared with vanilla and existing recursive\nbaselines. These gains demonstrate that MoR is an effective path towards\nlarge-model quality without incurring large-model cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models unlocks impressive capabilities, but the accompanying\ncomputational and memory demands make both training and deployment expensive.\nExisting efficiency efforts typically target either parameter sharing or\nadaptive computation, leaving open the question of how to attain both\nsimultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework\nthat combines the two axes of efficiency inside a single Recursive Transformer.\nMoR reuses a shared stack of layers across recursion steps to achieve parameter\nefficiency, while lightweight routers enable adaptive token-level thinking by\ndynamically assigning different recursion depths to individual tokens. This\nallows MoR to focus quadratic attention computation only among tokens still\nactive at a given recursion depth, further improving memory access efficiency\nby selectively caching only their key-value pairs. Beyond these core\nmechanisms, we also propose a KV sharing variant that reuses KV pairs from the\nfirst recursion, specifically designed to decrease prefill latency and memory\nfootprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms\na new Pareto frontier: at equal training FLOPs and smaller model sizes, it\nsignificantly lowers validation perplexity and improves few-shot accuracy,\nwhile delivering higher throughput compared with vanilla and existing recursive\nbaselines. These gains demonstrate that MoR is an effective path towards\nlarge-model quality without incurring large-model cost."
                },
                "authors": [
                    {
                        "name": "Sangmin Bae"
                    },
                    {
                        "name": "Yujin Kim"
                    },
                    {
                        "name": "Reza Bayat"
                    },
                    {
                        "name": "Sungnyun Kim"
                    },
                    {
                        "name": "Jiyoun Ha"
                    },
                    {
                        "name": "Tal Schuster"
                    },
                    {
                        "name": "Adam Fisch"
                    },
                    {
                        "name": "Hrayr Harutyunyan"
                    },
                    {
                        "name": "Ziwei Ji"
                    },
                    {
                        "name": "Aaron Courville"
                    },
                    {
                        "name": "Se-Young Yun"
                    }
                ],
                "author_detail": {
                    "name": "Se-Young Yun"
                },
                "author": "Se-Young Yun",
                "arxiv_comment": "36 pages, 9 figures, 14 tables, codes at\n  https://github.com/raymin0223/mixture_of_recursions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10524v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10524v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09025v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09025v2",
                "updated": "2025-07-20T03:49:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    20,
                    3,
                    49,
                    3,
                    6,
                    201,
                    0
                ],
                "published": "2025-07-11T21:19:18Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    21,
                    19,
                    18,
                    4,
                    192,
                    0
                ],
                "title": "Lizard: An Efficient Linearization Framework for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lizard: An Efficient Linearization Framework for Large Language Models"
                },
                "summary": "We propose Lizard, a linearization framework that transforms pretrained\nTransformer-based Large Language Models (LLMs) into flexible, subquadratic\narchitectures for infinite-context generation. Transformer-based LLMs face\nsignificant memory and computational bottlenecks as context lengths increase,\ndue to the quadratic complexity of softmax attention and the growing key-value\n(KV) cache. Lizard addresses these limitations by introducing a subquadratic\nattention mechanism that closely approximates softmax attention while\npreserving the output quality. Unlike previous linearization methods, which are\noften limited by fixed model structures and therefore exclude gating\nmechanisms, Lizard incorporates a gating module inspired by recent\nstate-of-the-art linear models. This enables adaptive memory control, supports\nconstant-memory inference, offers strong length generalization, and allows more\nflexible model design. Lizard combines gated linear attention for global\ncontext compression with sliding window attention enhanced by meta memory,\nforming a hybrid mechanism that captures both long-range dependencies and\nfine-grained local interactions. Moreover, we introduce a hardware-aware\nalgorithm that accelerates the training speed of our models. Extensive\nexperiments show that Lizard achieves near-lossless recovery of the teacher\nmodel's performance across standard language modeling tasks, while\nsignificantly outperforming previous linearization methods. On the 5-shot MMLU\nbenchmark, Lizard improves over prior models by 18 points and shows significant\nimprovements on associative recall tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Lizard, a linearization framework that transforms pretrained\nTransformer-based Large Language Models (LLMs) into flexible, subquadratic\narchitectures for infinite-context generation. Transformer-based LLMs face\nsignificant memory and computational bottlenecks as context lengths increase,\ndue to the quadratic complexity of softmax attention and the growing key-value\n(KV) cache. Lizard addresses these limitations by introducing a subquadratic\nattention mechanism that closely approximates softmax attention while\npreserving the output quality. Unlike previous linearization methods, which are\noften limited by fixed model structures and therefore exclude gating\nmechanisms, Lizard incorporates a gating module inspired by recent\nstate-of-the-art linear models. This enables adaptive memory control, supports\nconstant-memory inference, offers strong length generalization, and allows more\nflexible model design. Lizard combines gated linear attention for global\ncontext compression with sliding window attention enhanced by meta memory,\nforming a hybrid mechanism that captures both long-range dependencies and\nfine-grained local interactions. Moreover, we introduce a hardware-aware\nalgorithm that accelerates the training speed of our models. Extensive\nexperiments show that Lizard achieves near-lossless recovery of the teacher\nmodel's performance across standard language modeling tasks, while\nsignificantly outperforming previous linearization methods. On the 5-shot MMLU\nbenchmark, Lizard improves over prior models by 18 points and shows significant\nimprovements on associative recall tasks."
                },
                "authors": [
                    {
                        "name": "Chien Van Nguyen"
                    },
                    {
                        "name": "Ruiyi Zhang"
                    },
                    {
                        "name": "Hanieh Deilamsalehy"
                    },
                    {
                        "name": "Puneet Mathur"
                    },
                    {
                        "name": "Viet Dac Lai"
                    },
                    {
                        "name": "Haoliang Wang"
                    },
                    {
                        "name": "Jayakumar Subramanian"
                    },
                    {
                        "name": "Ryan A. Rossi"
                    },
                    {
                        "name": "Trung Bui"
                    },
                    {
                        "name": "Nikos Vlassis"
                    },
                    {
                        "name": "Franck Dernoncourt"
                    },
                    {
                        "name": "Thien Huu Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Thien Huu Nguyen"
                },
                "author": "Thien Huu Nguyen",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09025v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09025v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2508.05635v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05635v1",
                "updated": "2025-08-07T17:59:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    59,
                    44,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T17:59:44Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    59,
                    44,
                    3,
                    219,
                    0
                ],
                "title": "Genie Envisioner: A Unified World Foundation Platform for Robotic\n  Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Genie Envisioner: A Unified World Foundation Platform for Robotic\n  Manipulation"
                },
                "summary": "We introduce Genie Envisioner (GE), a unified world foundation platform for\nrobotic manipulation that integrates policy learning, evaluation, and\nsimulation within a single video-generative framework. At its core, GE-Base is\na large-scale, instruction-conditioned video diffusion model that captures the\nspatial, temporal, and semantic dynamics of real-world robotic interactions in\na structured latent space. Built upon this foundation, GE-Act maps latent\nrepresentations to executable action trajectories through a lightweight,\nflow-matching decoder, enabling precise and generalizable policy inference\nacross diverse embodiments with minimal supervision. To support scalable\nevaluation and training, GE-Sim serves as an action-conditioned neural\nsimulator, producing high-fidelity rollouts for closed-loop policy development.\nThe platform is further equipped with EWMBench, a standardized benchmark suite\nmeasuring visual fidelity, physical consistency, and instruction-action\nalignment. Together, these components establish Genie Envisioner as a scalable\nand practical foundation for instruction-driven, general-purpose embodied\nintelligence. All code, models, and benchmarks will be released publicly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Genie Envisioner (GE), a unified world foundation platform for\nrobotic manipulation that integrates policy learning, evaluation, and\nsimulation within a single video-generative framework. At its core, GE-Base is\na large-scale, instruction-conditioned video diffusion model that captures the\nspatial, temporal, and semantic dynamics of real-world robotic interactions in\na structured latent space. Built upon this foundation, GE-Act maps latent\nrepresentations to executable action trajectories through a lightweight,\nflow-matching decoder, enabling precise and generalizable policy inference\nacross diverse embodiments with minimal supervision. To support scalable\nevaluation and training, GE-Sim serves as an action-conditioned neural\nsimulator, producing high-fidelity rollouts for closed-loop policy development.\nThe platform is further equipped with EWMBench, a standardized benchmark suite\nmeasuring visual fidelity, physical consistency, and instruction-action\nalignment. Together, these components establish Genie Envisioner as a scalable\nand practical foundation for instruction-driven, general-purpose embodied\nintelligence. All code, models, and benchmarks will be released publicly."
                },
                "authors": [
                    {
                        "name": "Yue Liao"
                    },
                    {
                        "name": "Pengfei Zhou"
                    },
                    {
                        "name": "Siyuan Huang"
                    },
                    {
                        "name": "Donglin Yang"
                    },
                    {
                        "name": "Shengcong Chen"
                    },
                    {
                        "name": "Yuxin Jiang"
                    },
                    {
                        "name": "Yue Hu"
                    },
                    {
                        "name": "Jingbin Cai"
                    },
                    {
                        "name": "Si Liu"
                    },
                    {
                        "name": "Jianlan Luo"
                    },
                    {
                        "name": "Liliang Chen"
                    },
                    {
                        "name": "Shuicheng Yan"
                    },
                    {
                        "name": "Maoqing Yao"
                    },
                    {
                        "name": "Guanghui Ren"
                    }
                ],
                "author_detail": {
                    "name": "Guanghui Ren"
                },
                "author": "Guanghui Ren",
                "arxiv_comment": "https://genie-envisioner.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05635v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05634v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05634v1",
                "updated": "2025-08-07T17:59:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    59,
                    43,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T17:59:43Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    59,
                    43,
                    3,
                    219,
                    0
                ],
                "title": "Towards Generalizable Safety in Crowd Navigation via Conformal\n  Uncertainty Handling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Generalizable Safety in Crowd Navigation via Conformal\n  Uncertainty Handling"
                },
                "summary": "Mobile robots navigating in crowds trained using reinforcement learning are\nknown to suffer performance degradation when faced with out-of-distribution\nscenarios. We propose that by properly accounting for the uncertainties of\npedestrians, a robot can learn safe navigation policies that are robust to\ndistribution shifts. Our method augments agent observations with prediction\nuncertainty estimates generated by adaptive conformal inference, and it uses\nthese estimates to guide the agent's behavior through constrained reinforcement\nlearning. The system helps regulate the agent's actions and enables it to adapt\nto distribution shifts. In the in-distribution setting, our approach achieves a\n96.93% success rate, which is over 8.80% higher than the previous\nstate-of-the-art baselines with over 3.72 times fewer collisions and 2.43 times\nfewer intrusions into ground-truth human future trajectories. In three\nout-of-distribution scenarios, our method shows much stronger robustness when\nfacing distribution shifts in velocity variations, policy changes, and\ntransitions from individual to group dynamics. We deploy our method on a real\nrobot, and experiments show that the robot makes safe and robust decisions when\ninteracting with both sparse and dense crowds. Our code and videos are\navailable on https://gen-safe-nav.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile robots navigating in crowds trained using reinforcement learning are\nknown to suffer performance degradation when faced with out-of-distribution\nscenarios. We propose that by properly accounting for the uncertainties of\npedestrians, a robot can learn safe navigation policies that are robust to\ndistribution shifts. Our method augments agent observations with prediction\nuncertainty estimates generated by adaptive conformal inference, and it uses\nthese estimates to guide the agent's behavior through constrained reinforcement\nlearning. The system helps regulate the agent's actions and enables it to adapt\nto distribution shifts. In the in-distribution setting, our approach achieves a\n96.93% success rate, which is over 8.80% higher than the previous\nstate-of-the-art baselines with over 3.72 times fewer collisions and 2.43 times\nfewer intrusions into ground-truth human future trajectories. In three\nout-of-distribution scenarios, our method shows much stronger robustness when\nfacing distribution shifts in velocity variations, policy changes, and\ntransitions from individual to group dynamics. We deploy our method on a real\nrobot, and experiments show that the robot makes safe and robust decisions when\ninteracting with both sparse and dense crowds. Our code and videos are\navailable on https://gen-safe-nav.github.io/."
                },
                "authors": [
                    {
                        "name": "Jianpeng Yao"
                    },
                    {
                        "name": "Xiaopan Zhang"
                    },
                    {
                        "name": "Yu Xia"
                    },
                    {
                        "name": "Zejin Wang"
                    },
                    {
                        "name": "Amit K. Roy-Chowdhury"
                    },
                    {
                        "name": "Jiachen Li"
                    }
                ],
                "author_detail": {
                    "name": "Jiachen Li"
                },
                "author": "Jiachen Li",
                "arxiv_comment": "9th Conference on Robot Learning (CoRL 2025); Project website:\n  https://gen-safe-nav.github.io/. arXiv admin note: text overlap with\n  arXiv:2407.17460",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05634v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05634v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05629v1",
                "updated": "2025-08-07T17:59:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    59,
                    4,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T17:59:04Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    59,
                    4,
                    3,
                    219,
                    0
                ],
                "title": "On the Generalization of SFT: A Reinforcement Learning Perspective with\n  Reward Rectification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Generalization of SFT: A Reinforcement Learning Perspective with\n  Reward Rectification"
                },
                "summary": "We present a simple yet theoretically motivated improvement to Supervised\nFine-Tuning (SFT) for the Large Language Model (LLM), addressing its limited\ngeneralization compared to reinforcement learning (RL). Through mathematical\nanalysis, we reveal that standard SFT gradients implicitly encode a problematic\nreward structure that may severely restrict the generalization capabilities of\nmodel. To rectify this, we propose Dynamic Fine-Tuning (DFT), stabilizing\ngradient updates for each token by dynamically rescaling the objective function\nwith the probability of this token. Remarkably, this single-line code change\nsignificantly outperforms standard SFT across multiple challenging benchmarks\nand base models, demonstrating greatly improved generalization. Additionally,\nour approach shows competitive results in offline RL settings, offering an\neffective yet simpler alternative. This work bridges theoretical insight and\npractical solutions, substantially advancing SFT performance. The code will be\navailable at https://github.com/yongliang-wu/DFT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a simple yet theoretically motivated improvement to Supervised\nFine-Tuning (SFT) for the Large Language Model (LLM), addressing its limited\ngeneralization compared to reinforcement learning (RL). Through mathematical\nanalysis, we reveal that standard SFT gradients implicitly encode a problematic\nreward structure that may severely restrict the generalization capabilities of\nmodel. To rectify this, we propose Dynamic Fine-Tuning (DFT), stabilizing\ngradient updates for each token by dynamically rescaling the objective function\nwith the probability of this token. Remarkably, this single-line code change\nsignificantly outperforms standard SFT across multiple challenging benchmarks\nand base models, demonstrating greatly improved generalization. Additionally,\nour approach shows competitive results in offline RL settings, offering an\neffective yet simpler alternative. This work bridges theoretical insight and\npractical solutions, substantially advancing SFT performance. The code will be\navailable at https://github.com/yongliang-wu/DFT."
                },
                "authors": [
                    {
                        "name": "Yongliang Wu"
                    },
                    {
                        "name": "Yizhou Zhou"
                    },
                    {
                        "name": "Zhou Ziheng"
                    },
                    {
                        "name": "Yingzhe Peng"
                    },
                    {
                        "name": "Xinyu Ye"
                    },
                    {
                        "name": "Xinting Hu"
                    },
                    {
                        "name": "Wenbo Zhu"
                    },
                    {
                        "name": "Lu Qi"
                    },
                    {
                        "name": "Ming-Hsuan Yang"
                    },
                    {
                        "name": "Xu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xu Yang"
                },
                "author": "Xu Yang",
                "arxiv_comment": "14 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05626v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05626v1",
                "updated": "2025-08-07T17:58:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    58,
                    42,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T17:58:42Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    58,
                    42,
                    3,
                    219,
                    0
                ],
                "title": "Physically Controllable Relighting of Photographs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physically Controllable Relighting of Photographs"
                },
                "summary": "We present a self-supervised approach to in-the-wild image relighting that\nenables fully controllable, physically based illumination editing. We achieve\nthis by combining the physical accuracy of traditional rendering with the\nphotorealistic appearance made possible by neural rendering. Our pipeline works\nby inferring a colored mesh representation of a given scene using monocular\nestimates of geometry and intrinsic components. This representation allows\nusers to define their desired illumination configuration in 3D. The scene under\nthe new lighting can then be rendered using a path-tracing engine. We send this\napproximate rendering of the scene through a feed-forward neural renderer to\npredict the final photorealistic relighting result. We develop a differentiable\nrendering process to reconstruct in-the-wild scene illumination, enabling\nself-supervised training of our neural renderer on raw image collections. Our\nmethod represents a significant step in bringing the explicit physical control\nover lights available in typical 3D computer graphics tools, such as Blender,\nto in-the-wild relighting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a self-supervised approach to in-the-wild image relighting that\nenables fully controllable, physically based illumination editing. We achieve\nthis by combining the physical accuracy of traditional rendering with the\nphotorealistic appearance made possible by neural rendering. Our pipeline works\nby inferring a colored mesh representation of a given scene using monocular\nestimates of geometry and intrinsic components. This representation allows\nusers to define their desired illumination configuration in 3D. The scene under\nthe new lighting can then be rendered using a path-tracing engine. We send this\napproximate rendering of the scene through a feed-forward neural renderer to\npredict the final photorealistic relighting result. We develop a differentiable\nrendering process to reconstruct in-the-wild scene illumination, enabling\nself-supervised training of our neural renderer on raw image collections. Our\nmethod represents a significant step in bringing the explicit physical control\nover lights available in typical 3D computer graphics tools, such as Blender,\nto in-the-wild relighting."
                },
                "authors": [
                    {
                        "name": "Chris Careaga"
                    },
                    {
                        "name": "Yaz Aksoy"
                    }
                ],
                "author_detail": {
                    "name": "Yaz Aksoy"
                },
                "author": "Yaz Aksoy",
                "arxiv_doi": "10.1145/3721238.3730666",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3721238.3730666",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.05626v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05626v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Proc. SIGGRAPH 2025, 10 pages, 9 figures",
                "arxiv_journal_ref": "SIGGRAPH Conference Papers, Year 2025, Article No. 105, Pages 1 -\n  10",
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05625v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05625v1",
                "updated": "2025-08-07T17:58:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    58,
                    41,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T17:58:41Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    58,
                    41,
                    3,
                    219,
                    0
                ],
                "title": "How Do LLMs Persuade? Linear Probes Can Uncover Persuasion Dynamics in\n  Multi-Turn Conversations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Do LLMs Persuade? Linear Probes Can Uncover Persuasion Dynamics in\n  Multi-Turn Conversations"
                },
                "summary": "Large Language Models (LLMs) have started to demonstrate the ability to\npersuade humans, yet our understanding of how this dynamic transpires is\nlimited. Recent work has used linear probes, lightweight tools for analyzing\nmodel representations, to study various LLM skills such as the ability to model\nuser sentiment and political perspective. Motivated by this, we apply probes to\nstudy persuasion dynamics in natural, multi-turn conversations. We leverage\ninsights from cognitive science to train probes on distinct aspects of\npersuasion: persuasion success, persuadee personality, and persuasion strategy.\nDespite their simplicity, we show that they capture various aspects of\npersuasion at both the sample and dataset levels. For instance, probes can\nidentify the point in a conversation where the persuadee was persuaded or where\npersuasive success generally occurs across the entire dataset. We also show\nthat in addition to being faster than expensive prompting-based approaches,\nprobes can do just as well and even outperform prompting in some settings, such\nas when uncovering persuasion strategy. This suggests probes as a plausible\navenue for studying other complex behaviours such as deception and\nmanipulation, especially in multi-turn settings and large-scale dataset\nanalysis where prompting-based methods would be computationally inefficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have started to demonstrate the ability to\npersuade humans, yet our understanding of how this dynamic transpires is\nlimited. Recent work has used linear probes, lightweight tools for analyzing\nmodel representations, to study various LLM skills such as the ability to model\nuser sentiment and political perspective. Motivated by this, we apply probes to\nstudy persuasion dynamics in natural, multi-turn conversations. We leverage\ninsights from cognitive science to train probes on distinct aspects of\npersuasion: persuasion success, persuadee personality, and persuasion strategy.\nDespite their simplicity, we show that they capture various aspects of\npersuasion at both the sample and dataset levels. For instance, probes can\nidentify the point in a conversation where the persuadee was persuaded or where\npersuasive success generally occurs across the entire dataset. We also show\nthat in addition to being faster than expensive prompting-based approaches,\nprobes can do just as well and even outperform prompting in some settings, such\nas when uncovering persuasion strategy. This suggests probes as a plausible\navenue for studying other complex behaviours such as deception and\nmanipulation, especially in multi-turn settings and large-scale dataset\nanalysis where prompting-based methods would be computationally inefficient."
                },
                "authors": [
                    {
                        "name": "Brandon Jaipersaud"
                    },
                    {
                        "name": "David Krueger"
                    },
                    {
                        "name": "Ekdeep Singh Lubana"
                    }
                ],
                "author_detail": {
                    "name": "Ekdeep Singh Lubana"
                },
                "author": "Ekdeep Singh Lubana",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05625v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05625v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05622v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05622v1",
                "updated": "2025-08-07T17:57:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    57,
                    46,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T17:57:46Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    57,
                    46,
                    3,
                    219,
                    0
                ],
                "title": "Simulating Human-Like Learning Dynamics with LLM-Empowered Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating Human-Like Learning Dynamics with LLM-Empowered Agents"
                },
                "summary": "Capturing human learning behavior based on deep learning methods has become a\nmajor research focus in both psychology and intelligent systems. Recent\napproaches rely on controlled experiments or rule-based models to explore\ncognitive processes. However, they struggle to capture learning dynamics, track\nprogress over time, or provide explainability. To address these challenges, we\nintroduce LearnerAgent, a novel multi-agent framework based on Large Language\nModels (LLMs) to simulate a realistic teaching environment. To explore\nhuman-like learning dynamics, we construct learners with psychologically\ngrounded profiles-such as Deep, Surface, and Lazy-as well as a persona-free\nGeneral Learner to inspect the base LLM's default behavior. Through weekly\nknowledge acquisition, monthly strategic choices, periodic tests, and peer\ninteraction, we can track the dynamic learning progress of individual learners\nover a full-year journey. Our findings are fourfold: 1) Longitudinal analysis\nreveals that only Deep Learner achieves sustained cognitive growth. Our\nspecially designed \"trap questions\" effectively diagnose Surface Learner's\nshallow knowledge. 2) The behavioral and cognitive patterns of distinct\nlearners align closely with their psychological profiles. 3) Learners'\nself-concept scores evolve realistically, with the General Learner developing\nsurprisingly high self-efficacy despite its cognitive limitations. 4)\nCritically, the default profile of base LLM is a \"diligent but brittle Surface\nLearner\"-an agent that mimics the behaviors of a good student but lacks true,\ngeneralizable understanding. Extensive simulation experiments demonstrate that\nLearnerAgent aligns well with real scenarios, yielding more insightful findings\nabout LLMs' behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Capturing human learning behavior based on deep learning methods has become a\nmajor research focus in both psychology and intelligent systems. Recent\napproaches rely on controlled experiments or rule-based models to explore\ncognitive processes. However, they struggle to capture learning dynamics, track\nprogress over time, or provide explainability. To address these challenges, we\nintroduce LearnerAgent, a novel multi-agent framework based on Large Language\nModels (LLMs) to simulate a realistic teaching environment. To explore\nhuman-like learning dynamics, we construct learners with psychologically\ngrounded profiles-such as Deep, Surface, and Lazy-as well as a persona-free\nGeneral Learner to inspect the base LLM's default behavior. Through weekly\nknowledge acquisition, monthly strategic choices, periodic tests, and peer\ninteraction, we can track the dynamic learning progress of individual learners\nover a full-year journey. Our findings are fourfold: 1) Longitudinal analysis\nreveals that only Deep Learner achieves sustained cognitive growth. Our\nspecially designed \"trap questions\" effectively diagnose Surface Learner's\nshallow knowledge. 2) The behavioral and cognitive patterns of distinct\nlearners align closely with their psychological profiles. 3) Learners'\nself-concept scores evolve realistically, with the General Learner developing\nsurprisingly high self-efficacy despite its cognitive limitations. 4)\nCritically, the default profile of base LLM is a \"diligent but brittle Surface\nLearner\"-an agent that mimics the behaviors of a good student but lacks true,\ngeneralizable understanding. Extensive simulation experiments demonstrate that\nLearnerAgent aligns well with real scenarios, yielding more insightful findings\nabout LLMs' behavior."
                },
                "authors": [
                    {
                        "name": "Yu Yuan"
                    },
                    {
                        "name": "Lili Zhao"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Guangting Zheng"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Mengdi Zhang"
                    },
                    {
                        "name": "Qi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Qi Liu"
                },
                "author": "Qi Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05622v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05622v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05619v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05619v1",
                "updated": "2025-08-07T17:57:12Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    57,
                    12,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T17:57:12Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    57,
                    12,
                    3,
                    219,
                    0
                ],
                "title": "The Missing Reward: Active Inference in the Era of Experience",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Missing Reward: Active Inference in the Era of Experience"
                },
                "summary": "This paper argues that Active Inference (AIF) provides a crucial foundation\nfor developing autonomous AI agents capable of learning from experience without\ncontinuous human reward engineering. As AI systems begin to exhaust\nhigh-quality training data and rely on increasingly large human workforces for\nreward design, the current paradigm faces significant scalability challenges\nthat could impede progress toward genuinely autonomous intelligence. The\nproposal for an ``Era of Experience,'' where agents learn from self-generated\ndata, is a promising step forward. However, this vision still depends on\nextensive human engineering of reward functions, effectively shifting the\nbottleneck from data curation to reward curation. This highlights what we\nidentify as the \\textbf{grounded-agency gap}: the inability of contemporary AI\nsystems to autonomously formulate, adapt, and pursue objectives in response to\nchanging circumstances. We propose that AIF can bridge this gap by replacing\nexternal reward signals with an intrinsic drive to minimize free energy,\nallowing agents to naturally balance exploration and exploitation through a\nunified Bayesian objective. By integrating Large Language Models as generative\nworld models with AIF's principled decision-making framework, we can create\nagents that learn efficiently from experience while remaining aligned with\nhuman values. This synthesis offers a compelling path toward AI systems that\ncan develop autonomously while adhering to both computational and physical\nconstraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper argues that Active Inference (AIF) provides a crucial foundation\nfor developing autonomous AI agents capable of learning from experience without\ncontinuous human reward engineering. As AI systems begin to exhaust\nhigh-quality training data and rely on increasingly large human workforces for\nreward design, the current paradigm faces significant scalability challenges\nthat could impede progress toward genuinely autonomous intelligence. The\nproposal for an ``Era of Experience,'' where agents learn from self-generated\ndata, is a promising step forward. However, this vision still depends on\nextensive human engineering of reward functions, effectively shifting the\nbottleneck from data curation to reward curation. This highlights what we\nidentify as the \\textbf{grounded-agency gap}: the inability of contemporary AI\nsystems to autonomously formulate, adapt, and pursue objectives in response to\nchanging circumstances. We propose that AIF can bridge this gap by replacing\nexternal reward signals with an intrinsic drive to minimize free energy,\nallowing agents to naturally balance exploration and exploitation through a\nunified Bayesian objective. By integrating Large Language Models as generative\nworld models with AIF's principled decision-making framework, we can create\nagents that learn efficiently from experience while remaining aligned with\nhuman values. This synthesis offers a compelling path toward AI systems that\ncan develop autonomously while adhering to both computational and physical\nconstraints."
                },
                "authors": [
                    {
                        "name": "Bo Wen"
                    }
                ],
                "author_detail": {
                    "name": "Bo Wen"
                },
                "author": "Bo Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05619v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05619v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nlin.AO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.hist-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05616v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05616v1",
                "updated": "2025-08-07T17:55:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    55,
                    10,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T17:55:10Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    55,
                    10,
                    3,
                    219,
                    0
                ],
                "title": "TrajEvo: Trajectory Prediction Heuristics Design via LLM-driven\n  Evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrajEvo: Trajectory Prediction Heuristics Design via LLM-driven\n  Evolution"
                },
                "summary": "Trajectory prediction is a critical task in modeling human behavior,\nespecially in safety-critical domains such as social robotics and autonomous\nvehicle navigation. Traditional heuristics based on handcrafted rules often\nlack accuracy and generalizability. Although deep learning approaches offer\nimproved performance, they typically suffer from high computational cost,\nlimited explainability, and, importantly, poor generalization to\nout-of-distribution (OOD) scenarios. In this paper, we introduce TrajEvo, a\nframework that leverages Large Language Models (LLMs) to automatically design\ntrajectory prediction heuristics. TrajEvo employs an evolutionary algorithm to\ngenerate and refine prediction heuristics from past trajectory data. We propose\ntwo key innovations: Cross-Generation Elite Sampling to encourage population\ndiversity, and a Statistics Feedback Loop that enables the LLM to analyze and\nimprove alternative predictions. Our evaluations demonstrate that TrajEvo\noutperforms existing heuristic methods across multiple real-world datasets, and\nnotably surpasses both heuristic and deep learning methods in generalizing to\nan unseen OOD real-world dataset. TrajEvo marks a promising step toward the\nautomated design of fast, explainable, and generalizable trajectory prediction\nheuristics. We release our source code to facilitate future research at\nhttps://github.com/ai4co/trajevo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trajectory prediction is a critical task in modeling human behavior,\nespecially in safety-critical domains such as social robotics and autonomous\nvehicle navigation. Traditional heuristics based on handcrafted rules often\nlack accuracy and generalizability. Although deep learning approaches offer\nimproved performance, they typically suffer from high computational cost,\nlimited explainability, and, importantly, poor generalization to\nout-of-distribution (OOD) scenarios. In this paper, we introduce TrajEvo, a\nframework that leverages Large Language Models (LLMs) to automatically design\ntrajectory prediction heuristics. TrajEvo employs an evolutionary algorithm to\ngenerate and refine prediction heuristics from past trajectory data. We propose\ntwo key innovations: Cross-Generation Elite Sampling to encourage population\ndiversity, and a Statistics Feedback Loop that enables the LLM to analyze and\nimprove alternative predictions. Our evaluations demonstrate that TrajEvo\noutperforms existing heuristic methods across multiple real-world datasets, and\nnotably surpasses both heuristic and deep learning methods in generalizing to\nan unseen OOD real-world dataset. TrajEvo marks a promising step toward the\nautomated design of fast, explainable, and generalizable trajectory prediction\nheuristics. We release our source code to facilitate future research at\nhttps://github.com/ai4co/trajevo."
                },
                "authors": [
                    {
                        "name": "Zhikai Zhao"
                    },
                    {
                        "name": "Chuanbo Hua"
                    },
                    {
                        "name": "Federico Berto"
                    },
                    {
                        "name": "Kanghoon Lee"
                    },
                    {
                        "name": "Zihan Ma"
                    },
                    {
                        "name": "Jiachen Li"
                    },
                    {
                        "name": "Jinkyoo Park"
                    }
                ],
                "author_detail": {
                    "name": "Jinkyoo Park"
                },
                "author": "Jinkyoo Park",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2505.04480",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05616v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05616v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05615v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05615v1",
                "updated": "2025-08-07T17:54:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    54,
                    27,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T17:54:27Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    54,
                    27,
                    3,
                    219,
                    0
                ],
                "title": "Test-Time Reinforcement Learning for GUI Grounding via Region\n  Consistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-Time Reinforcement Learning for GUI Grounding via Region\n  Consistency"
                },
                "summary": "Graphical User Interface (GUI) grounding, the task of mapping natural\nlanguage instructions to precise screen coordinates, is fundamental to\nautonomous GUI agents. While existing methods achieve strong performance\nthrough extensive supervised training or reinforcement learning with labeled\nrewards, they remain constrained by the cost and availability of pixel-level\nannotations. We observe that when models generate multiple predictions for the\nsame GUI element, the spatial overlap patterns reveal implicit confidence\nsignals that can guide more accurate localization. Leveraging this insight, we\npropose GUI-RC (Region Consistency), a test-time scaling method that constructs\nspatial voting grids from multiple sampled predictions to identify consensus\nregions where models show highest agreement. Without any training, GUI-RC\nimproves accuracy by 2-3% across various architectures on ScreenSpot\nbenchmarks. We further introduce GUI-RCPO (Region Consistency Policy\nOptimization), which transforms these consistency patterns into rewards for\ntest-time reinforcement learning. By computing how well each prediction aligns\nwith the collective consensus, GUI-RCPO enables models to iteratively refine\ntheir outputs on unlabeled data during inference. Extensive experiments\ndemonstrate the generality of our approach: GUI-RC boosts\nQwen2.5-VL-3B-Instruct from 80.11% to 83.57% on ScreenSpot-v2, while GUI-RCPO\nfurther improves it to 85.14% through self-supervised optimization. Our\napproach reveals the untapped potential of test-time scaling and test-time\nreinforcement learning for GUI grounding, offering a promising path toward more\nrobust and data-efficient GUI agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphical User Interface (GUI) grounding, the task of mapping natural\nlanguage instructions to precise screen coordinates, is fundamental to\nautonomous GUI agents. While existing methods achieve strong performance\nthrough extensive supervised training or reinforcement learning with labeled\nrewards, they remain constrained by the cost and availability of pixel-level\nannotations. We observe that when models generate multiple predictions for the\nsame GUI element, the spatial overlap patterns reveal implicit confidence\nsignals that can guide more accurate localization. Leveraging this insight, we\npropose GUI-RC (Region Consistency), a test-time scaling method that constructs\nspatial voting grids from multiple sampled predictions to identify consensus\nregions where models show highest agreement. Without any training, GUI-RC\nimproves accuracy by 2-3% across various architectures on ScreenSpot\nbenchmarks. We further introduce GUI-RCPO (Region Consistency Policy\nOptimization), which transforms these consistency patterns into rewards for\ntest-time reinforcement learning. By computing how well each prediction aligns\nwith the collective consensus, GUI-RCPO enables models to iteratively refine\ntheir outputs on unlabeled data during inference. Extensive experiments\ndemonstrate the generality of our approach: GUI-RC boosts\nQwen2.5-VL-3B-Instruct from 80.11% to 83.57% on ScreenSpot-v2, while GUI-RCPO\nfurther improves it to 85.14% through self-supervised optimization. Our\napproach reveals the untapped potential of test-time scaling and test-time\nreinforcement learning for GUI grounding, offering a promising path toward more\nrobust and data-efficient GUI agents."
                },
                "authors": [
                    {
                        "name": "Yong Du"
                    },
                    {
                        "name": "Yuchen Yan"
                    },
                    {
                        "name": "Fei Tang"
                    },
                    {
                        "name": "Zhengxi Lu"
                    },
                    {
                        "name": "Chang Zong"
                    },
                    {
                        "name": "Weiming Lu"
                    },
                    {
                        "name": "Shengpei Jiang"
                    },
                    {
                        "name": "Yongliang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Yongliang Shen"
                },
                "author": "Yongliang Shen",
                "arxiv_comment": "Project Page: https://zju-real.github.io/gui-rcpo Code:\n  https://github.com/zju-real/gui-rcpo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05615v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05615v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05613v1",
                "updated": "2025-08-07T17:53:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    53,
                    56,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T17:53:56Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    53,
                    56,
                    3,
                    219,
                    0
                ],
                "title": "Cooper: Co-Optimizing Policy and Reward Models in Reinforcement Learning\n  for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cooper: Co-Optimizing Policy and Reward Models in Reinforcement Learning\n  for Large Language Models"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable performance in\nreasoning tasks, where reinforcement learning (RL) serves as a key algorithm\nfor enhancing their reasoning capabilities. Currently, there are two mainstream\nreward paradigms: model-based rewards and rule-based rewards. However, both\napproaches suffer from limitations: rule-based rewards lack robustness, while\nmodel-based rewards are vulnerable to reward hacking. To address these issues,\nwe propose Cooper(Co-optimizing Policy Model and Reward Model), a RL framework\nthat jointly optimizes both the policy model and the reward model. Cooper\nleverages the high precision of rule-based rewards when identifying correct\nresponses, and dynamically constructs and selects positive-negative sample\npairs for continued training the reward model. This design enhances robustness\nand mitigates the risk of reward hacking. To further support Cooper, we\nintroduce a hybrid annotation strategy that efficiently and accurately\ngenerates training data for the reward model. We also propose a reference-based\nreward modeling paradigm, where the reward model takes a reference answer as\ninput. Based on this design, we train a reward model named VerifyRM, which\nachieves higher accuracy on VerifyBench compared to other models of the same\nsize. We conduct reinforcement learning using both VerifyRM and Cooper. Our\nexperiments show that Cooper not only alleviates reward hacking but also\nimproves end-to-end RL performance, for instance, achieving a 0.54% gain in\naverage accuracy on Qwen2.5-1.5B-Instruct. Our findings demonstrate that\ndynamically updating reward model is an effective way to combat reward hacking,\nproviding a reference for better integrating reward models into RL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable performance in\nreasoning tasks, where reinforcement learning (RL) serves as a key algorithm\nfor enhancing their reasoning capabilities. Currently, there are two mainstream\nreward paradigms: model-based rewards and rule-based rewards. However, both\napproaches suffer from limitations: rule-based rewards lack robustness, while\nmodel-based rewards are vulnerable to reward hacking. To address these issues,\nwe propose Cooper(Co-optimizing Policy Model and Reward Model), a RL framework\nthat jointly optimizes both the policy model and the reward model. Cooper\nleverages the high precision of rule-based rewards when identifying correct\nresponses, and dynamically constructs and selects positive-negative sample\npairs for continued training the reward model. This design enhances robustness\nand mitigates the risk of reward hacking. To further support Cooper, we\nintroduce a hybrid annotation strategy that efficiently and accurately\ngenerates training data for the reward model. We also propose a reference-based\nreward modeling paradigm, where the reward model takes a reference answer as\ninput. Based on this design, we train a reward model named VerifyRM, which\nachieves higher accuracy on VerifyBench compared to other models of the same\nsize. We conduct reinforcement learning using both VerifyRM and Cooper. Our\nexperiments show that Cooper not only alleviates reward hacking but also\nimproves end-to-end RL performance, for instance, achieving a 0.54% gain in\naverage accuracy on Qwen2.5-1.5B-Instruct. Our findings demonstrate that\ndynamically updating reward model is an effective way to combat reward hacking,\nproviding a reference for better integrating reward models into RL."
                },
                "authors": [
                    {
                        "name": "Haitao Hong"
                    },
                    {
                        "name": "Yuchen Yan"
                    },
                    {
                        "name": "Xingyu Wu"
                    },
                    {
                        "name": "Guiyang Hou"
                    },
                    {
                        "name": "Wenqi Zhang"
                    },
                    {
                        "name": "Weiming Lu"
                    },
                    {
                        "name": "Yongliang Shen"
                    },
                    {
                        "name": "Jun Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Jun Xiao"
                },
                "author": "Jun Xiao",
                "arxiv_comment": "Project Page: https://zju-real.github.io/cooper Code:\n  https://github.com/zju-real/cooper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01473v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01473v2",
                "updated": "2025-08-07T17:46:00Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    46,
                    0,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-02T19:46:09Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    19,
                    46,
                    9,
                    5,
                    214,
                    0
                ],
                "title": "TreeDiff: AST-Guided Code Generation with Diffusion LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TreeDiff: AST-Guided Code Generation with Diffusion LLMs"
                },
                "summary": "Recent advances in diffusion-based language models have opened new\npossibilities for controllable and bidirectional sequence generation. These\nmodels provide an alternative to traditional autoregressive approaches by\nframing text generation as an iterative denoising process. However, applying\ndiffusion models to structured domains such as source code remains a\nsignificant challenge. Programming languages differ from natural language in\nthat they follow strict syntactic and semantic rules, with hierarchical\norganization that must be preserved for correctness. Standard token-level\ncorruption techniques used during training often ignore this structure, which\nmay hinder the model's ability to learn meaningful representations of code. To\naddress this limitation, we propose a syntax-aware diffusion framework that\nincorporates structural priors from Abstract Syntax Trees (ASTs) into the\ndenoising process. Instead of masking individual tokens at random, we\nselectively corrupt syntactically meaningful code spans derived from AST\nsubtrees. This enables the model to reconstruct programs in a way that respects\ngrammatical boundaries and captures long-range dependencies. Experimental\nresults demonstrate that syntax-aware corruption significantly improves\nsyntactic correctness, reconstruction accuracy, and generalization to unseen\ncode patterns. These findings highlight the potential of incorporating\nstructural information into diffusion-based training and suggest that\nsyntax-guided denoising is a promising direction for advancing diffusion-based\nlanguage models in code generation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in diffusion-based language models have opened new\npossibilities for controllable and bidirectional sequence generation. These\nmodels provide an alternative to traditional autoregressive approaches by\nframing text generation as an iterative denoising process. However, applying\ndiffusion models to structured domains such as source code remains a\nsignificant challenge. Programming languages differ from natural language in\nthat they follow strict syntactic and semantic rules, with hierarchical\norganization that must be preserved for correctness. Standard token-level\ncorruption techniques used during training often ignore this structure, which\nmay hinder the model's ability to learn meaningful representations of code. To\naddress this limitation, we propose a syntax-aware diffusion framework that\nincorporates structural priors from Abstract Syntax Trees (ASTs) into the\ndenoising process. Instead of masking individual tokens at random, we\nselectively corrupt syntactically meaningful code spans derived from AST\nsubtrees. This enables the model to reconstruct programs in a way that respects\ngrammatical boundaries and captures long-range dependencies. Experimental\nresults demonstrate that syntax-aware corruption significantly improves\nsyntactic correctness, reconstruction accuracy, and generalization to unseen\ncode patterns. These findings highlight the potential of incorporating\nstructural information into diffusion-based training and suggest that\nsyntax-guided denoising is a promising direction for advancing diffusion-based\nlanguage models in code generation tasks."
                },
                "authors": [
                    {
                        "name": "Yiming Zeng"
                    },
                    {
                        "name": "Jinghan Cao"
                    },
                    {
                        "name": "Zexin Li"
                    },
                    {
                        "name": "Yiming Chen"
                    },
                    {
                        "name": "Tao Ren"
                    },
                    {
                        "name": "Dawei Xiang"
                    },
                    {
                        "name": "Xidong Wu"
                    },
                    {
                        "name": "Shangqian Gao"
                    },
                    {
                        "name": "Tingting Yu"
                    }
                ],
                "author_detail": {
                    "name": "Tingting Yu"
                },
                "author": "Tingting Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01473v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01473v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05606v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05606v1",
                "updated": "2025-08-07T17:45:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    45,
                    17,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T17:45:17Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    45,
                    17,
                    3,
                    219,
                    0
                ],
                "title": "Uni-cot: Towards Unified Chain-of-Thought Reasoning Across Text and\n  Vision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uni-cot: Towards Unified Chain-of-Thought Reasoning Across Text and\n  Vision"
                },
                "summary": "Chain-of-Thought (CoT) reasoning has been widely adopted to enhance Large\nLanguage Models (LLMs) by decomposing complex tasks into simpler, sequential\nsubtasks. However, extending CoT to vision-language reasoning tasks remains\nchallenging, as it often requires interpreting transitions of visual states to\nsupport reasoning. Existing methods often struggle with this due to limited\ncapacity of modeling visual state transitions or incoherent visual trajectories\ncaused by fragmented architectures.\n  To overcome these limitations, we propose Uni-CoT, a Unified Chain-of-Thought\nframework that enables coherent and grounded multimodal reasoning within a\nsingle unified model. The key idea is to leverage a model capable of both image\nunderstanding and generation to reason over visual content and model evolving\nvisual states. However, empowering a unified model to achieve that is\nnon-trivial, given the high computational cost and the burden of training. To\naddress this, Uni-CoT introduces a novel two-level reasoning paradigm: A\nMacro-Level CoT for high-level task planning and A Micro-Level CoT for subtask\nexecution. This design significantly reduces the computational overhead.\nFurthermore, we introduce a structured training paradigm that combines\ninterleaved image-text supervision for macro-level CoT with multi-task\nobjectives for micro-level CoT. Together, these innovations allow Uni-CoT to\nperform scalable and coherent multi-modal reasoning. Furthermore, thanks to our\ndesign, all experiments can be efficiently completed using only 8 A100 GPUs\nwith 80GB VRAM each. Experimental results on reasoning-driven image generation\nbenchmark (WISE) and editing benchmarks (RISE and KRIS) indicates that Uni-CoT\ndemonstrates SOTA performance and strong generalization, establishing Uni-CoT\nas a promising solution for multi-modal reasoning. Project Page and Code:\nhttps://sais-fuxi.github.io/projects/uni-cot/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) reasoning has been widely adopted to enhance Large\nLanguage Models (LLMs) by decomposing complex tasks into simpler, sequential\nsubtasks. However, extending CoT to vision-language reasoning tasks remains\nchallenging, as it often requires interpreting transitions of visual states to\nsupport reasoning. Existing methods often struggle with this due to limited\ncapacity of modeling visual state transitions or incoherent visual trajectories\ncaused by fragmented architectures.\n  To overcome these limitations, we propose Uni-CoT, a Unified Chain-of-Thought\nframework that enables coherent and grounded multimodal reasoning within a\nsingle unified model. The key idea is to leverage a model capable of both image\nunderstanding and generation to reason over visual content and model evolving\nvisual states. However, empowering a unified model to achieve that is\nnon-trivial, given the high computational cost and the burden of training. To\naddress this, Uni-CoT introduces a novel two-level reasoning paradigm: A\nMacro-Level CoT for high-level task planning and A Micro-Level CoT for subtask\nexecution. This design significantly reduces the computational overhead.\nFurthermore, we introduce a structured training paradigm that combines\ninterleaved image-text supervision for macro-level CoT with multi-task\nobjectives for micro-level CoT. Together, these innovations allow Uni-CoT to\nperform scalable and coherent multi-modal reasoning. Furthermore, thanks to our\ndesign, all experiments can be efficiently completed using only 8 A100 GPUs\nwith 80GB VRAM each. Experimental results on reasoning-driven image generation\nbenchmark (WISE) and editing benchmarks (RISE and KRIS) indicates that Uni-CoT\ndemonstrates SOTA performance and strong generalization, establishing Uni-CoT\nas a promising solution for multi-modal reasoning. Project Page and Code:\nhttps://sais-fuxi.github.io/projects/uni-cot/"
                },
                "authors": [
                    {
                        "name": "Luozheng Qin"
                    },
                    {
                        "name": "Jia Gong"
                    },
                    {
                        "name": "Yuqing Sun"
                    },
                    {
                        "name": "Tianjiao Li"
                    },
                    {
                        "name": "Mengping Yang"
                    },
                    {
                        "name": "Xiaomeng Yang"
                    },
                    {
                        "name": "Chao Qu"
                    },
                    {
                        "name": "Zhiyu Tan"
                    },
                    {
                        "name": "Hao Li"
                    }
                ],
                "author_detail": {
                    "name": "Hao Li"
                },
                "author": "Hao Li",
                "arxiv_comment": "https://sais-fuxi.github.io/projects/uni-cot/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05606v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05606v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.01370v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.01370v2",
                "updated": "2025-08-07T17:34:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    34,
                    27,
                    3,
                    219,
                    0
                ],
                "published": "2023-11-02T16:27:58Z",
                "published_parsed": [
                    2023,
                    11,
                    2,
                    16,
                    27,
                    58,
                    3,
                    306,
                    0
                ],
                "title": "Quantum Sensing with Topological-Paired Bound States",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Sensing with Topological-Paired Bound States"
                },
                "summary": "We present an efficient and robust protocol for quantum-enhanced sensing\nusing a single qubit in the topological waveguide system. Our method relies on\nthe topological-paired bound states, which are localized near the qubit and can\nbe effectively regarded as a two-level system. Through the lens of Bayesian\ninference theory, we show that the sensitivity can reach the Heisenberg limit\nacross a large field range. Inheriting from the topological robustness of the\nwaveguide, our sensing protocol is robust against local perturbations. Besides,\nour sensing protocol utilizes a product state as the initial state, which can\nbe easily prepared in experiments. We expect this approach would pave the way\ntoward robust topological quantum sensors based on near-term quantum platforms\nsuch as superconducting qubits and Rydberg arrays.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an efficient and robust protocol for quantum-enhanced sensing\nusing a single qubit in the topological waveguide system. Our method relies on\nthe topological-paired bound states, which are localized near the qubit and can\nbe effectively regarded as a two-level system. Through the lens of Bayesian\ninference theory, we show that the sensitivity can reach the Heisenberg limit\nacross a large field range. Inheriting from the topological robustness of the\nwaveguide, our sensing protocol is robust against local perturbations. Besides,\nour sensing protocol utilizes a product state as the initial state, which can\nbe easily prepared in experiments. We expect this approach would pave the way\ntoward robust topological quantum sensors based on near-term quantum platforms\nsuch as superconducting qubits and Rydberg arrays."
                },
                "authors": [
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Peng Xu"
                    },
                    {
                        "name": "Jiazhong Hu"
                    },
                    {
                        "name": "Xingze Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xingze Qiu"
                },
                "author": "Xingze Qiu",
                "arxiv_doi": "10.1088/1367-2630/add8b0",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1088/1367-2630/add8b0",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2311.01370v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.01370v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "9 pages, 6 figures",
                "arxiv_journal_ref": "New J. Phys. 27, 054509 (2025)",
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.quant-gas",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05592v1",
                "updated": "2025-08-07T17:32:14Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    32,
                    14,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T17:32:14Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    32,
                    14,
                    3,
                    219,
                    0
                ],
                "title": "MathSmith: Towards Extremely Hard Mathematical Reasoning by Forging\n  Synthetic Problems with a Reinforced Policy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MathSmith: Towards Extremely Hard Mathematical Reasoning by Forging\n  Synthetic Problems with a Reinforced Policy"
                },
                "summary": "Large language models have achieved substantial progress in mathematical\nreasoning, yet their advancement is limited by the scarcity of high-quality,\nhigh-difficulty training data. Existing synthesis methods largely rely on\ntransforming human-written templates, limiting both diversity and scalability.\nWe propose MathSmith, a novel framework for synthesizing challenging\nmathematical problems to enhance LLM reasoning. Rather than modifying existing\nproblems, MathSmith constructs new ones from scratch by randomly sampling\nconcept-explanation pairs from PlanetMath, ensuring data independence and\navoiding contamination. To increase difficulty, we design nine predefined\nstrategies as soft constraints during rationales. We further adopts\nreinforcement learning to jointly optimize structural validity, reasoning\ncomplexity, and answer consistency. The length of the reasoning trace generated\nunder autoregressive prompting is used to reflect cognitive complexity,\nencouraging the creation of more demanding problems aligned with\nlong-chain-of-thought reasoning. Experiments across five benchmarks,\ncategorized as easy & medium (GSM8K, MATH-500) and hard (AIME2024, AIME2025,\nOlympiadBench), show that MathSmith consistently outperforms existing baselines\nunder both short and long CoT settings. Additionally, a weakness-focused\nvariant generation module enables targeted improvement on specific concepts.\nOverall, MathSmith exhibits strong scalability, generalization, and\ntransferability, highlighting the promise of high-difficulty synthetic data in\nadvancing LLM reasoning capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have achieved substantial progress in mathematical\nreasoning, yet their advancement is limited by the scarcity of high-quality,\nhigh-difficulty training data. Existing synthesis methods largely rely on\ntransforming human-written templates, limiting both diversity and scalability.\nWe propose MathSmith, a novel framework for synthesizing challenging\nmathematical problems to enhance LLM reasoning. Rather than modifying existing\nproblems, MathSmith constructs new ones from scratch by randomly sampling\nconcept-explanation pairs from PlanetMath, ensuring data independence and\navoiding contamination. To increase difficulty, we design nine predefined\nstrategies as soft constraints during rationales. We further adopts\nreinforcement learning to jointly optimize structural validity, reasoning\ncomplexity, and answer consistency. The length of the reasoning trace generated\nunder autoregressive prompting is used to reflect cognitive complexity,\nencouraging the creation of more demanding problems aligned with\nlong-chain-of-thought reasoning. Experiments across five benchmarks,\ncategorized as easy & medium (GSM8K, MATH-500) and hard (AIME2024, AIME2025,\nOlympiadBench), show that MathSmith consistently outperforms existing baselines\nunder both short and long CoT settings. Additionally, a weakness-focused\nvariant generation module enables targeted improvement on specific concepts.\nOverall, MathSmith exhibits strong scalability, generalization, and\ntransferability, highlighting the promise of high-difficulty synthetic data in\nadvancing LLM reasoning capabilities."
                },
                "authors": [
                    {
                        "name": "Shaoxiong Zhan"
                    },
                    {
                        "name": "Yanlin Lai"
                    },
                    {
                        "name": "Ziyu Lu"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Ziqing Yang"
                    },
                    {
                        "name": "Fei Tang"
                    }
                ],
                "author_detail": {
                    "name": "Fei Tang"
                },
                "author": "Fei Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05585v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05585v1",
                "updated": "2025-08-07T17:22:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    22,
                    33,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T17:22:33Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    22,
                    33,
                    3,
                    219,
                    0
                ],
                "title": "DART: Dual Adaptive Refinement Transfer for Open-Vocabulary Multi-Label\n  Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DART: Dual Adaptive Refinement Transfer for Open-Vocabulary Multi-Label\n  Recognition"
                },
                "summary": "Open-Vocabulary Multi-Label Recognition (OV-MLR) aims to identify multiple\nseen and unseen object categories within an image, requiring both precise\nintra-class localization to pinpoint objects and effective inter-class\nreasoning to model complex category dependencies. While Vision-Language\nPre-training (VLP) models offer a strong open-vocabulary foundation, they often\nstruggle with fine-grained localization under weak supervision and typically\nfail to explicitly leverage structured relational knowledge beyond basic\nsemantics, limiting performance especially for unseen classes. To overcome\nthese limitations, we propose the Dual Adaptive Refinement Transfer (DART)\nframework. DART enhances a frozen VLP backbone via two synergistic adaptive\nmodules. For intra-class refinement, an Adaptive Refinement Module (ARM)\nrefines patch features adaptively, coupled with a novel Weakly Supervised Patch\nSelecting (WPS) loss that enables discriminative localization using only\nimage-level labels. Concurrently, for inter-class transfer, an Adaptive\nTransfer Module (ATM) leverages a Class Relationship Graph (CRG), constructed\nusing structured knowledge mined from a Large Language Model (LLM), and employs\ngraph attention network to adaptively transfer relational information between\nclass representations. DART is the first framework, to our knowledge, to\nexplicitly integrate external LLM-derived relational knowledge for adaptive\ninter-class transfer while simultaneously performing adaptive intra-class\nrefinement under weak supervision for OV-MLR. Extensive experiments on\nchallenging benchmarks demonstrate that our DART achieves new state-of-the-art\nperformance, validating its effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-Vocabulary Multi-Label Recognition (OV-MLR) aims to identify multiple\nseen and unseen object categories within an image, requiring both precise\nintra-class localization to pinpoint objects and effective inter-class\nreasoning to model complex category dependencies. While Vision-Language\nPre-training (VLP) models offer a strong open-vocabulary foundation, they often\nstruggle with fine-grained localization under weak supervision and typically\nfail to explicitly leverage structured relational knowledge beyond basic\nsemantics, limiting performance especially for unseen classes. To overcome\nthese limitations, we propose the Dual Adaptive Refinement Transfer (DART)\nframework. DART enhances a frozen VLP backbone via two synergistic adaptive\nmodules. For intra-class refinement, an Adaptive Refinement Module (ARM)\nrefines patch features adaptively, coupled with a novel Weakly Supervised Patch\nSelecting (WPS) loss that enables discriminative localization using only\nimage-level labels. Concurrently, for inter-class transfer, an Adaptive\nTransfer Module (ATM) leverages a Class Relationship Graph (CRG), constructed\nusing structured knowledge mined from a Large Language Model (LLM), and employs\ngraph attention network to adaptively transfer relational information between\nclass representations. DART is the first framework, to our knowledge, to\nexplicitly integrate external LLM-derived relational knowledge for adaptive\ninter-class transfer while simultaneously performing adaptive intra-class\nrefinement under weak supervision for OV-MLR. Extensive experiments on\nchallenging benchmarks demonstrate that our DART achieves new state-of-the-art\nperformance, validating its effectiveness."
                },
                "authors": [
                    {
                        "name": "Haijing Liu"
                    },
                    {
                        "name": "Tao Pu"
                    },
                    {
                        "name": "Hefeng Wu"
                    },
                    {
                        "name": "Keze Wang"
                    },
                    {
                        "name": "Liang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Liang Lin"
                },
                "author": "Liang Lin",
                "arxiv_comment": "Accepted by ACM MM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05585v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05585v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05581v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05581v1",
                "updated": "2025-08-07T17:15:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    15,
                    17,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T17:15:17Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    15,
                    17,
                    3,
                    219,
                    0
                ],
                "title": "Iterative Learning of Computable Phenotypes for Treatment Resistant\n  Hypertension using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iterative Learning of Computable Phenotypes for Treatment Resistant\n  Hypertension using Large Language Models"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities for\nmedical question answering and programming, but their potential for generating\ninterpretable computable phenotypes (CPs) is under-explored. In this work, we\ninvestigate whether LLMs can generate accurate and concise CPs for six clinical\nphenotypes of varying complexity, which could be leveraged to enable scalable\nclinical decision support to improve care for patients with hypertension. In\naddition to evaluating zero-short performance, we propose and test a\nsynthesize, execute, debug, instruct strategy that uses LLMs to generate and\niteratively refine CPs using data-driven feedback. Our results show that LLMs,\ncoupled with iterative learning, can generate interpretable and reasonably\naccurate programs that approach the performance of state-of-the-art ML methods\nwhile requiring significantly fewer training examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities for\nmedical question answering and programming, but their potential for generating\ninterpretable computable phenotypes (CPs) is under-explored. In this work, we\ninvestigate whether LLMs can generate accurate and concise CPs for six clinical\nphenotypes of varying complexity, which could be leveraged to enable scalable\nclinical decision support to improve care for patients with hypertension. In\naddition to evaluating zero-short performance, we propose and test a\nsynthesize, execute, debug, instruct strategy that uses LLMs to generate and\niteratively refine CPs using data-driven feedback. Our results show that LLMs,\ncoupled with iterative learning, can generate interpretable and reasonably\naccurate programs that approach the performance of state-of-the-art ML methods\nwhile requiring significantly fewer training examples."
                },
                "authors": [
                    {
                        "name": "Guilherme Seidyo Imai Aldeia"
                    },
                    {
                        "name": "Daniel S. Herman"
                    },
                    {
                        "name": "William G. La Cava"
                    }
                ],
                "author_detail": {
                    "name": "William G. La Cava"
                },
                "author": "William G. La Cava",
                "arxiv_comment": "To appear in PMLR, Volume 298, Machine Learning for Healthcare, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05581v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05581v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10855v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10855v3",
                "updated": "2025-08-07T17:04:23Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    4,
                    23,
                    3,
                    219,
                    0
                ],
                "published": "2024-12-14T15:03:33Z",
                "published_parsed": [
                    2024,
                    12,
                    14,
                    15,
                    3,
                    33,
                    5,
                    349,
                    0
                ],
                "title": "Fast and Robust Visuomotor Riemannian Flow Matching Policy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and Robust Visuomotor Riemannian Flow Matching Policy"
                },
                "summary": "Diffusion-based visuomotor policies excel at learning complex robotic tasks\nby effectively combining visual data with high-dimensional, multi-modal action\ndistributions. However, diffusion models often suffer from slow inference due\nto costly denoising processes or require complex sequential training arising\nfrom recent distilling approaches. This paper introduces Riemannian Flow\nMatching Policy (RFMP), a model that inherits the easy training and fast\ninference capabilities of flow matching (FM). Moreover, RFMP inherently\nincorporates geometric constraints commonly found in realistic robotic\napplications, as the robot state resides on a Riemannian manifold. To enhance\nthe robustness of RFMP, we propose Stable RFMP (SRFMP), which leverages\nLaSalle's invariance principle to equip the dynamics of FM with stability to\nthe support of a target Riemannian distribution. Rigorous evaluation on ten\nsimulated and real-world tasks show that RFMP successfully learns and\nsynthesizes complex sensorimotor policies on Euclidean and Riemannian spaces\nwith efficient training and inference phases, outperforming Diffusion Policies\nand Consistency Policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based visuomotor policies excel at learning complex robotic tasks\nby effectively combining visual data with high-dimensional, multi-modal action\ndistributions. However, diffusion models often suffer from slow inference due\nto costly denoising processes or require complex sequential training arising\nfrom recent distilling approaches. This paper introduces Riemannian Flow\nMatching Policy (RFMP), a model that inherits the easy training and fast\ninference capabilities of flow matching (FM). Moreover, RFMP inherently\nincorporates geometric constraints commonly found in realistic robotic\napplications, as the robot state resides on a Riemannian manifold. To enhance\nthe robustness of RFMP, we propose Stable RFMP (SRFMP), which leverages\nLaSalle's invariance principle to equip the dynamics of FM with stability to\nthe support of a target Riemannian distribution. Rigorous evaluation on ten\nsimulated and real-world tasks show that RFMP successfully learns and\nsynthesizes complex sensorimotor policies on Euclidean and Riemannian spaces\nwith efficient training and inference phases, outperforming Diffusion Policies\nand Consistency Policies."
                },
                "authors": [
                    {
                        "name": "Haoran Ding"
                    },
                    {
                        "name": "Nomie Jaquier"
                    },
                    {
                        "name": "Jan Peters"
                    },
                    {
                        "name": "Leonel Rozo"
                    }
                ],
                "author_detail": {
                    "name": "Leonel Rozo"
                },
                "author": "Leonel Rozo",
                "arxiv_comment": "Accepted for publication in IEEE T-RO. Project website:\n  https://sites.google.com/view/rfmp 17 pages, 12 figures, 12 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10855v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10855v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05571v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05571v1",
                "updated": "2025-08-07T17:02:23Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    2,
                    23,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T17:02:23Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    2,
                    23,
                    3,
                    219,
                    0
                ],
                "title": "Fairy$\\pm i$: the First 2-bit Complex LLM with All Parameters in\n  $\\{\\pm1, \\pm i\\}$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fairy$\\pm i$: the First 2-bit Complex LLM with All Parameters in\n  $\\{\\pm1, \\pm i\\}$"
                },
                "summary": "Quantization-Aware Training (QAT) integrates quantization into the training\nloop, enabling LLMs to learn robust low-bit representations, and is widely\nrecognized as one of the most promising research directions. All current QAT\nresearch focuses on minimizing quantization error on full-precision models,\nwhere the full-precision accuracy acts as an upper bound (accuracy ceiling). No\nexisting method has even attempted to surpass this ceiling. To break this\nceiling, we propose a new paradigm: raising the ceiling (full-precision model),\nand then still quantizing it efficiently into 2 bits. We propose Fairy$\\pm i$,\nthe first 2-bit quantization framework for complex-valued LLMs. Specifically,\nour method leverages the representational advantages of the complex domain to\nboost full-precision accuracy. We map weights to the fourth roots of unity\n$\\{\\pm1, \\pm i\\}$, forming a perfectly symmetric and information-theoretically\noptimal 2-bit representation. Importantly, each quantized weight has either a\nzero real or imaginary part, enabling multiplication-free inference using only\nadditions and element swaps. Experimental results show that Fairy$\\pm i$\noutperforms the ceiling of existing 2-bit quantization approaches in terms of\nboth PPL and downstream tasks, while maintaining strict storage and compute\nefficiency. This work opens a new direction for building highly accurate and\npractical LLMs under extremely low-bit constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization-Aware Training (QAT) integrates quantization into the training\nloop, enabling LLMs to learn robust low-bit representations, and is widely\nrecognized as one of the most promising research directions. All current QAT\nresearch focuses on minimizing quantization error on full-precision models,\nwhere the full-precision accuracy acts as an upper bound (accuracy ceiling). No\nexisting method has even attempted to surpass this ceiling. To break this\nceiling, we propose a new paradigm: raising the ceiling (full-precision model),\nand then still quantizing it efficiently into 2 bits. We propose Fairy$\\pm i$,\nthe first 2-bit quantization framework for complex-valued LLMs. Specifically,\nour method leverages the representational advantages of the complex domain to\nboost full-precision accuracy. We map weights to the fourth roots of unity\n$\\{\\pm1, \\pm i\\}$, forming a perfectly symmetric and information-theoretically\noptimal 2-bit representation. Importantly, each quantized weight has either a\nzero real or imaginary part, enabling multiplication-free inference using only\nadditions and element swaps. Experimental results show that Fairy$\\pm i$\noutperforms the ceiling of existing 2-bit quantization approaches in terms of\nboth PPL and downstream tasks, while maintaining strict storage and compute\nefficiency. This work opens a new direction for building highly accurate and\npractical LLMs under extremely low-bit constraints."
                },
                "authors": [
                    {
                        "name": "Feiyu Wang"
                    },
                    {
                        "name": "Guoan Wang"
                    },
                    {
                        "name": "Yihao Zhang"
                    },
                    {
                        "name": "Shengfan Wang"
                    },
                    {
                        "name": "Weitao Li"
                    },
                    {
                        "name": "Bokai Huang"
                    },
                    {
                        "name": "Shimao Chen"
                    },
                    {
                        "name": "Zihan Jiang"
                    },
                    {
                        "name": "Rui Xu"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "arxiv_comment": "13 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05571v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05571v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05568v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05568v1",
                "updated": "2025-08-07T17:00:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    0,
                    47,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T17:00:47Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    0,
                    47,
                    3,
                    219,
                    0
                ],
                "title": "X-VFL: A New Vertical Federated Learning Framework with Cross Completion\n  and Decision Subspace Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-VFL: A New Vertical Federated Learning Framework with Cross Completion\n  and Decision Subspace Alignment"
                },
                "summary": "Vertical Federated Learning (VFL) enables collaborative learning by\nintegrating disjoint feature subsets from multiple clients/parties. However,\nVFL typically faces two key challenges: i) the requirement for perfectly\naligned data samples across all clients (missing features are not allowed); ii)\nthe requirement for joint collaborative inference/prediction involving all\nclients (it does not support locally independent inference on a single client).\nTo address these challenges, we propose X-VFL, a new VFL framework designed to\ndeal with the non-aligned data samples with (partially) missing features and to\nsupport locally independent inference of new data samples for each client. In\nparticular, we design two novel modules in X-VFL: Cross Completion (XCom) and\nDecision Subspace Alignment (DS-Align). XCom can complete/reconstruct missing\nfeatures for non-aligned data samples by leveraging information from other\nclients. DS-Align aligns local features with completed and global features\nacross all clients within the decision subspace, thus enabling locally\nindependent inference at each client. Moreover, we provide convergence theorems\nfor different algorithms used in training X-VFL, showing an $O(1/\\sqrt{T})$\nconvergence rate for SGD-type algorithms and an $O(1/T)$ rate for PAGE-type\nalgorithms, where $T$ denotes the number of training update steps. Extensive\nexperiments on real-world datasets demonstrate that X-VFL significantly\noutperforms existing methods, e.g., achieving a 15% improvement in accuracy on\nthe image CIFAR-10 dataset and a 43% improvement on the medical MIMIC-III\ndataset. These results validate the practical effectiveness and superiority of\nX-VFL, particularly in scenarios involving partially missing features and\nlocally independent inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vertical Federated Learning (VFL) enables collaborative learning by\nintegrating disjoint feature subsets from multiple clients/parties. However,\nVFL typically faces two key challenges: i) the requirement for perfectly\naligned data samples across all clients (missing features are not allowed); ii)\nthe requirement for joint collaborative inference/prediction involving all\nclients (it does not support locally independent inference on a single client).\nTo address these challenges, we propose X-VFL, a new VFL framework designed to\ndeal with the non-aligned data samples with (partially) missing features and to\nsupport locally independent inference of new data samples for each client. In\nparticular, we design two novel modules in X-VFL: Cross Completion (XCom) and\nDecision Subspace Alignment (DS-Align). XCom can complete/reconstruct missing\nfeatures for non-aligned data samples by leveraging information from other\nclients. DS-Align aligns local features with completed and global features\nacross all clients within the decision subspace, thus enabling locally\nindependent inference at each client. Moreover, we provide convergence theorems\nfor different algorithms used in training X-VFL, showing an $O(1/\\sqrt{T})$\nconvergence rate for SGD-type algorithms and an $O(1/T)$ rate for PAGE-type\nalgorithms, where $T$ denotes the number of training update steps. Extensive\nexperiments on real-world datasets demonstrate that X-VFL significantly\noutperforms existing methods, e.g., achieving a 15% improvement in accuracy on\nthe image CIFAR-10 dataset and a 43% improvement on the medical MIMIC-III\ndataset. These results validate the practical effectiveness and superiority of\nX-VFL, particularly in scenarios involving partially missing features and\nlocally independent inference."
                },
                "authors": [
                    {
                        "name": "Qinghua Yao"
                    },
                    {
                        "name": "Xiangrui Xu"
                    },
                    {
                        "name": "Zhize Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhize Li"
                },
                "author": "Zhize Li",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05568v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05568v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02085v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02085v3",
                "updated": "2025-08-07T16:46:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    16,
                    46,
                    44,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-04T05:51:55Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    5,
                    51,
                    55,
                    0,
                    216,
                    0
                ],
                "title": "SE-Agent: Self-Evolution Trajectory Optimization in Multi-Step Reasoning\n  with LLM-Based Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SE-Agent: Self-Evolution Trajectory Optimization in Multi-Step Reasoning\n  with LLM-Based Agents"
                },
                "summary": "Large Language Model (LLM)-based agents have recently shown impressive\ncapabilities in complex reasoning and tool use via multi-step interactions with\ntheir environments. While these agents have the potential to tackle complicated\ntasks, their problem-solving process, i.e., agents' interaction trajectory\nleading to task completion, remains underexploited. These trajectories contain\nrich feedback that can navigate agents toward the right directions for solving\nproblems correctly. Although prevailing approaches, such as Monte Carlo Tree\nSearch (MCTS), can effectively balance exploration and exploitation, they\nignore the interdependence among various trajectories and lack the diversity of\nsearch spaces, which leads to redundant reasoning and suboptimal outcomes. To\naddress these challenges, we propose SE-Agent, a Self-Evolution framework that\nenables Agents to optimize their reasoning processes iteratively. Our approach\nrevisits and enhances former pilot trajectories through three key operations:\nrevision, recombination, and refinement. This evolutionary mechanism enables\ntwo critical advantages: (1) it expands the search space beyond local optima by\nintelligently exploring diverse solution paths guided by previous trajectories,\nand (2) it leverages cross-trajectory inspiration to efficiently enhance\nperformance while mitigating the impact of suboptimal reasoning paths. Through\nthese mechanisms, SE-Agent achieves continuous self-evolution that\nincrementally improves reasoning quality. We evaluate SE-Agent on SWE-bench\nVerified to resolve real-world GitHub issues. Experimental results across five\nstrong LLMs show that integrating SE-Agent delivers up to 55% relative\nimprovement, achieving state-of-the-art performance among all open-source\nagents on SWE-bench Verified. Our code and demonstration materials are publicly\navailable at https://github.com/JARVIS-Xs/SE-Agent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based agents have recently shown impressive\ncapabilities in complex reasoning and tool use via multi-step interactions with\ntheir environments. While these agents have the potential to tackle complicated\ntasks, their problem-solving process, i.e., agents' interaction trajectory\nleading to task completion, remains underexploited. These trajectories contain\nrich feedback that can navigate agents toward the right directions for solving\nproblems correctly. Although prevailing approaches, such as Monte Carlo Tree\nSearch (MCTS), can effectively balance exploration and exploitation, they\nignore the interdependence among various trajectories and lack the diversity of\nsearch spaces, which leads to redundant reasoning and suboptimal outcomes. To\naddress these challenges, we propose SE-Agent, a Self-Evolution framework that\nenables Agents to optimize their reasoning processes iteratively. Our approach\nrevisits and enhances former pilot trajectories through three key operations:\nrevision, recombination, and refinement. This evolutionary mechanism enables\ntwo critical advantages: (1) it expands the search space beyond local optima by\nintelligently exploring diverse solution paths guided by previous trajectories,\nand (2) it leverages cross-trajectory inspiration to efficiently enhance\nperformance while mitigating the impact of suboptimal reasoning paths. Through\nthese mechanisms, SE-Agent achieves continuous self-evolution that\nincrementally improves reasoning quality. We evaluate SE-Agent on SWE-bench\nVerified to resolve real-world GitHub issues. Experimental results across five\nstrong LLMs show that integrating SE-Agent delivers up to 55% relative\nimprovement, achieving state-of-the-art performance among all open-source\nagents on SWE-bench Verified. Our code and demonstration materials are publicly\navailable at https://github.com/JARVIS-Xs/SE-Agent."
                },
                "authors": [
                    {
                        "name": "Jiaye Lin"
                    },
                    {
                        "name": "Yifu Guo"
                    },
                    {
                        "name": "Yuzhen Han"
                    },
                    {
                        "name": "Sen Hu"
                    },
                    {
                        "name": "Ziyi Ni"
                    },
                    {
                        "name": "Licheng Wang"
                    },
                    {
                        "name": "Mingguang Chen"
                    },
                    {
                        "name": "Daxin Jiang"
                    },
                    {
                        "name": "Binxing Jiao"
                    },
                    {
                        "name": "Chen Hu"
                    },
                    {
                        "name": "Huacan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huacan Wang"
                },
                "author": "Huacan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02085v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02085v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02934v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02934v2",
                "updated": "2025-08-07T16:39:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    16,
                    39,
                    32,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-04T22:20:34Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    22,
                    20,
                    34,
                    0,
                    216,
                    0
                ],
                "title": "Hubble Space Telescope Observations of the Interstellar Interloper\n  3I/ATLAS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hubble Space Telescope Observations of the Interstellar Interloper\n  3I/ATLAS"
                },
                "summary": "We present high angular resolution observations of the third known\ninterstellar interloper, 3I/ATLAS, from the Hubble Space Telescope. The object\nis clearly active at 3.8 au pre-perihelion, showing dust emitted from the hot\nSun-facing side of the nucleus and a weak, radiation pressure swept tail away\nfrom the Sun. We apply a simple model to estimate the mass loss rate in dust as\ndM/dt = 6 sqrt(a) kg/s, where a is the mean particle size in microns. With 1 <\na < 100, we infer dM/dt = 6 to 60 kg/s. A fit to the surface brightness\ndistribution of the inner coma limits the effective radius of the nucleus to be\nr < 2.8 km, assuming red geometric albedo 0.04. Conversely, the nucleus cannot\nbe smaller than 0.16 km in radius if its coma is supplied by sublimation of\ncarbon monoxide, and must be larger if a less volatile molecule drives the mass\nloss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present high angular resolution observations of the third known\ninterstellar interloper, 3I/ATLAS, from the Hubble Space Telescope. The object\nis clearly active at 3.8 au pre-perihelion, showing dust emitted from the hot\nSun-facing side of the nucleus and a weak, radiation pressure swept tail away\nfrom the Sun. We apply a simple model to estimate the mass loss rate in dust as\ndM/dt = 6 sqrt(a) kg/s, where a is the mean particle size in microns. With 1 <\na < 100, we infer dM/dt = 6 to 60 kg/s. A fit to the surface brightness\ndistribution of the inner coma limits the effective radius of the nucleus to be\nr < 2.8 km, assuming red geometric albedo 0.04. Conversely, the nucleus cannot\nbe smaller than 0.16 km in radius if its coma is supplied by sublimation of\ncarbon monoxide, and must be larger if a less volatile molecule drives the mass\nloss."
                },
                "authors": [
                    {
                        "name": "David Jewitt"
                    },
                    {
                        "name": "Man-To Hui"
                    },
                    {
                        "name": "Max Mutchler"
                    },
                    {
                        "name": "Yoonyoung Kim"
                    },
                    {
                        "name": "Jessica Agarwal"
                    }
                ],
                "author_detail": {
                    "name": "Jessica Agarwal"
                },
                "author": "Jessica Agarwal",
                "arxiv_comment": "13 pages, 4 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02934v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02934v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05553v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05553v1",
                "updated": "2025-08-07T16:33:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    16,
                    33,
                    45,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T16:33:45Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    16,
                    33,
                    45,
                    3,
                    219,
                    0
                ],
                "title": "Do Political Opinions Transfer Between Western Languages? An Analysis of\n  Unaligned and Aligned Multilingual LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Political Opinions Transfer Between Western Languages? An Analysis of\n  Unaligned and Aligned Multilingual LLMs"
                },
                "summary": "Public opinion surveys show cross-cultural differences in political opinions\nbetween socio-cultural contexts. However, there is no clear evidence whether\nthese differences translate to cross-lingual differences in multilingual large\nlanguage models (MLLMs). We analyze whether opinions transfer between languages\nor whether there are separate opinions for each language in MLLMs of various\nsizes across five Western languages. We evaluate MLLMs' opinions by prompting\nthem to report their (dis)agreement with political statements from voting\nadvice applications. To better understand the interaction between languages in\nthe models, we evaluate them both before and after aligning them with more left\nor right views using direct preference optimization and English alignment data\nonly. Our findings reveal that unaligned models show only very few significant\ncross-lingual differences in the political opinions they reflect. The political\nalignment shifts opinions almost uniformly across all five languages. We\nconclude that in Western language contexts, political opinions transfer between\nlanguages, demonstrating the challenges in achieving explicit socio-linguistic,\ncultural, and political alignment of MLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Public opinion surveys show cross-cultural differences in political opinions\nbetween socio-cultural contexts. However, there is no clear evidence whether\nthese differences translate to cross-lingual differences in multilingual large\nlanguage models (MLLMs). We analyze whether opinions transfer between languages\nor whether there are separate opinions for each language in MLLMs of various\nsizes across five Western languages. We evaluate MLLMs' opinions by prompting\nthem to report their (dis)agreement with political statements from voting\nadvice applications. To better understand the interaction between languages in\nthe models, we evaluate them both before and after aligning them with more left\nor right views using direct preference optimization and English alignment data\nonly. Our findings reveal that unaligned models show only very few significant\ncross-lingual differences in the political opinions they reflect. The political\nalignment shifts opinions almost uniformly across all five languages. We\nconclude that in Western language contexts, political opinions transfer between\nlanguages, demonstrating the challenges in achieving explicit socio-linguistic,\ncultural, and political alignment of MLLMs."
                },
                "authors": [
                    {
                        "name": "Franziska Weeber"
                    },
                    {
                        "name": "Tanise Ceron"
                    },
                    {
                        "name": "Sebastian Pad"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Pad"
                },
                "author": "Sebastian Pad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05553v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05553v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; J.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01834v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01834v2",
                "updated": "2025-08-07T16:33:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    16,
                    33,
                    16,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-03T16:44:05Z",
                "published_parsed": [
                    2025,
                    8,
                    3,
                    16,
                    44,
                    5,
                    6,
                    215,
                    0
                ],
                "title": "Efficient optimization of expensive black-box simulators via marginal\n  means, with application to neutrino detector design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient optimization of expensive black-box simulators via marginal\n  means, with application to neutrino detector design"
                },
                "summary": "With advances in scientific computing, computer experiments are increasingly\nused for optimizing complex systems. However, for modern applications, e.g.,\nthe optimization of nuclear physics detectors, each experiment run can require\nhundreds of CPU hours, making the optimization of its black-box simulator over\na high-dimensional space a challenging task. Given limited runs at inputs\n$\\mathbf{x}_1, \\cdots, \\mathbf{x}_n$, the best solution from these evaluated\ninputs can be far from optimal, particularly as dimensionality increases.\nExisting black-box methods, however, largely employ this ''pick-the-winner''\n(PW) solution, which leads to mediocre optimization performance. To address\nthis, we propose a new Black-box Optimization via Marginal Means (BOMM)\napproach. The key idea is a new estimator of a global optimizer $\\mathbf{x}^*$\nthat leverages the so-called marginal mean functions, which can be efficiently\ninferred with limited runs in high dimensions. Unlike PW, this estimator can\nselect solutions beyond evaluated inputs for improved optimization performance.\nAssuming the objective function follows a generalized additive model with\nunknown link function and under mild conditions, we prove that the BOMM\nestimator not only is consistent for optimization, but also has an optimization\nrate that tempers the ''curse-of-dimensionality'' faced by existing methods,\nthus enabling better performance as dimensionality increases. We present a\npractical framework for implementing BOMM using the transformed additive\nGaussian process surrogate model. Finally, we demonstrate the effectiveness of\nBOMM in numerical experiments and an application on neutrino detector\noptimization in nuclear physics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With advances in scientific computing, computer experiments are increasingly\nused for optimizing complex systems. However, for modern applications, e.g.,\nthe optimization of nuclear physics detectors, each experiment run can require\nhundreds of CPU hours, making the optimization of its black-box simulator over\na high-dimensional space a challenging task. Given limited runs at inputs\n$\\mathbf{x}_1, \\cdots, \\mathbf{x}_n$, the best solution from these evaluated\ninputs can be far from optimal, particularly as dimensionality increases.\nExisting black-box methods, however, largely employ this ''pick-the-winner''\n(PW) solution, which leads to mediocre optimization performance. To address\nthis, we propose a new Black-box Optimization via Marginal Means (BOMM)\napproach. The key idea is a new estimator of a global optimizer $\\mathbf{x}^*$\nthat leverages the so-called marginal mean functions, which can be efficiently\ninferred with limited runs in high dimensions. Unlike PW, this estimator can\nselect solutions beyond evaluated inputs for improved optimization performance.\nAssuming the objective function follows a generalized additive model with\nunknown link function and under mild conditions, we prove that the BOMM\nestimator not only is consistent for optimization, but also has an optimization\nrate that tempers the ''curse-of-dimensionality'' faced by existing methods,\nthus enabling better performance as dimensionality increases. We present a\npractical framework for implementing BOMM using the transformed additive\nGaussian process surrogate model. Finally, we demonstrate the effectiveness of\nBOMM in numerical experiments and an application on neutrino detector\noptimization in nuclear physics."
                },
                "authors": [
                    {
                        "name": "Hwanwoo Kim"
                    },
                    {
                        "name": "Simon Mak"
                    },
                    {
                        "name": "Ann-Kathrin Schuetz"
                    },
                    {
                        "name": "Alan Poon"
                    }
                ],
                "author_detail": {
                    "name": "Alan Poon"
                },
                "author": "Alan Poon",
                "arxiv_comment": "updated funding information",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01834v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01834v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00255v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00255v2",
                "updated": "2025-08-07T16:31:54Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    16,
                    31,
                    54,
                    3,
                    219,
                    0
                ],
                "published": "2025-03-31T22:02:24Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    22,
                    2,
                    24,
                    0,
                    90,
                    0
                ],
                "title": "SciReplicate-Bench: Benchmarking LLMs in Agent-driven Algorithmic\n  Reproduction from Research Papers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SciReplicate-Bench: Benchmarking LLMs in Agent-driven Algorithmic\n  Reproduction from Research Papers"
                },
                "summary": "This study evaluates large language models (LLMs) in generating code from\nalgorithm descriptions in recent NLP papers. The task requires two key\ncompetencies: (1) algorithm comprehension: synthesizing information from papers\nand academic literature to understand implementation logic, and (2) coding\nexpertise: identifying dependencies and correctly implementing necessary APIs.\nTo facilitate rigorous evaluation, we introduce SciReplicate-Bench, a benchmark\nof 100 tasks from 36 NLP papers published in 2024, featuring detailed\nannotations and comprehensive test cases. Building on SciReplicate-Bench, we\npropose Sci-Reproducer, a dual-agent framework consisting of a Paper Agent that\ninterprets algorithmic concepts from literature and a Code Agent that retrieves\ndependencies from repositories and implements solutions. To assess algorithm\nunderstanding, we introduce reasoning graph accuracy, which quantifies\nsimilarity between generated and reference reasoning graphs derived from code\ncomments and structure. For evaluating implementation quality, we employ\nexecution accuracy, CodeBLEU, and repository dependency/API recall metrics. In\nour experiments, we evaluate various powerful non-reasoning and reasoning LLMs\nas foundational models. The best-performing LLM using \\ModelName~achieves only\n39% execution accuracy, highlighting the benchmark's difficulty. Our analysis\nidentifies missing or inconsistent algorithm descriptions as key barriers to\nsuccessful reproduction. We make available our benchmark and code at\nhttps://github.com/xyzCS/SciReplicate-Bench and project homepage at\nhttps://xyzcs.github.io/scireplicate.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study evaluates large language models (LLMs) in generating code from\nalgorithm descriptions in recent NLP papers. The task requires two key\ncompetencies: (1) algorithm comprehension: synthesizing information from papers\nand academic literature to understand implementation logic, and (2) coding\nexpertise: identifying dependencies and correctly implementing necessary APIs.\nTo facilitate rigorous evaluation, we introduce SciReplicate-Bench, a benchmark\nof 100 tasks from 36 NLP papers published in 2024, featuring detailed\nannotations and comprehensive test cases. Building on SciReplicate-Bench, we\npropose Sci-Reproducer, a dual-agent framework consisting of a Paper Agent that\ninterprets algorithmic concepts from literature and a Code Agent that retrieves\ndependencies from repositories and implements solutions. To assess algorithm\nunderstanding, we introduce reasoning graph accuracy, which quantifies\nsimilarity between generated and reference reasoning graphs derived from code\ncomments and structure. For evaluating implementation quality, we employ\nexecution accuracy, CodeBLEU, and repository dependency/API recall metrics. In\nour experiments, we evaluate various powerful non-reasoning and reasoning LLMs\nas foundational models. The best-performing LLM using \\ModelName~achieves only\n39% execution accuracy, highlighting the benchmark's difficulty. Our analysis\nidentifies missing or inconsistent algorithm descriptions as key barriers to\nsuccessful reproduction. We make available our benchmark and code at\nhttps://github.com/xyzCS/SciReplicate-Bench and project homepage at\nhttps://xyzcs.github.io/scireplicate.github.io/."
                },
                "authors": [
                    {
                        "name": "Yanzheng Xiang"
                    },
                    {
                        "name": "Hanqi Yan"
                    },
                    {
                        "name": "Shuyin Ouyang"
                    },
                    {
                        "name": "Lin Gui"
                    },
                    {
                        "name": "Yulan He"
                    }
                ],
                "author_detail": {
                    "name": "Yulan He"
                },
                "author": "Yulan He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00255v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00255v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12496v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12496v2",
                "updated": "2025-08-07T16:23:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    16,
                    23,
                    33,
                    3,
                    219,
                    0
                ],
                "published": "2025-06-14T13:17:27Z",
                "published_parsed": [
                    2025,
                    6,
                    14,
                    13,
                    17,
                    27,
                    5,
                    165,
                    0
                ],
                "title": "Improving Factuality for Dialogue Response Generation via Graph-Based\n  Knowledge Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Factuality for Dialogue Response Generation via Graph-Based\n  Knowledge Augmentation"
                },
                "summary": "Large Language Models (LLMs) succeed in many natural language processing\ntasks. However, their tendency to hallucinate - generate plausible but\ninconsistent or factually incorrect text - can cause significant problems in\ncertain tasks, including response generation in dialogue. To mitigate this\nissue, we propose two novel graph knowledge-augmented frameworks, Dialogue\nResponse Generation via Textualised Graphs (TG-DRG) and Graph-Aware Dialogue\nResponse Generation (GA-DRG), which combine reasoning-guided dialogue\nreformulation, dialogue sense knowledge selection, and graph-enhanced response\ngeneration to improve the factuality of dialogue responses. To evaluate the\nfactuality of generated responses, we propose a dialogue fact score that\naddresses the limitations of existing fact-score methods in dialogue settings,\nproviding a more reliable assessment of factual consistency. We evaluate our\nmethods using different baselines on the OpendialKG and HybriDialogue datasets.\nOur methods noticeably improve factuality compared to other graph\nknowledge-augmentation baselines, including the state-of-the-art G-retriever,\nachieving improvements of 3.47% on OpendialKG and 3.12% on HybriDialogue in\nterms of dialogue fact score. The code will be released on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) succeed in many natural language processing\ntasks. However, their tendency to hallucinate - generate plausible but\ninconsistent or factually incorrect text - can cause significant problems in\ncertain tasks, including response generation in dialogue. To mitigate this\nissue, we propose two novel graph knowledge-augmented frameworks, Dialogue\nResponse Generation via Textualised Graphs (TG-DRG) and Graph-Aware Dialogue\nResponse Generation (GA-DRG), which combine reasoning-guided dialogue\nreformulation, dialogue sense knowledge selection, and graph-enhanced response\ngeneration to improve the factuality of dialogue responses. To evaluate the\nfactuality of generated responses, we propose a dialogue fact score that\naddresses the limitations of existing fact-score methods in dialogue settings,\nproviding a more reliable assessment of factual consistency. We evaluate our\nmethods using different baselines on the OpendialKG and HybriDialogue datasets.\nOur methods noticeably improve factuality compared to other graph\nknowledge-augmentation baselines, including the state-of-the-art G-retriever,\nachieving improvements of 3.47% on OpendialKG and 3.12% on HybriDialogue in\nterms of dialogue fact score. The code will be released on GitHub."
                },
                "authors": [
                    {
                        "name": "Xiangyan Chen"
                    },
                    {
                        "name": "Yujian Gan"
                    },
                    {
                        "name": "Yimeng Gu"
                    },
                    {
                        "name": "Matthew Purver"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Purver"
                },
                "author": "Matthew Purver",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12496v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12496v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05544v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05544v1",
                "updated": "2025-08-07T16:22:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    16,
                    22,
                    49,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T16:22:49Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    16,
                    22,
                    49,
                    3,
                    219,
                    0
                ],
                "title": "Conformal Sets in Multiple-Choice Question Answering under Black-Box\n  Settings with Provable Coverage Guarantees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conformal Sets in Multiple-Choice Question Answering under Black-Box\n  Settings with Provable Coverage Guarantees"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable progress in\nmultiple-choice question answering (MCQA), but their inherent unreliability,\nsuch as hallucination and overconfidence, limits their application in high-risk\ndomains. To address this, we propose a frequency-based uncertainty\nquantification method under black-box settings, leveraging conformal prediction\n(CP) to ensure provable coverage guarantees. Our approach involves multiple\nindependent samplings of the model's output distribution for each input, with\nthe most frequent sample serving as a reference to calculate predictive entropy\n(PE). Experimental evaluations across six LLMs and four datasets (MedMCQA,\nMedQA, MMLU, MMLU-Pro) demonstrate that frequency-based PE outperforms\nlogit-based PE in distinguishing between correct and incorrect predictions, as\nmeasured by AUROC. Furthermore, the method effectively controls the empirical\nmiscoverage rate under user-specified risk levels, validating that sampling\nfrequency can serve as a viable substitute for logit-based probabilities in\nblack-box scenarios. This work provides a distribution-free model-agnostic\nframework for reliable uncertainty quantification in MCQA with guaranteed\ncoverage, enhancing the trustworthiness of LLMs in practical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable progress in\nmultiple-choice question answering (MCQA), but their inherent unreliability,\nsuch as hallucination and overconfidence, limits their application in high-risk\ndomains. To address this, we propose a frequency-based uncertainty\nquantification method under black-box settings, leveraging conformal prediction\n(CP) to ensure provable coverage guarantees. Our approach involves multiple\nindependent samplings of the model's output distribution for each input, with\nthe most frequent sample serving as a reference to calculate predictive entropy\n(PE). Experimental evaluations across six LLMs and four datasets (MedMCQA,\nMedQA, MMLU, MMLU-Pro) demonstrate that frequency-based PE outperforms\nlogit-based PE in distinguishing between correct and incorrect predictions, as\nmeasured by AUROC. Furthermore, the method effectively controls the empirical\nmiscoverage rate under user-specified risk levels, validating that sampling\nfrequency can serve as a viable substitute for logit-based probabilities in\nblack-box scenarios. This work provides a distribution-free model-agnostic\nframework for reliable uncertainty quantification in MCQA with guaranteed\ncoverage, enhancing the trustworthiness of LLMs in practical applications."
                },
                "authors": [
                    {
                        "name": "Guang Yang"
                    },
                    {
                        "name": "Xinyang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xinyang Liu"
                },
                "author": "Xinyang Liu",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05544v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05544v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05545v1",
                "updated": "2025-08-07T16:22:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    16,
                    22,
                    49,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T16:22:49Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    16,
                    22,
                    49,
                    3,
                    219,
                    0
                ],
                "title": "PRvL: Quantifying the Capabilities and Risks of Large Language Models\n  for PII Redaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRvL: Quantifying the Capabilities and Risks of Large Language Models\n  for PII Redaction"
                },
                "summary": "Redacting Personally Identifiable Information (PII) from unstructured text is\ncritical for ensuring data privacy in regulated domains. While earlier\napproaches have relied on rule-based systems and domain-specific Named Entity\nRecognition (NER) models, these methods fail to generalize across formats and\ncontexts. Recent advances in Large Language Models (LLMs) offer a promising\nalternative, yet the effect of architectural and training choices on redaction\nperformance remains underexplored. LLMs have demonstrated strong performance in\ntasks that require contextual language understanding, including the redaction\nof PII in free-form text. Prior work suggests that with appropriate adaptation,\nLLMs can become effective contextual privacy learners. However, the\nconsequences of architectural and training choices for PII Redaction remain\nunderexplored. In this work, we present a comprehensive analysis of LLMs as\nprivacy-preserving PII Redaction systems. We evaluate a range of LLM\narchitectures and training strategies for their effectiveness in PII Redaction.\nOur analysis measures redaction performance, semantic preservation, and PII\nleakage, and compares these outcomes against latency and computational cost.\nThe results provide practical guidance for configuring LLM-based redactors that\nare accurate, efficient, and privacy-aware. To support reproducibility and\nreal-world deployment, we release PRvL, an open-source suite of fine-tuned\nmodels, and evaluation tools for general-purpose PII Redaction. PRvL is built\nentirely on open-source LLMs and supports multiple inference settings for\nflexibility and compliance. It is designed to be easily customized for\ndifferent domains and fully operable within secure, self-managed environments.\nThis enables data owners to perform redactions without relying on third-party\nservices or exposing sensitive content beyond their own infrastructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Redacting Personally Identifiable Information (PII) from unstructured text is\ncritical for ensuring data privacy in regulated domains. While earlier\napproaches have relied on rule-based systems and domain-specific Named Entity\nRecognition (NER) models, these methods fail to generalize across formats and\ncontexts. Recent advances in Large Language Models (LLMs) offer a promising\nalternative, yet the effect of architectural and training choices on redaction\nperformance remains underexplored. LLMs have demonstrated strong performance in\ntasks that require contextual language understanding, including the redaction\nof PII in free-form text. Prior work suggests that with appropriate adaptation,\nLLMs can become effective contextual privacy learners. However, the\nconsequences of architectural and training choices for PII Redaction remain\nunderexplored. In this work, we present a comprehensive analysis of LLMs as\nprivacy-preserving PII Redaction systems. We evaluate a range of LLM\narchitectures and training strategies for their effectiveness in PII Redaction.\nOur analysis measures redaction performance, semantic preservation, and PII\nleakage, and compares these outcomes against latency and computational cost.\nThe results provide practical guidance for configuring LLM-based redactors that\nare accurate, efficient, and privacy-aware. To support reproducibility and\nreal-world deployment, we release PRvL, an open-source suite of fine-tuned\nmodels, and evaluation tools for general-purpose PII Redaction. PRvL is built\nentirely on open-source LLMs and supports multiple inference settings for\nflexibility and compliance. It is designed to be easily customized for\ndifferent domains and fully operable within secure, self-managed environments.\nThis enables data owners to perform redactions without relying on third-party\nservices or exposing sensitive content beyond their own infrastructure."
                },
                "authors": [
                    {
                        "name": "Leon Garza"
                    },
                    {
                        "name": "Anantaa Kotal"
                    },
                    {
                        "name": "Aritran Piplai"
                    },
                    {
                        "name": "Lavanya Elluri"
                    },
                    {
                        "name": "Prajit Das"
                    },
                    {
                        "name": "Aman Chadha"
                    }
                ],
                "author_detail": {
                    "name": "Aman Chadha"
                },
                "author": "Aman Chadha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.08112v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.08112v5",
                "updated": "2025-08-07T16:19:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    16,
                    19,
                    6,
                    3,
                    219,
                    0
                ],
                "published": "2023-03-14T17:47:09Z",
                "published_parsed": [
                    2023,
                    3,
                    14,
                    17,
                    47,
                    9,
                    1,
                    73,
                    0
                ],
                "title": "Eliciting Latent Predictions from Transformers with the Tuned Lens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eliciting Latent Predictions from Transformers with the Tuned Lens"
                },
                "summary": "We analyze transformers from the perspective of iterative inference, seeking\nto understand how model predictions are refined layer by layer. To do so, we\ntrain an affine probe for each block in a frozen pretrained model, making it\npossible to decode every hidden state into a distribution over the vocabulary.\nOur method, the tuned lens, is a refinement of the earlier \"logit lens\"\ntechnique, which yielded useful insights but is often brittle.\n  We test our method on various autoregressive language models with up to 20B\nparameters, showing it to be more predictive, reliable and unbiased than the\nlogit lens. With causal experiments, we show the tuned lens uses similar\nfeatures to the model itself. We also find the trajectory of latent predictions\ncan be used to detect malicious inputs with high accuracy. All code needed to\nreproduce our results can be found at\nhttps://github.com/AlignmentResearch/tuned-lens.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We analyze transformers from the perspective of iterative inference, seeking\nto understand how model predictions are refined layer by layer. To do so, we\ntrain an affine probe for each block in a frozen pretrained model, making it\npossible to decode every hidden state into a distribution over the vocabulary.\nOur method, the tuned lens, is a refinement of the earlier \"logit lens\"\ntechnique, which yielded useful insights but is often brittle.\n  We test our method on various autoregressive language models with up to 20B\nparameters, showing it to be more predictive, reliable and unbiased than the\nlogit lens. With causal experiments, we show the tuned lens uses similar\nfeatures to the model itself. We also find the trajectory of latent predictions\ncan be used to detect malicious inputs with high accuracy. All code needed to\nreproduce our results can be found at\nhttps://github.com/AlignmentResearch/tuned-lens."
                },
                "authors": [
                    {
                        "name": "Nora Belrose"
                    },
                    {
                        "name": "Igor Ostrovsky"
                    },
                    {
                        "name": "Lev McKinney"
                    },
                    {
                        "name": "Zach Furman"
                    },
                    {
                        "name": "Logan Smith"
                    },
                    {
                        "name": "Danny Halawi"
                    },
                    {
                        "name": "Stella Biderman"
                    },
                    {
                        "name": "Jacob Steinhardt"
                    }
                ],
                "author_detail": {
                    "name": "Jacob Steinhardt"
                },
                "author": "Jacob Steinhardt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2303.08112v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.08112v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05537v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05537v1",
                "updated": "2025-08-07T16:13:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    16,
                    13,
                    24,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T16:13:24Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    16,
                    13,
                    24,
                    3,
                    219,
                    0
                ],
                "title": "Tractable Sharpness-Aware Learning of Probabilistic Circuits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tractable Sharpness-Aware Learning of Probabilistic Circuits"
                },
                "summary": "Probabilistic Circuits (PCs) are a class of generative models that allow\nexact and tractable inference for a wide range of queries. While recent\ndevelopments have enabled the learning of deep and expressive PCs, this\nincreased capacity can often lead to overfitting, especially when data is\nlimited. We analyze PC overfitting from a log-likelihood-landscape perspective\nand show that it is often caused by convergence to sharp optima that generalize\npoorly. Inspired by sharpness aware minimization in neural networks, we propose\na Hessian-based regularizer for training PCs. As a key contribution, we show\nthat the trace of the Hessian of the log-likelihood-a sharpness proxy that is\ntypically intractable in deep neural networks-can be computed efficiently for\nPCs. Minimizing this Hessian trace induces a gradient-norm-based regularizer\nthat yields simple closed-form parameter updates for EM, and integrates\nseamlessly with gradient based learning methods. Experiments on synthetic and\nreal-world datasets demonstrate that our method consistently guides PCs toward\nflatter minima, improves generalization performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probabilistic Circuits (PCs) are a class of generative models that allow\nexact and tractable inference for a wide range of queries. While recent\ndevelopments have enabled the learning of deep and expressive PCs, this\nincreased capacity can often lead to overfitting, especially when data is\nlimited. We analyze PC overfitting from a log-likelihood-landscape perspective\nand show that it is often caused by convergence to sharp optima that generalize\npoorly. Inspired by sharpness aware minimization in neural networks, we propose\na Hessian-based regularizer for training PCs. As a key contribution, we show\nthat the trace of the Hessian of the log-likelihood-a sharpness proxy that is\ntypically intractable in deep neural networks-can be computed efficiently for\nPCs. Minimizing this Hessian trace induces a gradient-norm-based regularizer\nthat yields simple closed-form parameter updates for EM, and integrates\nseamlessly with gradient based learning methods. Experiments on synthetic and\nreal-world datasets demonstrate that our method consistently guides PCs toward\nflatter minima, improves generalization performance."
                },
                "authors": [
                    {
                        "name": "Hrithik Suresh"
                    },
                    {
                        "name": "Sahil Sidheekh"
                    },
                    {
                        "name": "Vishnu Shreeram M. P"
                    },
                    {
                        "name": "Sriraam Natarajan"
                    },
                    {
                        "name": "Narayanan C. Krishnan"
                    }
                ],
                "author_detail": {
                    "name": "Narayanan C. Krishnan"
                },
                "author": "Narayanan C. Krishnan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05537v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05537v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05535v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05535v1",
                "updated": "2025-08-07T16:09:12Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    16,
                    9,
                    12,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T16:09:12Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    16,
                    9,
                    12,
                    3,
                    219,
                    0
                ],
                "title": "Mixed-Initiative Dialog for Human-Robot Collaborative Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixed-Initiative Dialog for Human-Robot Collaborative Manipulation"
                },
                "summary": "Effective robotic systems for long-horizon human-robot collaboration must\nadapt to a wide range of human partners, whose physical behavior, willingness\nto assist, and understanding of the robot's capabilities may change over time.\nThis demands a tightly coupled communication loop that grants both agents the\nflexibility to propose, accept, or decline requests as they coordinate toward\ncompleting the task effectively. We apply a Mixed-Initiative dialog paradigm to\nCollaborative human-roBot teaming and propose MICoBot, a system that handles\nthe common scenario where both agents, using natural language, take initiative\nin formulating, accepting, or rejecting proposals on who can best complete\ndifferent steps of a task. To handle diverse, task-directed dialog, and find\nsuccessful collaborative strategies that minimize human effort, MICoBot makes\ndecisions at three levels: (1) a meta-planner considers human dialog to\nformulate and code a high-level collaboration strategy, (2) a planner optimally\nallocates the remaining steps to either agent based on the robot's capabilities\n(measured by a simulation-pretrained affordance model) and the human's\nestimated availability to help, and (3) an action executor decides the\nlow-level actions to perform or words to say to the human. Our extensive\nevaluations in simulation and real-world -- on a physical robot with 18 unique\nhuman participants over 27 hours -- demonstrate the ability of our method to\neffectively collaborate with diverse human users, yielding significantly\nimproved task success and user experience than a pure LLM baseline and other\nagent allocation models. See additional videos and materials at\nhttps://robin-lab.cs.utexas.edu/MicoBot/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective robotic systems for long-horizon human-robot collaboration must\nadapt to a wide range of human partners, whose physical behavior, willingness\nto assist, and understanding of the robot's capabilities may change over time.\nThis demands a tightly coupled communication loop that grants both agents the\nflexibility to propose, accept, or decline requests as they coordinate toward\ncompleting the task effectively. We apply a Mixed-Initiative dialog paradigm to\nCollaborative human-roBot teaming and propose MICoBot, a system that handles\nthe common scenario where both agents, using natural language, take initiative\nin formulating, accepting, or rejecting proposals on who can best complete\ndifferent steps of a task. To handle diverse, task-directed dialog, and find\nsuccessful collaborative strategies that minimize human effort, MICoBot makes\ndecisions at three levels: (1) a meta-planner considers human dialog to\nformulate and code a high-level collaboration strategy, (2) a planner optimally\nallocates the remaining steps to either agent based on the robot's capabilities\n(measured by a simulation-pretrained affordance model) and the human's\nestimated availability to help, and (3) an action executor decides the\nlow-level actions to perform or words to say to the human. Our extensive\nevaluations in simulation and real-world -- on a physical robot with 18 unique\nhuman participants over 27 hours -- demonstrate the ability of our method to\neffectively collaborate with diverse human users, yielding significantly\nimproved task success and user experience than a pure LLM baseline and other\nagent allocation models. See additional videos and materials at\nhttps://robin-lab.cs.utexas.edu/MicoBot/."
                },
                "authors": [
                    {
                        "name": "Albert Yu"
                    },
                    {
                        "name": "Chengshu Li"
                    },
                    {
                        "name": "Luca Macesanu"
                    },
                    {
                        "name": "Arnav Balaji"
                    },
                    {
                        "name": "Ruchira Ray"
                    },
                    {
                        "name": "Raymond Mooney"
                    },
                    {
                        "name": "Roberto Martn-Martn"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Martn-Martn"
                },
                "author": "Roberto Martn-Martn",
                "arxiv_comment": "Project website at https://robin-lab.cs.utexas.edu/MicoBot/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05535v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05535v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.9; I.2.7; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10589v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10589v2",
                "updated": "2025-08-07T16:08:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    16,
                    8,
                    9,
                    3,
                    219,
                    0
                ],
                "published": "2025-04-14T18:00:05Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    18,
                    0,
                    5,
                    0,
                    104,
                    0
                ],
                "title": "Mitigating Eddington and Malmquist Biases in Latent-Inclination\n  Inference of the Tully-Fisher Relation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Eddington and Malmquist Biases in Latent-Inclination\n  Inference of the Tully-Fisher Relation"
                },
                "summary": "The Tully-Fisher relation is a vital distance indicator, but its precise\ninference is challenged by selection bias, statistical bias, and uncertain\ninclination corrections. This study presents a Bayesian framework that\nsimultaneously addresses these issues. To eliminate the need for individual\ninclination corrections, inclination is treated as a latent variable with a\nknown probability distribution. To correct for the distance-dependent Malmqvist\nbias arising from sample selection, the model incorporates Gaussian scatter in\nthe dependent variable, the distribution of the independent variable, and the\nobservational selection function into the data likelihood. To mitigate the\nstatistical bias -- termed the ``general Eddington bias'' -- caused by Gaussian\nscatter and the non-uniform distribution of the independent variable, two\nmethods are introduced: (1) analytical bias corrections applied to the\ndependent variable before likelihood computation, and (2) a dual-scatter model\nthat accounts for Gaussian scatter in the independent variable within the\nlikelihood function. The effectiveness of these methods is demonstrated using\nsimulated datasets. By rigorously addressing selection and statistical biases\nin a latent-variable regression analysis, this work provides a robust approach\nfor unbiased distance estimates from standardizable candles, which is critical\nfor improving the accuracy of Hubble constant determinations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Tully-Fisher relation is a vital distance indicator, but its precise\ninference is challenged by selection bias, statistical bias, and uncertain\ninclination corrections. This study presents a Bayesian framework that\nsimultaneously addresses these issues. To eliminate the need for individual\ninclination corrections, inclination is treated as a latent variable with a\nknown probability distribution. To correct for the distance-dependent Malmqvist\nbias arising from sample selection, the model incorporates Gaussian scatter in\nthe dependent variable, the distribution of the independent variable, and the\nobservational selection function into the data likelihood. To mitigate the\nstatistical bias -- termed the ``general Eddington bias'' -- caused by Gaussian\nscatter and the non-uniform distribution of the independent variable, two\nmethods are introduced: (1) analytical bias corrections applied to the\ndependent variable before likelihood computation, and (2) a dual-scatter model\nthat accounts for Gaussian scatter in the independent variable within the\nlikelihood function. The effectiveness of these methods is demonstrated using\nsimulated datasets. By rigorously addressing selection and statistical biases\nin a latent-variable regression analysis, this work provides a robust approach\nfor unbiased distance estimates from standardizable candles, which is critical\nfor improving the accuracy of Hubble constant determinations."
                },
                "authors": [
                    {
                        "name": "Hai Fu"
                    }
                ],
                "author_detail": {
                    "name": "Hai Fu"
                },
                "author": "Hai Fu",
                "arxiv_comment": "ApJ accepted. Python functions and notebook are available at\n  https://github.com/fuhaiastro/TFR_biases",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10589v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10589v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05534v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05534v1",
                "updated": "2025-08-07T16:06:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    16,
                    6,
                    58,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T16:06:58Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    16,
                    6,
                    58,
                    3,
                    219,
                    0
                ],
                "title": "CoCoLex: Confidence-guided Copy-based Decoding for Grounded Legal Text\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoCoLex: Confidence-guided Copy-based Decoding for Grounded Legal Text\n  Generation"
                },
                "summary": "Due to their ability to process long and complex contexts, LLMs can offer key\nbenefits to the Legal domain, but their adoption has been hindered by their\ntendency to generate unfaithful, ungrounded, or hallucinatory outputs. While\nRetrieval-Augmented Generation offers a promising solution by grounding\ngenerations in external knowledge, it offers no guarantee that the provided\ncontext will be effectively integrated. To address this, context-aware decoding\nstrategies have been proposed to amplify the influence of relevant context, but\nthey usually do not explicitly enforce faithfulness to the context. In this\nwork, we introduce Confidence-guided Copy-based Decoding for Legal Text\nGeneration (CoCoLex)-a decoding strategy that dynamically interpolates the\nmodel produced vocabulary distribution with a distribution derived based on\ncopying from the context. CoCoLex encourages direct copying based on the\nmodel's confidence, ensuring greater fidelity to the source. Experimental\nresults on five legal benchmarks demonstrate that CoCoLex outperforms existing\ncontext-aware decoding methods, particularly in long-form generation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to their ability to process long and complex contexts, LLMs can offer key\nbenefits to the Legal domain, but their adoption has been hindered by their\ntendency to generate unfaithful, ungrounded, or hallucinatory outputs. While\nRetrieval-Augmented Generation offers a promising solution by grounding\ngenerations in external knowledge, it offers no guarantee that the provided\ncontext will be effectively integrated. To address this, context-aware decoding\nstrategies have been proposed to amplify the influence of relevant context, but\nthey usually do not explicitly enforce faithfulness to the context. In this\nwork, we introduce Confidence-guided Copy-based Decoding for Legal Text\nGeneration (CoCoLex)-a decoding strategy that dynamically interpolates the\nmodel produced vocabulary distribution with a distribution derived based on\ncopying from the context. CoCoLex encourages direct copying based on the\nmodel's confidence, ensuring greater fidelity to the source. Experimental\nresults on five legal benchmarks demonstrate that CoCoLex outperforms existing\ncontext-aware decoding methods, particularly in long-form generation tasks."
                },
                "authors": [
                    {
                        "name": "Santosh T. Y. S. S"
                    },
                    {
                        "name": "Youssef Tarek Elkhayat"
                    },
                    {
                        "name": "Oana Ichim"
                    },
                    {
                        "name": "Pranav Shetty"
                    },
                    {
                        "name": "Dongsheng Wang"
                    },
                    {
                        "name": "Zhiqiang Ma"
                    },
                    {
                        "name": "Armineh Nourbakhsh"
                    },
                    {
                        "name": "Xiaomo Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaomo Liu"
                },
                "author": "Xiaomo Liu",
                "arxiv_doi": "10.18653/v1/2025.acl-long.931",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.18653/v1/2025.acl-long.931",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.05534v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05534v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to ACL 2025-Main Conference",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05531v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05531v1",
                "updated": "2025-08-07T16:02:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    16,
                    2,
                    15,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T16:02:15Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    16,
                    2,
                    15,
                    3,
                    219,
                    0
                ],
                "title": "Point cloud segmentation for 3D Clothed Human Layering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Point cloud segmentation for 3D Clothed Human Layering"
                },
                "summary": "3D Cloth modeling and simulation is essential for avatars creation in several\nfields, such as fashion, entertainment, and animation. Achieving high-quality\nresults is challenging due to the large variability of clothed body especially\nin the generation of realistic wrinkles. 3D scan acquisitions provide more\naccuracy in the representation of real-world objects but lack semantic\ninformation that can be inferred with a reliable semantic reconstruction\npipeline. To this aim, shape segmentation plays a crucial role in identifying\nthe semantic shape parts. However, current 3D shape segmentation methods are\ndesigned for scene understanding and interpretation and only few work is\ndevoted to modeling. In the context of clothed body modeling the segmentation\nis a preliminary step for fully semantic shape parts reconstruction namely the\nunderlying body and the involved garments. These parts represent several layers\nwith strong overlap in contrast with standard segmentation methods that provide\ndisjoint sets. In this work we propose a new 3D point cloud segmentation\nparadigm where each 3D point can be simultaneously associated to different\nlayers. In this fashion we can estimate the underlying body parts and the\nunseen clothed regions, i.e., the part of a cloth occluded by the clothed-layer\nabove. We name this segmentation paradigm clothed human layering. We create a\nnew synthetic dataset that simulates very realistic 3D scans with the ground\ntruth of the involved clothing layers. We propose and evaluate different neural\nnetwork settings to deal with 3D clothing layering. We considered both coarse\nand fine grained per-layer garment identification. Our experiments demonstrates\nthe benefit in introducing proper strategies for the segmentation on the\ngarment domain on both the synthetic and real-world scan datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Cloth modeling and simulation is essential for avatars creation in several\nfields, such as fashion, entertainment, and animation. Achieving high-quality\nresults is challenging due to the large variability of clothed body especially\nin the generation of realistic wrinkles. 3D scan acquisitions provide more\naccuracy in the representation of real-world objects but lack semantic\ninformation that can be inferred with a reliable semantic reconstruction\npipeline. To this aim, shape segmentation plays a crucial role in identifying\nthe semantic shape parts. However, current 3D shape segmentation methods are\ndesigned for scene understanding and interpretation and only few work is\ndevoted to modeling. In the context of clothed body modeling the segmentation\nis a preliminary step for fully semantic shape parts reconstruction namely the\nunderlying body and the involved garments. These parts represent several layers\nwith strong overlap in contrast with standard segmentation methods that provide\ndisjoint sets. In this work we propose a new 3D point cloud segmentation\nparadigm where each 3D point can be simultaneously associated to different\nlayers. In this fashion we can estimate the underlying body parts and the\nunseen clothed regions, i.e., the part of a cloth occluded by the clothed-layer\nabove. We name this segmentation paradigm clothed human layering. We create a\nnew synthetic dataset that simulates very realistic 3D scans with the ground\ntruth of the involved clothing layers. We propose and evaluate different neural\nnetwork settings to deal with 3D clothing layering. We considered both coarse\nand fine grained per-layer garment identification. Our experiments demonstrates\nthe benefit in introducing proper strategies for the segmentation on the\ngarment domain on both the synthetic and real-world scan datasets."
                },
                "authors": [
                    {
                        "name": "Davide Garavaso"
                    },
                    {
                        "name": "Federico Masi"
                    },
                    {
                        "name": "Pietro Musoni"
                    },
                    {
                        "name": "Umberto Castellani"
                    }
                ],
                "author_detail": {
                    "name": "Umberto Castellani"
                },
                "author": "Umberto Castellani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05531v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05531v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09032v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09032v2",
                "updated": "2025-08-07T16:01:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    16,
                    1,
                    43,
                    3,
                    219,
                    0
                ],
                "published": "2025-03-12T03:45:53Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    3,
                    45,
                    53,
                    2,
                    71,
                    0
                ],
                "title": "Teaching LLMs How to Learn with Contextual Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaching LLMs How to Learn with Contextual Fine-Tuning"
                },
                "summary": "Prompting Large Language Models (LLMs), or providing context on the expected\nmodel of operation, is an effective way to steer the outputs of such models to\nsatisfy human desiderata after they have been trained. But in rapidly evolving\ndomains, there is often need to fine-tune LLMs to improve either the kind of\nknowledge in their memory or their abilities to perform open ended reasoning in\nnew domains. When human's learn new concepts, we often do so by linking the new\nmaterial that we are studying to concepts we have already learned before. To\nthat end, we ask, \"can prompting help us teach LLMs how to learn\". In this\nwork, we study a novel generalization of instruction tuning, called contextual\nfine-tuning, to fine-tune LLMs. Our method leverages instructional prompts\ndesigned to mimic human cognitive strategies in learning and problem-solving to\nguide the learning process during training, aiming to improve the model's\ninterpretation and understanding of domain-specific knowledge. We empirically\ndemonstrate that this simple yet effective modification improves the ability of\nLLMs to be fine-tuned rapidly on new datasets both within the medical and\nfinancial domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompting Large Language Models (LLMs), or providing context on the expected\nmodel of operation, is an effective way to steer the outputs of such models to\nsatisfy human desiderata after they have been trained. But in rapidly evolving\ndomains, there is often need to fine-tune LLMs to improve either the kind of\nknowledge in their memory or their abilities to perform open ended reasoning in\nnew domains. When human's learn new concepts, we often do so by linking the new\nmaterial that we are studying to concepts we have already learned before. To\nthat end, we ask, \"can prompting help us teach LLMs how to learn\". In this\nwork, we study a novel generalization of instruction tuning, called contextual\nfine-tuning, to fine-tune LLMs. Our method leverages instructional prompts\ndesigned to mimic human cognitive strategies in learning and problem-solving to\nguide the learning process during training, aiming to improve the model's\ninterpretation and understanding of domain-specific knowledge. We empirically\ndemonstrate that this simple yet effective modification improves the ability of\nLLMs to be fine-tuned rapidly on new datasets both within the medical and\nfinancial domains."
                },
                "authors": [
                    {
                        "name": "Younwoo Choi"
                    },
                    {
                        "name": "Muhammad Adil Asif"
                    },
                    {
                        "name": "Ziwen Han"
                    },
                    {
                        "name": "John Willes"
                    },
                    {
                        "name": "Rahul G. Krishnan"
                    }
                ],
                "author_detail": {
                    "name": "Rahul G. Krishnan"
                },
                "author": "Rahul G. Krishnan",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09032v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09032v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07836v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07836v2",
                "updated": "2025-08-07T15:59:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    59,
                    19,
                    3,
                    219,
                    0
                ],
                "published": "2024-07-31T19:05:00Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    19,
                    5,
                    0,
                    2,
                    213,
                    0
                ],
                "title": "Learned Single-Pass Multitasking Perceptual Graphics for Immersive\n  Displays",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learned Single-Pass Multitasking Perceptual Graphics for Immersive\n  Displays"
                },
                "summary": "Emerging immersive display technologies efficiently utilize resources with\nperceptual graphics methods such as foveated rendering and denoising. Running\nmultiple perceptual graphics methods challenges devices with limited power and\ncomputational resources. We propose a computationally-lightweight learned\nmultitasking perceptual graphics model. Given RGB images and text-prompts, our\nmodel performs text-described perceptual tasks in a single inference step.\nSimply daisy-chaining multiple models or training dedicated models can lead to\nmodel management issues and exhaust computational resources. In contrast, our\nflexible method unlocks consistent high quality perceptual effects with\nreasonable compute, supporting various permutations at varied intensities using\nadjectives in text prompts (e.g. mildly, lightly). Text-guidance provides ease\nof use for dynamic requirements such as creative processes. To train our model,\nwe propose a dataset containing source and perceptually enhanced images with\ncorresponding text prompts. We evaluate our model on desktop and embedded\nplatforms and validate perceptual quality through a user study.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging immersive display technologies efficiently utilize resources with\nperceptual graphics methods such as foveated rendering and denoising. Running\nmultiple perceptual graphics methods challenges devices with limited power and\ncomputational resources. We propose a computationally-lightweight learned\nmultitasking perceptual graphics model. Given RGB images and text-prompts, our\nmodel performs text-described perceptual tasks in a single inference step.\nSimply daisy-chaining multiple models or training dedicated models can lead to\nmodel management issues and exhaust computational resources. In contrast, our\nflexible method unlocks consistent high quality perceptual effects with\nreasonable compute, supporting various permutations at varied intensities using\nadjectives in text prompts (e.g. mildly, lightly). Text-guidance provides ease\nof use for dynamic requirements such as creative processes. To train our model,\nwe propose a dataset containing source and perceptually enhanced images with\ncorresponding text prompts. We evaluate our model on desktop and embedded\nplatforms and validate perceptual quality through a user study."
                },
                "authors": [
                    {
                        "name": "Doa Ylmaz"
                    },
                    {
                        "name": "He Wang"
                    },
                    {
                        "name": "Towaki Takikawa"
                    },
                    {
                        "name": "Duygu Ceylan"
                    },
                    {
                        "name": "Kaan Akit"
                    }
                ],
                "author_detail": {
                    "name": "Kaan Akit"
                },
                "author": "Kaan Akit",
                "arxiv_doi": "10.1145/3746027.3754801",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746027.3754801",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.07836v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07836v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02583v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02583v2",
                "updated": "2025-08-07T15:57:23Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    57,
                    23,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-04T16:39:24Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    39,
                    24,
                    0,
                    216,
                    0
                ],
                "title": "CAMA: Enhancing Mathematical Reasoning in Large Language Models with\n  Causal Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAMA: Enhancing Mathematical Reasoning in Large Language Models with\n  Causal Knowledge"
                },
                "summary": "Large Language Models (LLMs) have demonstrated strong performance across a\nwide range of tasks, yet they still struggle with complex mathematical\nreasoning, a challenge fundamentally rooted in deep structural dependencies. To\naddress this challenge, we propose \\textbf{CA}usal \\textbf{MA}thematician\n(\\textbf{CAMA}), a two-stage causal framework that equips LLMs with explicit,\nreusable mathematical structure. In the learning stage, CAMA first constructs\nthe \\textbf{M}athematical \\textbf{C}ausal \\textbf{G}raph (\\textbf{MCG}), a\nhigh-level representation of solution strategies, by combining LLM priors with\ncausal discovery algorithms applied to a corpus of question-solution pairs. The\nresulting MCG encodes essential knowledge points and their causal dependencies.\nTo better align the graph with downstream reasoning tasks, CAMA further refines\nthe MCG through iterative feedback derived from a selected subset of the\nquestion-solution pairs. In the reasoning stage, given a new question, CAMA\ndynamically extracts a task-relevant subgraph from the MCG, conditioned on both\nthe question content and the LLM's intermediate reasoning trace. This subgraph,\nwhich encodes the most pertinent knowledge points and their causal\ndependencies, is then injected back into the LLM to guide its reasoning\nprocess. Empirical results on real-world datasets show that CAMA significantly\nimproves LLM performance on challenging mathematical problems. Furthermore, our\nexperiments demonstrate that structured guidance consistently outperforms\nunstructured alternatives, and that incorporating asymmetric causal\nrelationships yields greater improvements than using symmetric associations\nalone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated strong performance across a\nwide range of tasks, yet they still struggle with complex mathematical\nreasoning, a challenge fundamentally rooted in deep structural dependencies. To\naddress this challenge, we propose \\textbf{CA}usal \\textbf{MA}thematician\n(\\textbf{CAMA}), a two-stage causal framework that equips LLMs with explicit,\nreusable mathematical structure. In the learning stage, CAMA first constructs\nthe \\textbf{M}athematical \\textbf{C}ausal \\textbf{G}raph (\\textbf{MCG}), a\nhigh-level representation of solution strategies, by combining LLM priors with\ncausal discovery algorithms applied to a corpus of question-solution pairs. The\nresulting MCG encodes essential knowledge points and their causal dependencies.\nTo better align the graph with downstream reasoning tasks, CAMA further refines\nthe MCG through iterative feedback derived from a selected subset of the\nquestion-solution pairs. In the reasoning stage, given a new question, CAMA\ndynamically extracts a task-relevant subgraph from the MCG, conditioned on both\nthe question content and the LLM's intermediate reasoning trace. This subgraph,\nwhich encodes the most pertinent knowledge points and their causal\ndependencies, is then injected back into the LLM to guide its reasoning\nprocess. Empirical results on real-world datasets show that CAMA significantly\nimproves LLM performance on challenging mathematical problems. Furthermore, our\nexperiments demonstrate that structured guidance consistently outperforms\nunstructured alternatives, and that incorporating asymmetric causal\nrelationships yields greater improvements than using symmetric associations\nalone."
                },
                "authors": [
                    {
                        "name": "Lei Zan"
                    },
                    {
                        "name": "Keli Zhang"
                    },
                    {
                        "name": "Ruichu Cai"
                    },
                    {
                        "name": "Lujia Pan"
                    }
                ],
                "author_detail": {
                    "name": "Lujia Pan"
                },
                "author": "Lujia Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02583v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02583v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05527v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05527v1",
                "updated": "2025-08-07T15:55:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    55,
                    46,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T15:55:46Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    55,
                    46,
                    3,
                    219,
                    0
                ],
                "title": "AI vs. Human Moderators: A Comparative Evaluation of Multimodal LLMs in\n  Content Moderation for Brand Safety",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI vs. Human Moderators: A Comparative Evaluation of Multimodal LLMs in\n  Content Moderation for Brand Safety"
                },
                "summary": "As the volume of video content online grows exponentially, the demand for\nmoderation of unsafe videos has surpassed human capabilities, posing both\noperational and mental health challenges. While recent studies demonstrated the\nmerits of Multimodal Large Language Models (MLLMs) in various video\nunderstanding tasks, their application to multimodal content moderation, a\ndomain that requires nuanced understanding of both visual and textual cues,\nremains relatively underexplored. In this work, we benchmark the capabilities\nof MLLMs in brand safety classification, a critical subset of content\nmoderation for safe-guarding advertising integrity. To this end, we introduce a\nnovel, multimodal and multilingual dataset, meticulously labeled by\nprofessional reviewers in a multitude of risk categories. Through a detailed\ncomparative analysis, we demonstrate the effectiveness of MLLMs such as Gemini,\nGPT, and Llama in multimodal brand safety, and evaluate their accuracy and cost\nefficiency compared to professional human reviewers. Furthermore, we present an\nin-depth discussion shedding light on limitations of MLLMs and failure cases.\nWe are releasing our dataset alongside this paper to facilitate future research\non effective and responsible brand safety and content moderation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the volume of video content online grows exponentially, the demand for\nmoderation of unsafe videos has surpassed human capabilities, posing both\noperational and mental health challenges. While recent studies demonstrated the\nmerits of Multimodal Large Language Models (MLLMs) in various video\nunderstanding tasks, their application to multimodal content moderation, a\ndomain that requires nuanced understanding of both visual and textual cues,\nremains relatively underexplored. In this work, we benchmark the capabilities\nof MLLMs in brand safety classification, a critical subset of content\nmoderation for safe-guarding advertising integrity. To this end, we introduce a\nnovel, multimodal and multilingual dataset, meticulously labeled by\nprofessional reviewers in a multitude of risk categories. Through a detailed\ncomparative analysis, we demonstrate the effectiveness of MLLMs such as Gemini,\nGPT, and Llama in multimodal brand safety, and evaluate their accuracy and cost\nefficiency compared to professional human reviewers. Furthermore, we present an\nin-depth discussion shedding light on limitations of MLLMs and failure cases.\nWe are releasing our dataset alongside this paper to facilitate future research\non effective and responsible brand safety and content moderation."
                },
                "authors": [
                    {
                        "name": "Adi Levi"
                    },
                    {
                        "name": "Or Levi"
                    },
                    {
                        "name": "Sardhendu Mishra"
                    },
                    {
                        "name": "Jonathan Morra"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Morra"
                },
                "author": "Jonathan Morra",
                "arxiv_comment": "Accepted to the Computer Vision in Advertising and Marketing (CVAM)\n  workshop at ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05527v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05527v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.10; I.2.7; H.3.3; H.4.3; K.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05525v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05525v1",
                "updated": "2025-08-07T15:53:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    53,
                    30,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T15:53:30Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    53,
                    30,
                    3,
                    219,
                    0
                ],
                "title": "The World According to LLMs: How Geographic Origin Influences LLMs'\n  Entity Deduction Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The World According to LLMs: How Geographic Origin Influences LLMs'\n  Entity Deduction Capabilities"
                },
                "summary": "Large Language Models (LLMs) have been extensively tuned to mitigate explicit\nbiases, yet they often exhibit subtle implicit biases rooted in their\npre-training data. Rather than directly probing LLMs with human-crafted\nquestions that may trigger guardrails, we propose studying how models behave\nwhen they proactively ask questions themselves. The 20 Questions game, a\nmulti-turn deduction task, serves as an ideal testbed for this purpose. We\nsystematically evaluate geographic performance disparities in entity deduction\nusing a new dataset, Geo20Q+, consisting of both notable people and culturally\nsignificant objects (e.g., foods, landmarks, animals) from diverse regions. We\ntest popular LLMs across two gameplay configurations (canonical 20-question and\nunlimited turns) and in seven languages (English, Hindi, Mandarin, Japanese,\nFrench, Spanish, and Turkish). Our results reveal geographic disparities: LLMs\nare substantially more successful at deducing entities from the Global North\nthan the Global South, and the Global West than the Global East. While\nWikipedia pageviews and pre-training corpus frequency correlate mildly with\nperformance, they fail to fully explain these disparities. Notably, the\nlanguage in which the game is played has minimal impact on performance gaps.\nThese findings demonstrate the value of creative, free-form evaluation\nframeworks for uncovering subtle biases in LLMs that remain hidden in standard\nprompting setups. By analyzing how models initiate and pursue reasoning goals\nover multiple turns, we find geographic and cultural disparities embedded in\ntheir reasoning processes. We release the dataset (Geo20Q+) and code at\nhttps://sites.google.com/view/llmbias20q/home.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been extensively tuned to mitigate explicit\nbiases, yet they often exhibit subtle implicit biases rooted in their\npre-training data. Rather than directly probing LLMs with human-crafted\nquestions that may trigger guardrails, we propose studying how models behave\nwhen they proactively ask questions themselves. The 20 Questions game, a\nmulti-turn deduction task, serves as an ideal testbed for this purpose. We\nsystematically evaluate geographic performance disparities in entity deduction\nusing a new dataset, Geo20Q+, consisting of both notable people and culturally\nsignificant objects (e.g., foods, landmarks, animals) from diverse regions. We\ntest popular LLMs across two gameplay configurations (canonical 20-question and\nunlimited turns) and in seven languages (English, Hindi, Mandarin, Japanese,\nFrench, Spanish, and Turkish). Our results reveal geographic disparities: LLMs\nare substantially more successful at deducing entities from the Global North\nthan the Global South, and the Global West than the Global East. While\nWikipedia pageviews and pre-training corpus frequency correlate mildly with\nperformance, they fail to fully explain these disparities. Notably, the\nlanguage in which the game is played has minimal impact on performance gaps.\nThese findings demonstrate the value of creative, free-form evaluation\nframeworks for uncovering subtle biases in LLMs that remain hidden in standard\nprompting setups. By analyzing how models initiate and pursue reasoning goals\nover multiple turns, we find geographic and cultural disparities embedded in\ntheir reasoning processes. We release the dataset (Geo20Q+) and code at\nhttps://sites.google.com/view/llmbias20q/home."
                },
                "authors": [
                    {
                        "name": "Harsh Nishant Lalai"
                    },
                    {
                        "name": "Raj Sanjay Shah"
                    },
                    {
                        "name": "Jiaxin Pei"
                    },
                    {
                        "name": "Sashank Varma"
                    },
                    {
                        "name": "Yi-Chia Wang"
                    },
                    {
                        "name": "Ali Emami"
                    }
                ],
                "author_detail": {
                    "name": "Ali Emami"
                },
                "author": "Ali Emami",
                "arxiv_comment": "Conference on Language Modeling 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05525v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05525v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05516v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05516v1",
                "updated": "2025-08-07T15:47:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    47,
                    55,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T15:47:55Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    47,
                    55,
                    3,
                    219,
                    0
                ],
                "title": "FS-IQA: Certified Feature Smoothing for Robust Image Quality Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FS-IQA: Certified Feature Smoothing for Robust Image Quality Assessment"
                },
                "summary": "We propose a novel certified defense method for Image Quality Assessment\n(IQA) models based on randomized smoothing with noise applied in the feature\nspace rather than the input space. Unlike prior approaches that inject Gaussian\nnoise directly into input images, often degrading visual quality, our method\npreserves image fidelity while providing robustness guarantees. To formally\nconnect noise levels in the feature space with corresponding input-space\nperturbations, we analyze the maximum singular value of the backbone network's\nJacobian. Our approach supports both full-reference (FR) and no-reference (NR)\nIQA models without requiring any architectural modifications, suitable for\nvarious scenarios. It is also computationally efficient, requiring a single\nbackbone forward pass per image. Compared to previous methods, it reduces\ninference time by 99.5% without certification and by 20.6% when certification\nis applied. We validate our method with extensive experiments on two benchmark\ndatasets, involving six widely-used FR and NR IQA models and comparisons\nagainst five state-of-the-art certified defenses. Our results demonstrate\nconsistent improvements in correlation with subjective quality scores by up to\n30.9%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a novel certified defense method for Image Quality Assessment\n(IQA) models based on randomized smoothing with noise applied in the feature\nspace rather than the input space. Unlike prior approaches that inject Gaussian\nnoise directly into input images, often degrading visual quality, our method\npreserves image fidelity while providing robustness guarantees. To formally\nconnect noise levels in the feature space with corresponding input-space\nperturbations, we analyze the maximum singular value of the backbone network's\nJacobian. Our approach supports both full-reference (FR) and no-reference (NR)\nIQA models without requiring any architectural modifications, suitable for\nvarious scenarios. It is also computationally efficient, requiring a single\nbackbone forward pass per image. Compared to previous methods, it reduces\ninference time by 99.5% without certification and by 20.6% when certification\nis applied. We validate our method with extensive experiments on two benchmark\ndatasets, involving six widely-used FR and NR IQA models and comparisons\nagainst five state-of-the-art certified defenses. Our results demonstrate\nconsistent improvements in correlation with subjective quality scores by up to\n30.9%."
                },
                "authors": [
                    {
                        "name": "Ekaterina Shumitskaya"
                    },
                    {
                        "name": "Dmitriy Vatolin"
                    },
                    {
                        "name": "Anastasia Antsiferova"
                    }
                ],
                "author_detail": {
                    "name": "Anastasia Antsiferova"
                },
                "author": "Anastasia Antsiferova",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05516v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05516v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05512v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05512v1",
                "updated": "2025-08-07T15:46:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    46,
                    53,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T15:46:53Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    46,
                    53,
                    3,
                    219,
                    0
                ],
                "title": "RankArena: A Unified Platform for Evaluating Retrieval, Reranking and\n  RAG with Human and LLM Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RankArena: A Unified Platform for Evaluating Retrieval, Reranking and\n  RAG with Human and LLM Feedback"
                },
                "summary": "Evaluating the quality of retrieval-augmented generation (RAG) and document\nreranking systems remains challenging due to the lack of scalable,\nuser-centric, and multi-perspective evaluation tools. We introduce RankArena, a\nunified platform for comparing and analysing the performance of retrieval\npipelines, rerankers, and RAG systems using structured human and LLM-based\nfeedback as well as for collecting such feedback. RankArena supports multiple\nevaluation modes: direct reranking visualisation, blind pairwise comparisons\nwith human or LLM voting, supervised manual document annotation, and end-to-end\nRAG answer quality assessment. It captures fine-grained relevance feedback\nthrough both pairwise preferences and full-list annotations, along with\nauxiliary metadata such as movement metrics, annotation time, and quality\nratings. The platform also integrates LLM-as-a-judge evaluation, enabling\ncomparison between model-generated rankings and human ground truth annotations.\nAll interactions are stored as structured evaluation datasets that can be used\nto train rerankers, reward models, judgment agents, or retrieval strategy\nselectors. Our platform is publicly available at https://rankarena.ngrok.io/,\nand the Demo video is provided https://youtu.be/jIYAP4PaSSI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the quality of retrieval-augmented generation (RAG) and document\nreranking systems remains challenging due to the lack of scalable,\nuser-centric, and multi-perspective evaluation tools. We introduce RankArena, a\nunified platform for comparing and analysing the performance of retrieval\npipelines, rerankers, and RAG systems using structured human and LLM-based\nfeedback as well as for collecting such feedback. RankArena supports multiple\nevaluation modes: direct reranking visualisation, blind pairwise comparisons\nwith human or LLM voting, supervised manual document annotation, and end-to-end\nRAG answer quality assessment. It captures fine-grained relevance feedback\nthrough both pairwise preferences and full-list annotations, along with\nauxiliary metadata such as movement metrics, annotation time, and quality\nratings. The platform also integrates LLM-as-a-judge evaluation, enabling\ncomparison between model-generated rankings and human ground truth annotations.\nAll interactions are stored as structured evaluation datasets that can be used\nto train rerankers, reward models, judgment agents, or retrieval strategy\nselectors. Our platform is publicly available at https://rankarena.ngrok.io/,\nand the Demo video is provided https://youtu.be/jIYAP4PaSSI."
                },
                "authors": [
                    {
                        "name": "Abdelrahman Abdallah"
                    },
                    {
                        "name": "Mahmoud Abdalla"
                    },
                    {
                        "name": "Bhawna Piryani"
                    },
                    {
                        "name": "Jamshid Mozafari"
                    },
                    {
                        "name": "Mohammed Ali"
                    },
                    {
                        "name": "Adam Jatowt"
                    }
                ],
                "author_detail": {
                    "name": "Adam Jatowt"
                },
                "author": "Adam Jatowt",
                "arxiv_comment": "Accept at CIKM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05512v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05512v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05509v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05509v1",
                "updated": "2025-08-07T15:42:00Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    42,
                    0,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T15:42:00Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    42,
                    0,
                    3,
                    219,
                    0
                ],
                "title": "LAG: Logic-Augmented Generation from a Cartesian Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAG: Logic-Augmented Generation from a Cartesian Perspective"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\na wide range of tasks, yet exhibit critical limitations in knowledge-intensive\ntasks, often generating hallucinations when faced with questions requiring\nspecialized expertise. While retrieval-augmented generation (RAG) mitigates\nthis by integrating external knowledge, it struggles with complex reasoning\nscenarios due to its reliance on direct semantic retrieval and lack of\nstructured logical organization. Inspired by Cartesian principles from\n\\textit{Discours de la m\\'ethode}, this paper introduces Logic-Augmented\nGeneration (LAG), a novel paradigm that reframes knowledge augmentation through\nsystematic question decomposition and dependency-aware reasoning. Specifically,\nLAG first decomposes complex questions into atomic sub-questions ordered by\nlogical dependencies. It then resolves these sequentially, using prior answers\nto guide context retrieval for subsequent sub-questions, ensuring stepwise\ngrounding in logical chain. To prevent error propagation, LAG incorporates a\nlogical termination mechanism that halts inference upon encountering\nunanswerable sub-questions and reduces wasted computation on excessive\nreasoning. Finally, it synthesizes all sub-resolutions to generate verified\nresponses. Experiments on four benchmark datasets demonstrate that LAG\nsignificantly enhances reasoning robustness, reduces hallucination, and aligns\nLLM problem-solving with human cognition, offering a principled alternative to\nexisting RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities across\na wide range of tasks, yet exhibit critical limitations in knowledge-intensive\ntasks, often generating hallucinations when faced with questions requiring\nspecialized expertise. While retrieval-augmented generation (RAG) mitigates\nthis by integrating external knowledge, it struggles with complex reasoning\nscenarios due to its reliance on direct semantic retrieval and lack of\nstructured logical organization. Inspired by Cartesian principles from\n\\textit{Discours de la m\\'ethode}, this paper introduces Logic-Augmented\nGeneration (LAG), a novel paradigm that reframes knowledge augmentation through\nsystematic question decomposition and dependency-aware reasoning. Specifically,\nLAG first decomposes complex questions into atomic sub-questions ordered by\nlogical dependencies. It then resolves these sequentially, using prior answers\nto guide context retrieval for subsequent sub-questions, ensuring stepwise\ngrounding in logical chain. To prevent error propagation, LAG incorporates a\nlogical termination mechanism that halts inference upon encountering\nunanswerable sub-questions and reduces wasted computation on excessive\nreasoning. Finally, it synthesizes all sub-resolutions to generate verified\nresponses. Experiments on four benchmark datasets demonstrate that LAG\nsignificantly enhances reasoning robustness, reduces hallucination, and aligns\nLLM problem-solving with human cognition, offering a principled alternative to\nexisting RAG systems."
                },
                "authors": [
                    {
                        "name": "Yilin Xiao"
                    },
                    {
                        "name": "Chuang Zhou"
                    },
                    {
                        "name": "Qinggang Zhang"
                    },
                    {
                        "name": "Su Dong"
                    },
                    {
                        "name": "Shengyuan Chen"
                    },
                    {
                        "name": "Xiao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xiao Huang"
                },
                "author": "Xiao Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05509v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05509v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19230v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19230v3",
                "updated": "2025-08-07T15:41:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    41,
                    44,
                    3,
                    219,
                    0
                ],
                "published": "2025-06-24T01:28:29Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    1,
                    28,
                    29,
                    1,
                    175,
                    0
                ],
                "title": "gcor: A Python Implementation of Categorical Gini Correlation and Its\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "gcor: A Python Implementation of Categorical Gini Correlation and Its\n  Inference"
                },
                "summary": "Categorical Gini Correlation (CGC), introduced by Dang et al. (2020), is a\nnovel dependence measure designed to quantify the association between a\nnumerical variable and a categorical variable. It has appealing properties\ncompared to existing dependence measures, such as zero correlation mutually\nimplying independence between the variables. It has also shown superior\nperformance over existing methods when applied to feature screening for\nclassification. This article presents a Python implementation for computing\nCGC, constructing confidence intervals, and performing independence tests based\non it. Efficient algorithms have been implemented for all procedures, and they\nhave been optimized using vectorization and parallelization to enhance\ncomputational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Categorical Gini Correlation (CGC), introduced by Dang et al. (2020), is a\nnovel dependence measure designed to quantify the association between a\nnumerical variable and a categorical variable. It has appealing properties\ncompared to existing dependence measures, such as zero correlation mutually\nimplying independence between the variables. It has also shown superior\nperformance over existing methods when applied to feature screening for\nclassification. This article presents a Python implementation for computing\nCGC, constructing confidence intervals, and performing independence tests based\non it. Efficient algorithms have been implemented for all procedures, and they\nhave been optimized using vectorization and parallelization to enhance\ncomputational efficiency."
                },
                "authors": [
                    {
                        "name": "Sameera Hewage"
                    }
                ],
                "author_detail": {
                    "name": "Sameera Hewage"
                },
                "author": "Sameera Hewage",
                "arxiv_comment": "Corrected typos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19230v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19230v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62H20, 62G20, 62-07, 62R07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04094v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04094v2",
                "updated": "2025-08-07T15:40:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    40,
                    48,
                    3,
                    219,
                    0
                ],
                "published": "2024-10-05T09:27:52Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    9,
                    27,
                    52,
                    5,
                    279,
                    0
                ],
                "title": "BloomWise: Enhancing Problem-Solving capabilities of Large Language\n  Models using Bloom's-Taxonomy-Inspired Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BloomWise: Enhancing Problem-Solving capabilities of Large Language\n  Models using Bloom's-Taxonomy-Inspired Prompts"
                },
                "summary": "Despite the remarkable capabilities of large language models (LLMs) across a\nrange of tasks, mathematical reasoning remains a challenging frontier.\nMotivated by the observation that humans learn more effectively when prompted\nnot what to think but how to think, we introduce BloomWise, a\ncognitively-inspired prompting technique designed to enhance LLMs' performance\non mathematical problem solving while making their solutions more explainable.\nBloomWise encourages LLMs to generate solutions - in the form of explanations -\nby progressing through a sequence of cognitive operations-from basic (e.g.,\nremembering) to more advanced reasoning skills (e.g., evaluating) - mirroring\nhow humans build understanding. The process iterates through these levels,\nhalting early if a convergence criterion is met: specifically, if two or more\nconsecutive levels yield the same answer, the solution from the earliest such\nlevel is output; otherwise, the process continues until all levels are\ncompleted. Through extensive experiments across five popular math reasoning\ndatasets, we demonstrate the effectiveness of BloomWise. We also present\ncomprehensive ablation studies to analyze the strengths of each component\nwithin our system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable capabilities of large language models (LLMs) across a\nrange of tasks, mathematical reasoning remains a challenging frontier.\nMotivated by the observation that humans learn more effectively when prompted\nnot what to think but how to think, we introduce BloomWise, a\ncognitively-inspired prompting technique designed to enhance LLMs' performance\non mathematical problem solving while making their solutions more explainable.\nBloomWise encourages LLMs to generate solutions - in the form of explanations -\nby progressing through a sequence of cognitive operations-from basic (e.g.,\nremembering) to more advanced reasoning skills (e.g., evaluating) - mirroring\nhow humans build understanding. The process iterates through these levels,\nhalting early if a convergence criterion is met: specifically, if two or more\nconsecutive levels yield the same answer, the solution from the earliest such\nlevel is output; otherwise, the process continues until all levels are\ncompleted. Through extensive experiments across five popular math reasoning\ndatasets, we demonstrate the effectiveness of BloomWise. We also present\ncomprehensive ablation studies to analyze the strengths of each component\nwithin our system."
                },
                "authors": [
                    {
                        "name": "Maria-Eleni Zoumpoulidi"
                    },
                    {
                        "name": "Georgios Paraskevopoulos"
                    },
                    {
                        "name": "Alexandros Potamianos"
                    }
                ],
                "author_detail": {
                    "name": "Alexandros Potamianos"
                },
                "author": "Alexandros Potamianos",
                "arxiv_comment": "16 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04094v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04094v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05508v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05508v1",
                "updated": "2025-08-07T15:39:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    39,
                    48,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T15:39:48Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    39,
                    48,
                    3,
                    219,
                    0
                ],
                "title": "Auto-Eval Judge: Towards a General Agentic Framework for Task Completion\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-Eval Judge: Towards a General Agentic Framework for Task Completion\n  Evaluation"
                },
                "summary": "The increasing adoption of foundation models as agents across diverse domains\nnecessitates a robust evaluation framework. Current methods, such as\nLLM-as-a-Judge, focus only on final outputs, overlooking the step-by-step\nreasoning that drives agentic decision-making. Meanwhile, existing\nAgent-as-a-Judge systems, where one agent evaluates another's task completion,\nare typically designed for narrow, domain-specific settings. To address this\ngap, we propose a generalizable, modular framework for evaluating agent task\ncompletion independent of the task domain. The framework emulates human-like\nevaluation by decomposing tasks into sub-tasks and validating each step using\navailable information, such as the agent's output and reasoning. Each module\ncontributes to a specific aspect of the evaluation process, and their outputs\nare aggregated to produce a final verdict on task completion. We validate our\nframework by evaluating the Magentic-One Actor Agent on two benchmarks, GAIA\nand BigCodeBench. Our Judge Agent predicts task success with closer agreement\nto human evaluations, achieving 4.76% and 10.52% higher alignment accuracy,\nrespectively, compared to the GPT-4o based LLM-as-a-Judge baseline. This\ndemonstrates the potential of our proposed general-purpose evaluation\nframework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing adoption of foundation models as agents across diverse domains\nnecessitates a robust evaluation framework. Current methods, such as\nLLM-as-a-Judge, focus only on final outputs, overlooking the step-by-step\nreasoning that drives agentic decision-making. Meanwhile, existing\nAgent-as-a-Judge systems, where one agent evaluates another's task completion,\nare typically designed for narrow, domain-specific settings. To address this\ngap, we propose a generalizable, modular framework for evaluating agent task\ncompletion independent of the task domain. The framework emulates human-like\nevaluation by decomposing tasks into sub-tasks and validating each step using\navailable information, such as the agent's output and reasoning. Each module\ncontributes to a specific aspect of the evaluation process, and their outputs\nare aggregated to produce a final verdict on task completion. We validate our\nframework by evaluating the Magentic-One Actor Agent on two benchmarks, GAIA\nand BigCodeBench. Our Judge Agent predicts task success with closer agreement\nto human evaluations, achieving 4.76% and 10.52% higher alignment accuracy,\nrespectively, compared to the GPT-4o based LLM-as-a-Judge baseline. This\ndemonstrates the potential of our proposed general-purpose evaluation\nframework."
                },
                "authors": [
                    {
                        "name": "Roshita Bhonsle"
                    },
                    {
                        "name": "Rishav Dutta"
                    },
                    {
                        "name": "Sneha Vavilapalli"
                    },
                    {
                        "name": "Harsh Seth"
                    },
                    {
                        "name": "Abubakarr Jaye"
                    },
                    {
                        "name": "Yapei Chang"
                    },
                    {
                        "name": "Mukund Rungta"
                    },
                    {
                        "name": "Emmanuel Aboah Boateng"
                    },
                    {
                        "name": "Sadid Hasan"
                    },
                    {
                        "name": "Ehi Nosakhare"
                    },
                    {
                        "name": "Soundar Srinivasan"
                    }
                ],
                "author_detail": {
                    "name": "Soundar Srinivasan"
                },
                "author": "Soundar Srinivasan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05508v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05508v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16435v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16435v2",
                "updated": "2025-08-07T15:39:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    39,
                    27,
                    3,
                    219,
                    0
                ],
                "published": "2025-02-23T04:21:32Z",
                "published_parsed": [
                    2025,
                    2,
                    23,
                    4,
                    21,
                    32,
                    6,
                    54,
                    0
                ],
                "title": "Human Cognitive Benchmarks Reveal Foundational Visual Gaps in MLLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human Cognitive Benchmarks Reveal Foundational Visual Gaps in MLLMs"
                },
                "summary": "Despite significant progress on popular multimodal benchmarks,\nstate-of-the-art Multimodal Large Language Models (MLLMs) continue to struggle\nwith basic visual reasoning tasks that are trivially solved by humans, such as\nrecognizing spatial relationships. To systematically investigate this gap, we\nintroduce VisFactor, a benchmark that digitizes 20 vision-centric subtests from\na well-established cognitive psychology assessment. These subtests span four\ncore domains of human visual cognition: (1) Visualization and Spatial\nProcessing, (2) Perceptual and Closure, (3) Memory, and (4) Reasoning. We\nevaluate 20 frontier MLLMs from GPT, Gemini, Claude, LLaMA, Qwen, and SEED\nfamilies. The best-performing model achieves a score of only 25.19 out of 100,\nwith consistent failures on tasks such as mental rotation, spatial relation\ninference, and figure-ground discrimination, regardless of model size or\nprompting strategy. These findings suggest that current MLLM performance gains\non high-level benchmarks do not reflect human-like low-level visual cognition,\nchallenging the assumption that large-scale pretraining naturally induces\ngestalt-like perceptual capabilities. The dataset and evaluation toolkit are\npublicly available at: https://github.com/CUHK-ARISE/VisFactor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite significant progress on popular multimodal benchmarks,\nstate-of-the-art Multimodal Large Language Models (MLLMs) continue to struggle\nwith basic visual reasoning tasks that are trivially solved by humans, such as\nrecognizing spatial relationships. To systematically investigate this gap, we\nintroduce VisFactor, a benchmark that digitizes 20 vision-centric subtests from\na well-established cognitive psychology assessment. These subtests span four\ncore domains of human visual cognition: (1) Visualization and Spatial\nProcessing, (2) Perceptual and Closure, (3) Memory, and (4) Reasoning. We\nevaluate 20 frontier MLLMs from GPT, Gemini, Claude, LLaMA, Qwen, and SEED\nfamilies. The best-performing model achieves a score of only 25.19 out of 100,\nwith consistent failures on tasks such as mental rotation, spatial relation\ninference, and figure-ground discrimination, regardless of model size or\nprompting strategy. These findings suggest that current MLLM performance gains\non high-level benchmarks do not reflect human-like low-level visual cognition,\nchallenging the assumption that large-scale pretraining naturally induces\ngestalt-like perceptual capabilities. The dataset and evaluation toolkit are\npublicly available at: https://github.com/CUHK-ARISE/VisFactor."
                },
                "authors": [
                    {
                        "name": "Jen-Tse Huang"
                    },
                    {
                        "name": "Dasen Dai"
                    },
                    {
                        "name": "Jen-Yuan Huang"
                    },
                    {
                        "name": "Youliang Yuan"
                    },
                    {
                        "name": "Xiaoyuan Liu"
                    },
                    {
                        "name": "Wenxuan Wang"
                    },
                    {
                        "name": "Wenxiang Jiao"
                    },
                    {
                        "name": "Pinjia He"
                    },
                    {
                        "name": "Zhaopeng Tu"
                    },
                    {
                        "name": "Haodong Duan"
                    }
                ],
                "author_detail": {
                    "name": "Haodong Duan"
                },
                "author": "Haodong Duan",
                "arxiv_comment": "Update: Evaluated 20 MLLMs; Added generated test cases",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16435v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16435v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05503v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05503v1",
                "updated": "2025-08-07T15:36:38Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    36,
                    38,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T15:36:38Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    36,
                    38,
                    3,
                    219,
                    0
                ],
                "title": "AutoIAD: Manager-Driven Multi-Agent Collaboration for Automated\n  Industrial Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoIAD: Manager-Driven Multi-Agent Collaboration for Automated\n  Industrial Anomaly Detection"
                },
                "summary": "Industrial anomaly detection (IAD) is critical for manufacturing quality\ncontrol, but conventionally requires significant manual effort for various\napplication scenarios. This paper introduces AutoIAD, a multi-agent\ncollaboration framework, specifically designed for end-to-end automated\ndevelopment of industrial visual anomaly detection. AutoIAD leverages a\nManager-Driven central agent to orchestrate specialized sub-agents (including\nData Preparation, Data Loader, Model Designer, Trainer) and integrates a\ndomain-specific knowledge base, which intelligently handles the entire pipeline\nusing raw industrial image data to develop a trained anomaly detection model.\nWe construct a comprehensive benchmark using MVTec AD datasets to evaluate\nAutoIAD across various LLM backends. Extensive experiments demonstrate that\nAutoIAD significantly outperforms existing general-purpose agentic\ncollaboration frameworks and traditional AutoML frameworks in task completion\nrate and model performance (AUROC), while effectively mitigating issues like\nhallucination through iterative refinement. Ablation studies further confirm\nthe crucial roles of the Manager central agent and the domain knowledge base\nmodule in producing robust and high-quality IAD solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Industrial anomaly detection (IAD) is critical for manufacturing quality\ncontrol, but conventionally requires significant manual effort for various\napplication scenarios. This paper introduces AutoIAD, a multi-agent\ncollaboration framework, specifically designed for end-to-end automated\ndevelopment of industrial visual anomaly detection. AutoIAD leverages a\nManager-Driven central agent to orchestrate specialized sub-agents (including\nData Preparation, Data Loader, Model Designer, Trainer) and integrates a\ndomain-specific knowledge base, which intelligently handles the entire pipeline\nusing raw industrial image data to develop a trained anomaly detection model.\nWe construct a comprehensive benchmark using MVTec AD datasets to evaluate\nAutoIAD across various LLM backends. Extensive experiments demonstrate that\nAutoIAD significantly outperforms existing general-purpose agentic\ncollaboration frameworks and traditional AutoML frameworks in task completion\nrate and model performance (AUROC), while effectively mitigating issues like\nhallucination through iterative refinement. Ablation studies further confirm\nthe crucial roles of the Manager central agent and the domain knowledge base\nmodule in producing robust and high-quality IAD solutions."
                },
                "authors": [
                    {
                        "name": "Dongwei Ji"
                    },
                    {
                        "name": "Bingzhang Hu"
                    },
                    {
                        "name": "Yi Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yi Zhou"
                },
                "author": "Yi Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05503v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05503v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21021v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21021v2",
                "updated": "2025-08-07T15:36:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    36,
                    21,
                    3,
                    219,
                    0
                ],
                "published": "2024-10-28T13:42:27Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    42,
                    27,
                    0,
                    302,
                    0
                ],
                "title": "A Stein Gradient Descent Approach for Doubly Intractable Distributions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Stein Gradient Descent Approach for Doubly Intractable Distributions"
                },
                "summary": "Bayesian inference for doubly intractable distributions is challenging\nbecause they include intractable terms, which are functions of parameters of\ninterest. Although several alternatives have been developed for such models,\nthey are computationally intensive due to repeated auxiliary variable\nsimulations. We propose a novel Monte Carlo Stein variational gradient descent\n(MC-SVGD) approach for inference for doubly intractable distributions. Through\nan efficient gradient approximation, our MC-SVGD approach rapidly transforms an\narbitrary reference distribution to approximate the posterior distribution of\ninterest, without necessitating any predefined variational distribution class\nfor the posterior. Such a transport map is obtained by minimizing\nKullback-Leibler divergence between the transformed and posterior distributions\nin a reproducing kernel Hilbert space (RKHS). We also investigate the\nconvergence rate of the proposed method. We illustrate the application of the\nmethod to challenging examples, including a Potts model, an exponential random\ngraph model, and a Conway--Maxwell--Poisson regression model. The proposed\nmethod achieves substantial computational gains over existing algorithms, while\nproviding comparable inferential performance for the posterior distributions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian inference for doubly intractable distributions is challenging\nbecause they include intractable terms, which are functions of parameters of\ninterest. Although several alternatives have been developed for such models,\nthey are computationally intensive due to repeated auxiliary variable\nsimulations. We propose a novel Monte Carlo Stein variational gradient descent\n(MC-SVGD) approach for inference for doubly intractable distributions. Through\nan efficient gradient approximation, our MC-SVGD approach rapidly transforms an\narbitrary reference distribution to approximate the posterior distribution of\ninterest, without necessitating any predefined variational distribution class\nfor the posterior. Such a transport map is obtained by minimizing\nKullback-Leibler divergence between the transformed and posterior distributions\nin a reproducing kernel Hilbert space (RKHS). We also investigate the\nconvergence rate of the proposed method. We illustrate the application of the\nmethod to challenging examples, including a Potts model, an exponential random\ngraph model, and a Conway--Maxwell--Poisson regression model. The proposed\nmethod achieves substantial computational gains over existing algorithms, while\nproviding comparable inferential performance for the posterior distributions."
                },
                "authors": [
                    {
                        "name": "Heesang Lee"
                    },
                    {
                        "name": "Songhee Kim"
                    },
                    {
                        "name": "Bokgyeong Kang"
                    },
                    {
                        "name": "Jaewoo Park"
                    }
                ],
                "author_detail": {
                    "name": "Jaewoo Park"
                },
                "author": "Jaewoo Park",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21021v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21021v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05501v1",
                "updated": "2025-08-07T15:36:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    36,
                    17,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T15:36:17Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    36,
                    17,
                    3,
                    219,
                    0
                ],
                "title": "SMOL-MapSeg: Show Me One Label",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SMOL-MapSeg: Show Me One Label"
                },
                "summary": "Historical maps are valuable for studying changes to the Earth's surface.\nWith the rise of deep learning, models like UNet have been used to extract\ninformation from these maps through semantic segmentation. Recently,\npre-trained foundation models have shown strong performance across domains such\nas autonomous driving, medical imaging, and industrial inspection. However,\nthey struggle with historical maps. These models are trained on modern or\ndomain-specific images, where patterns can be tied to predefined concepts\nthrough common sense or expert knowledge. Historical maps lack such consistency\n-- similar concepts can appear in vastly different shapes and styles. To\naddress this, we propose On-Need Declarative (OND) knowledge-based prompting,\nwhich introduces explicit prompts to guide the model on what patterns\ncorrespond to which concepts. This allows users to specify the target concept\nand pattern during inference (on-need inference). We implement this by\nreplacing the prompt encoder of the foundation model SAM with our OND prompting\nmechanism and fine-tune it on historical maps. The resulting model is called\nSMOL-MapSeg (Show Me One Label). Experiments show that SMOL-MapSeg can\naccurately segment classes defined by OND knowledge. It can also adapt to\nunseen classes through few-shot fine-tuning. Additionally, it outperforms a\nUNet-based baseline in average segmentation performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Historical maps are valuable for studying changes to the Earth's surface.\nWith the rise of deep learning, models like UNet have been used to extract\ninformation from these maps through semantic segmentation. Recently,\npre-trained foundation models have shown strong performance across domains such\nas autonomous driving, medical imaging, and industrial inspection. However,\nthey struggle with historical maps. These models are trained on modern or\ndomain-specific images, where patterns can be tied to predefined concepts\nthrough common sense or expert knowledge. Historical maps lack such consistency\n-- similar concepts can appear in vastly different shapes and styles. To\naddress this, we propose On-Need Declarative (OND) knowledge-based prompting,\nwhich introduces explicit prompts to guide the model on what patterns\ncorrespond to which concepts. This allows users to specify the target concept\nand pattern during inference (on-need inference). We implement this by\nreplacing the prompt encoder of the foundation model SAM with our OND prompting\nmechanism and fine-tune it on historical maps. The resulting model is called\nSMOL-MapSeg (Show Me One Label). Experiments show that SMOL-MapSeg can\naccurately segment classes defined by OND knowledge. It can also adapt to\nunseen classes through few-shot fine-tuning. Additionally, it outperforms a\nUNet-based baseline in average segmentation performance."
                },
                "authors": [
                    {
                        "name": "Yunshuang Yuan"
                    },
                    {
                        "name": "Frank Thiemann"
                    },
                    {
                        "name": "Thorsten Dahms"
                    },
                    {
                        "name": "Monika Sester"
                    }
                ],
                "author_detail": {
                    "name": "Monika Sester"
                },
                "author": "Monika Sester",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05498v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05498v1",
                "updated": "2025-08-07T15:34:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    34,
                    41,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T15:34:41Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    34,
                    41,
                    3,
                    219,
                    0
                ],
                "title": "GRAIL:Learning to Interact with Large Knowledge Graphs for Retrieval\n  Augmented Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GRAIL:Learning to Interact with Large Knowledge Graphs for Retrieval\n  Augmented Reasoning"
                },
                "summary": "Large Language Models (LLMs) integrated with Retrieval-Augmented Generation\n(RAG) techniques have exhibited remarkable performance across a wide range of\ndomains. However, existing RAG approaches primarily operate on unstructured\ndata and demonstrate limited capability in handling structured knowledge such\nas knowledge graphs. Meanwhile, current graph retrieval methods fundamentally\nstruggle to capture holistic graph structures while simultaneously facing\nprecision control challenges that manifest as either critical information gaps\nor excessive redundant connections, collectively undermining reasoning\nperformance. To address this challenge, we propose GRAIL: Graph-Retrieval\nAugmented Interactive Learning, a framework designed to interact with\nlarge-scale graphs for retrieval-augmented reasoning. Specifically, GRAIL\nintegrates LLM-guided random exploration with path filtering to establish a\ndata synthesis pipeline, where a fine-grained reasoning trajectory is\nautomatically generated for each task. Based on the synthesized data, we then\nemploy a two-stage training process to learn a policy that dynamically decides\nthe optimal actions at each reasoning step. The overall objective of\nprecision-conciseness balance in graph retrieval is decoupled into fine-grained\nprocess-supervised rewards to enhance data efficiency and training stability.\nIn practical deployment, GRAIL adopts an interactive retrieval paradigm,\nenabling the model to autonomously explore graph paths while dynamically\nbalancing retrieval breadth and precision. Extensive experiments have shown\nthat GRAIL achieves an average accuracy improvement of 21.01% and F1\nimprovement of 22.43% on three knowledge graph question-answering datasets. Our\nsource code and datasets is available at https://github.com/Changgeww/GRAIL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) integrated with Retrieval-Augmented Generation\n(RAG) techniques have exhibited remarkable performance across a wide range of\ndomains. However, existing RAG approaches primarily operate on unstructured\ndata and demonstrate limited capability in handling structured knowledge such\nas knowledge graphs. Meanwhile, current graph retrieval methods fundamentally\nstruggle to capture holistic graph structures while simultaneously facing\nprecision control challenges that manifest as either critical information gaps\nor excessive redundant connections, collectively undermining reasoning\nperformance. To address this challenge, we propose GRAIL: Graph-Retrieval\nAugmented Interactive Learning, a framework designed to interact with\nlarge-scale graphs for retrieval-augmented reasoning. Specifically, GRAIL\nintegrates LLM-guided random exploration with path filtering to establish a\ndata synthesis pipeline, where a fine-grained reasoning trajectory is\nautomatically generated for each task. Based on the synthesized data, we then\nemploy a two-stage training process to learn a policy that dynamically decides\nthe optimal actions at each reasoning step. The overall objective of\nprecision-conciseness balance in graph retrieval is decoupled into fine-grained\nprocess-supervised rewards to enhance data efficiency and training stability.\nIn practical deployment, GRAIL adopts an interactive retrieval paradigm,\nenabling the model to autonomously explore graph paths while dynamically\nbalancing retrieval breadth and precision. Extensive experiments have shown\nthat GRAIL achieves an average accuracy improvement of 21.01% and F1\nimprovement of 22.43% on three knowledge graph question-answering datasets. Our\nsource code and datasets is available at https://github.com/Changgeww/GRAIL."
                },
                "authors": [
                    {
                        "name": "Ge Chang"
                    },
                    {
                        "name": "Jinbo Su"
                    },
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Pengfei Yang"
                    },
                    {
                        "name": "Yuhao Shang"
                    },
                    {
                        "name": "Huiwen Zheng"
                    },
                    {
                        "name": "Hongli Ma"
                    },
                    {
                        "name": "Yan Liang"
                    },
                    {
                        "name": "Yuanchun Li"
                    },
                    {
                        "name": "Yunxin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yunxin Liu"
                },
                "author": "Yunxin Liu",
                "arxiv_comment": "9 pages,3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05498v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05498v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05496v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05496v1",
                "updated": "2025-08-07T15:34:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    34,
                    6,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T15:34:06Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    34,
                    6,
                    3,
                    219,
                    0
                ],
                "title": "InfiAlign: A Scalable and Sample-Efficient Framework for Aligning LLMs\n  to Enhance Reasoning Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiAlign: A Scalable and Sample-Efficient Framework for Aligning LLMs\n  to Enhance Reasoning Capabilities"
                },
                "summary": "Large language models (LLMs) have exhibited impressive reasoning abilities on\na wide range of complex tasks. However, enhancing these capabilities through\npost-training remains resource intensive, particularly in terms of data and\ncomputational cost. Although recent efforts have sought to improve sample\nefficiency through selective data curation, existing methods often rely on\nheuristic or task-specific strategies that hinder scalability. In this work, we\nintroduce InfiAlign, a scalable and sample-efficient post-training framework\nthat integrates supervised fine-tuning (SFT) with Direct Preference\nOptimization (DPO) to align LLMs for enhanced reasoning. At the core of\nInfiAlign is a robust data selection pipeline that automatically curates\nhigh-quality alignment data from open-source reasoning datasets using\nmultidimensional quality metrics. This pipeline enables significant performance\ngains while drastically reducing data requirements and remains extensible to\nnew data sources. When applied to the Qwen2.5-Math-7B-Base model, our SFT model\nachieves performance on par with DeepSeek-R1-Distill-Qwen-7B, while using only\napproximately 12% of the training data, and demonstrates strong generalization\nacross diverse reasoning tasks. Additional improvements are obtained through\nthe application of DPO, with particularly notable gains in mathematical\nreasoning tasks. The model achieves an average improvement of 3.89% on AIME\n24/25 benchmarks. Our results highlight the effectiveness of combining\nprincipled data selection with full-stage post-training, offering a practical\nsolution for aligning large reasoning models in a scalable and data-efficient\nmanner. The model checkpoints are available at\nhttps://huggingface.co/InfiX-ai/InfiAlign-Qwen-7B-SFT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have exhibited impressive reasoning abilities on\na wide range of complex tasks. However, enhancing these capabilities through\npost-training remains resource intensive, particularly in terms of data and\ncomputational cost. Although recent efforts have sought to improve sample\nefficiency through selective data curation, existing methods often rely on\nheuristic or task-specific strategies that hinder scalability. In this work, we\nintroduce InfiAlign, a scalable and sample-efficient post-training framework\nthat integrates supervised fine-tuning (SFT) with Direct Preference\nOptimization (DPO) to align LLMs for enhanced reasoning. At the core of\nInfiAlign is a robust data selection pipeline that automatically curates\nhigh-quality alignment data from open-source reasoning datasets using\nmultidimensional quality metrics. This pipeline enables significant performance\ngains while drastically reducing data requirements and remains extensible to\nnew data sources. When applied to the Qwen2.5-Math-7B-Base model, our SFT model\nachieves performance on par with DeepSeek-R1-Distill-Qwen-7B, while using only\napproximately 12% of the training data, and demonstrates strong generalization\nacross diverse reasoning tasks. Additional improvements are obtained through\nthe application of DPO, with particularly notable gains in mathematical\nreasoning tasks. The model achieves an average improvement of 3.89% on AIME\n24/25 benchmarks. Our results highlight the effectiveness of combining\nprincipled data selection with full-stage post-training, offering a practical\nsolution for aligning large reasoning models in a scalable and data-efficient\nmanner. The model checkpoints are available at\nhttps://huggingface.co/InfiX-ai/InfiAlign-Qwen-7B-SFT."
                },
                "authors": [
                    {
                        "name": "Shuo Cai"
                    },
                    {
                        "name": "Su Lu"
                    },
                    {
                        "name": "Qi Zhou"
                    },
                    {
                        "name": "Kejing Yang"
                    },
                    {
                        "name": "Zhijie Sang"
                    },
                    {
                        "name": "Congkai Xie"
                    },
                    {
                        "name": "Hongxia Yang"
                    }
                ],
                "author_detail": {
                    "name": "Hongxia Yang"
                },
                "author": "Hongxia Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05496v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05496v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05492v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05492v1",
                "updated": "2025-08-07T15:28:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    28,
                    34,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T15:28:34Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    28,
                    34,
                    3,
                    219,
                    0
                ],
                "title": "MoMA: A Mixture-of-Multimodal-Agents Architecture for Enhancing Clinical\n  Prediction Modelling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoMA: A Mixture-of-Multimodal-Agents Architecture for Enhancing Clinical\n  Prediction Modelling"
                },
                "summary": "Multimodal electronic health record (EHR) data provide richer, complementary\ninsights into patient health compared to single-modality data. However,\neffectively integrating diverse data modalities for clinical prediction\nmodeling remains challenging due to the substantial data requirements. We\nintroduce a novel architecture, Mixture-of-Multimodal-Agents (MoMA), designed\nto leverage multiple large language model (LLM) agents for clinical prediction\ntasks using multimodal EHR data. MoMA employs specialized LLM agents\n(\"specialist agents\") to convert non-textual modalities, such as medical images\nand laboratory results, into structured textual summaries. These summaries,\ntogether with clinical notes, are combined by another LLM (\"aggregator agent\")\nto generate a unified multimodal summary, which is then used by a third LLM\n(\"predictor agent\") to produce clinical predictions. Evaluating MoMA on three\nprediction tasks using real-world datasets with different modality combinations\nand prediction settings, MoMA outperforms current state-of-the-art methods,\nhighlighting its enhanced accuracy and flexibility across various tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal electronic health record (EHR) data provide richer, complementary\ninsights into patient health compared to single-modality data. However,\neffectively integrating diverse data modalities for clinical prediction\nmodeling remains challenging due to the substantial data requirements. We\nintroduce a novel architecture, Mixture-of-Multimodal-Agents (MoMA), designed\nto leverage multiple large language model (LLM) agents for clinical prediction\ntasks using multimodal EHR data. MoMA employs specialized LLM agents\n(\"specialist agents\") to convert non-textual modalities, such as medical images\nand laboratory results, into structured textual summaries. These summaries,\ntogether with clinical notes, are combined by another LLM (\"aggregator agent\")\nto generate a unified multimodal summary, which is then used by a third LLM\n(\"predictor agent\") to produce clinical predictions. Evaluating MoMA on three\nprediction tasks using real-world datasets with different modality combinations\nand prediction settings, MoMA outperforms current state-of-the-art methods,\nhighlighting its enhanced accuracy and flexibility across various tasks."
                },
                "authors": [
                    {
                        "name": "Jifan Gao"
                    },
                    {
                        "name": "Mahmudur Rahman"
                    },
                    {
                        "name": "John Caskey"
                    },
                    {
                        "name": "Madeline Oguss"
                    },
                    {
                        "name": "Ann O'Rourke"
                    },
                    {
                        "name": "Randy Brown"
                    },
                    {
                        "name": "Anne Stey"
                    },
                    {
                        "name": "Anoop Mayampurath"
                    },
                    {
                        "name": "Matthew M. Churpek"
                    },
                    {
                        "name": "Guanhua Chen"
                    },
                    {
                        "name": "Majid Afshar"
                    }
                ],
                "author_detail": {
                    "name": "Majid Afshar"
                },
                "author": "Majid Afshar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05492v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05492v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08754v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08754v5",
                "updated": "2025-08-07T15:23:38Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    23,
                    38,
                    3,
                    219,
                    0
                ],
                "published": "2025-03-28T15:49:52Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    15,
                    49,
                    52,
                    4,
                    87,
                    0
                ],
                "title": "Towards Personalized Conversational Sales Agents: Contextual User\n  Profiling for Strategic Action",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Personalized Conversational Sales Agents: Contextual User\n  Profiling for Strategic Action"
                },
                "summary": "Conversational Recommender Systems (CRSs)aim to engage users in dialogue to\nprovide tailored recommendations. While traditional CRSs focus on eliciting\npreferences and retrieving items, real-world e-commerce interactions involve\nmore complex decision-making, where users consider multiple factors beyond\nsimple attributes. To capture this complexity, we introduce Conversational\nSales (CSALES), a novel task that integrates preference elicitation,\nrecommendation, and persuasion within a unified conversational framework. To\nsupport realistic and systematic evaluation, we present CSUSER, an evaluation\nprotocol with LLM-based user simulator grounded in real-world behavioral data\nby modeling fine-grained user profiles for personalized interaction. We also\npropose CSI, a conversational sales agent that proactively infers contextual\nuser profiles and strategically selects actions through conversation.\nComprehensive experiments show that CSI significantly improves both\nrecommendation success and persuasive effectiveness across diverse user\nprofiles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational Recommender Systems (CRSs)aim to engage users in dialogue to\nprovide tailored recommendations. While traditional CRSs focus on eliciting\npreferences and retrieving items, real-world e-commerce interactions involve\nmore complex decision-making, where users consider multiple factors beyond\nsimple attributes. To capture this complexity, we introduce Conversational\nSales (CSALES), a novel task that integrates preference elicitation,\nrecommendation, and persuasion within a unified conversational framework. To\nsupport realistic and systematic evaluation, we present CSUSER, an evaluation\nprotocol with LLM-based user simulator grounded in real-world behavioral data\nby modeling fine-grained user profiles for personalized interaction. We also\npropose CSI, a conversational sales agent that proactively infers contextual\nuser profiles and strategically selects actions through conversation.\nComprehensive experiments show that CSI significantly improves both\nrecommendation success and persuasive effectiveness across diverse user\nprofiles."
                },
                "authors": [
                    {
                        "name": "Tongyoung Kim"
                    },
                    {
                        "name": "Jeongeun Lee"
                    },
                    {
                        "name": "Soojin Yoon"
                    },
                    {
                        "name": "Sunghwan Kim"
                    },
                    {
                        "name": "Dongha Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongha Lee"
                },
                "author": "Dongha Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08754v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08754v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05474v1",
                "updated": "2025-08-07T15:13:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    13,
                    55,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T15:13:55Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    13,
                    55,
                    3,
                    219,
                    0
                ],
                "title": "Can Large Language Models Generate Effective Datasets for Emotion\n  Recognition in Conversations?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Generate Effective Datasets for Emotion\n  Recognition in Conversations?"
                },
                "summary": "Emotion recognition in conversations (ERC) focuses on identifying emotion\nshifts within interactions, representing a significant step toward advancing\nmachine intelligence. However, ERC data remains scarce, and existing datasets\nface numerous challenges due to their highly biased sources and the inherent\nsubjectivity of soft labels. Even though Large Language Models (LLMs) have\ndemonstrated their quality in many affective tasks, they are typically\nexpensive to train, and their application to ERC tasks--particularly in data\ngeneration--remains limited. To address these challenges, we employ a small,\nresource-efficient, and general-purpose LLM to synthesize ERC datasets with\ndiverse properties, supplementing the three most widely used ERC benchmarks. We\ngenerate six novel datasets, with two tailored to enhance each benchmark. We\nevaluate the utility of these datasets to (1) supplement existing datasets for\nERC classification, and (2) analyze the effects of label imbalance in ERC. Our\nexperimental results indicate that ERC classifier models trained on the\ngenerated datasets exhibit strong robustness and consistently achieve\nstatistically significant performance improvements on existing ERC benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emotion recognition in conversations (ERC) focuses on identifying emotion\nshifts within interactions, representing a significant step toward advancing\nmachine intelligence. However, ERC data remains scarce, and existing datasets\nface numerous challenges due to their highly biased sources and the inherent\nsubjectivity of soft labels. Even though Large Language Models (LLMs) have\ndemonstrated their quality in many affective tasks, they are typically\nexpensive to train, and their application to ERC tasks--particularly in data\ngeneration--remains limited. To address these challenges, we employ a small,\nresource-efficient, and general-purpose LLM to synthesize ERC datasets with\ndiverse properties, supplementing the three most widely used ERC benchmarks. We\ngenerate six novel datasets, with two tailored to enhance each benchmark. We\nevaluate the utility of these datasets to (1) supplement existing datasets for\nERC classification, and (2) analyze the effects of label imbalance in ERC. Our\nexperimental results indicate that ERC classifier models trained on the\ngenerated datasets exhibit strong robustness and consistently achieve\nstatistically significant performance improvements on existing ERC benchmarks."
                },
                "authors": [
                    {
                        "name": "Burak Can Kaplan"
                    },
                    {
                        "name": "Hugo Cesar De Castro Carneiro"
                    },
                    {
                        "name": "Stefan Wermter"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Wermter"
                },
                "author": "Stefan Wermter",
                "arxiv_comment": "8 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05473v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05473v1",
                "updated": "2025-08-07T15:13:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    13,
                    42,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T15:13:42Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    13,
                    42,
                    3,
                    219,
                    0
                ],
                "title": "Embedding Alignment in Code Generation for Audio",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embedding Alignment in Code Generation for Audio"
                },
                "summary": "LLM-powered code generation has the potential to revolutionize creative\ncoding endeavors, such as live-coding, by enabling users to focus on structural\nmotifs over syntactic details. In such domains, when prompting an LLM, users\nmay benefit from considering multiple varied code candidates to better realize\ntheir musical intentions. Code generation models, however, struggle to present\nunique and diverse code candidates, with no direct insight into the code's\naudio output. To better establish a relationship between code candidates and\nproduced audio, we investigate the topology of the mapping between code and\naudio embedding spaces. We find that code and audio embeddings do not exhibit a\nsimple linear relationship, but supplement this with a constructed predictive\nmodel that shows an embedding alignment map could be learned. Supplementing the\naim for musically diverse output, we present a model that given code predicts\noutput audio embedding, constructing a code-audio embedding alignment map.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-powered code generation has the potential to revolutionize creative\ncoding endeavors, such as live-coding, by enabling users to focus on structural\nmotifs over syntactic details. In such domains, when prompting an LLM, users\nmay benefit from considering multiple varied code candidates to better realize\ntheir musical intentions. Code generation models, however, struggle to present\nunique and diverse code candidates, with no direct insight into the code's\naudio output. To better establish a relationship between code candidates and\nproduced audio, we investigate the topology of the mapping between code and\naudio embedding spaces. We find that code and audio embeddings do not exhibit a\nsimple linear relationship, but supplement this with a constructed predictive\nmodel that shows an embedding alignment map could be learned. Supplementing the\naim for musically diverse output, we present a model that given code predicts\noutput audio embedding, constructing a code-audio embedding alignment map."
                },
                "authors": [
                    {
                        "name": "Sam Kouteili"
                    },
                    {
                        "name": "Hiren Madhu"
                    },
                    {
                        "name": "George Typaldos"
                    },
                    {
                        "name": "Mark Santolucito"
                    }
                ],
                "author_detail": {
                    "name": "Mark Santolucito"
                },
                "author": "Mark Santolucito",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05473v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05473v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05470v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05470v1",
                "updated": "2025-08-07T15:11:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    11,
                    48,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T15:11:48Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    11,
                    48,
                    3,
                    219,
                    0
                ],
                "title": "Rethinking Creativity Evaluation: A Critical Analysis of Existing\n  Creativity Evaluations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Creativity Evaluation: A Critical Analysis of Existing\n  Creativity Evaluations"
                },
                "summary": "We systematically examine, analyze, and compare representative creativity\nmeasures--creativity index, perplexity, syntactic templates, and\nLLM-as-a-Judge--across diverse creative domains, including creative writing,\nunconventional problem-solving, and research ideation. Our analyses reveal that\nthese metrics exhibit limited consistency, capturing different dimensions of\ncreativity. We highlight key limitations, including the creativity index's\nfocus on lexical diversity, perplexity's sensitivity to model confidence, and\nsyntactic templates' inability to capture conceptual creativity. Additionally,\nLLM-as-a-Judge shows instability and bias. Our findings underscore the need for\nmore robust, generalizable evaluation frameworks that better align with human\njudgments of creativity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We systematically examine, analyze, and compare representative creativity\nmeasures--creativity index, perplexity, syntactic templates, and\nLLM-as-a-Judge--across diverse creative domains, including creative writing,\nunconventional problem-solving, and research ideation. Our analyses reveal that\nthese metrics exhibit limited consistency, capturing different dimensions of\ncreativity. We highlight key limitations, including the creativity index's\nfocus on lexical diversity, perplexity's sensitivity to model confidence, and\nsyntactic templates' inability to capture conceptual creativity. Additionally,\nLLM-as-a-Judge shows instability and bias. Our findings underscore the need for\nmore robust, generalizable evaluation frameworks that better align with human\njudgments of creativity."
                },
                "authors": [
                    {
                        "name": "Li-Chun Lu"
                    },
                    {
                        "name": "Miri Liu"
                    },
                    {
                        "name": "Pin-Chun Lu"
                    },
                    {
                        "name": "Yufei Tian"
                    },
                    {
                        "name": "Shao-Hua Sun"
                    },
                    {
                        "name": "Nanyun Peng"
                    }
                ],
                "author_detail": {
                    "name": "Nanyun Peng"
                },
                "author": "Nanyun Peng",
                "arxiv_comment": "15 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05470v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05470v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05469v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05469v1",
                "updated": "2025-08-07T15:11:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    11,
                    43,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T15:11:43Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    11,
                    43,
                    3,
                    219,
                    0
                ],
                "title": "Let's Measure Information Step-by-Step: LLM-Based Evaluation Beyond\n  Vibes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let's Measure Information Step-by-Step: LLM-Based Evaluation Beyond\n  Vibes"
                },
                "summary": "We develop mechanisms for evaluating AI systems without ground truth by\nexploiting a connection between gaming resistance and output quality. The data\nprocessing inequality ensures post-hoc attempts to game a metric degrades both\ninformation content and task performance. We prove that f-mutual information\nmeasures are the unique gaming resistant mechanisms under natural conditions,\nwith the overseer acting as an agent. While Shannon mutual information faces\nexponential sample complexity, bounded measures like total variation distance\nremain tractable. Empirically, across ten domains from translation to peer\nreview, all information-theoretic mechanisms achieve perfect discrimination (d\n> 0.5) between faithful and strategic agents. In contrast, LLM judges exhibit\nsystematic evaluation inversion, preferring fabricated content over accurate\nsummaries. Our mechanisms show 10-100x better robustness to adversarial\nmanipulation than current practices. We also find performance follows an\ninverted-U curve with compression ratio, peaking at 10:1 where agent responses\nexhibit optimal information diversity (3 effective dimensions), giving a\nbias-variance perspective on when our approach is expected to be most\neffective.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop mechanisms for evaluating AI systems without ground truth by\nexploiting a connection between gaming resistance and output quality. The data\nprocessing inequality ensures post-hoc attempts to game a metric degrades both\ninformation content and task performance. We prove that f-mutual information\nmeasures are the unique gaming resistant mechanisms under natural conditions,\nwith the overseer acting as an agent. While Shannon mutual information faces\nexponential sample complexity, bounded measures like total variation distance\nremain tractable. Empirically, across ten domains from translation to peer\nreview, all information-theoretic mechanisms achieve perfect discrimination (d\n> 0.5) between faithful and strategic agents. In contrast, LLM judges exhibit\nsystematic evaluation inversion, preferring fabricated content over accurate\nsummaries. Our mechanisms show 10-100x better robustness to adversarial\nmanipulation than current practices. We also find performance follows an\ninverted-U curve with compression ratio, peaking at 10:1 where agent responses\nexhibit optimal information diversity (3 effective dimensions), giving a\nbias-variance perspective on when our approach is expected to be most\neffective."
                },
                "authors": [
                    {
                        "name": "Zachary Robertson"
                    },
                    {
                        "name": "Sanmi Koyejo"
                    }
                ],
                "author_detail": {
                    "name": "Sanmi Koyejo"
                },
                "author": "Sanmi Koyejo",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05469v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05469v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05468v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05468v1",
                "updated": "2025-08-07T15:11:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    11,
                    17,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T15:11:17Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    11,
                    17,
                    3,
                    219,
                    0
                ],
                "title": "TASE: Token Awareness and Structured Evaluation for Multilingual\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TASE: Token Awareness and Structured Evaluation for Multilingual\n  Language Models"
                },
                "summary": "While large language models (LLMs) have demonstrated remarkable performance\non high-level semantic tasks, they often struggle with fine-grained,\ntoken-level understanding and structural reasoning--capabilities that are\nessential for applications requiring precision and control. We introduce TASE,\na comprehensive benchmark designed to evaluate LLMs' ability to perceive and\nreason about token-level information across languages. TASE covers 10 tasks\nunder two core categories: token awareness and structural understanding,\nspanning Chinese, English, and Korean, with a 35,927-instance evaluation set\nand a scalable synthetic data generation pipeline for training. Tasks include\ncharacter counting, token alignment, syntactic structure parsing, and length\nconstraint satisfaction. We evaluate over 30 leading commercial and open-source\nLLMs, including O3, Claude 4, Gemini 2.5 Pro, and DeepSeek-R1, and train a\ncustom Qwen2.5-14B model using the GRPO training method. Results show that\nhuman performance significantly outpaces current LLMs, revealing persistent\nweaknesses in token-level reasoning. TASE sheds light on these limitations and\nprovides a new diagnostic lens for future improvements in low-level language\nunderstanding and cross-lingual generalization. Our code and dataset are\npublicly available at https://github.com/cyzcz/Tase .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have demonstrated remarkable performance\non high-level semantic tasks, they often struggle with fine-grained,\ntoken-level understanding and structural reasoning--capabilities that are\nessential for applications requiring precision and control. We introduce TASE,\na comprehensive benchmark designed to evaluate LLMs' ability to perceive and\nreason about token-level information across languages. TASE covers 10 tasks\nunder two core categories: token awareness and structural understanding,\nspanning Chinese, English, and Korean, with a 35,927-instance evaluation set\nand a scalable synthetic data generation pipeline for training. Tasks include\ncharacter counting, token alignment, syntactic structure parsing, and length\nconstraint satisfaction. We evaluate over 30 leading commercial and open-source\nLLMs, including O3, Claude 4, Gemini 2.5 Pro, and DeepSeek-R1, and train a\ncustom Qwen2.5-14B model using the GRPO training method. Results show that\nhuman performance significantly outpaces current LLMs, revealing persistent\nweaknesses in token-level reasoning. TASE sheds light on these limitations and\nprovides a new diagnostic lens for future improvements in low-level language\nunderstanding and cross-lingual generalization. Our code and dataset are\npublicly available at https://github.com/cyzcz/Tase ."
                },
                "authors": [
                    {
                        "name": "Chenzhuo Zhao"
                    },
                    {
                        "name": "Xinda Wang"
                    },
                    {
                        "name": "Yue Huang"
                    },
                    {
                        "name": "Junting Lu"
                    },
                    {
                        "name": "Ziqian Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ziqian Liu"
                },
                "author": "Ziqian Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05468v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05468v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01545v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01545v2",
                "updated": "2025-08-07T15:04:14Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    4,
                    14,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-03T01:58:38Z",
                "published_parsed": [
                    2025,
                    8,
                    3,
                    1,
                    58,
                    38,
                    6,
                    215,
                    0
                ],
                "title": "Getting out of the Big-Muddy: Escalation of Commitment in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Getting out of the Big-Muddy: Escalation of Commitment in LLMs"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed in autonomous\ndecision-making roles across high-stakes domains. However, since models are\ntrained on human-generated data, they may inherit cognitive biases that\nsystematically distort human judgment, including escalation of commitment,\nwhere decision-makers continue investing in failing courses of action due to\nprior investment. Understanding when LLMs exhibit such biases presents a unique\nchallenge. While these biases are well-documented in humans, it remains unclear\nwhether they manifest consistently in LLMs or require specific triggering\nconditions. This paper investigates this question using a two-stage investment\ntask across four experimental conditions: model as investor, model as advisor,\nmulti-agent deliberation, and compound pressure scenario. Across N = 6,500\ntrials, we find that bias manifestation in LLMs is highly context-dependent. In\nindividual decision-making contexts (Studies 1-2, N = 4,000), LLMs demonstrate\nstrong rational cost-benefit logic with minimal escalation of commitment.\nHowever, multi-agent deliberation reveals a striking hierarchy effect (Study 3,\nN = 500): while asymmetrical hierarchies show moderate escalation rates\n(46.2%), symmetrical peer-based decision-making produces near-universal\nescalation (99.2%). Similarly, when subjected to compound organizational and\npersonal pressures (Study 4, N = 2,000), models exhibit high degrees of\nescalation of commitment (68.95% average allocation to failing divisions).\nThese findings reveal that LLM bias manifestation depends critically on social\nand organizational context rather than being inherent, with significant\nimplications for the deployment of multi-agent systems and unsupervised\noperations where such conditions may emerge naturally.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in autonomous\ndecision-making roles across high-stakes domains. However, since models are\ntrained on human-generated data, they may inherit cognitive biases that\nsystematically distort human judgment, including escalation of commitment,\nwhere decision-makers continue investing in failing courses of action due to\nprior investment. Understanding when LLMs exhibit such biases presents a unique\nchallenge. While these biases are well-documented in humans, it remains unclear\nwhether they manifest consistently in LLMs or require specific triggering\nconditions. This paper investigates this question using a two-stage investment\ntask across four experimental conditions: model as investor, model as advisor,\nmulti-agent deliberation, and compound pressure scenario. Across N = 6,500\ntrials, we find that bias manifestation in LLMs is highly context-dependent. In\nindividual decision-making contexts (Studies 1-2, N = 4,000), LLMs demonstrate\nstrong rational cost-benefit logic with minimal escalation of commitment.\nHowever, multi-agent deliberation reveals a striking hierarchy effect (Study 3,\nN = 500): while asymmetrical hierarchies show moderate escalation rates\n(46.2%), symmetrical peer-based decision-making produces near-universal\nescalation (99.2%). Similarly, when subjected to compound organizational and\npersonal pressures (Study 4, N = 2,000), models exhibit high degrees of\nescalation of commitment (68.95% average allocation to failing divisions).\nThese findings reveal that LLM bias manifestation depends critically on social\nand organizational context rather than being inherent, with significant\nimplications for the deployment of multi-agent systems and unsupervised\noperations where such conditions may emerge naturally."
                },
                "authors": [
                    {
                        "name": "Emilio Barkett"
                    },
                    {
                        "name": "Olivia Long"
                    },
                    {
                        "name": "Paul Krger"
                    }
                ],
                "author_detail": {
                    "name": "Paul Krger"
                },
                "author": "Paul Krger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01545v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01545v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05464v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05464v1",
                "updated": "2025-08-07T15:03:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    3,
                    39,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T15:03:39Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    3,
                    39,
                    3,
                    219,
                    0
                ],
                "title": "Bench-2-CoP: Can We Trust Benchmarking for EU AI Compliance?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bench-2-CoP: Can We Trust Benchmarking for EU AI Compliance?"
                },
                "summary": "The rapid advancement of General Purpose AI (GPAI) models necessitates robust\nevaluation frameworks, especially with emerging regulations like the EU AI Act\nand its associated Code of Practice (CoP). Current AI evaluation practices\ndepend heavily on established benchmarks, but these tools were not designed to\nmeasure the systemic risks that are the focus of the new regulatory landscape.\nThis research addresses the urgent need to quantify this \"benchmark-regulation\ngap.\" We introduce Bench-2-CoP, a novel, systematic framework that uses\nvalidated LLM-as-judge analysis to map the coverage of 194,955 questions from\nwidely-used benchmarks against the EU AI Act's taxonomy of model capabilities\nand propensities. Our findings reveal a profound misalignment: the evaluation\necosystem is overwhelmingly focused on a narrow set of behavioral propensities,\nsuch as \"Tendency to hallucinate\" (53.7% of the corpus) and \"Discriminatory\nbias\" (28.9%), while critical functional capabilities are dangerously\nneglected. Crucially, capabilities central to loss-of-control scenarios,\nincluding evading human oversight, self-replication, and autonomous AI\ndevelopment, receive zero coverage in the entire benchmark corpus. This\ntranslates to a near-total evaluation gap for systemic risks like \"Loss of\nControl\" (0.4% coverage) and \"Cyber Offence\" (0.8% coverage). This study\nprovides the first comprehensive, quantitative analysis of this gap, offering\ncritical insights for policymakers to refine the CoP and for developers to\nbuild the next generation of evaluation tools, ultimately fostering safer and\nmore compliant AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of General Purpose AI (GPAI) models necessitates robust\nevaluation frameworks, especially with emerging regulations like the EU AI Act\nand its associated Code of Practice (CoP). Current AI evaluation practices\ndepend heavily on established benchmarks, but these tools were not designed to\nmeasure the systemic risks that are the focus of the new regulatory landscape.\nThis research addresses the urgent need to quantify this \"benchmark-regulation\ngap.\" We introduce Bench-2-CoP, a novel, systematic framework that uses\nvalidated LLM-as-judge analysis to map the coverage of 194,955 questions from\nwidely-used benchmarks against the EU AI Act's taxonomy of model capabilities\nand propensities. Our findings reveal a profound misalignment: the evaluation\necosystem is overwhelmingly focused on a narrow set of behavioral propensities,\nsuch as \"Tendency to hallucinate\" (53.7% of the corpus) and \"Discriminatory\nbias\" (28.9%), while critical functional capabilities are dangerously\nneglected. Crucially, capabilities central to loss-of-control scenarios,\nincluding evading human oversight, self-replication, and autonomous AI\ndevelopment, receive zero coverage in the entire benchmark corpus. This\ntranslates to a near-total evaluation gap for systemic risks like \"Loss of\nControl\" (0.4% coverage) and \"Cyber Offence\" (0.8% coverage). This study\nprovides the first comprehensive, quantitative analysis of this gap, offering\ncritical insights for policymakers to refine the CoP and for developers to\nbuild the next generation of evaluation tools, ultimately fostering safer and\nmore compliant AI."
                },
                "authors": [
                    {
                        "name": "Matteo Prandi"
                    },
                    {
                        "name": "Vincenzo Suriani"
                    },
                    {
                        "name": "Federico Pierucci"
                    },
                    {
                        "name": "Marcello Galisai"
                    },
                    {
                        "name": "Daniele Nardi"
                    },
                    {
                        "name": "Piercosma Bisconti"
                    }
                ],
                "author_detail": {
                    "name": "Piercosma Bisconti"
                },
                "author": "Piercosma Bisconti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05464v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05464v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19168v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19168v2",
                "updated": "2025-08-07T14:58:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    14,
                    58,
                    9,
                    3,
                    219,
                    0
                ],
                "published": "2025-03-24T21:43:47Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    21,
                    43,
                    47,
                    0,
                    83,
                    0
                ],
                "title": "Language Model Uncertainty Quantification with Attention Chain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Model Uncertainty Quantification with Attention Chain"
                },
                "summary": "Accurately quantifying a large language model's (LLM) predictive uncertainty\nis crucial for judging the reliability of its answers. While most existing\nresearch focuses on short, directly answerable questions with closed-form\noutputs (e.g., multiple-choice), involving intermediate reasoning steps in LLM\nresponses is increasingly important. This added complexity complicates\nuncertainty quantification (UQ) because the probabilities assigned to answer\ntokens are conditioned on a vast space of preceding reasoning tokens. Direct\nmarginalization is infeasible, and the dependency inflates probability\nestimates, causing overconfidence in UQ. To address this, we propose UQAC, an\nefficient method that narrows the reasoning space to a tractable size for\nmarginalization. UQAC iteratively constructs an \"attention chain\" of tokens\ndeemed \"semantically crucial\" to the final answer via a backtracking procedure.\nStarting from the answer tokens, it uses attention weights to identify the most\ninfluential predecessors, then iterates this process until reaching the input\ntokens. The resulting chain is further refined with similarity filtering and\nprobability thresholding, which reduce the reasoning space, facilitating the\napproximation of the marginal answer token probabilities. We validate UQAC on\nmultiple reasoning benchmarks with advanced open-source LLMs, demonstrating\nthat it consistently delivers reliable UQ estimates with high computational\nefficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately quantifying a large language model's (LLM) predictive uncertainty\nis crucial for judging the reliability of its answers. While most existing\nresearch focuses on short, directly answerable questions with closed-form\noutputs (e.g., multiple-choice), involving intermediate reasoning steps in LLM\nresponses is increasingly important. This added complexity complicates\nuncertainty quantification (UQ) because the probabilities assigned to answer\ntokens are conditioned on a vast space of preceding reasoning tokens. Direct\nmarginalization is infeasible, and the dependency inflates probability\nestimates, causing overconfidence in UQ. To address this, we propose UQAC, an\nefficient method that narrows the reasoning space to a tractable size for\nmarginalization. UQAC iteratively constructs an \"attention chain\" of tokens\ndeemed \"semantically crucial\" to the final answer via a backtracking procedure.\nStarting from the answer tokens, it uses attention weights to identify the most\ninfluential predecessors, then iterates this process until reaching the input\ntokens. The resulting chain is further refined with similarity filtering and\nprobability thresholding, which reduce the reasoning space, facilitating the\napproximation of the marginal answer token probabilities. We validate UQAC on\nmultiple reasoning benchmarks with advanced open-source LLMs, demonstrating\nthat it consistently delivers reliable UQ estimates with high computational\nefficiency."
                },
                "authors": [
                    {
                        "name": "Yinghao Li"
                    },
                    {
                        "name": "Rushi Qiang"
                    },
                    {
                        "name": "Lama Moukheiber"
                    },
                    {
                        "name": "Chao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Zhang"
                },
                "author": "Chao Zhang",
                "arxiv_comment": "36 pages, 7 figures, 36 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19168v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19168v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11105v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11105v3",
                "updated": "2025-08-07T14:57:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    14,
                    57,
                    45,
                    3,
                    219,
                    0
                ],
                "published": "2025-06-07T01:37:42Z",
                "published_parsed": [
                    2025,
                    6,
                    7,
                    1,
                    37,
                    42,
                    5,
                    158,
                    0
                ],
                "title": "Enabling On-Device Medical AI Assistants via Input-Driven Saliency\n  Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling On-Device Medical AI Assistants via Input-Driven Saliency\n  Adaptation"
                },
                "summary": "Large Language Models (LLMs) have significant impact on the healthcare\nscenarios but remain prohibitively large for deployment in real-time,\nresource-constrained environments such as edge devices. In this work, we\nintroduce a novel medical assistant system, optimized through our\ngeneral-purpose compression framework, which tailors Large Language Models\n(LLMs) for deployment in specialized domains. By measuring neuron saliency on\ndomain-specific data, our method can aggressively prune irrelevant neurons,\nreducing model size while preserving performance. Following pruning, we apply\npost-training quantization to further reduce the memory footprint, and evaluate\nthe compressed model across medical benchmarks including MedMCQA, MedQA, and\nPubMedQA. We also deploy the 50\\% compressed Gemma and the 67\\% compressed\nLLaMA3 models on Jetson Orin Nano (18.7W peak) and Raspberry Pi 5 (6.3W peak),\nachieving real-time, energy-efficient inference under hardware constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have significant impact on the healthcare\nscenarios but remain prohibitively large for deployment in real-time,\nresource-constrained environments such as edge devices. In this work, we\nintroduce a novel medical assistant system, optimized through our\ngeneral-purpose compression framework, which tailors Large Language Models\n(LLMs) for deployment in specialized domains. By measuring neuron saliency on\ndomain-specific data, our method can aggressively prune irrelevant neurons,\nreducing model size while preserving performance. Following pruning, we apply\npost-training quantization to further reduce the memory footprint, and evaluate\nthe compressed model across medical benchmarks including MedMCQA, MedQA, and\nPubMedQA. We also deploy the 50\\% compressed Gemma and the 67\\% compressed\nLLaMA3 models on Jetson Orin Nano (18.7W peak) and Raspberry Pi 5 (6.3W peak),\nachieving real-time, energy-efficient inference under hardware constraints."
                },
                "authors": [
                    {
                        "name": "Uttej Kallakurik"
                    },
                    {
                        "name": "Edward Humes"
                    },
                    {
                        "name": "Rithvik Jonna"
                    },
                    {
                        "name": "Xiaomin Lin"
                    },
                    {
                        "name": "Tinoosh Mohsenin"
                    }
                ],
                "author_detail": {
                    "name": "Tinoosh Mohsenin"
                },
                "author": "Tinoosh Mohsenin",
                "arxiv_comment": "Accepted for publication in the Proceedings of IEEE BioCAS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11105v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11105v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05871v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05871v2",
                "updated": "2025-08-07T14:50:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    14,
                    50,
                    30,
                    3,
                    219,
                    0
                ],
                "published": "2025-04-08T09:54:49Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    54,
                    49,
                    1,
                    98,
                    0
                ],
                "title": "Agent Guide: A Simple Agent Behavioral Watermarking Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent Guide: A Simple Agent Behavioral Watermarking Framework"
                },
                "summary": "The increasing deployment of intelligent agents in digital ecosystems, such\nas social media platforms, has raised significant concerns about traceability\nand accountability, particularly in cybersecurity and digital content\nprotection. Traditional large language model (LLM) watermarking techniques,\nwhich rely on token-level manipulations, are ill-suited for agents due to the\nchallenges of behavior tokenization and information loss during\nbehavior-to-action translation. To address these issues, we propose Agent\nGuide, a novel behavioral watermarking framework that embeds watermarks by\nguiding the agent's high-level decisions (behavior) through probability biases,\nwhile preserving the naturalness of specific executions (action). Our approach\ndecouples agent behavior into two levels, behavior (e.g., choosing to bookmark)\nand action (e.g., bookmarking with specific tags), and applies watermark-guided\nbiases to the behavior probability distribution. We employ a z-statistic-based\nstatistical analysis to detect the watermark, ensuring reliable extraction over\nmultiple rounds. Experiments in a social media scenario with diverse agent\nprofiles demonstrate that Agent Guide achieves effective watermark detection\nwith a low false positive rate. Our framework provides a practical and robust\nsolution for agent watermarking, with applications in identifying malicious\nagents and protecting proprietary agent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing deployment of intelligent agents in digital ecosystems, such\nas social media platforms, has raised significant concerns about traceability\nand accountability, particularly in cybersecurity and digital content\nprotection. Traditional large language model (LLM) watermarking techniques,\nwhich rely on token-level manipulations, are ill-suited for agents due to the\nchallenges of behavior tokenization and information loss during\nbehavior-to-action translation. To address these issues, we propose Agent\nGuide, a novel behavioral watermarking framework that embeds watermarks by\nguiding the agent's high-level decisions (behavior) through probability biases,\nwhile preserving the naturalness of specific executions (action). Our approach\ndecouples agent behavior into two levels, behavior (e.g., choosing to bookmark)\nand action (e.g., bookmarking with specific tags), and applies watermark-guided\nbiases to the behavior probability distribution. We employ a z-statistic-based\nstatistical analysis to detect the watermark, ensuring reliable extraction over\nmultiple rounds. Experiments in a social media scenario with diverse agent\nprofiles demonstrate that Agent Guide achieves effective watermark detection\nwith a low false positive rate. Our framework provides a practical and robust\nsolution for agent watermarking, with applications in identifying malicious\nagents and protecting proprietary agent systems."
                },
                "authors": [
                    {
                        "name": "Kaibo Huang"
                    },
                    {
                        "name": "Zipei Zhang"
                    },
                    {
                        "name": "Zhongliang Yang"
                    },
                    {
                        "name": "Linna Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Linna Zhou"
                },
                "author": "Linna Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05871v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05871v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.00708v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.00708v2",
                "updated": "2025-08-07T14:48:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    14,
                    48,
                    25,
                    3,
                    219,
                    0
                ],
                "published": "2024-04-23T19:57:03Z",
                "published_parsed": [
                    2024,
                    4,
                    23,
                    19,
                    57,
                    3,
                    1,
                    114,
                    0
                ],
                "title": "Understanding Large Language Model Behaviors through Interactive\n  Counterfactual Generation and Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Large Language Model Behaviors through Interactive\n  Counterfactual Generation and Analysis"
                },
                "summary": "Understanding the behavior of large language models (LLMs) is crucial for\nensuring their safe and reliable use. However, existing explainable AI (XAI)\nmethods for LLMs primarily rely on word-level explanations, which are often\ncomputationally inefficient and misaligned with human reasoning processes.\nMoreover, these methods often treat explanation as a one-time output,\noverlooking its inherently interactive and iterative nature. In this paper, we\npresent LLM Analyzer, an interactive visualization system that addresses these\nlimitations by enabling intuitive and efficient exploration of LLM behaviors\nthrough counterfactual analysis. Our system features a novel algorithm that\ngenerates fluent and semantically meaningful counterfactuals via targeted\nremoval and replacement operations at user-defined levels of granularity. These\ncounterfactuals are used to compute feature attribution scores, which are then\nintegrated with concrete examples in a table-based visualization, supporting\ndynamic analysis of model behavior. A user study with LLM practitioners and\ninterviews with experts demonstrate the system's usability and effectiveness,\nemphasizing the importance of involving humans in the explanation process as\nactive participants rather than passive recipients.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the behavior of large language models (LLMs) is crucial for\nensuring their safe and reliable use. However, existing explainable AI (XAI)\nmethods for LLMs primarily rely on word-level explanations, which are often\ncomputationally inefficient and misaligned with human reasoning processes.\nMoreover, these methods often treat explanation as a one-time output,\noverlooking its inherently interactive and iterative nature. In this paper, we\npresent LLM Analyzer, an interactive visualization system that addresses these\nlimitations by enabling intuitive and efficient exploration of LLM behaviors\nthrough counterfactual analysis. Our system features a novel algorithm that\ngenerates fluent and semantically meaningful counterfactuals via targeted\nremoval and replacement operations at user-defined levels of granularity. These\ncounterfactuals are used to compute feature attribution scores, which are then\nintegrated with concrete examples in a table-based visualization, supporting\ndynamic analysis of model behavior. A user study with LLM practitioners and\ninterviews with experts demonstrate the system's usability and effectiveness,\nemphasizing the importance of involving humans in the explanation process as\nactive participants rather than passive recipients."
                },
                "authors": [
                    {
                        "name": "Furui Cheng"
                    },
                    {
                        "name": "Vilm Zouhar"
                    },
                    {
                        "name": "Robin Shing Moon Chan"
                    },
                    {
                        "name": "Daniel Frst"
                    },
                    {
                        "name": "Hendrik Strobelt"
                    },
                    {
                        "name": "Mennatallah El-Assady"
                    }
                ],
                "author_detail": {
                    "name": "Mennatallah El-Assady"
                },
                "author": "Mennatallah El-Assady",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.00708v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.00708v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; H.5.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05452v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05452v1",
                "updated": "2025-08-07T14:46:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    14,
                    46,
                    30,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T14:46:30Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    14,
                    46,
                    30,
                    3,
                    219,
                    0
                ],
                "title": "LLMEval-3: A Large-Scale Longitudinal Study on Robust and Fair\n  Evaluation of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMEval-3: A Large-Scale Longitudinal Study on Robust and Fair\n  Evaluation of Large Language Models"
                },
                "summary": "Existing evaluation of Large Language Models (LLMs) on static benchmarks is\nvulnerable to data contamination and leaderboard overfitting, critical issues\nthat obscure true model capabilities. To address this, we introduce LLMEval-3,\na framework for dynamic evaluation of LLMs. LLMEval-3 is built on a proprietary\nbank of 220k graduate-level questions, from which it dynamically samples unseen\ntest sets for each evaluation run. Its automated pipeline ensures integrity via\ncontamination-resistant data curation, a novel anti-cheating architecture, and\na calibrated LLM-as-a-judge process achieving 90% agreement with human experts,\ncomplemented by a relative ranking system for fair comparison. An 20-month\nlongitudinal study of nearly 50 leading models reveals a performance ceiling on\nknowledge memorization and exposes data contamination vulnerabilities\nundetectable by static benchmarks. The framework demonstrates exceptional\nrobustness in ranking stability and consistency, providing strong empirical\nvalidation for the dynamic evaluation paradigm. LLMEval-3 offers a robust and\ncredible methodology for assessing the true capabilities of LLMs beyond\nleaderboard scores, promoting the development of more trustworthy evaluation\nstandards.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing evaluation of Large Language Models (LLMs) on static benchmarks is\nvulnerable to data contamination and leaderboard overfitting, critical issues\nthat obscure true model capabilities. To address this, we introduce LLMEval-3,\na framework for dynamic evaluation of LLMs. LLMEval-3 is built on a proprietary\nbank of 220k graduate-level questions, from which it dynamically samples unseen\ntest sets for each evaluation run. Its automated pipeline ensures integrity via\ncontamination-resistant data curation, a novel anti-cheating architecture, and\na calibrated LLM-as-a-judge process achieving 90% agreement with human experts,\ncomplemented by a relative ranking system for fair comparison. An 20-month\nlongitudinal study of nearly 50 leading models reveals a performance ceiling on\nknowledge memorization and exposes data contamination vulnerabilities\nundetectable by static benchmarks. The framework demonstrates exceptional\nrobustness in ranking stability and consistency, providing strong empirical\nvalidation for the dynamic evaluation paradigm. LLMEval-3 offers a robust and\ncredible methodology for assessing the true capabilities of LLMs beyond\nleaderboard scores, promoting the development of more trustworthy evaluation\nstandards."
                },
                "authors": [
                    {
                        "name": "Ming Zhang"
                    },
                    {
                        "name": "Yujiong Shen"
                    },
                    {
                        "name": "Jingyi Deng"
                    },
                    {
                        "name": "Yuhui Wang"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Junzhe Wang"
                    },
                    {
                        "name": "Shichun Liu"
                    },
                    {
                        "name": "Shihan Dou"
                    },
                    {
                        "name": "Huayu Sha"
                    },
                    {
                        "name": "Qiyuan Peng"
                    },
                    {
                        "name": "Changhao Jiang"
                    },
                    {
                        "name": "Jingqi Tong"
                    },
                    {
                        "name": "Yilong Wu"
                    },
                    {
                        "name": "Zhihao Zhang"
                    },
                    {
                        "name": "Mingqi Wu"
                    },
                    {
                        "name": "Zhiheng Xi"
                    },
                    {
                        "name": "Mingxu Chai"
                    },
                    {
                        "name": "Tao Liang"
                    },
                    {
                        "name": "Zhihui Fei"
                    },
                    {
                        "name": "Zhen Wang"
                    },
                    {
                        "name": "Mingyang Wan"
                    },
                    {
                        "name": "Guojun Ma"
                    },
                    {
                        "name": "Tao Gui"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05452v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05452v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12106v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12106v4",
                "updated": "2025-08-07T14:43:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    14,
                    43,
                    9,
                    3,
                    219,
                    0
                ],
                "published": "2025-01-21T12:56:47Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    12,
                    56,
                    47,
                    1,
                    21,
                    0
                ],
                "title": "Can open source large language models be used for tumor documentation in\n  Germany? -- An evaluation on urological doctors' notes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can open source large language models be used for tumor documentation in\n  Germany? -- An evaluation on urological doctors' notes"
                },
                "summary": "Tumor documentation in Germany is largely done manually, requiring reading\npatient records and entering data into structured databases. Large language\nmodels (LLMs) could potentially enhance this process by improving efficiency\nand reliability. This evaluation tests eleven different open source LLMs with\nsizes ranging from 1-70 billion model parameters on three basic tasks of the\ntumor documentation process: identifying tumor diagnoses, assigning ICD-10\ncodes, and extracting the date of first diagnosis. For evaluating the LLMs on\nthese tasks, a dataset of annotated text snippets based on anonymized doctors'\nnotes from urology was prepared. Different prompting strategies were used to\ninvestigate the effect of the number of examples in few-shot prompting and to\nexplore the capabilities of the LLMs in general. The models Llama 3.1 8B,\nMistral 7B, and Mistral NeMo 12 B performed comparably well in the tasks.\nModels with less extensive training data or having fewer than 7 billion\nparameters showed notably lower performance, while larger models did not\ndisplay performance gains. Examples from a different medical domain than\nurology could also improve the outcome in few-shot prompting, which\ndemonstrates the ability of LLMs to handle tasks needed for tumor\ndocumentation. Open source LLMs show a strong potential for automating tumor\ndocumentation. Models from 7-12 billion parameters could offer an optimal\nbalance between performance and resource efficiency. With tailored fine-tuning\nand well-designed prompting, these models might become important tools for\nclinical documentation in the future. The code for the evaluation is available\nfrom https://github.com/stefan-m-lenz/UroLlmEval. We also release the dataset\nas a new valuable resource that addresses the shortage of authentic and easily\naccessible benchmarks in German-language medical NLP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tumor documentation in Germany is largely done manually, requiring reading\npatient records and entering data into structured databases. Large language\nmodels (LLMs) could potentially enhance this process by improving efficiency\nand reliability. This evaluation tests eleven different open source LLMs with\nsizes ranging from 1-70 billion model parameters on three basic tasks of the\ntumor documentation process: identifying tumor diagnoses, assigning ICD-10\ncodes, and extracting the date of first diagnosis. For evaluating the LLMs on\nthese tasks, a dataset of annotated text snippets based on anonymized doctors'\nnotes from urology was prepared. Different prompting strategies were used to\ninvestigate the effect of the number of examples in few-shot prompting and to\nexplore the capabilities of the LLMs in general. The models Llama 3.1 8B,\nMistral 7B, and Mistral NeMo 12 B performed comparably well in the tasks.\nModels with less extensive training data or having fewer than 7 billion\nparameters showed notably lower performance, while larger models did not\ndisplay performance gains. Examples from a different medical domain than\nurology could also improve the outcome in few-shot prompting, which\ndemonstrates the ability of LLMs to handle tasks needed for tumor\ndocumentation. Open source LLMs show a strong potential for automating tumor\ndocumentation. Models from 7-12 billion parameters could offer an optimal\nbalance between performance and resource efficiency. With tailored fine-tuning\nand well-designed prompting, these models might become important tools for\nclinical documentation in the future. The code for the evaluation is available\nfrom https://github.com/stefan-m-lenz/UroLlmEval. We also release the dataset\nas a new valuable resource that addresses the shortage of authentic and easily\naccessible benchmarks in German-language medical NLP."
                },
                "authors": [
                    {
                        "name": "Stefan Lenz"
                    },
                    {
                        "name": "Arsenij Ustjanzew"
                    },
                    {
                        "name": "Marco Jeray"
                    },
                    {
                        "name": "Meike Ressing"
                    },
                    {
                        "name": "Torsten Panholzer"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Panholzer"
                },
                "author": "Torsten Panholzer",
                "arxiv_doi": "10.1186/s13040-025-00463-8",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1186/s13040-025-00463-8",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.12106v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12106v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "53 pages, 5 figures",
                "arxiv_journal_ref": "BioData Mining volume 18, Article number 48 (2025)",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05433v1",
                "updated": "2025-08-07T14:24:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    14,
                    24,
                    3,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T14:24:03Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    14,
                    24,
                    3,
                    3,
                    219,
                    0
                ],
                "title": "Discovering Interpretable Programmatic Policies via Multimodal\n  LLM-assisted Evolutionary Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discovering Interpretable Programmatic Policies via Multimodal\n  LLM-assisted Evolutionary Search"
                },
                "summary": "Interpretability and high performance are essential goals in designing\ncontrol policies, particularly for safety-critical tasks. Deep reinforcement\nlearning has greatly enhanced performance, yet its inherent lack of\ninterpretability often undermines trust and hinders real-world deployment. This\nwork addresses these dual challenges by introducing a novel approach for\nprogrammatic policy discovery, called Multimodal Large Language Model-assisted\nEvolutionary Search (MLES). MLES utilizes multimodal large language models as\npolicy generators, combining them with evolutionary mechanisms for automatic\npolicy optimization. It integrates visual feedback-driven behavior analysis\nwithin the policy generation process to identify failure patterns and\nfacilitate targeted improvements, enhancing the efficiency of policy discovery\nand producing adaptable, human-aligned policies. Experimental results show that\nMLES achieves policy discovery capabilities and efficiency comparable to\nProximal Policy Optimization (PPO) across two control tasks, while offering\ntransparent control logic and traceable design processes. This paradigm\novercomes the limitations of predefined domain-specific languages, facilitates\nknowledge transfer and reuse, and is scalable across various control tasks.\nMLES shows promise as a leading approach for the next generation of\ninterpretable control policy discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpretability and high performance are essential goals in designing\ncontrol policies, particularly for safety-critical tasks. Deep reinforcement\nlearning has greatly enhanced performance, yet its inherent lack of\ninterpretability often undermines trust and hinders real-world deployment. This\nwork addresses these dual challenges by introducing a novel approach for\nprogrammatic policy discovery, called Multimodal Large Language Model-assisted\nEvolutionary Search (MLES). MLES utilizes multimodal large language models as\npolicy generators, combining them with evolutionary mechanisms for automatic\npolicy optimization. It integrates visual feedback-driven behavior analysis\nwithin the policy generation process to identify failure patterns and\nfacilitate targeted improvements, enhancing the efficiency of policy discovery\nand producing adaptable, human-aligned policies. Experimental results show that\nMLES achieves policy discovery capabilities and efficiency comparable to\nProximal Policy Optimization (PPO) across two control tasks, while offering\ntransparent control logic and traceable design processes. This paradigm\novercomes the limitations of predefined domain-specific languages, facilitates\nknowledge transfer and reuse, and is scalable across various control tasks.\nMLES shows promise as a leading approach for the next generation of\ninterpretable control policy discovery."
                },
                "authors": [
                    {
                        "name": "Qinglong Hu"
                    },
                    {
                        "name": "Xialiang Tong"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    },
                    {
                        "name": "Fei Liu"
                    },
                    {
                        "name": "Zhichao Lu"
                    },
                    {
                        "name": "Qingfu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qingfu Zhang"
                },
                "author": "Qingfu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05429v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05429v1",
                "updated": "2025-08-07T14:17:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    14,
                    17,
                    43,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T14:17:43Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    14,
                    17,
                    43,
                    3,
                    219,
                    0
                ],
                "title": "MyCulture: Exploring Malaysia's Diverse Culture under Low-Resource\n  Language Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MyCulture: Exploring Malaysia's Diverse Culture under Low-Resource\n  Language Constraints"
                },
                "summary": "Large Language Models (LLMs) often exhibit cultural biases due to training\ndata dominated by high-resource languages like English and Chinese. This poses\nchallenges for accurately representing and evaluating diverse cultural\ncontexts, particularly in low-resource language settings. To address this, we\nintroduce MyCulture, a benchmark designed to comprehensively evaluate LLMs on\nMalaysian culture across six pillars: arts, attire, customs, entertainment,\nfood, and religion presented in Bahasa Melayu. Unlike conventional benchmarks,\nMyCulture employs a novel open-ended multiple-choice question format without\npredefined options, thereby reducing guessing and mitigating format bias. We\nprovide a theoretical justification for the effectiveness of this open-ended\nstructure in improving both fairness and discriminative power. Furthermore, we\nanalyze structural bias by comparing model performance on structured versus\nfree-form outputs, and assess language bias through multilingual prompt\nvariations. Our evaluation across a range of regional and international LLMs\nreveals significant disparities in cultural comprehension, highlighting the\nurgent need for culturally grounded and linguistically inclusive benchmarks in\nthe development and assessment of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often exhibit cultural biases due to training\ndata dominated by high-resource languages like English and Chinese. This poses\nchallenges for accurately representing and evaluating diverse cultural\ncontexts, particularly in low-resource language settings. To address this, we\nintroduce MyCulture, a benchmark designed to comprehensively evaluate LLMs on\nMalaysian culture across six pillars: arts, attire, customs, entertainment,\nfood, and religion presented in Bahasa Melayu. Unlike conventional benchmarks,\nMyCulture employs a novel open-ended multiple-choice question format without\npredefined options, thereby reducing guessing and mitigating format bias. We\nprovide a theoretical justification for the effectiveness of this open-ended\nstructure in improving both fairness and discriminative power. Furthermore, we\nanalyze structural bias by comparing model performance on structured versus\nfree-form outputs, and assess language bias through multilingual prompt\nvariations. Our evaluation across a range of regional and international LLMs\nreveals significant disparities in cultural comprehension, highlighting the\nurgent need for culturally grounded and linguistically inclusive benchmarks in\nthe development and assessment of LLMs."
                },
                "authors": [
                    {
                        "name": "Zhong Ken Hew"
                    },
                    {
                        "name": "Jia Xin Low"
                    },
                    {
                        "name": "Sze Jue Yang"
                    },
                    {
                        "name": "Chee Seng chan"
                    }
                ],
                "author_detail": {
                    "name": "Chee Seng chan"
                },
                "author": "Chee Seng chan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05429v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05429v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11790v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11790v3",
                "updated": "2025-08-07T14:17:38Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    14,
                    17,
                    38,
                    3,
                    219,
                    0
                ],
                "published": "2025-05-17T02:28:12Z",
                "published_parsed": [
                    2025,
                    5,
                    17,
                    2,
                    28,
                    12,
                    5,
                    137,
                    0
                ],
                "title": "JULI: Jailbreak Large Language Models by Self-Introspection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JULI: Jailbreak Large Language Models by Self-Introspection"
                },
                "summary": "Large Language Models (LLMs) are trained with safety alignment to prevent\ngenerating malicious content. Although some attacks have highlighted\nvulnerabilities in these safety-aligned LLMs, they typically have limitations,\nsuch as necessitating access to the model weights or the generation process.\nSince proprietary models through API-calling do not grant users such\npermissions, these attacks find it challenging to compromise them. In this\npaper, we propose Jailbreaking Using LLM Introspection (JULI), which jailbreaks\nLLMs by manipulating the token log probabilities, using a tiny plug-in block,\nBiasNet. JULI relies solely on the knowledge of the target LLM's predicted\ntoken log probabilities. It can effectively jailbreak API-calling LLMs under a\nblack-box setting and knowing only top-$5$ token log probabilities. Our\napproach demonstrates superior effectiveness, outperforming existing\nstate-of-the-art (SOTA) approaches across multiple metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are trained with safety alignment to prevent\ngenerating malicious content. Although some attacks have highlighted\nvulnerabilities in these safety-aligned LLMs, they typically have limitations,\nsuch as necessitating access to the model weights or the generation process.\nSince proprietary models through API-calling do not grant users such\npermissions, these attacks find it challenging to compromise them. In this\npaper, we propose Jailbreaking Using LLM Introspection (JULI), which jailbreaks\nLLMs by manipulating the token log probabilities, using a tiny plug-in block,\nBiasNet. JULI relies solely on the knowledge of the target LLM's predicted\ntoken log probabilities. It can effectively jailbreak API-calling LLMs under a\nblack-box setting and knowing only top-$5$ token log probabilities. Our\napproach demonstrates superior effectiveness, outperforming existing\nstate-of-the-art (SOTA) approaches across multiple metrics."
                },
                "authors": [
                    {
                        "name": "Jesson Wang"
                    },
                    {
                        "name": "Zhanhao Hu"
                    },
                    {
                        "name": "David Wagner"
                    }
                ],
                "author_detail": {
                    "name": "David Wagner"
                },
                "author": "David Wagner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11790v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11790v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05428v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05428v1",
                "updated": "2025-08-07T14:17:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    14,
                    17,
                    28,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T14:17:28Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    14,
                    17,
                    28,
                    3,
                    219,
                    0
                ],
                "title": "Group Causal Policy Optimization for Post-Training Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Group Causal Policy Optimization for Post-Training Large Language Models"
                },
                "summary": "Recent advances in large language models (LLMs) have broadened their\napplicability across diverse tasks, yet specialized domains still require\ntargeted post training. Among existing methods, Group Relative Policy\nOptimization (GRPO) stands out for its efficiency, leveraging groupwise\nrelative rewards while avoiding costly value function learning. However, GRPO\ntreats candidate responses as independent, overlooking semantic interactions\nsuch as complementarity and contradiction. To address this challenge, we first\nintroduce a Structural Causal Model (SCM) that reveals hidden dependencies\namong candidate responses induced by conditioning on a final integrated output\nforming a collider structure. Then, our causal analysis leads to two insights:\n(1) projecting responses onto a causally informed subspace improves prediction\nquality, and (2) this projection yields a better baseline than query only\nconditioning. Building on these insights, we propose Group Causal Policy\nOptimization (GCPO), which integrates causal structure into optimization\nthrough two key components: a causally informed reward adjustment and a novel\nKL regularization term that aligns the policy with a causally projected\nreference distribution. Comprehensive experimental evaluations demonstrate that\nGCPO consistently surpasses existing methods, including GRPO across multiple\nreasoning benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have broadened their\napplicability across diverse tasks, yet specialized domains still require\ntargeted post training. Among existing methods, Group Relative Policy\nOptimization (GRPO) stands out for its efficiency, leveraging groupwise\nrelative rewards while avoiding costly value function learning. However, GRPO\ntreats candidate responses as independent, overlooking semantic interactions\nsuch as complementarity and contradiction. To address this challenge, we first\nintroduce a Structural Causal Model (SCM) that reveals hidden dependencies\namong candidate responses induced by conditioning on a final integrated output\nforming a collider structure. Then, our causal analysis leads to two insights:\n(1) projecting responses onto a causally informed subspace improves prediction\nquality, and (2) this projection yields a better baseline than query only\nconditioning. Building on these insights, we propose Group Causal Policy\nOptimization (GCPO), which integrates causal structure into optimization\nthrough two key components: a causally informed reward adjustment and a novel\nKL regularization term that aligns the policy with a causally projected\nreference distribution. Comprehensive experimental evaluations demonstrate that\nGCPO consistently surpasses existing methods, including GRPO across multiple\nreasoning benchmarks."
                },
                "authors": [
                    {
                        "name": "Ziyin Gu"
                    },
                    {
                        "name": "Jingyao Wang"
                    },
                    {
                        "name": "Ran Zuo"
                    },
                    {
                        "name": "Chuxiong Sun"
                    },
                    {
                        "name": "Zeen Song"
                    },
                    {
                        "name": "Changwen Zheng"
                    },
                    {
                        "name": "Wenwen Qiang"
                    }
                ],
                "author_detail": {
                    "name": "Wenwen Qiang"
                },
                "author": "Wenwen Qiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05428v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05428v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05427v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05427v1",
                "updated": "2025-08-07T14:17:23Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    14,
                    17,
                    23,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T14:17:23Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    14,
                    17,
                    23,
                    3,
                    219,
                    0
                ],
                "title": "Large Language Models Transform Organic Synthesis From Reaction\n  Prediction to Automation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Transform Organic Synthesis From Reaction\n  Prediction to Automation"
                },
                "summary": "Large language models (LLMs) are beginning to reshape how chemists plan and\nrun reactions in organic synthesis. Trained on millions of reported\ntransformations, these text-based models can propose synthetic routes, forecast\nreaction outcomes and even instruct robots that execute experiments without\nhuman supervision. Here we survey the milestones that turned LLMs from\nspeculative tools into practical lab partners. We show how coupling LLMs with\ngraph neural networks, quantum calculations and real-time spectroscopy shrinks\ndiscovery cycles and supports greener, data-driven chemistry. We discuss\nlimitations, including biased datasets, opaque reasoning and the need for\nsafety gates that prevent unintentional hazards. Finally, we outline community\ninitiatives open benchmarks, federated learning and explainable interfaces that\naim to democratize access while keeping humans firmly in control. These\nadvances chart a path towards rapid, reliable and inclusive molecular\ninnovation powered by artificial intelligence and automation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are beginning to reshape how chemists plan and\nrun reactions in organic synthesis. Trained on millions of reported\ntransformations, these text-based models can propose synthetic routes, forecast\nreaction outcomes and even instruct robots that execute experiments without\nhuman supervision. Here we survey the milestones that turned LLMs from\nspeculative tools into practical lab partners. We show how coupling LLMs with\ngraph neural networks, quantum calculations and real-time spectroscopy shrinks\ndiscovery cycles and supports greener, data-driven chemistry. We discuss\nlimitations, including biased datasets, opaque reasoning and the need for\nsafety gates that prevent unintentional hazards. Finally, we outline community\ninitiatives open benchmarks, federated learning and explainable interfaces that\naim to democratize access while keeping humans firmly in control. These\nadvances chart a path towards rapid, reliable and inclusive molecular\ninnovation powered by artificial intelligence and automation."
                },
                "authors": [
                    {
                        "name": "Kartar Kumar Lohana Tharwani"
                    },
                    {
                        "name": "Rajesh Kumar"
                    },
                    {
                        "name": "Sumita"
                    },
                    {
                        "name": "Numan Ahmed"
                    },
                    {
                        "name": "Yong Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yong Tang"
                },
                "author": "Yong Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05427v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05427v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09994v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09994v2",
                "updated": "2025-08-07T14:15:07Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    14,
                    15,
                    7,
                    3,
                    219,
                    0
                ],
                "published": "2025-03-13T03:05:11Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    3,
                    5,
                    11,
                    3,
                    72,
                    0
                ],
                "title": "TIME: Temporal-Sensitive Multi-Dimensional Instruction Tuning and Robust\n  Benchmarking for Video-LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TIME: Temporal-Sensitive Multi-Dimensional Instruction Tuning and Robust\n  Benchmarking for Video-LLMs"
                },
                "summary": "Video large language models have achieved remarkable performance in tasks\nsuch as video question answering, however, their temporal understanding remains\nsuboptimal. To address this limitation, we curate a dedicated instruction\nfine-tuning dataset that focuses on enhancing temporal comprehension across\nfive key dimensions. In order to reduce reliance on costly temporal\nannotations, we introduce a multi-task prompt fine-tuning approach that\nseamlessly integrates temporal-sensitive tasks into existing instruction\ndatasets without requiring additional annotations. Furthermore, we develop a\nnovel benchmark for temporal-sensitive video understanding that not only fills\nthe gaps in dimension coverage left by existing benchmarks but also rigorously\nfilters out potential shortcuts, ensuring a more accurate evaluation. Extensive\nexperimental results demonstrate that our approach significantly enhances the\ntemporal understanding of video-LLMs while avoiding reliance on shortcuts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models have achieved remarkable performance in tasks\nsuch as video question answering, however, their temporal understanding remains\nsuboptimal. To address this limitation, we curate a dedicated instruction\nfine-tuning dataset that focuses on enhancing temporal comprehension across\nfive key dimensions. In order to reduce reliance on costly temporal\nannotations, we introduce a multi-task prompt fine-tuning approach that\nseamlessly integrates temporal-sensitive tasks into existing instruction\ndatasets without requiring additional annotations. Furthermore, we develop a\nnovel benchmark for temporal-sensitive video understanding that not only fills\nthe gaps in dimension coverage left by existing benchmarks but also rigorously\nfilters out potential shortcuts, ensuring a more accurate evaluation. Extensive\nexperimental results demonstrate that our approach significantly enhances the\ntemporal understanding of video-LLMs while avoiding reliance on shortcuts."
                },
                "authors": [
                    {
                        "name": "Yunxiao Wang"
                    },
                    {
                        "name": "Meng Liu"
                    },
                    {
                        "name": "Wenqi Liu"
                    },
                    {
                        "name": "Xuemeng Song"
                    },
                    {
                        "name": "Bin Wen"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Tingting Gao"
                    },
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Guorui Zhou"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09994v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09994v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05421v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05421v1",
                "updated": "2025-08-07T14:14:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    14,
                    14,
                    8,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T14:14:08Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    14,
                    14,
                    8,
                    3,
                    219,
                    0
                ],
                "title": "LLM-based Multi-Agent Copilot for Quantum Sensor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based Multi-Agent Copilot for Quantum Sensor"
                },
                "summary": "Large language models (LLM) exhibit broad utility but face limitations in\nquantum sensor development, stemming from interdisciplinary knowledge barriers\nand involving complex optimization processes. Here we present QCopilot, an\nLLM-based multi-agent framework integrating external knowledge access, active\nlearning, and uncertainty quantification for quantum sensor design and\ndiagnosis. Comprising commercial LLMs with few-shot prompt engineering and\nvector knowledge base, QCopilot employs specialized agents to adaptively select\noptimization methods, automate modeling analysis, and independently perform\nproblem diagnosis. Applying QCopilot to atom cooling experiments, we generated\n10${}^{\\rm{8}}$ sub-$\\rm{\\mu}$K atoms without any human intervention within a\nfew hours, representing $\\sim$100$\\times$ speedup over manual experimentation.\nNotably, by continuously accumulating prior knowledge and enabling dynamic\nmodeling, QCopilot can autonomously identify anomalous parameters in\nmulti-parameter experimental settings. Our work reduces barriers to large-scale\nquantum sensor deployment and readily extends to other quantum information\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLM) exhibit broad utility but face limitations in\nquantum sensor development, stemming from interdisciplinary knowledge barriers\nand involving complex optimization processes. Here we present QCopilot, an\nLLM-based multi-agent framework integrating external knowledge access, active\nlearning, and uncertainty quantification for quantum sensor design and\ndiagnosis. Comprising commercial LLMs with few-shot prompt engineering and\nvector knowledge base, QCopilot employs specialized agents to adaptively select\noptimization methods, automate modeling analysis, and independently perform\nproblem diagnosis. Applying QCopilot to atom cooling experiments, we generated\n10${}^{\\rm{8}}$ sub-$\\rm{\\mu}$K atoms without any human intervention within a\nfew hours, representing $\\sim$100$\\times$ speedup over manual experimentation.\nNotably, by continuously accumulating prior knowledge and enabling dynamic\nmodeling, QCopilot can autonomously identify anomalous parameters in\nmulti-parameter experimental settings. Our work reduces barriers to large-scale\nquantum sensor deployment and readily extends to other quantum information\nsystems."
                },
                "authors": [
                    {
                        "name": "Rong Sha"
                    },
                    {
                        "name": "Binglin Wang"
                    },
                    {
                        "name": "Jun Yang"
                    },
                    {
                        "name": "Xiaoxiao Ma"
                    },
                    {
                        "name": "Chengkun Wu"
                    },
                    {
                        "name": "Liang Yan"
                    },
                    {
                        "name": "Chao Zhou"
                    },
                    {
                        "name": "Jixun Liu"
                    },
                    {
                        "name": "Guochao Wang"
                    },
                    {
                        "name": "Shuhua Yan"
                    },
                    {
                        "name": "Lingxiao Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Lingxiao Zhu"
                },
                "author": "Lingxiao Zhu",
                "arxiv_comment": "13 pages,4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05421v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05421v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05418v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05418v1",
                "updated": "2025-08-07T14:10:07Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    14,
                    10,
                    7,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T14:10:07Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    14,
                    10,
                    7,
                    3,
                    219,
                    0
                ],
                "title": "Fuzzy Decisions on Fluid Instabilities: Autoencoder-Based Reconstruction\n  meets Rule-Based Anomaly Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fuzzy Decisions on Fluid Instabilities: Autoencoder-Based Reconstruction\n  meets Rule-Based Anomaly Classification"
                },
                "summary": "Shockwave classification in shadowgraph imaging is challenging due to limited\nlabeled data and complex flow structures. This study presents a hybrid\nframework that combines unsupervised autoencoder models with a fuzzy inference\nsystem to generate and interpret anomaly maps. Among the evaluated methods, the\nhybrid $\\beta$-VAE autoencoder with a fuzzy rule-based system most effectively\ncaptured coherent shock features, integrating spatial context to enhance\nanomaly classification. The resulting approach enables interpretable,\nunsupervised classification of flow disruptions and lays the groundwork for\nreal-time, physics-informed diagnostics in experimental and industrial fluid\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shockwave classification in shadowgraph imaging is challenging due to limited\nlabeled data and complex flow structures. This study presents a hybrid\nframework that combines unsupervised autoencoder models with a fuzzy inference\nsystem to generate and interpret anomaly maps. Among the evaluated methods, the\nhybrid $\\beta$-VAE autoencoder with a fuzzy rule-based system most effectively\ncaptured coherent shock features, integrating spatial context to enhance\nanomaly classification. The resulting approach enables interpretable,\nunsupervised classification of flow disruptions and lays the groundwork for\nreal-time, physics-informed diagnostics in experimental and industrial fluid\napplications."
                },
                "authors": [
                    {
                        "name": "Bharadwaj Dogga"
                    },
                    {
                        "name": "Gibin M. Raju"
                    },
                    {
                        "name": "Wilhelm Louw"
                    },
                    {
                        "name": "Kelly Cohen"
                    }
                ],
                "author_detail": {
                    "name": "Kelly Cohen"
                },
                "author": "Kelly Cohen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05418v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05418v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04377v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04377v2",
                "updated": "2025-08-07T14:05:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    14,
                    5,
                    19,
                    3,
                    219,
                    0
                ],
                "published": "2025-04-06T06:09:21Z",
                "published_parsed": [
                    2025,
                    4,
                    6,
                    6,
                    9,
                    21,
                    6,
                    96,
                    0
                ],
                "title": "PolyGuard: A Multilingual Safety Moderation Tool for 17 Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PolyGuard: A Multilingual Safety Moderation Tool for 17 Languages"
                },
                "summary": "Truly multilingual safety moderation efforts for Large Language Models (LLMs)\nhave been hindered by a narrow focus on a small set of languages (e.g.,\nEnglish, Chinese) as well as a limited scope of safety definition, resulting in\nsignificant gaps in moderation capabilities. To bridge these gaps, we release\nPOLYGUARD, a new state-of-the-art multilingual safety model for safeguarding\nLLM generations, and the corresponding training and evaluation datasets.\nPOLYGUARD is trained on POLYGUARDMIX, the largest multilingual safety training\ncorpus to date containing 1.91M samples across 17 languages (e.g., Chinese,\nCzech, English, Hindi). We also introduce POLYGUARDPROMPTS, a high quality\nmultilingual benchmark with 29K samples for the evaluation of safety\nguardrails. Created by combining naturally occurring multilingual human-LLM\ninteractions and human-verified machine translations of an English-only safety\ndataset (WildGuardMix; Han et al., 2024), our datasets contain prompt-output\npairs with labels of prompt harmfulness, response harmfulness, and response\nrefusal. Through extensive evaluations across multiple safety and toxicity\nbenchmarks, we demonstrate that POLYGUARD outperforms existing state-of-the-art\nopen-weight and commercial safety classifiers by 5.5%. Our contributions\nadvance efforts toward safer multilingual LLMs for all global users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Truly multilingual safety moderation efforts for Large Language Models (LLMs)\nhave been hindered by a narrow focus on a small set of languages (e.g.,\nEnglish, Chinese) as well as a limited scope of safety definition, resulting in\nsignificant gaps in moderation capabilities. To bridge these gaps, we release\nPOLYGUARD, a new state-of-the-art multilingual safety model for safeguarding\nLLM generations, and the corresponding training and evaluation datasets.\nPOLYGUARD is trained on POLYGUARDMIX, the largest multilingual safety training\ncorpus to date containing 1.91M samples across 17 languages (e.g., Chinese,\nCzech, English, Hindi). We also introduce POLYGUARDPROMPTS, a high quality\nmultilingual benchmark with 29K samples for the evaluation of safety\nguardrails. Created by combining naturally occurring multilingual human-LLM\ninteractions and human-verified machine translations of an English-only safety\ndataset (WildGuardMix; Han et al., 2024), our datasets contain prompt-output\npairs with labels of prompt harmfulness, response harmfulness, and response\nrefusal. Through extensive evaluations across multiple safety and toxicity\nbenchmarks, we demonstrate that POLYGUARD outperforms existing state-of-the-art\nopen-weight and commercial safety classifiers by 5.5%. Our contributions\nadvance efforts toward safer multilingual LLMs for all global users."
                },
                "authors": [
                    {
                        "name": "Priyanshu Kumar"
                    },
                    {
                        "name": "Devansh Jain"
                    },
                    {
                        "name": "Akhila Yerukola"
                    },
                    {
                        "name": "Liwei Jiang"
                    },
                    {
                        "name": "Himanshu Beniwal"
                    },
                    {
                        "name": "Thomas Hartvigsen"
                    },
                    {
                        "name": "Maarten Sap"
                    }
                ],
                "author_detail": {
                    "name": "Maarten Sap"
                },
                "author": "Maarten Sap",
                "arxiv_comment": "Accepted to COLM 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04377v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04377v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05411v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05411v1",
                "updated": "2025-08-07T14:04:12Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    14,
                    4,
                    12,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T14:04:12Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    14,
                    4,
                    12,
                    3,
                    219,
                    0
                ],
                "title": "MolSnap: Snap-Fast Molecular Generation with Latent Variational Mean\n  Flow",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MolSnap: Snap-Fast Molecular Generation with Latent Variational Mean\n  Flow"
                },
                "summary": "Molecular generation conditioned on textual descriptions is a fundamental\ntask in computational chemistry and drug discovery. Existing methods often\nstruggle to simultaneously ensure high-quality, diverse generation and fast\ninference. In this work, we propose a novel causality-aware framework that\naddresses these challenges through two key innovations. First, we introduce a\nCausality-Aware Transformer (CAT) that jointly encodes molecular graph tokens\nand text instructions while enforcing causal dependencies during generation.\nSecond, we develop a Variational Mean Flow (VMF) framework that generalizes\nexisting flow-based methods by modeling the latent space as a mixture of\nGaussians, enhancing expressiveness beyond unimodal priors. VMF enables\nefficient one-step inference while maintaining strong generation quality and\ndiversity. Extensive experiments on four standard molecular benchmarks\ndemonstrate that our model outperforms state-of-the-art baselines, achieving\nhigher novelty (up to 74.5\\%), diversity (up to 70.3\\%), and 100\\% validity\nacross all datasets. Moreover, VMF requires only one number of function\nevaluation (NFE) during conditional generation and up to five NFEs for\nunconditional generation, offering substantial computational efficiency over\ndiffusion-based methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Molecular generation conditioned on textual descriptions is a fundamental\ntask in computational chemistry and drug discovery. Existing methods often\nstruggle to simultaneously ensure high-quality, diverse generation and fast\ninference. In this work, we propose a novel causality-aware framework that\naddresses these challenges through two key innovations. First, we introduce a\nCausality-Aware Transformer (CAT) that jointly encodes molecular graph tokens\nand text instructions while enforcing causal dependencies during generation.\nSecond, we develop a Variational Mean Flow (VMF) framework that generalizes\nexisting flow-based methods by modeling the latent space as a mixture of\nGaussians, enhancing expressiveness beyond unimodal priors. VMF enables\nefficient one-step inference while maintaining strong generation quality and\ndiversity. Extensive experiments on four standard molecular benchmarks\ndemonstrate that our model outperforms state-of-the-art baselines, achieving\nhigher novelty (up to 74.5\\%), diversity (up to 70.3\\%), and 100\\% validity\nacross all datasets. Moreover, VMF requires only one number of function\nevaluation (NFE) during conditional generation and up to five NFEs for\nunconditional generation, offering substantial computational efficiency over\ndiffusion-based methods."
                },
                "authors": [
                    {
                        "name": "Md Atik Ahamed"
                    },
                    {
                        "name": "Qiang Ye"
                    },
                    {
                        "name": "Qiang Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Qiang Cheng"
                },
                "author": "Qiang Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05411v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05411v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05399v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05399v1",
                "updated": "2025-08-07T13:51:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    13,
                    51,
                    17,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T13:51:17Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    13,
                    51,
                    17,
                    3,
                    219,
                    0
                ],
                "title": "UNCAGE: Contrastive Attention Guidance for Masked Generative\n  Transformers in Text-to-Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UNCAGE: Contrastive Attention Guidance for Masked Generative\n  Transformers in Text-to-Image Generation"
                },
                "summary": "Text-to-image (T2I) generation has been actively studied using Diffusion\nModels and Autoregressive Models. Recently, Masked Generative Transformers have\ngained attention as an alternative to Autoregressive Models to overcome the\ninherent limitations of causal attention and autoregressive decoding through\nbidirectional attention and parallel decoding, enabling efficient and\nhigh-quality image generation. However, compositional T2I generation remains\nchallenging, as even state-of-the-art Diffusion Models often fail to accurately\nbind attributes and achieve proper text-image alignment. While Diffusion Models\nhave been extensively studied for this issue, Masked Generative Transformers\nexhibit similar limitations but have not been explored in this context. To\naddress this, we propose Unmasking with Contrastive Attention Guidance\n(UNCAGE), a novel training-free method that improves compositional fidelity by\nleveraging attention maps to prioritize the unmasking of tokens that clearly\nrepresent individual objects. UNCAGE consistently improves performance in both\nquantitative and qualitative evaluations across multiple benchmarks and\nmetrics, with negligible inference overhead. Our code is available at\nhttps://github.com/furiosa-ai/uncage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image (T2I) generation has been actively studied using Diffusion\nModels and Autoregressive Models. Recently, Masked Generative Transformers have\ngained attention as an alternative to Autoregressive Models to overcome the\ninherent limitations of causal attention and autoregressive decoding through\nbidirectional attention and parallel decoding, enabling efficient and\nhigh-quality image generation. However, compositional T2I generation remains\nchallenging, as even state-of-the-art Diffusion Models often fail to accurately\nbind attributes and achieve proper text-image alignment. While Diffusion Models\nhave been extensively studied for this issue, Masked Generative Transformers\nexhibit similar limitations but have not been explored in this context. To\naddress this, we propose Unmasking with Contrastive Attention Guidance\n(UNCAGE), a novel training-free method that improves compositional fidelity by\nleveraging attention maps to prioritize the unmasking of tokens that clearly\nrepresent individual objects. UNCAGE consistently improves performance in both\nquantitative and qualitative evaluations across multiple benchmarks and\nmetrics, with negligible inference overhead. Our code is available at\nhttps://github.com/furiosa-ai/uncage."
                },
                "authors": [
                    {
                        "name": "Wonjun Kang"
                    },
                    {
                        "name": "Byeongkeun Ahn"
                    },
                    {
                        "name": "Minjae Lee"
                    },
                    {
                        "name": "Kevin Galim"
                    },
                    {
                        "name": "Seunghyuk Oh"
                    },
                    {
                        "name": "Hyung Il Koo"
                    },
                    {
                        "name": "Nam Ik Cho"
                    }
                ],
                "author_detail": {
                    "name": "Nam Ik Cho"
                },
                "author": "Nam Ik Cho",
                "arxiv_comment": "Code is available at https://github.com/furiosa-ai/uncage",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05399v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05399v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05396v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05396v1",
                "updated": "2025-08-07T13:49:00Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    13,
                    49,
                    0,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T13:49:00Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    13,
                    49,
                    0,
                    3,
                    219,
                    0
                ],
                "title": "Real-Time Iteration Scheme for Diffusion Policy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-Time Iteration Scheme for Diffusion Policy"
                },
                "summary": "Diffusion Policies have demonstrated impressive performance in robotic\nmanipulation tasks. However, their long inference time, resulting from an\nextensive iterative denoising process, and the need to execute an action chunk\nbefore the next prediction to maintain consistent actions limit their\napplicability to latency-critical tasks or simple tasks with a short cycle\ntime. While recent methods explored distillation or alternative policy\nstructures to accelerate inference, these often demand additional training,\nwhich can be resource-intensive for large robotic models. In this paper, we\nintroduce a novel approach inspired by the Real-Time Iteration (RTI) Scheme, a\nmethod from optimal control that accelerates optimization by leveraging\nsolutions from previous time steps as initial guesses for subsequent\niterations. We explore the application of this scheme in diffusion inference\nand propose a scaling-based method to effectively handle discrete actions, such\nas grasping, in robotic manipulation. The proposed scheme significantly reduces\nruntime computational costs without the need for distillation or policy\nredesign. This enables a seamless integration into many pre-trained\ndiffusion-based models, in particular, to resource-demanding large models. We\nalso provide theoretical conditions for the contractivity which could be useful\nfor estimating the initial denoising step. Quantitative results from extensive\nsimulation experiments show a substantial reduction in inference time, with\ncomparable overall performance compared with Diffusion Policy using full-step\ndenoising. Our project page with additional resources is available at:\nhttps://rti-dp.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Policies have demonstrated impressive performance in robotic\nmanipulation tasks. However, their long inference time, resulting from an\nextensive iterative denoising process, and the need to execute an action chunk\nbefore the next prediction to maintain consistent actions limit their\napplicability to latency-critical tasks or simple tasks with a short cycle\ntime. While recent methods explored distillation or alternative policy\nstructures to accelerate inference, these often demand additional training,\nwhich can be resource-intensive for large robotic models. In this paper, we\nintroduce a novel approach inspired by the Real-Time Iteration (RTI) Scheme, a\nmethod from optimal control that accelerates optimization by leveraging\nsolutions from previous time steps as initial guesses for subsequent\niterations. We explore the application of this scheme in diffusion inference\nand propose a scaling-based method to effectively handle discrete actions, such\nas grasping, in robotic manipulation. The proposed scheme significantly reduces\nruntime computational costs without the need for distillation or policy\nredesign. This enables a seamless integration into many pre-trained\ndiffusion-based models, in particular, to resource-demanding large models. We\nalso provide theoretical conditions for the contractivity which could be useful\nfor estimating the initial denoising step. Quantitative results from extensive\nsimulation experiments show a substantial reduction in inference time, with\ncomparable overall performance compared with Diffusion Policy using full-step\ndenoising. Our project page with additional resources is available at:\nhttps://rti-dp.github.io/."
                },
                "authors": [
                    {
                        "name": "Yufei Duan"
                    },
                    {
                        "name": "Hang Yin"
                    },
                    {
                        "name": "Danica Kragic"
                    }
                ],
                "author_detail": {
                    "name": "Danica Kragic"
                },
                "author": "Danica Kragic",
                "arxiv_comment": "\\c{opyright} 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05396v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05396v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04016v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04016v2",
                "updated": "2025-08-07T13:44:38Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    13,
                    44,
                    38,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-06T02:12:29Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    2,
                    12,
                    29,
                    2,
                    218,
                    0
                ],
                "title": "S$^2$Q-VDiT: Accurate Quantized Video Diffusion Transformer with Salient\n  Data and Sparse Token Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "S$^2$Q-VDiT: Accurate Quantized Video Diffusion Transformer with Salient\n  Data and Sparse Token Distillation"
                },
                "summary": "Diffusion transformers have emerged as the mainstream paradigm for video\ngeneration models. However, the use of up to billions of parameters incurs\nsignificant computational costs. Quantization offers a promising solution by\nreducing memory usage and accelerating inference. Nonetheless, we observe that\nthe joint modeling of spatial and temporal information in video diffusion\nmodels (V-DMs) leads to extremely long token sequences, which introduces high\ncalibration variance and learning challenges. To address these issues, we\npropose S$^2$Q-VDiT, a post-training quantization framework for V-DMs that\nleverages Salient data and Sparse token distillation. During the calibration\nphase, we identify that quantization performance is highly sensitive to the\nchoice of calibration data. To mitigate this, we introduce\n\\textit{Hessian-aware Salient Data Selection}, which constructs high-quality\ncalibration datasets by considering both diffusion and quantization\ncharacteristics unique to V-DMs. To tackle the learning challenges, we further\nanalyze the sparse attention patterns inherent in V-DMs. Based on this\nobservation, we propose \\textit{Attention-guided Sparse Token Distillation},\nwhich exploits token-wise attention distributions to emphasize tokens that are\nmore influential to the model's output. Under W4A6 quantization, S$^2$Q-VDiT\nachieves lossless performance while delivering $3.9\\times$ model compression\nand $1.3\\times$ inference acceleration. Code will be available at\nhttps://github.com/wlfeng0509/s2q-vdit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have emerged as the mainstream paradigm for video\ngeneration models. However, the use of up to billions of parameters incurs\nsignificant computational costs. Quantization offers a promising solution by\nreducing memory usage and accelerating inference. Nonetheless, we observe that\nthe joint modeling of spatial and temporal information in video diffusion\nmodels (V-DMs) leads to extremely long token sequences, which introduces high\ncalibration variance and learning challenges. To address these issues, we\npropose S$^2$Q-VDiT, a post-training quantization framework for V-DMs that\nleverages Salient data and Sparse token distillation. During the calibration\nphase, we identify that quantization performance is highly sensitive to the\nchoice of calibration data. To mitigate this, we introduce\n\\textit{Hessian-aware Salient Data Selection}, which constructs high-quality\ncalibration datasets by considering both diffusion and quantization\ncharacteristics unique to V-DMs. To tackle the learning challenges, we further\nanalyze the sparse attention patterns inherent in V-DMs. Based on this\nobservation, we propose \\textit{Attention-guided Sparse Token Distillation},\nwhich exploits token-wise attention distributions to emphasize tokens that are\nmore influential to the model's output. Under W4A6 quantization, S$^2$Q-VDiT\nachieves lossless performance while delivering $3.9\\times$ model compression\nand $1.3\\times$ inference acceleration. Code will be available at\nhttps://github.com/wlfeng0509/s2q-vdit."
                },
                "authors": [
                    {
                        "name": "Weilun Feng"
                    },
                    {
                        "name": "Haotong Qin"
                    },
                    {
                        "name": "Chuanguang Yang"
                    },
                    {
                        "name": "Xiangqi Li"
                    },
                    {
                        "name": "Han Yang"
                    },
                    {
                        "name": "Yuqi Li"
                    },
                    {
                        "name": "Zhulin An"
                    },
                    {
                        "name": "Libo Huang"
                    },
                    {
                        "name": "Michele Magno"
                    },
                    {
                        "name": "Yongjun Xu"
                    }
                ],
                "author_detail": {
                    "name": "Yongjun Xu"
                },
                "author": "Yongjun Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04016v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04016v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05387v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05387v1",
                "updated": "2025-08-07T13:37:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    13,
                    37,
                    4,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T13:37:04Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    13,
                    37,
                    4,
                    3,
                    219,
                    0
                ],
                "title": "Echo: Decoupling Inference and Training for Large-Scale RL Alignment on\n  Heterogeneous Swarms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Echo: Decoupling Inference and Training for Large-Scale RL Alignment on\n  Heterogeneous Swarms"
                },
                "summary": "Modern RL-based post-training for large language models (LLMs) co-locate\ntrajectory sampling and policy optimisation on the same GPU cluster, forcing\nthe system to switch between inference and training workloads. This serial\ncontext switching violates the single-program-multiple-data (SPMD) assumption\nunderlying today's distributed training systems. We present Echo, the RL system\nthat cleanly decouples these two phases across heterogeneous \"inference\" and\n\"training\" swarms while preserving statistical efficiency. Echo introduces two\nlightweight synchronization protocols: a sequential pull mode that refreshes\nsampler weights on every API call for minimal bias, and an asynchronous\npush-pull mode that streams version-tagged rollouts through a replay buffer to\nmaximise hardware utilisation. Training three representative RL workloads with\nQwen3-4B, Qwen2.5-7B and Qwen3-32B on a geographically distributed cluster,\nEcho matches a fully co-located Verl baseline in convergence speed and final\nreward while off-loading trajectory generation to commodity edge hardware.\nThese promising results demonstrate that large-scale RL for LLMs could achieve\ndatacentre-grade performance using decentralised, heterogeneous resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern RL-based post-training for large language models (LLMs) co-locate\ntrajectory sampling and policy optimisation on the same GPU cluster, forcing\nthe system to switch between inference and training workloads. This serial\ncontext switching violates the single-program-multiple-data (SPMD) assumption\nunderlying today's distributed training systems. We present Echo, the RL system\nthat cleanly decouples these two phases across heterogeneous \"inference\" and\n\"training\" swarms while preserving statistical efficiency. Echo introduces two\nlightweight synchronization protocols: a sequential pull mode that refreshes\nsampler weights on every API call for minimal bias, and an asynchronous\npush-pull mode that streams version-tagged rollouts through a replay buffer to\nmaximise hardware utilisation. Training three representative RL workloads with\nQwen3-4B, Qwen2.5-7B and Qwen3-32B on a geographically distributed cluster,\nEcho matches a fully co-located Verl baseline in convergence speed and final\nreward while off-loading trajectory generation to commodity edge hardware.\nThese promising results demonstrate that large-scale RL for LLMs could achieve\ndatacentre-grade performance using decentralised, heterogeneous resources."
                },
                "authors": [
                    {
                        "name": "Jie Xiao"
                    },
                    {
                        "name": "Shaoduo Gan"
                    },
                    {
                        "name": "Changyuan Fan"
                    },
                    {
                        "name": "Qingnan Ren"
                    },
                    {
                        "name": "Alfred Long"
                    },
                    {
                        "name": "Yuchen Zhang"
                    },
                    {
                        "name": "Rymon Yu"
                    },
                    {
                        "name": "Eric Yang"
                    },
                    {
                        "name": "Lynn Ai"
                    }
                ],
                "author_detail": {
                    "name": "Lynn Ai"
                },
                "author": "Lynn Ai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05387v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05387v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03703v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03703v4",
                "updated": "2025-08-07T13:34:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    13,
                    34,
                    51,
                    3,
                    219,
                    0
                ],
                "published": "2025-07-04T16:38:09Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    16,
                    38,
                    9,
                    4,
                    185,
                    0
                ],
                "title": "Sign Spotting Disambiguation using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sign Spotting Disambiguation using Large Language Models"
                },
                "summary": "Sign spotting, the task of identifying and localizing individual signs within\ncontinuous sign language video, plays a pivotal role in scaling dataset\nannotations and addressing the severe data scarcity issue in sign language\ntranslation. While automatic sign spotting holds great promise for enabling\nframe-level supervision at scale, it grapples with challenges such as\nvocabulary inflexibility and ambiguity inherent in continuous sign streams.\nHence, we introduce a novel, training-free framework that integrates Large\nLanguage Models (LLMs) to significantly enhance sign spotting quality. Our\napproach extracts global spatio-temporal and hand shape features, which are\nthen matched against a large-scale sign dictionary using dynamic time warping\nand cosine similarity. This dictionary-based matching inherently offers\nsuperior vocabulary flexibility without requiring model retraining. To mitigate\nnoise and ambiguity from the matching process, an LLM performs context-aware\ngloss disambiguation via beam search, notably without fine-tuning. Extensive\nexperiments on both synthetic and real-world sign language datasets demonstrate\nour method's superior accuracy and sentence fluency compared to traditional\napproaches, highlighting the potential of LLMs in advancing sign spotting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sign spotting, the task of identifying and localizing individual signs within\ncontinuous sign language video, plays a pivotal role in scaling dataset\nannotations and addressing the severe data scarcity issue in sign language\ntranslation. While automatic sign spotting holds great promise for enabling\nframe-level supervision at scale, it grapples with challenges such as\nvocabulary inflexibility and ambiguity inherent in continuous sign streams.\nHence, we introduce a novel, training-free framework that integrates Large\nLanguage Models (LLMs) to significantly enhance sign spotting quality. Our\napproach extracts global spatio-temporal and hand shape features, which are\nthen matched against a large-scale sign dictionary using dynamic time warping\nand cosine similarity. This dictionary-based matching inherently offers\nsuperior vocabulary flexibility without requiring model retraining. To mitigate\nnoise and ambiguity from the matching process, an LLM performs context-aware\ngloss disambiguation via beam search, notably without fine-tuning. Extensive\nexperiments on both synthetic and real-world sign language datasets demonstrate\nour method's superior accuracy and sentence fluency compared to traditional\napproaches, highlighting the potential of LLMs in advancing sign spotting."
                },
                "authors": [
                    {
                        "name": "JianHe Low"
                    },
                    {
                        "name": "Ozge Mercanoglu Sincan"
                    },
                    {
                        "name": "Richard Bowden"
                    }
                ],
                "author_detail": {
                    "name": "Richard Bowden"
                },
                "author": "Richard Bowden",
                "arxiv_comment": "Accepted in the international conference on Intelligent Virtual\n  Agents (IVA Adjunct)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03703v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03703v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03751v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03751v4",
                "updated": "2025-08-07T13:29:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    13,
                    29,
                    43,
                    3,
                    219,
                    0
                ],
                "published": "2024-10-01T21:48:12Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    21,
                    48,
                    12,
                    1,
                    275,
                    0
                ],
                "title": "Recent Advances in Speech Language Models: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Advances in Speech Language Models: A Survey"
                },
                "summary": "Large Language Models (LLMs) have recently garnered significant attention,\nprimarily for their capabilities in text-based interactions. However, natural\nhuman interaction often relies on speech, necessitating a shift towards\nvoice-based models. A straightforward approach to achieve this involves a\npipeline of ``Automatic Speech Recognition (ASR) + LLM + Text-to-Speech (TTS)\",\nwhere input speech is transcribed to text, processed by an LLM, and then\nconverted back to speech. Despite being straightforward, this method suffers\nfrom inherent limitations, such as information loss during modality conversion,\nsignificant latency due to the complex pipeline, and error accumulation across\nthe three stages. To address these issues, Speech Language Models (SpeechLMs)\n-- end-to-end models that generate speech without converting from text -- have\nemerged as a promising alternative. This survey paper provides the first\ncomprehensive overview of recent methodologies for constructing SpeechLMs,\ndetailing the key components of their architecture and the various training\nrecipes integral to their development. Additionally, we systematically survey\nthe various capabilities of SpeechLMs, categorize their evaluation metrics, and\ndiscuss the challenges and future research directions in this rapidly evolving\nfield. The GitHub repository is available at\nhttps://github.com/dreamtheater123/Awesome-SpeechLM-Survey",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently garnered significant attention,\nprimarily for their capabilities in text-based interactions. However, natural\nhuman interaction often relies on speech, necessitating a shift towards\nvoice-based models. A straightforward approach to achieve this involves a\npipeline of ``Automatic Speech Recognition (ASR) + LLM + Text-to-Speech (TTS)\",\nwhere input speech is transcribed to text, processed by an LLM, and then\nconverted back to speech. Despite being straightforward, this method suffers\nfrom inherent limitations, such as information loss during modality conversion,\nsignificant latency due to the complex pipeline, and error accumulation across\nthe three stages. To address these issues, Speech Language Models (SpeechLMs)\n-- end-to-end models that generate speech without converting from text -- have\nemerged as a promising alternative. This survey paper provides the first\ncomprehensive overview of recent methodologies for constructing SpeechLMs,\ndetailing the key components of their architecture and the various training\nrecipes integral to their development. Additionally, we systematically survey\nthe various capabilities of SpeechLMs, categorize their evaluation metrics, and\ndiscuss the challenges and future research directions in this rapidly evolving\nfield. The GitHub repository is available at\nhttps://github.com/dreamtheater123/Awesome-SpeechLM-Survey"
                },
                "authors": [
                    {
                        "name": "Wenqian Cui"
                    },
                    {
                        "name": "Dianzhi Yu"
                    },
                    {
                        "name": "Xiaoqi Jiao"
                    },
                    {
                        "name": "Ziqiao Meng"
                    },
                    {
                        "name": "Guangyan Zhang"
                    },
                    {
                        "name": "Qichao Wang"
                    },
                    {
                        "name": "Yiwen Guo"
                    },
                    {
                        "name": "Irwin King"
                    }
                ],
                "author_detail": {
                    "name": "Irwin King"
                },
                "author": "Irwin King",
                "arxiv_comment": "The reduced version of this paper has been accepted at ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03751v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03751v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05370v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05370v1",
                "updated": "2025-08-07T13:15:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    13,
                    15,
                    59,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T13:15:59Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    13,
                    15,
                    59,
                    3,
                    219,
                    0
                ],
                "title": "Simulating LLM training workloads for heterogeneous compute and network\n  infrastructure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating LLM training workloads for heterogeneous compute and network\n  infrastructure"
                },
                "summary": "The growing demand for large-scale GPU clusters in distributed model training\npresents a significant barrier to innovation, particularly in model\noptimization, performance tuning, and system-level enhancements. To address\nthis challenge, LLM training simulators are employed to estimate training time\nand guide design decisions. However, the state-of-the-art LLM training\nsimulators assume homogeneous compute and network infrastructure. In practice,\ndevice heterogeneity is inevitable due to resource sharing in cloud\nenvironments, frequent shifts in device generations, and inherent intra-chip\ninterconnect heterogeneity. To address the gap between state-of-the-art and\npractical requirements, we propose the design of a heterogeneity-aware\ndistributed LLM simulator capable of predicting training time while enabling\nabstractions to specify custom configurations for device groups and\ndevice-to-parallelism mapping. We present the design requirements and\nchallenges in building a heterogeneity-aware distributed ML training simulator,\nand design components such as non-uniform workload partitioning. Our initial\nsimulation results demonstrate the impact of heterogeneity on the model\ncomputation and communication time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing demand for large-scale GPU clusters in distributed model training\npresents a significant barrier to innovation, particularly in model\noptimization, performance tuning, and system-level enhancements. To address\nthis challenge, LLM training simulators are employed to estimate training time\nand guide design decisions. However, the state-of-the-art LLM training\nsimulators assume homogeneous compute and network infrastructure. In practice,\ndevice heterogeneity is inevitable due to resource sharing in cloud\nenvironments, frequent shifts in device generations, and inherent intra-chip\ninterconnect heterogeneity. To address the gap between state-of-the-art and\npractical requirements, we propose the design of a heterogeneity-aware\ndistributed LLM simulator capable of predicting training time while enabling\nabstractions to specify custom configurations for device groups and\ndevice-to-parallelism mapping. We present the design requirements and\nchallenges in building a heterogeneity-aware distributed ML training simulator,\nand design components such as non-uniform workload partitioning. Our initial\nsimulation results demonstrate the impact of heterogeneity on the model\ncomputation and communication time."
                },
                "authors": [
                    {
                        "name": "Sumit Kumar"
                    },
                    {
                        "name": "Arjun Temura"
                    },
                    {
                        "name": "Naman Sharma"
                    },
                    {
                        "name": "Ramanjeet Singh"
                    },
                    {
                        "name": "Meet Dadhania"
                    },
                    {
                        "name": "Praveen Tammana"
                    },
                    {
                        "name": "Satananda Burla"
                    },
                    {
                        "name": "Abed Mohammad Kamaluddin"
                    },
                    {
                        "name": "Rinku Shah"
                    }
                ],
                "author_detail": {
                    "name": "Rinku Shah"
                },
                "author": "Rinku Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05370v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05370v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05366v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05366v1",
                "updated": "2025-08-07T13:13:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    13,
                    13,
                    19,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T13:13:19Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    13,
                    13,
                    19,
                    3,
                    219,
                    0
                ],
                "title": "Can Language Models Critique Themselves? Investigating Self-Feedback for\n  Retrieval Augmented Generation at BioASQ 2025",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Language Models Critique Themselves? Investigating Self-Feedback for\n  Retrieval Augmented Generation at BioASQ 2025"
                },
                "summary": "Agentic Retrieval Augmented Generation (RAG) and 'deep research' systems aim\nto enable autonomous search processes where Large Language Models (LLMs)\niteratively refine outputs. However, applying these systems to domain-specific\nprofessional search, such as biomedical research, presents challenges, as\nautomated systems may reduce user involvement and misalign with expert\ninformation needs. Professional search tasks often demand high levels of user\nexpertise and transparency. The BioASQ CLEF 2025 challenge, using\nexpert-formulated questions, can serve as a platform to study these issues. We\nexplored the performance of current reasoning and nonreasoning LLMs like\nGemini-Flash 2.0, o3-mini, o4-mini and DeepSeek-R1. A key aspect of our\nmethodology was a self-feedback mechanism where LLMs generated, evaluated, and\nthen refined their outputs for query expansion and for multiple answer types\n(yes/no, factoid, list, ideal). We investigated whether this iterative\nself-correction improves performance and if reasoning models are more capable\nof generating useful feedback. Preliminary results indicate varied performance\nfor the self-feedback strategy across models and tasks. This work offers\ninsights into LLM self-correction and informs future work on comparing the\neffectiveness of LLM-generated feedback with direct human expert input in these\nsearch systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic Retrieval Augmented Generation (RAG) and 'deep research' systems aim\nto enable autonomous search processes where Large Language Models (LLMs)\niteratively refine outputs. However, applying these systems to domain-specific\nprofessional search, such as biomedical research, presents challenges, as\nautomated systems may reduce user involvement and misalign with expert\ninformation needs. Professional search tasks often demand high levels of user\nexpertise and transparency. The BioASQ CLEF 2025 challenge, using\nexpert-formulated questions, can serve as a platform to study these issues. We\nexplored the performance of current reasoning and nonreasoning LLMs like\nGemini-Flash 2.0, o3-mini, o4-mini and DeepSeek-R1. A key aspect of our\nmethodology was a self-feedback mechanism where LLMs generated, evaluated, and\nthen refined their outputs for query expansion and for multiple answer types\n(yes/no, factoid, list, ideal). We investigated whether this iterative\nself-correction improves performance and if reasoning models are more capable\nof generating useful feedback. Preliminary results indicate varied performance\nfor the self-feedback strategy across models and tasks. This work offers\ninsights into LLM self-correction and informs future work on comparing the\neffectiveness of LLM-generated feedback with direct human expert input in these\nsearch systems."
                },
                "authors": [
                    {
                        "name": "Samy Ateia"
                    },
                    {
                        "name": "Udo Kruschwitz"
                    }
                ],
                "author_detail": {
                    "name": "Udo Kruschwitz"
                },
                "author": "Udo Kruschwitz",
                "arxiv_comment": "Version as accepted at the BioASQ Lab at CLEF 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05366v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05366v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05364v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05364v1",
                "updated": "2025-08-07T13:12:26Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    13,
                    12,
                    26,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T13:12:26Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    13,
                    12,
                    26,
                    3,
                    219,
                    0
                ],
                "title": "Optimal Corpus Aware Training for Neural Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Corpus Aware Training for Neural Machine Translation"
                },
                "summary": "Corpus Aware Training (CAT) leverages valuable corpus metadata during\ntraining by injecting corpus information into each training example, and has\nbeen found effective in the literature, commonly known as the \"tagging\"\napproach. Models trained with CAT inherently learn the quality, domain and\nnuance between corpora directly from data, and can easily switch to different\ninference behavior. To achieve the best evaluation, CAT models pre-define a\ngroup of high quality data before training starts which can be error-prone and\ninefficient. In this work, we propose Optimal Corpus Aware Training (OCAT),\nwhich fine-tunes a CAT pre-trained model by freezing most of the model\nparameters and only tuning small set of corpus-related parameters. We show that\nOCAT is lightweight, resilient to overfitting, and effective in boosting model\naccuracy. We use WMT23 English to Chinese and English to German translation\ntasks as our test ground and show +3.6 and +1.8 chrF improvement, respectively,\nover vanilla training. Furthermore, our approach is on-par or slightly better\nthan other state-of-the-art fine-tuning techniques while being less sensitive\nto hyperparameter settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Corpus Aware Training (CAT) leverages valuable corpus metadata during\ntraining by injecting corpus information into each training example, and has\nbeen found effective in the literature, commonly known as the \"tagging\"\napproach. Models trained with CAT inherently learn the quality, domain and\nnuance between corpora directly from data, and can easily switch to different\ninference behavior. To achieve the best evaluation, CAT models pre-define a\ngroup of high quality data before training starts which can be error-prone and\ninefficient. In this work, we propose Optimal Corpus Aware Training (OCAT),\nwhich fine-tunes a CAT pre-trained model by freezing most of the model\nparameters and only tuning small set of corpus-related parameters. We show that\nOCAT is lightweight, resilient to overfitting, and effective in boosting model\naccuracy. We use WMT23 English to Chinese and English to German translation\ntasks as our test ground and show +3.6 and +1.8 chrF improvement, respectively,\nover vanilla training. Furthermore, our approach is on-par or slightly better\nthan other state-of-the-art fine-tuning techniques while being less sensitive\nto hyperparameter settings."
                },
                "authors": [
                    {
                        "name": "Yi-Hsiu Liao"
                    },
                    {
                        "name": "Cheng Shen"
                    },
                    {
                        "name": "Brenda"
                    },
                    {
                        "name": "Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yang"
                },
                "arxiv_affiliation": "Zixiaofan",
                "author": "Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05364v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05364v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.07480v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.07480v3",
                "updated": "2025-08-07T13:12:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    13,
                    12,
                    25,
                    3,
                    219,
                    0
                ],
                "published": "2024-07-10T09:11:03Z",
                "published_parsed": [
                    2024,
                    7,
                    10,
                    9,
                    11,
                    3,
                    2,
                    192,
                    0
                ],
                "title": "CHIME/FRB/Pulsar discovery of a nearby long period radio transient with\n  a timing glitch",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CHIME/FRB/Pulsar discovery of a nearby long period radio transient with\n  a timing glitch"
                },
                "summary": "We present the discovery of a 421 s long period radio transient (LPT) using\nthe CHIME telescope, CHIME J0630+25. The source is localized to\nRA=06:30:38.4$\\pm1'$ Dec=25:26:24$\\pm1'$ using voltage data acquired with the\nCHIME baseband system. A timing analysis shows that a model including a glitch\nis preferred over a non-glitch model with $dF/F=1.3\\times10^{-6}$, consistent\nwith other glitching neutron stars. The timing model suggests a surface\nmagnetic field of $\\sim1.5\\times10^{15}$ G and a characteristic age of\n$\\sim1.28\\times10^{6}$ yrs. A separate line of evidence to support a strong\nlocal magnetic field is an abnormally high rotation measure of $RM=-347.8(6)\n\\mathrm{rad\\, m^{-2}}$ relative to CHIME J0630+25's modest dispersion measure\nof 22(1) pc cm$^{-2}$, implying a dense local magneto-ionic structure. As a\nresult, we believe that CHIME J0630+25 is a magnetized, slowly spinning,\nisolated neutron star. This marks CHIME J0630+25 as the longest period neutron\nstar and the second long period neutron star with an inferred magnetar-like\nfield. Based on dispersion measure models and comparison with pulsars with\ndistance measurements, CHIME J0630+25 is located at a nearby distance of\n170$^{+310}_{-100}$ pc (95.4\\%), making it an ideal candidate for follow-up\nstudies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the discovery of a 421 s long period radio transient (LPT) using\nthe CHIME telescope, CHIME J0630+25. The source is localized to\nRA=06:30:38.4$\\pm1'$ Dec=25:26:24$\\pm1'$ using voltage data acquired with the\nCHIME baseband system. A timing analysis shows that a model including a glitch\nis preferred over a non-glitch model with $dF/F=1.3\\times10^{-6}$, consistent\nwith other glitching neutron stars. The timing model suggests a surface\nmagnetic field of $\\sim1.5\\times10^{15}$ G and a characteristic age of\n$\\sim1.28\\times10^{6}$ yrs. A separate line of evidence to support a strong\nlocal magnetic field is an abnormally high rotation measure of $RM=-347.8(6)\n\\mathrm{rad\\, m^{-2}}$ relative to CHIME J0630+25's modest dispersion measure\nof 22(1) pc cm$^{-2}$, implying a dense local magneto-ionic structure. As a\nresult, we believe that CHIME J0630+25 is a magnetized, slowly spinning,\nisolated neutron star. This marks CHIME J0630+25 as the longest period neutron\nstar and the second long period neutron star with an inferred magnetar-like\nfield. Based on dispersion measure models and comparison with pulsars with\ndistance measurements, CHIME J0630+25 is located at a nearby distance of\n170$^{+310}_{-100}$ pc (95.4\\%), making it an ideal candidate for follow-up\nstudies."
                },
                "authors": [
                    {
                        "name": "Fengqiu Adam Dong"
                    },
                    {
                        "name": "Tracy E Clarke"
                    },
                    {
                        "name": "Alice Curtin"
                    },
                    {
                        "name": "Ajay Kumar"
                    },
                    {
                        "name": "Ryan Mckinven"
                    },
                    {
                        "name": "Kaitlyn Shin"
                    },
                    {
                        "name": "Ingrid Stairs"
                    },
                    {
                        "name": "Charanjot Brar"
                    },
                    {
                        "name": "Kevin Burdge"
                    },
                    {
                        "name": "Shami Chatterjee"
                    },
                    {
                        "name": "Amanda M. Cook"
                    },
                    {
                        "name": "Emmanuel Fonseca"
                    },
                    {
                        "name": "B. M. Gaensler"
                    },
                    {
                        "name": "Jason W. Hessels"
                    },
                    {
                        "name": "Victoria M. Kaspi"
                    },
                    {
                        "name": "Mattias Lazda"
                    },
                    {
                        "name": "Robert Main"
                    },
                    {
                        "name": "Kiyoshi W. Masui"
                    },
                    {
                        "name": "James W. McKee"
                    },
                    {
                        "name": "Bradley W. Meyers"
                    },
                    {
                        "name": "Aaron B. Pearlman"
                    },
                    {
                        "name": "Scott M. Ransom"
                    },
                    {
                        "name": "Paul Scholz"
                    },
                    {
                        "name": "Kendrick M. Smith"
                    },
                    {
                        "name": "Chia Min Tan"
                    }
                ],
                "author_detail": {
                    "name": "Chia Min Tan"
                },
                "author": "Chia Min Tan",
                "arxiv_comment": "V3: Typos fixed. V2:The previous submission was delayed due to other\n  commitments of the lead author. Because of that, this new version of the\n  paper has a) more data, b) baseband/raw volatage data, d) more analysis on\n  the timing and polarisation, c) reformatted. This publication has been\n  accepted to Astrophysical Journal Letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.07480v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.07480v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.10587v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.10587v2",
                "updated": "2025-08-07T13:11:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    13,
                    11,
                    9,
                    3,
                    219,
                    0
                ],
                "published": "2024-06-15T10:35:22Z",
                "published_parsed": [
                    2024,
                    6,
                    15,
                    10,
                    35,
                    22,
                    5,
                    167,
                    0
                ],
                "title": "Polytopal mesh agglomeration via geometrical deep learning for\n  three-dimensional heterogeneous domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Polytopal mesh agglomeration via geometrical deep learning for\n  three-dimensional heterogeneous domains"
                },
                "summary": "Agglomeration techniques can be successfully employed to reduce the\ncomputational costs of numerical simulations and stand at the basis of\nmultilevel algebraic solvers. To automatically perform mesh agglomeration, we\npropose a novel Geometrical Deep Learning-based algorithm that can exploit the\ngeometrical and physical information of the underlying computational domain to\nconstruct the agglomerated grid and -- simultaneously -- guarantee the\nagglomerated grid's quality. In particular, we propose a bisection model based\non Graph Neural Networks (GNNs) to partition a suitable connectivity graph of\ncomputational three-dimensional meshes. The new approach has a high online\ninference speed. It can simultaneously process the graph structure of the mesh,\nthe geometrical information of the mesh (e.g., elements' volumes, centers'\ncoordinates), and the physical information of the domain (e.g., physical\nparameters). Taking advantage of this new approach, our algorithm can\nagglomerate meshes of a domain composed of heterogeneous media, automatically\nrespecting the underlying heterogeneities. The proposed GNN approach is\ncompared with the k-means algorithm and METIS, which are widely employed\napproaches for graph partitioning and are meant to process only the\nconnectivity information on the mesh. We demonstrate that the performance of\nour algorithms outperforms the k-means and METIS algorithms in terms of quality\nmetrics and runtimes. Moreover, we demonstrate that our algorithm also shows a\ngood level of generalization when applied to complex geometries, such as\nthree-dimensional geometries reconstructed from medical images. Finally, the\nmodel's capability to perform agglomeration in heterogeneous domains is\nevaluated when integrated into a polytopal discontinuous Galerkin finite\nelement solver.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agglomeration techniques can be successfully employed to reduce the\ncomputational costs of numerical simulations and stand at the basis of\nmultilevel algebraic solvers. To automatically perform mesh agglomeration, we\npropose a novel Geometrical Deep Learning-based algorithm that can exploit the\ngeometrical and physical information of the underlying computational domain to\nconstruct the agglomerated grid and -- simultaneously -- guarantee the\nagglomerated grid's quality. In particular, we propose a bisection model based\non Graph Neural Networks (GNNs) to partition a suitable connectivity graph of\ncomputational three-dimensional meshes. The new approach has a high online\ninference speed. It can simultaneously process the graph structure of the mesh,\nthe geometrical information of the mesh (e.g., elements' volumes, centers'\ncoordinates), and the physical information of the domain (e.g., physical\nparameters). Taking advantage of this new approach, our algorithm can\nagglomerate meshes of a domain composed of heterogeneous media, automatically\nrespecting the underlying heterogeneities. The proposed GNN approach is\ncompared with the k-means algorithm and METIS, which are widely employed\napproaches for graph partitioning and are meant to process only the\nconnectivity information on the mesh. We demonstrate that the performance of\nour algorithms outperforms the k-means and METIS algorithms in terms of quality\nmetrics and runtimes. Moreover, we demonstrate that our algorithm also shows a\ngood level of generalization when applied to complex geometries, such as\nthree-dimensional geometries reconstructed from medical images. Finally, the\nmodel's capability to perform agglomeration in heterogeneous domains is\nevaluated when integrated into a polytopal discontinuous Galerkin finite\nelement solver."
                },
                "authors": [
                    {
                        "name": "Paola F. Antonietti"
                    },
                    {
                        "name": "Mattia Corti"
                    },
                    {
                        "name": "Gabriele Martinelli"
                    }
                ],
                "author_detail": {
                    "name": "Gabriele Martinelli"
                },
                "author": "Gabriele Martinelli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.10587v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.10587v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01215v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01215v3",
                "updated": "2025-08-07T13:10:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    13,
                    10,
                    30,
                    3,
                    219,
                    0
                ],
                "published": "2024-10-02T03:57:21Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    3,
                    57,
                    21,
                    2,
                    276,
                    0
                ],
                "title": "From Code to Correctness: Closing the Last Mile of Code Generation with\n  Hierarchical Debugging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Code to Correctness: Closing the Last Mile of Code Generation with\n  Hierarchical Debugging"
                },
                "summary": "While large language models have made significant strides in code generation,\nthe pass rate of the generated code is bottlenecked on subtle errors, often\nrequiring human intervention to pass tests, especially for complex problems.\nExisting LLM-based debugging systems treat generated programs as monolithic\nunits, failing to address bugs at multiple levels of granularity, from\nlow-level syntax errors to high-level algorithmic flaws. In this paper, we\nintroduce Multi-Granularity Debugger (MGDebugger), a hierarchical code debugger\nby isolating, identifying, and resolving bugs at various levels of granularity.\nMGDebugger decomposes problematic code into a hierarchical tree structure of\nsubfunctions, with each level representing a particular granularity of error.\nDuring debugging, it analyzes each subfunction and iteratively resolves bugs in\na bottom-up manner. To effectively test each subfunction, we propose an\nLLM-simulated Python executor, which traces code execution and tracks important\nvariable states to pinpoint errors accurately. Extensive experiments\ndemonstrate that MGDebugger outperforms existing debugging systems, achieving\nan 18.9% improvement in accuracy over seed generations in HumanEval and a 97.6%\nrepair success rate in HumanEvalFix. Furthermore, MGDebugger effectively fixes\nbugs across different categories and difficulty levels, demonstrating its\nrobustness and effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models have made significant strides in code generation,\nthe pass rate of the generated code is bottlenecked on subtle errors, often\nrequiring human intervention to pass tests, especially for complex problems.\nExisting LLM-based debugging systems treat generated programs as monolithic\nunits, failing to address bugs at multiple levels of granularity, from\nlow-level syntax errors to high-level algorithmic flaws. In this paper, we\nintroduce Multi-Granularity Debugger (MGDebugger), a hierarchical code debugger\nby isolating, identifying, and resolving bugs at various levels of granularity.\nMGDebugger decomposes problematic code into a hierarchical tree structure of\nsubfunctions, with each level representing a particular granularity of error.\nDuring debugging, it analyzes each subfunction and iteratively resolves bugs in\na bottom-up manner. To effectively test each subfunction, we propose an\nLLM-simulated Python executor, which traces code execution and tracks important\nvariable states to pinpoint errors accurately. Extensive experiments\ndemonstrate that MGDebugger outperforms existing debugging systems, achieving\nan 18.9% improvement in accuracy over seed generations in HumanEval and a 97.6%\nrepair success rate in HumanEvalFix. Furthermore, MGDebugger effectively fixes\nbugs across different categories and difficulty levels, demonstrating its\nrobustness and effectiveness."
                },
                "authors": [
                    {
                        "name": "Yuling Shi"
                    },
                    {
                        "name": "Songsong Wang"
                    },
                    {
                        "name": "Chengcheng Wan"
                    },
                    {
                        "name": "Min Wang"
                    },
                    {
                        "name": "Xiaodong Gu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaodong Gu"
                },
                "author": "Xiaodong Gu",
                "arxiv_comment": "Code and data available at https://github.com/YerbaPage/MGDebugger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01215v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01215v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20471v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20471v3",
                "updated": "2025-08-07T13:07:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    13,
                    7,
                    35,
                    3,
                    219,
                    0
                ],
                "published": "2025-05-26T19:10:47Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    19,
                    10,
                    47,
                    0,
                    146,
                    0
                ],
                "title": "WeatherEdit: Controllable Weather Editing with 4D Gaussian Field",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WeatherEdit: Controllable Weather Editing with 4D Gaussian Field"
                },
                "summary": "In this work, we present WeatherEdit, a novel weather editing pipeline for\ngenerating realistic weather effects with controllable types and severity in 3D\nscenes. Our approach is structured into two key components: weather background\nediting and weather particle construction. For weather background editing, we\nintroduce an all-in-one adapter that integrates multiple weather styles into a\nsingle pretrained diffusion model, enabling the generation of diverse weather\neffects in 2D image backgrounds. During inference, we design a Temporal-View\n(TV-) attention mechanism that follows a specific order to aggregate temporal\nand spatial information, ensuring consistent editing across multi-frame and\nmulti-view images. To construct the weather particles, we first reconstruct a\n3D scene using the edited images and then introduce a dynamic 4D Gaussian field\nto generate snowflakes, raindrops and fog in the scene. The attributes and\ndynamics of these particles are precisely controlled through physical-based\nmodelling and simulation, ensuring realistic weather representation and\nflexible severity adjustments. Finally, we integrate the 4D Gaussian field with\nthe 3D scene to render consistent and highly realistic weather effects.\nExperiments on multiple driving datasets demonstrate that WeatherEdit can\ngenerate diverse weather effects with controllable condition severity,\nhighlighting its potential for autonomous driving simulation in adverse\nweather. See project page: https://jumponthemoon.github.io/w-edit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we present WeatherEdit, a novel weather editing pipeline for\ngenerating realistic weather effects with controllable types and severity in 3D\nscenes. Our approach is structured into two key components: weather background\nediting and weather particle construction. For weather background editing, we\nintroduce an all-in-one adapter that integrates multiple weather styles into a\nsingle pretrained diffusion model, enabling the generation of diverse weather\neffects in 2D image backgrounds. During inference, we design a Temporal-View\n(TV-) attention mechanism that follows a specific order to aggregate temporal\nand spatial information, ensuring consistent editing across multi-frame and\nmulti-view images. To construct the weather particles, we first reconstruct a\n3D scene using the edited images and then introduce a dynamic 4D Gaussian field\nto generate snowflakes, raindrops and fog in the scene. The attributes and\ndynamics of these particles are precisely controlled through physical-based\nmodelling and simulation, ensuring realistic weather representation and\nflexible severity adjustments. Finally, we integrate the 4D Gaussian field with\nthe 3D scene to render consistent and highly realistic weather effects.\nExperiments on multiple driving datasets demonstrate that WeatherEdit can\ngenerate diverse weather effects with controllable condition severity,\nhighlighting its potential for autonomous driving simulation in adverse\nweather. See project page: https://jumponthemoon.github.io/w-edit"
                },
                "authors": [
                    {
                        "name": "Chenghao Qian"
                    },
                    {
                        "name": "Wenjing Li"
                    },
                    {
                        "name": "Yuhu Guo"
                    },
                    {
                        "name": "Gustav Markkula"
                    }
                ],
                "author_detail": {
                    "name": "Gustav Markkula"
                },
                "author": "Gustav Markkula",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20471v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20471v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17803v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17803v2",
                "updated": "2025-08-07T13:00:26Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    13,
                    0,
                    26,
                    3,
                    219,
                    0
                ],
                "published": "2025-05-23T12:20:48Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    12,
                    20,
                    48,
                    4,
                    143,
                    0
                ],
                "title": "Anytime-valid simultaneous lower confidence bounds for the true\n  discovery proportion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anytime-valid simultaneous lower confidence bounds for the true\n  discovery proportion"
                },
                "summary": "We propose a method that combines the closed testing framework with the\nconcept of safe anytime-valid inference (SAVI) to compute lower confidence\nbounds for the true discovery proportion in a multiple testing setting. The\nproposed procedure provides confidence bounds that are valid at every\nobservation time point and that are simultaneous for all possible subsets of\nhypotheses. While the hypotheses are assumed to be fixed over time, the subsets\nof interest may vary. Anytime-valid simultaneous confidence bounds allow us to\nsequentially update the bounds over time and allow for optional stopping. This\nis a desirable property in practical applications such as neuroscience, where\ndata acquisition is costly and time-consuming. We also present a computational\nshortcut which makes the application of the proposed procedure feasible when\nthe number of hypotheses under consideration is large. We illustrate the\nperformance of the proposed method in a simulation study and give some\npractical guidelines on the implementation of the proposed procedure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a method that combines the closed testing framework with the\nconcept of safe anytime-valid inference (SAVI) to compute lower confidence\nbounds for the true discovery proportion in a multiple testing setting. The\nproposed procedure provides confidence bounds that are valid at every\nobservation time point and that are simultaneous for all possible subsets of\nhypotheses. While the hypotheses are assumed to be fixed over time, the subsets\nof interest may vary. Anytime-valid simultaneous confidence bounds allow us to\nsequentially update the bounds over time and allow for optional stopping. This\nis a desirable property in practical applications such as neuroscience, where\ndata acquisition is costly and time-consuming. We also present a computational\nshortcut which makes the application of the proposed procedure feasible when\nthe number of hypotheses under consideration is large. We illustrate the\nperformance of the proposed method in a simulation study and give some\npractical guidelines on the implementation of the proposed procedure."
                },
                "authors": [
                    {
                        "name": "Friederike Preusse"
                    }
                ],
                "author_detail": {
                    "name": "Friederike Preusse"
                },
                "author": "Friederike Preusse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17803v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17803v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05344v1",
                "updated": "2025-08-07T12:49:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    12,
                    49,
                    44,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T12:49:44Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    12,
                    49,
                    44,
                    3,
                    219,
                    0
                ],
                "title": "NomicLaw: Emergent Trust and Strategic Argumentation in LLMs During\n  Collaborative Law-Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NomicLaw: Emergent Trust and Strategic Argumentation in LLMs During\n  Collaborative Law-Making"
                },
                "summary": "Recent advancements in large language models (LLMs) have extended their\ncapabilities from basic text processing to complex reasoning tasks, including\nlegal interpretation, argumentation, and strategic interaction. However,\nempirical understanding of LLM behavior in open-ended, multi-agent settings\nespecially those involving deliberation over legal and ethical dilemmas remains\nlimited. We introduce NomicLaw, a structured multi-agent simulation where LLMs\nengage in collaborative law-making, responding to complex legal vignettes by\nproposing rules, justifying them, and voting on peer proposals. We\nquantitatively measure trust and reciprocity via voting patterns and\nqualitatively assess how agents use strategic language to justify proposals and\ninfluence outcomes. Experiments involving homogeneous and heterogeneous LLM\ngroups demonstrate how agents spontaneously form alliances, betray trust, and\nadapt their rhetoric to shape collective decisions. Our results highlight the\nlatent social reasoning and persuasive capabilities of ten open-source LLMs and\nprovide insights into the design of future AI systems capable of autonomous\nnegotiation, coordination and drafting legislation in legal settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have extended their\ncapabilities from basic text processing to complex reasoning tasks, including\nlegal interpretation, argumentation, and strategic interaction. However,\nempirical understanding of LLM behavior in open-ended, multi-agent settings\nespecially those involving deliberation over legal and ethical dilemmas remains\nlimited. We introduce NomicLaw, a structured multi-agent simulation where LLMs\nengage in collaborative law-making, responding to complex legal vignettes by\nproposing rules, justifying them, and voting on peer proposals. We\nquantitatively measure trust and reciprocity via voting patterns and\nqualitatively assess how agents use strategic language to justify proposals and\ninfluence outcomes. Experiments involving homogeneous and heterogeneous LLM\ngroups demonstrate how agents spontaneously form alliances, betray trust, and\nadapt their rhetoric to shape collective decisions. Our results highlight the\nlatent social reasoning and persuasive capabilities of ten open-source LLMs and\nprovide insights into the design of future AI systems capable of autonomous\nnegotiation, coordination and drafting legislation in legal settings."
                },
                "authors": [
                    {
                        "name": "Asutosh Hota"
                    },
                    {
                        "name": "Jussi P. P. Jokinen"
                    }
                ],
                "author_detail": {
                    "name": "Jussi P. P. Jokinen"
                },
                "author": "Jussi P. P. Jokinen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12611v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12611v2",
                "updated": "2025-08-07T12:49:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    12,
                    49,
                    40,
                    3,
                    219,
                    0
                ],
                "published": "2025-03-16T18:48:18Z",
                "published_parsed": [
                    2025,
                    3,
                    16,
                    18,
                    48,
                    18,
                    6,
                    75,
                    0
                ],
                "title": "Functional Factor Regression with an Application to Electricity Price\n  Curve Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Functional Factor Regression with an Application to Electricity Price\n  Curve Modeling"
                },
                "summary": "We propose a function-on-function linear regression model for time-dependent\ncurve data that is consistently estimated by imposing factor structures on the\nregressors. An integral operator based on cross-covariances identifies two\ncomponents for each functional regressor: a predictive low-dimensional\ncomponent, along with associated factors that are guaranteed to be correlated\nwith the dependent variable, and an infinite-dimensional component that has no\npredictive power. In order to consistently estimate the correct number of\nfactors for each regressor, we introduce a functional eigenvalue difference\ntest. While conventional estimators for functional linear models fail to\nconverge in distribution, we establish asymptotic normality, making it possible\nto construct confidence bands and conduct statistical inference. The model is\napplied to forecast electricity price curves in three different energy markets.\nIts prediction accuracy is found to be comparable to popular machine learning\napproaches, while providing statistically valid inference and interpretable\ninsights into the conditional correlation structures of electricity prices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a function-on-function linear regression model for time-dependent\ncurve data that is consistently estimated by imposing factor structures on the\nregressors. An integral operator based on cross-covariances identifies two\ncomponents for each functional regressor: a predictive low-dimensional\ncomponent, along with associated factors that are guaranteed to be correlated\nwith the dependent variable, and an infinite-dimensional component that has no\npredictive power. In order to consistently estimate the correct number of\nfactors for each regressor, we introduce a functional eigenvalue difference\ntest. While conventional estimators for functional linear models fail to\nconverge in distribution, we establish asymptotic normality, making it possible\nto construct confidence bands and conduct statistical inference. The model is\napplied to forecast electricity price curves in three different energy markets.\nIts prediction accuracy is found to be comparable to popular machine learning\napproaches, while providing statistically valid inference and interpretable\ninsights into the conditional correlation structures of electricity prices."
                },
                "authors": [
                    {
                        "name": "Sven Otto"
                    },
                    {
                        "name": "Luis Winter"
                    }
                ],
                "author_detail": {
                    "name": "Luis Winter"
                },
                "author": "Luis Winter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12611v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12611v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05342v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05342v1",
                "updated": "2025-08-07T12:48:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    12,
                    48,
                    9,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T12:48:09Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    12,
                    48,
                    9,
                    3,
                    219,
                    0
                ],
                "title": "Information-Theoretic Graph Fusion with Vision-Language-Action Model for\n  Policy Reasoning and Dual Robotic Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information-Theoretic Graph Fusion with Vision-Language-Action Model for\n  Policy Reasoning and Dual Robotic Control"
                },
                "summary": "Teaching robots dexterous skills from human videos remains challenging due to\nthe reliance on low-level trajectory imitation, which fails to generalize\nacross object types, spatial layouts, and manipulator configurations. We\npropose Graph-Fused Vision-Language-Action (GF-VLA), a framework that enables\ndual-arm robotic systems to perform task-level reasoning and execution directly\nfrom RGB and Depth human demonstrations. GF-VLA first extracts\nShannon-information-based cues to identify hands and objects with the highest\ntask relevance, then encodes these cues into temporally ordered scene graphs\nthat capture both hand-object and object-object interactions. These graphs are\nfused with a language-conditioned transformer that generates hierarchical\nbehavior trees and interpretable Cartesian motion commands. To improve\nexecution efficiency in bimanual settings, we further introduce a cross-hand\nselection policy that infers optimal gripper assignment without explicit\ngeometric reasoning. We evaluate GF-VLA on four structured dual-arm block\nassembly tasks involving symbolic shape construction and spatial\ngeneralization. Experimental results show that the information-theoretic scene\nrepresentation achieves over 95 percent graph accuracy and 93 percent subtask\nsegmentation, supporting the LLM planner in generating reliable and\nhuman-readable task policies. When executed by the dual-arm robot, these\npolicies yield 94 percent grasp success, 89 percent placement accuracy, and 90\npercent overall task success across stacking, letter-building, and geometric\nreconfiguration scenarios, demonstrating strong generalization and robustness\nacross diverse spatial and semantic variations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaching robots dexterous skills from human videos remains challenging due to\nthe reliance on low-level trajectory imitation, which fails to generalize\nacross object types, spatial layouts, and manipulator configurations. We\npropose Graph-Fused Vision-Language-Action (GF-VLA), a framework that enables\ndual-arm robotic systems to perform task-level reasoning and execution directly\nfrom RGB and Depth human demonstrations. GF-VLA first extracts\nShannon-information-based cues to identify hands and objects with the highest\ntask relevance, then encodes these cues into temporally ordered scene graphs\nthat capture both hand-object and object-object interactions. These graphs are\nfused with a language-conditioned transformer that generates hierarchical\nbehavior trees and interpretable Cartesian motion commands. To improve\nexecution efficiency in bimanual settings, we further introduce a cross-hand\nselection policy that infers optimal gripper assignment without explicit\ngeometric reasoning. We evaluate GF-VLA on four structured dual-arm block\nassembly tasks involving symbolic shape construction and spatial\ngeneralization. Experimental results show that the information-theoretic scene\nrepresentation achieves over 95 percent graph accuracy and 93 percent subtask\nsegmentation, supporting the LLM planner in generating reliable and\nhuman-readable task policies. When executed by the dual-arm robot, these\npolicies yield 94 percent grasp success, 89 percent placement accuracy, and 90\npercent overall task success across stacking, letter-building, and geometric\nreconfiguration scenarios, demonstrating strong generalization and robustness\nacross diverse spatial and semantic variations."
                },
                "authors": [
                    {
                        "name": "Shunlei Li"
                    },
                    {
                        "name": "Longsen Gao"
                    },
                    {
                        "name": "Jin Wang"
                    },
                    {
                        "name": "Chang Che"
                    },
                    {
                        "name": "Xi Xiao"
                    },
                    {
                        "name": "Jiuwen Cao"
                    },
                    {
                        "name": "Yingbai Hu"
                    },
                    {
                        "name": "Hamid Reza Karimi"
                    }
                ],
                "author_detail": {
                    "name": "Hamid Reza Karimi"
                },
                "author": "Hamid Reza Karimi",
                "arxiv_comment": "Journal under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05342v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05342v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05337v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05337v1",
                "updated": "2025-08-07T12:38:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    12,
                    38,
                    22,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T12:38:22Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    12,
                    38,
                    22,
                    3,
                    219,
                    0
                ],
                "title": "Efficient Reasoning for Large Reasoning Language Models via\n  Certainty-Guided Reflection Suppression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Reasoning for Large Reasoning Language Models via\n  Certainty-Guided Reflection Suppression"
                },
                "summary": "Recent Large Reasoning Language Models (LRLMs) employ long chain-of-thought\nreasoning with complex reflection behaviors, typically signaled by specific\ntrigger words (e.g., \"Wait\" and \"Alternatively\") to enhance performance.\nHowever, these reflection behaviors can lead to the overthinking problem where\nthe generation of redundant reasoning steps that unnecessarily increase token\nusage, raise inference costs, and reduce practical utility. In this paper, we\npropose Certainty-Guided Reflection Suppression (CGRS), a novel method that\nmitigates overthinking in LRLMs while maintaining reasoning accuracy. CGRS\noperates by dynamically suppressing the model's generation of reflection\ntriggers when it exhibits high confidence in its current response, thereby\npreventing redundant reflection cycles without compromising output quality. Our\napproach is model-agnostic, requires no retraining or architectural\nmodifications, and can be integrated seamlessly with existing autoregressive\ngeneration pipelines. Extensive experiments across four reasoning benchmarks\n(i.e., AIME24, AMC23, MATH500, and GPQA-D) demonstrate CGRS's effectiveness: it\nreduces token usage by an average of 18.5% to 41.9% while preserving accuracy.\nIt also achieves the optimal balance between length reduction and performance\ncompared to state-of-the-art baselines. These results hold consistently across\nmodel architectures (e.g., DeepSeek-R1-Distill series, QwQ-32B, and Qwen3\nfamily) and scales (4B to 32B parameters), highlighting CGRS's practical value\nfor efficient reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Large Reasoning Language Models (LRLMs) employ long chain-of-thought\nreasoning with complex reflection behaviors, typically signaled by specific\ntrigger words (e.g., \"Wait\" and \"Alternatively\") to enhance performance.\nHowever, these reflection behaviors can lead to the overthinking problem where\nthe generation of redundant reasoning steps that unnecessarily increase token\nusage, raise inference costs, and reduce practical utility. In this paper, we\npropose Certainty-Guided Reflection Suppression (CGRS), a novel method that\nmitigates overthinking in LRLMs while maintaining reasoning accuracy. CGRS\noperates by dynamically suppressing the model's generation of reflection\ntriggers when it exhibits high confidence in its current response, thereby\npreventing redundant reflection cycles without compromising output quality. Our\napproach is model-agnostic, requires no retraining or architectural\nmodifications, and can be integrated seamlessly with existing autoregressive\ngeneration pipelines. Extensive experiments across four reasoning benchmarks\n(i.e., AIME24, AMC23, MATH500, and GPQA-D) demonstrate CGRS's effectiveness: it\nreduces token usage by an average of 18.5% to 41.9% while preserving accuracy.\nIt also achieves the optimal balance between length reduction and performance\ncompared to state-of-the-art baselines. These results hold consistently across\nmodel architectures (e.g., DeepSeek-R1-Distill series, QwQ-32B, and Qwen3\nfamily) and scales (4B to 32B parameters), highlighting CGRS's practical value\nfor efficient reasoning."
                },
                "authors": [
                    {
                        "name": "Jiameng Huang"
                    },
                    {
                        "name": "Baijiong Lin"
                    },
                    {
                        "name": "Guhao Feng"
                    },
                    {
                        "name": "Jierun Chen"
                    },
                    {
                        "name": "Di He"
                    },
                    {
                        "name": "Lu Hou"
                    }
                ],
                "author_detail": {
                    "name": "Lu Hou"
                },
                "author": "Lu Hou",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05337v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05337v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02253v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02253v3",
                "updated": "2025-08-07T12:29:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    12,
                    29,
                    47,
                    3,
                    219,
                    0
                ],
                "published": "2025-07-03T03:02:49Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    3,
                    2,
                    49,
                    3,
                    184,
                    0
                ],
                "title": "Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and\n  Rigorous Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and\n  Rigorous Evaluation"
                },
                "summary": "Effective agent performance relies on the ability to compose tools and agents\ninto effective workflows. However, progress in Large Language Model (LLM)\nplanning and reasoning is limited by the scarcity of scalable, reliable\nevaluation data. This study addresses this limitation by identifying a suitable\nworkflow domain for LLM application. I introduce NL2Flow, a fully automated\nsystem for parametrically generating planning problems, which are expressed in\nnatural language, a structured intermediate representation, and formal PDDL,\nand rigorously evaluating the quality of generated plans. NL2Flow generates a\ndataset of 2296 low-difficulty problems in automated workflow generation and\nevaluates multiple open-sourced, instruct-tuned LLMs without task-specific\noptimization or architectural modifications. Results reveal that the highest\nperforming model achieved 86% success in generating valid plans and 69% in\ngenerating optimal plans, specifically for problems with feasible plans.\nRegression analysis shows that the influence of problem characteristics on plan\ngeneration is contingent on both model and prompt design. To investigate the\npotential of LLMs as natural language-to-JSON translators for workflow\ndefinition, and to facilitate integration with downstream symbolic computation\ntools and a symbolic planner, I evaluated the LLM's translation performance on\nnatural language workflow descriptions. I observed that translating natural\nlanguage into a JSON representation of a workflow problem yielded a lower\nsuccess rate than generating a plan directly, suggesting that unnecessary\ndecomposition of the reasoning task may degrade performance and highlighting\nthe benefit of models capable of reasoning directly from natural language to\naction. As LLM reasoning scales to increasingly complex problems, understanding\nthe shifting bottlenecks and sources of error within these systems will be\ncrucial.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective agent performance relies on the ability to compose tools and agents\ninto effective workflows. However, progress in Large Language Model (LLM)\nplanning and reasoning is limited by the scarcity of scalable, reliable\nevaluation data. This study addresses this limitation by identifying a suitable\nworkflow domain for LLM application. I introduce NL2Flow, a fully automated\nsystem for parametrically generating planning problems, which are expressed in\nnatural language, a structured intermediate representation, and formal PDDL,\nand rigorously evaluating the quality of generated plans. NL2Flow generates a\ndataset of 2296 low-difficulty problems in automated workflow generation and\nevaluates multiple open-sourced, instruct-tuned LLMs without task-specific\noptimization or architectural modifications. Results reveal that the highest\nperforming model achieved 86% success in generating valid plans and 69% in\ngenerating optimal plans, specifically for problems with feasible plans.\nRegression analysis shows that the influence of problem characteristics on plan\ngeneration is contingent on both model and prompt design. To investigate the\npotential of LLMs as natural language-to-JSON translators for workflow\ndefinition, and to facilitate integration with downstream symbolic computation\ntools and a symbolic planner, I evaluated the LLM's translation performance on\nnatural language workflow descriptions. I observed that translating natural\nlanguage into a JSON representation of a workflow problem yielded a lower\nsuccess rate than generating a plan directly, suggesting that unnecessary\ndecomposition of the reasoning task may degrade performance and highlighting\nthe benefit of models capable of reasoning directly from natural language to\naction. As LLM reasoning scales to increasingly complex problems, understanding\nthe shifting bottlenecks and sources of error within these systems will be\ncrucial."
                },
                "authors": [
                    {
                        "name": "Jungkoo Kang"
                    }
                ],
                "author_detail": {
                    "name": "Jungkoo Kang"
                },
                "author": "Jungkoo Kang",
                "arxiv_comment": "26 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02253v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02253v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06608v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06608v5",
                "updated": "2025-08-07T12:26:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    12,
                    26,
                    15,
                    3,
                    219,
                    0
                ],
                "published": "2025-07-09T07:27:18Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    7,
                    27,
                    18,
                    2,
                    190,
                    0
                ],
                "title": "Nexus:Proactive Intra-GPU Disaggregation of Prefill and Decode in LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nexus:Proactive Intra-GPU Disaggregation of Prefill and Decode in LLM\n  Serving"
                },
                "summary": "Monolithic serving with chunked prefill improves GPU utilization by batching\nprefill and decode together, but suffers from fine-grained phase interference.\nEngine-level prefill-decode (PD) disaggregation avoids interference but incurs\nhigher hardware and coordination overhead. Prior intra-GPU disaggregation\napproaches multiplex prefill and decode within a single GPU, using SLO-based\ntuning guided by heuristics from offline profiling or reactive feedback loops.\nHowever, these methods respond reactively to performance issues rather than\nanticipating them, limiting adaptability under dynamic workloads.\n  We ask: can we achieve proactive intra-GPU disaggregation that adapts\neffectively to dynamic workloads? The key challenge lies in managing the\nconflicting resource demands of prefill and decode under varying conditions. We\nfirst show that GPU resources exhibit diminishing returns -- beyond a\nsaturation point, more allocation yields minimal latency benefit. Second, we\nobserve that memory bandwidth contention becomes a critical bottleneck. These\ninsights motivate a design that dynamically partitions GPU resources across\nprefill and decode phases, while jointly considering compute capacity, memory\nfootprint, and bandwidth contention.\n  Evaluated on diverse LLMs and workloads, our system Nexus achieves up to 2.2x\nhigher throughput, 20x lower TTFT, and 2.5x lower TBT than vLLM; outperforms\nSGLang by up to 2x; and matches or exceeds disaggregated vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monolithic serving with chunked prefill improves GPU utilization by batching\nprefill and decode together, but suffers from fine-grained phase interference.\nEngine-level prefill-decode (PD) disaggregation avoids interference but incurs\nhigher hardware and coordination overhead. Prior intra-GPU disaggregation\napproaches multiplex prefill and decode within a single GPU, using SLO-based\ntuning guided by heuristics from offline profiling or reactive feedback loops.\nHowever, these methods respond reactively to performance issues rather than\nanticipating them, limiting adaptability under dynamic workloads.\n  We ask: can we achieve proactive intra-GPU disaggregation that adapts\neffectively to dynamic workloads? The key challenge lies in managing the\nconflicting resource demands of prefill and decode under varying conditions. We\nfirst show that GPU resources exhibit diminishing returns -- beyond a\nsaturation point, more allocation yields minimal latency benefit. Second, we\nobserve that memory bandwidth contention becomes a critical bottleneck. These\ninsights motivate a design that dynamically partitions GPU resources across\nprefill and decode phases, while jointly considering compute capacity, memory\nfootprint, and bandwidth contention.\n  Evaluated on diverse LLMs and workloads, our system Nexus achieves up to 2.2x\nhigher throughput, 20x lower TTFT, and 2.5x lower TBT than vLLM; outperforms\nSGLang by up to 2x; and matches or exceeds disaggregated vLLM."
                },
                "authors": [
                    {
                        "name": "Xiaoxiang Shi"
                    },
                    {
                        "name": "Colin Cai"
                    },
                    {
                        "name": "Junjia Du"
                    },
                    {
                        "name": "Zhihao Jia"
                    }
                ],
                "author_detail": {
                    "name": "Zhihao Jia"
                },
                "author": "Zhihao Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06608v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06608v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05311v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05311v1",
                "updated": "2025-08-07T12:11:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    12,
                    11,
                    53,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T12:11:53Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    12,
                    11,
                    53,
                    3,
                    219,
                    0
                ],
                "title": "A Novel Architecture for Symbolic Reasoning with Decision Trees and LLM\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Architecture for Symbolic Reasoning with Decision Trees and LLM\n  Agents"
                },
                "summary": "We propose a hybrid architecture that integrates decision tree-based symbolic\nreasoning with the generative capabilities of large language models (LLMs)\nwithin a coordinated multi-agent framework. Unlike prior approaches that\nloosely couple symbolic and neural modules, our design embeds decision trees\nand random forests as callable oracles within a unified reasoning system.\nTree-based modules enable interpretable rule inference and causal logic, while\nLLM agents handle abductive reasoning, generalization, and interactive\nplanning. A central orchestrator maintains belief state consistency and\nmediates communication across agents and external tools, enabling reasoning\nover both structured and unstructured inputs.\n  The system achieves strong performance on reasoning benchmarks. On\n\\textit{ProofWriter}, it improves entailment consistency by +7.2\\% through\nlogic-grounded tree validation. On GSM8k, it achieves +5.3\\% accuracy gains in\nmultistep mathematical problems via symbolic augmentation. On \\textit{ARC}, it\nboosts abstraction accuracy by +6.0\\% through integration of symbolic oracles.\nApplications in clinical decision support and scientific discovery show how the\nsystem encodes domain rules symbolically while leveraging LLMs for contextual\ninference and hypothesis generation. This architecture offers a robust,\ninterpretable, and extensible solution for general-purpose neuro-symbolic\nreasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a hybrid architecture that integrates decision tree-based symbolic\nreasoning with the generative capabilities of large language models (LLMs)\nwithin a coordinated multi-agent framework. Unlike prior approaches that\nloosely couple symbolic and neural modules, our design embeds decision trees\nand random forests as callable oracles within a unified reasoning system.\nTree-based modules enable interpretable rule inference and causal logic, while\nLLM agents handle abductive reasoning, generalization, and interactive\nplanning. A central orchestrator maintains belief state consistency and\nmediates communication across agents and external tools, enabling reasoning\nover both structured and unstructured inputs.\n  The system achieves strong performance on reasoning benchmarks. On\n\\textit{ProofWriter}, it improves entailment consistency by +7.2\\% through\nlogic-grounded tree validation. On GSM8k, it achieves +5.3\\% accuracy gains in\nmultistep mathematical problems via symbolic augmentation. On \\textit{ARC}, it\nboosts abstraction accuracy by +6.0\\% through integration of symbolic oracles.\nApplications in clinical decision support and scientific discovery show how the\nsystem encodes domain rules symbolically while leveraging LLMs for contextual\ninference and hypothesis generation. This architecture offers a robust,\ninterpretable, and extensible solution for general-purpose neuro-symbolic\nreasoning."
                },
                "authors": [
                    {
                        "name": "Andrew Kiruluta"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Kiruluta"
                },
                "author": "Andrew Kiruluta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05311v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05311v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05308v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05308v1",
                "updated": "2025-08-07T12:07:23Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    12,
                    7,
                    23,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T12:07:23Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    12,
                    7,
                    23,
                    3,
                    219,
                    0
                ],
                "title": "Identifying Optimal Regression Models For DEM Simulation Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying Optimal Regression Models For DEM Simulation Datasets"
                },
                "summary": "Developing fast regression models (surrogate/metamodels) from DEM data is key\nfor practical industrial application to allow real-time evaluations. However,\nbenchmarking different models is often overlooked in particle technology for\nregression tasks, as model selection is frequently not the primary research\nfocus. This can lead to the use of suboptimal models, resulting in subpar\npredictive accuracy, slow evaluations, or poor generalisation, hindering\neffective real-time decision-making and process optimisation. In this work, we\ndiscuss applying k-fold cross-validation to assess regression models for\ntabular DEM datasets and propose a simple framework for readers to follow to\nfind the optimal model for their data. An example demonstrates its application\nto a DEM dataset of packing fractions measured in a simple measuring beaker\nwith varying inter-particle properties, namely, average particle diameter,\ncoefficient of restitution, coefficient of sliding friction, coefficient of\nrolling resistance, and cohesive energy density. Out of 16 different models\ntested, a histogram-based gradient boosting model was found to be optimal,\nproviding a good fit with acceptable training and inference times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing fast regression models (surrogate/metamodels) from DEM data is key\nfor practical industrial application to allow real-time evaluations. However,\nbenchmarking different models is often overlooked in particle technology for\nregression tasks, as model selection is frequently not the primary research\nfocus. This can lead to the use of suboptimal models, resulting in subpar\npredictive accuracy, slow evaluations, or poor generalisation, hindering\neffective real-time decision-making and process optimisation. In this work, we\ndiscuss applying k-fold cross-validation to assess regression models for\ntabular DEM datasets and propose a simple framework for readers to follow to\nfind the optimal model for their data. An example demonstrates its application\nto a DEM dataset of packing fractions measured in a simple measuring beaker\nwith varying inter-particle properties, namely, average particle diameter,\ncoefficient of restitution, coefficient of sliding friction, coefficient of\nrolling resistance, and cohesive energy density. Out of 16 different models\ntested, a histogram-based gradient boosting model was found to be optimal,\nproviding a good fit with acceptable training and inference times."
                },
                "authors": [
                    {
                        "name": "B. D. Jenkins"
                    },
                    {
                        "name": "A. L. Nicusan"
                    },
                    {
                        "name": "A. Neveu"
                    },
                    {
                        "name": "G. Lumay"
                    },
                    {
                        "name": "F. Francqui"
                    },
                    {
                        "name": "J. P. K. Seville"
                    },
                    {
                        "name": "C. R. K. Windows-Yule"
                    }
                ],
                "author_detail": {
                    "name": "C. R. K. Windows-Yule"
                },
                "author": "C. R. K. Windows-Yule",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05308v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05308v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.comp-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03930v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03930v2",
                "updated": "2025-08-07T12:06:29Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    12,
                    6,
                    29,
                    3,
                    219,
                    0
                ],
                "published": "2024-12-05T07:12:53Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    7,
                    12,
                    53,
                    3,
                    340,
                    0
                ],
                "title": "GuARD: Effective Anomaly Detection through a Text-Rich and\n  Graph-Informed Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GuARD: Effective Anomaly Detection through a Text-Rich and\n  Graph-Informed Language Model"
                },
                "summary": "Anomaly detection on text-rich graphs is widely prevalent in real life, such\nas detecting incorrectly assigned academic papers to authors and detecting bots\nin social networks. The remarkable capabilities of large language models (LLMs)\npave a new revenue by utilizing rich-text information for effective anomaly\ndetection. However, simply introducing rich texts into LLMs can obscure\nessential detection cues and introduce high fine-tuning costs. Moreover, LLMs\noften overlook the intrinsic structural bias of graphs which is vital for\ndistinguishing normal from abnormal node patterns. To this end, this paper\nintroduces GuARD, a text-rich and graph-informed language model that combines\nkey structural features from graph-based methods with fine-grained semantic\nattributes extracted via small language models for effective anomaly detection\non text-rich graphs. GuARD is optimized with the progressive multi-modal\nmulti-turn instruction tuning framework in the task-guided instruction tuning\nregime tailed to incorporate both rich-text and structural modalities.\nExtensive experiments on four datasets reveal that GuARD outperforms\ngraph-based and LLM-based anomaly detection methods, while offering up to\n5$\\times$ times speedup in training and 5$\\times$ times speedup in inference\nover vanilla long-context LLMs on the large-scale WhoIsWho dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anomaly detection on text-rich graphs is widely prevalent in real life, such\nas detecting incorrectly assigned academic papers to authors and detecting bots\nin social networks. The remarkable capabilities of large language models (LLMs)\npave a new revenue by utilizing rich-text information for effective anomaly\ndetection. However, simply introducing rich texts into LLMs can obscure\nessential detection cues and introduce high fine-tuning costs. Moreover, LLMs\noften overlook the intrinsic structural bias of graphs which is vital for\ndistinguishing normal from abnormal node patterns. To this end, this paper\nintroduces GuARD, a text-rich and graph-informed language model that combines\nkey structural features from graph-based methods with fine-grained semantic\nattributes extracted via small language models for effective anomaly detection\non text-rich graphs. GuARD is optimized with the progressive multi-modal\nmulti-turn instruction tuning framework in the task-guided instruction tuning\nregime tailed to incorporate both rich-text and structural modalities.\nExtensive experiments on four datasets reveal that GuARD outperforms\ngraph-based and LLM-based anomaly detection methods, while offering up to\n5$\\times$ times speedup in training and 5$\\times$ times speedup in inference\nover vanilla long-context LLMs on the large-scale WhoIsWho dataset."
                },
                "authors": [
                    {
                        "name": "Yunhe Pang"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Fanjin Zhang"
                    },
                    {
                        "name": "Yanghui Rao"
                    },
                    {
                        "name": "Evgeny Kharlamov"
                    },
                    {
                        "name": "Jie Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Tang"
                },
                "author": "Jie Tang",
                "arxiv_doi": "10.1145/3711896.3736993",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3711896.3736993",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.03930v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03930v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at KDD 2025",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05305v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05305v1",
                "updated": "2025-08-07T12:03:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    12,
                    3,
                    44,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T12:03:44Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    12,
                    3,
                    44,
                    3,
                    219,
                    0
                ],
                "title": "SONAR-LLM: Autoregressive Transformer that Thinks in Sentence Embeddings\n  and Speaks in Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SONAR-LLM: Autoregressive Transformer that Thinks in Sentence Embeddings\n  and Speaks in Tokens"
                },
                "summary": "The recently proposed Large Concept Model (LCM) generates text by predicting\na sequence of sentence-level embeddings and training with either mean-squared\nerror or diffusion objectives. We present SONAR-LLM, a decoder-only transformer\nthat \"thinks\" in the same continuous SONAR embedding space, yet is supervised\nthrough token-level cross-entropy propagated via the frozen SONAR decoder. This\nhybrid objective retains the semantic abstraction of LCM while eliminating its\ndiffusion sampler and restoring a likelihood-based training signal. Across\nmodel sizes from 39M to 1.3B parameters, SONAR-LLM attains competitive\ngeneration quality. We report scaling trends, ablations, benchmark results, and\nrelease the complete training code and all pretrained checkpoints to foster\nreproducibility and future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recently proposed Large Concept Model (LCM) generates text by predicting\na sequence of sentence-level embeddings and training with either mean-squared\nerror or diffusion objectives. We present SONAR-LLM, a decoder-only transformer\nthat \"thinks\" in the same continuous SONAR embedding space, yet is supervised\nthrough token-level cross-entropy propagated via the frozen SONAR decoder. This\nhybrid objective retains the semantic abstraction of LCM while eliminating its\ndiffusion sampler and restoring a likelihood-based training signal. Across\nmodel sizes from 39M to 1.3B parameters, SONAR-LLM attains competitive\ngeneration quality. We report scaling trends, ablations, benchmark results, and\nrelease the complete training code and all pretrained checkpoints to foster\nreproducibility and future research."
                },
                "authors": [
                    {
                        "name": "Nikita Dragunov"
                    },
                    {
                        "name": "Temurbek Rahmatullaev"
                    },
                    {
                        "name": "Elizaveta Goncharova"
                    },
                    {
                        "name": "Andrey Kuznetsov"
                    },
                    {
                        "name": "Anton Razzhigaev"
                    }
                ],
                "author_detail": {
                    "name": "Anton Razzhigaev"
                },
                "author": "Anton Razzhigaev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05305v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05305v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05299v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05299v1",
                "updated": "2025-08-07T11:59:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    11,
                    59,
                    50,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T11:59:50Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    11,
                    59,
                    50,
                    3,
                    219,
                    0
                ],
                "title": "VS-LLM: Visual-Semantic Depression Assessment based on LLM for Drawing\n  Projection Test",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VS-LLM: Visual-Semantic Depression Assessment based on LLM for Drawing\n  Projection Test"
                },
                "summary": "The Drawing Projection Test (DPT) is an essential tool in art therapy,\nallowing psychologists to assess participants' mental states through their\nsketches. Specifically, through sketches with the theme of \"a person picking an\napple from a tree (PPAT)\", it can be revealed whether the participants are in\nmental states such as depression. Compared with scales, the DPT can enrich\npsychologists' understanding of an individual's mental state. However, the\ninterpretation of the PPAT is laborious and depends on the experience of the\npsychologists. To address this issue, we propose an effective identification\nmethod to support psychologists in conducting a large-scale automatic DPT.\nUnlike traditional sketch recognition, DPT more focus on the overall evaluation\nof the sketches, such as color usage and space utilization. Moreover, PPAT\nimposes a time limit and prohibits verbal reminders, resulting in low drawing\naccuracy and a lack of detailed depiction. To address these challenges, we\npropose the following efforts: (1) Providing an experimental environment for\nautomated analysis of PPAT sketches for depression assessment; (2) Offering a\nVisual-Semantic depression assessment based on LLM (VS-LLM) method; (3)\nExperimental results demonstrate that our method improves by 17.6% compared to\nthe psychologist assessment method. We anticipate that this work will\ncontribute to the research in mental state assessment based on PPAT sketches'\nelements recognition. Our datasets and codes are available at\nhttps://github.com/wmeiqi/VS-LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Drawing Projection Test (DPT) is an essential tool in art therapy,\nallowing psychologists to assess participants' mental states through their\nsketches. Specifically, through sketches with the theme of \"a person picking an\napple from a tree (PPAT)\", it can be revealed whether the participants are in\nmental states such as depression. Compared with scales, the DPT can enrich\npsychologists' understanding of an individual's mental state. However, the\ninterpretation of the PPAT is laborious and depends on the experience of the\npsychologists. To address this issue, we propose an effective identification\nmethod to support psychologists in conducting a large-scale automatic DPT.\nUnlike traditional sketch recognition, DPT more focus on the overall evaluation\nof the sketches, such as color usage and space utilization. Moreover, PPAT\nimposes a time limit and prohibits verbal reminders, resulting in low drawing\naccuracy and a lack of detailed depiction. To address these challenges, we\npropose the following efforts: (1) Providing an experimental environment for\nautomated analysis of PPAT sketches for depression assessment; (2) Offering a\nVisual-Semantic depression assessment based on LLM (VS-LLM) method; (3)\nExperimental results demonstrate that our method improves by 17.6% compared to\nthe psychologist assessment method. We anticipate that this work will\ncontribute to the research in mental state assessment based on PPAT sketches'\nelements recognition. Our datasets and codes are available at\nhttps://github.com/wmeiqi/VS-LLM."
                },
                "authors": [
                    {
                        "name": "Meiqi Wu"
                    },
                    {
                        "name": "Yaxuan Kang"
                    },
                    {
                        "name": "Xuchen Li"
                    },
                    {
                        "name": "Shiyu Hu"
                    },
                    {
                        "name": "Xiaotang Chen"
                    },
                    {
                        "name": "Yunfeng Kang"
                    },
                    {
                        "name": "Weiqiang Wang"
                    },
                    {
                        "name": "Kaiqi Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kaiqi Huang"
                },
                "author": "Kaiqi Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05299v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05299v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2508.05629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05629v1",
                "updated": "2025-08-07T17:59:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    59,
                    4,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T17:59:04Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    59,
                    4,
                    3,
                    219,
                    0
                ],
                "title": "On the Generalization of SFT: A Reinforcement Learning Perspective with\n  Reward Rectification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Generalization of SFT: A Reinforcement Learning Perspective with\n  Reward Rectification"
                },
                "summary": "We present a simple yet theoretically motivated improvement to Supervised\nFine-Tuning (SFT) for the Large Language Model (LLM), addressing its limited\ngeneralization compared to reinforcement learning (RL). Through mathematical\nanalysis, we reveal that standard SFT gradients implicitly encode a problematic\nreward structure that may severely restrict the generalization capabilities of\nmodel. To rectify this, we propose Dynamic Fine-Tuning (DFT), stabilizing\ngradient updates for each token by dynamically rescaling the objective function\nwith the probability of this token. Remarkably, this single-line code change\nsignificantly outperforms standard SFT across multiple challenging benchmarks\nand base models, demonstrating greatly improved generalization. Additionally,\nour approach shows competitive results in offline RL settings, offering an\neffective yet simpler alternative. This work bridges theoretical insight and\npractical solutions, substantially advancing SFT performance. The code will be\navailable at https://github.com/yongliang-wu/DFT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a simple yet theoretically motivated improvement to Supervised\nFine-Tuning (SFT) for the Large Language Model (LLM), addressing its limited\ngeneralization compared to reinforcement learning (RL). Through mathematical\nanalysis, we reveal that standard SFT gradients implicitly encode a problematic\nreward structure that may severely restrict the generalization capabilities of\nmodel. To rectify this, we propose Dynamic Fine-Tuning (DFT), stabilizing\ngradient updates for each token by dynamically rescaling the objective function\nwith the probability of this token. Remarkably, this single-line code change\nsignificantly outperforms standard SFT across multiple challenging benchmarks\nand base models, demonstrating greatly improved generalization. Additionally,\nour approach shows competitive results in offline RL settings, offering an\neffective yet simpler alternative. This work bridges theoretical insight and\npractical solutions, substantially advancing SFT performance. The code will be\navailable at https://github.com/yongliang-wu/DFT."
                },
                "authors": [
                    {
                        "name": "Yongliang Wu"
                    },
                    {
                        "name": "Yizhou Zhou"
                    },
                    {
                        "name": "Zhou Ziheng"
                    },
                    {
                        "name": "Yingzhe Peng"
                    },
                    {
                        "name": "Xinyu Ye"
                    },
                    {
                        "name": "Xinting Hu"
                    },
                    {
                        "name": "Wenbo Zhu"
                    },
                    {
                        "name": "Lu Qi"
                    },
                    {
                        "name": "Ming-Hsuan Yang"
                    },
                    {
                        "name": "Xu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xu Yang"
                },
                "author": "Xu Yang",
                "arxiv_comment": "14 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05625v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05625v1",
                "updated": "2025-08-07T17:58:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    58,
                    41,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T17:58:41Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    58,
                    41,
                    3,
                    219,
                    0
                ],
                "title": "How Do LLMs Persuade? Linear Probes Can Uncover Persuasion Dynamics in\n  Multi-Turn Conversations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Do LLMs Persuade? Linear Probes Can Uncover Persuasion Dynamics in\n  Multi-Turn Conversations"
                },
                "summary": "Large Language Models (LLMs) have started to demonstrate the ability to\npersuade humans, yet our understanding of how this dynamic transpires is\nlimited. Recent work has used linear probes, lightweight tools for analyzing\nmodel representations, to study various LLM skills such as the ability to model\nuser sentiment and political perspective. Motivated by this, we apply probes to\nstudy persuasion dynamics in natural, multi-turn conversations. We leverage\ninsights from cognitive science to train probes on distinct aspects of\npersuasion: persuasion success, persuadee personality, and persuasion strategy.\nDespite their simplicity, we show that they capture various aspects of\npersuasion at both the sample and dataset levels. For instance, probes can\nidentify the point in a conversation where the persuadee was persuaded or where\npersuasive success generally occurs across the entire dataset. We also show\nthat in addition to being faster than expensive prompting-based approaches,\nprobes can do just as well and even outperform prompting in some settings, such\nas when uncovering persuasion strategy. This suggests probes as a plausible\navenue for studying other complex behaviours such as deception and\nmanipulation, especially in multi-turn settings and large-scale dataset\nanalysis where prompting-based methods would be computationally inefficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have started to demonstrate the ability to\npersuade humans, yet our understanding of how this dynamic transpires is\nlimited. Recent work has used linear probes, lightweight tools for analyzing\nmodel representations, to study various LLM skills such as the ability to model\nuser sentiment and political perspective. Motivated by this, we apply probes to\nstudy persuasion dynamics in natural, multi-turn conversations. We leverage\ninsights from cognitive science to train probes on distinct aspects of\npersuasion: persuasion success, persuadee personality, and persuasion strategy.\nDespite their simplicity, we show that they capture various aspects of\npersuasion at both the sample and dataset levels. For instance, probes can\nidentify the point in a conversation where the persuadee was persuaded or where\npersuasive success generally occurs across the entire dataset. We also show\nthat in addition to being faster than expensive prompting-based approaches,\nprobes can do just as well and even outperform prompting in some settings, such\nas when uncovering persuasion strategy. This suggests probes as a plausible\navenue for studying other complex behaviours such as deception and\nmanipulation, especially in multi-turn settings and large-scale dataset\nanalysis where prompting-based methods would be computationally inefficient."
                },
                "authors": [
                    {
                        "name": "Brandon Jaipersaud"
                    },
                    {
                        "name": "David Krueger"
                    },
                    {
                        "name": "Ekdeep Singh Lubana"
                    }
                ],
                "author_detail": {
                    "name": "Ekdeep Singh Lubana"
                },
                "author": "Ekdeep Singh Lubana",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05625v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05625v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05622v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05622v1",
                "updated": "2025-08-07T17:57:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    57,
                    46,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T17:57:46Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    57,
                    46,
                    3,
                    219,
                    0
                ],
                "title": "Simulating Human-Like Learning Dynamics with LLM-Empowered Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating Human-Like Learning Dynamics with LLM-Empowered Agents"
                },
                "summary": "Capturing human learning behavior based on deep learning methods has become a\nmajor research focus in both psychology and intelligent systems. Recent\napproaches rely on controlled experiments or rule-based models to explore\ncognitive processes. However, they struggle to capture learning dynamics, track\nprogress over time, or provide explainability. To address these challenges, we\nintroduce LearnerAgent, a novel multi-agent framework based on Large Language\nModels (LLMs) to simulate a realistic teaching environment. To explore\nhuman-like learning dynamics, we construct learners with psychologically\ngrounded profiles-such as Deep, Surface, and Lazy-as well as a persona-free\nGeneral Learner to inspect the base LLM's default behavior. Through weekly\nknowledge acquisition, monthly strategic choices, periodic tests, and peer\ninteraction, we can track the dynamic learning progress of individual learners\nover a full-year journey. Our findings are fourfold: 1) Longitudinal analysis\nreveals that only Deep Learner achieves sustained cognitive growth. Our\nspecially designed \"trap questions\" effectively diagnose Surface Learner's\nshallow knowledge. 2) The behavioral and cognitive patterns of distinct\nlearners align closely with their psychological profiles. 3) Learners'\nself-concept scores evolve realistically, with the General Learner developing\nsurprisingly high self-efficacy despite its cognitive limitations. 4)\nCritically, the default profile of base LLM is a \"diligent but brittle Surface\nLearner\"-an agent that mimics the behaviors of a good student but lacks true,\ngeneralizable understanding. Extensive simulation experiments demonstrate that\nLearnerAgent aligns well with real scenarios, yielding more insightful findings\nabout LLMs' behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Capturing human learning behavior based on deep learning methods has become a\nmajor research focus in both psychology and intelligent systems. Recent\napproaches rely on controlled experiments or rule-based models to explore\ncognitive processes. However, they struggle to capture learning dynamics, track\nprogress over time, or provide explainability. To address these challenges, we\nintroduce LearnerAgent, a novel multi-agent framework based on Large Language\nModels (LLMs) to simulate a realistic teaching environment. To explore\nhuman-like learning dynamics, we construct learners with psychologically\ngrounded profiles-such as Deep, Surface, and Lazy-as well as a persona-free\nGeneral Learner to inspect the base LLM's default behavior. Through weekly\nknowledge acquisition, monthly strategic choices, periodic tests, and peer\ninteraction, we can track the dynamic learning progress of individual learners\nover a full-year journey. Our findings are fourfold: 1) Longitudinal analysis\nreveals that only Deep Learner achieves sustained cognitive growth. Our\nspecially designed \"trap questions\" effectively diagnose Surface Learner's\nshallow knowledge. 2) The behavioral and cognitive patterns of distinct\nlearners align closely with their psychological profiles. 3) Learners'\nself-concept scores evolve realistically, with the General Learner developing\nsurprisingly high self-efficacy despite its cognitive limitations. 4)\nCritically, the default profile of base LLM is a \"diligent but brittle Surface\nLearner\"-an agent that mimics the behaviors of a good student but lacks true,\ngeneralizable understanding. Extensive simulation experiments demonstrate that\nLearnerAgent aligns well with real scenarios, yielding more insightful findings\nabout LLMs' behavior."
                },
                "authors": [
                    {
                        "name": "Yu Yuan"
                    },
                    {
                        "name": "Lili Zhao"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Guangting Zheng"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Mengdi Zhang"
                    },
                    {
                        "name": "Qi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Qi Liu"
                },
                "author": "Qi Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05622v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05622v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05616v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05616v1",
                "updated": "2025-08-07T17:55:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    55,
                    10,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T17:55:10Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    55,
                    10,
                    3,
                    219,
                    0
                ],
                "title": "TrajEvo: Trajectory Prediction Heuristics Design via LLM-driven\n  Evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrajEvo: Trajectory Prediction Heuristics Design via LLM-driven\n  Evolution"
                },
                "summary": "Trajectory prediction is a critical task in modeling human behavior,\nespecially in safety-critical domains such as social robotics and autonomous\nvehicle navigation. Traditional heuristics based on handcrafted rules often\nlack accuracy and generalizability. Although deep learning approaches offer\nimproved performance, they typically suffer from high computational cost,\nlimited explainability, and, importantly, poor generalization to\nout-of-distribution (OOD) scenarios. In this paper, we introduce TrajEvo, a\nframework that leverages Large Language Models (LLMs) to automatically design\ntrajectory prediction heuristics. TrajEvo employs an evolutionary algorithm to\ngenerate and refine prediction heuristics from past trajectory data. We propose\ntwo key innovations: Cross-Generation Elite Sampling to encourage population\ndiversity, and a Statistics Feedback Loop that enables the LLM to analyze and\nimprove alternative predictions. Our evaluations demonstrate that TrajEvo\noutperforms existing heuristic methods across multiple real-world datasets, and\nnotably surpasses both heuristic and deep learning methods in generalizing to\nan unseen OOD real-world dataset. TrajEvo marks a promising step toward the\nautomated design of fast, explainable, and generalizable trajectory prediction\nheuristics. We release our source code to facilitate future research at\nhttps://github.com/ai4co/trajevo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trajectory prediction is a critical task in modeling human behavior,\nespecially in safety-critical domains such as social robotics and autonomous\nvehicle navigation. Traditional heuristics based on handcrafted rules often\nlack accuracy and generalizability. Although deep learning approaches offer\nimproved performance, they typically suffer from high computational cost,\nlimited explainability, and, importantly, poor generalization to\nout-of-distribution (OOD) scenarios. In this paper, we introduce TrajEvo, a\nframework that leverages Large Language Models (LLMs) to automatically design\ntrajectory prediction heuristics. TrajEvo employs an evolutionary algorithm to\ngenerate and refine prediction heuristics from past trajectory data. We propose\ntwo key innovations: Cross-Generation Elite Sampling to encourage population\ndiversity, and a Statistics Feedback Loop that enables the LLM to analyze and\nimprove alternative predictions. Our evaluations demonstrate that TrajEvo\noutperforms existing heuristic methods across multiple real-world datasets, and\nnotably surpasses both heuristic and deep learning methods in generalizing to\nan unseen OOD real-world dataset. TrajEvo marks a promising step toward the\nautomated design of fast, explainable, and generalizable trajectory prediction\nheuristics. We release our source code to facilitate future research at\nhttps://github.com/ai4co/trajevo."
                },
                "authors": [
                    {
                        "name": "Zhikai Zhao"
                    },
                    {
                        "name": "Chuanbo Hua"
                    },
                    {
                        "name": "Federico Berto"
                    },
                    {
                        "name": "Kanghoon Lee"
                    },
                    {
                        "name": "Zihan Ma"
                    },
                    {
                        "name": "Jiachen Li"
                    },
                    {
                        "name": "Jinkyoo Park"
                    }
                ],
                "author_detail": {
                    "name": "Jinkyoo Park"
                },
                "author": "Jinkyoo Park",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2505.04480",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05616v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05616v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05613v1",
                "updated": "2025-08-07T17:53:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    53,
                    56,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T17:53:56Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    53,
                    56,
                    3,
                    219,
                    0
                ],
                "title": "Cooper: Co-Optimizing Policy and Reward Models in Reinforcement Learning\n  for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cooper: Co-Optimizing Policy and Reward Models in Reinforcement Learning\n  for Large Language Models"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable performance in\nreasoning tasks, where reinforcement learning (RL) serves as a key algorithm\nfor enhancing their reasoning capabilities. Currently, there are two mainstream\nreward paradigms: model-based rewards and rule-based rewards. However, both\napproaches suffer from limitations: rule-based rewards lack robustness, while\nmodel-based rewards are vulnerable to reward hacking. To address these issues,\nwe propose Cooper(Co-optimizing Policy Model and Reward Model), a RL framework\nthat jointly optimizes both the policy model and the reward model. Cooper\nleverages the high precision of rule-based rewards when identifying correct\nresponses, and dynamically constructs and selects positive-negative sample\npairs for continued training the reward model. This design enhances robustness\nand mitigates the risk of reward hacking. To further support Cooper, we\nintroduce a hybrid annotation strategy that efficiently and accurately\ngenerates training data for the reward model. We also propose a reference-based\nreward modeling paradigm, where the reward model takes a reference answer as\ninput. Based on this design, we train a reward model named VerifyRM, which\nachieves higher accuracy on VerifyBench compared to other models of the same\nsize. We conduct reinforcement learning using both VerifyRM and Cooper. Our\nexperiments show that Cooper not only alleviates reward hacking but also\nimproves end-to-end RL performance, for instance, achieving a 0.54% gain in\naverage accuracy on Qwen2.5-1.5B-Instruct. Our findings demonstrate that\ndynamically updating reward model is an effective way to combat reward hacking,\nproviding a reference for better integrating reward models into RL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable performance in\nreasoning tasks, where reinforcement learning (RL) serves as a key algorithm\nfor enhancing their reasoning capabilities. Currently, there are two mainstream\nreward paradigms: model-based rewards and rule-based rewards. However, both\napproaches suffer from limitations: rule-based rewards lack robustness, while\nmodel-based rewards are vulnerable to reward hacking. To address these issues,\nwe propose Cooper(Co-optimizing Policy Model and Reward Model), a RL framework\nthat jointly optimizes both the policy model and the reward model. Cooper\nleverages the high precision of rule-based rewards when identifying correct\nresponses, and dynamically constructs and selects positive-negative sample\npairs for continued training the reward model. This design enhances robustness\nand mitigates the risk of reward hacking. To further support Cooper, we\nintroduce a hybrid annotation strategy that efficiently and accurately\ngenerates training data for the reward model. We also propose a reference-based\nreward modeling paradigm, where the reward model takes a reference answer as\ninput. Based on this design, we train a reward model named VerifyRM, which\nachieves higher accuracy on VerifyBench compared to other models of the same\nsize. We conduct reinforcement learning using both VerifyRM and Cooper. Our\nexperiments show that Cooper not only alleviates reward hacking but also\nimproves end-to-end RL performance, for instance, achieving a 0.54% gain in\naverage accuracy on Qwen2.5-1.5B-Instruct. Our findings demonstrate that\ndynamically updating reward model is an effective way to combat reward hacking,\nproviding a reference for better integrating reward models into RL."
                },
                "authors": [
                    {
                        "name": "Haitao Hong"
                    },
                    {
                        "name": "Yuchen Yan"
                    },
                    {
                        "name": "Xingyu Wu"
                    },
                    {
                        "name": "Guiyang Hou"
                    },
                    {
                        "name": "Wenqi Zhang"
                    },
                    {
                        "name": "Weiming Lu"
                    },
                    {
                        "name": "Yongliang Shen"
                    },
                    {
                        "name": "Jun Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Jun Xiao"
                },
                "author": "Jun Xiao",
                "arxiv_comment": "Project Page: https://zju-real.github.io/cooper Code:\n  https://github.com/zju-real/cooper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01473v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01473v2",
                "updated": "2025-08-07T17:46:00Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    46,
                    0,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-02T19:46:09Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    19,
                    46,
                    9,
                    5,
                    214,
                    0
                ],
                "title": "TreeDiff: AST-Guided Code Generation with Diffusion LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TreeDiff: AST-Guided Code Generation with Diffusion LLMs"
                },
                "summary": "Recent advances in diffusion-based language models have opened new\npossibilities for controllable and bidirectional sequence generation. These\nmodels provide an alternative to traditional autoregressive approaches by\nframing text generation as an iterative denoising process. However, applying\ndiffusion models to structured domains such as source code remains a\nsignificant challenge. Programming languages differ from natural language in\nthat they follow strict syntactic and semantic rules, with hierarchical\norganization that must be preserved for correctness. Standard token-level\ncorruption techniques used during training often ignore this structure, which\nmay hinder the model's ability to learn meaningful representations of code. To\naddress this limitation, we propose a syntax-aware diffusion framework that\nincorporates structural priors from Abstract Syntax Trees (ASTs) into the\ndenoising process. Instead of masking individual tokens at random, we\nselectively corrupt syntactically meaningful code spans derived from AST\nsubtrees. This enables the model to reconstruct programs in a way that respects\ngrammatical boundaries and captures long-range dependencies. Experimental\nresults demonstrate that syntax-aware corruption significantly improves\nsyntactic correctness, reconstruction accuracy, and generalization to unseen\ncode patterns. These findings highlight the potential of incorporating\nstructural information into diffusion-based training and suggest that\nsyntax-guided denoising is a promising direction for advancing diffusion-based\nlanguage models in code generation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in diffusion-based language models have opened new\npossibilities for controllable and bidirectional sequence generation. These\nmodels provide an alternative to traditional autoregressive approaches by\nframing text generation as an iterative denoising process. However, applying\ndiffusion models to structured domains such as source code remains a\nsignificant challenge. Programming languages differ from natural language in\nthat they follow strict syntactic and semantic rules, with hierarchical\norganization that must be preserved for correctness. Standard token-level\ncorruption techniques used during training often ignore this structure, which\nmay hinder the model's ability to learn meaningful representations of code. To\naddress this limitation, we propose a syntax-aware diffusion framework that\nincorporates structural priors from Abstract Syntax Trees (ASTs) into the\ndenoising process. Instead of masking individual tokens at random, we\nselectively corrupt syntactically meaningful code spans derived from AST\nsubtrees. This enables the model to reconstruct programs in a way that respects\ngrammatical boundaries and captures long-range dependencies. Experimental\nresults demonstrate that syntax-aware corruption significantly improves\nsyntactic correctness, reconstruction accuracy, and generalization to unseen\ncode patterns. These findings highlight the potential of incorporating\nstructural information into diffusion-based training and suggest that\nsyntax-guided denoising is a promising direction for advancing diffusion-based\nlanguage models in code generation tasks."
                },
                "authors": [
                    {
                        "name": "Yiming Zeng"
                    },
                    {
                        "name": "Jinghan Cao"
                    },
                    {
                        "name": "Zexin Li"
                    },
                    {
                        "name": "Yiming Chen"
                    },
                    {
                        "name": "Tao Ren"
                    },
                    {
                        "name": "Dawei Xiang"
                    },
                    {
                        "name": "Xidong Wu"
                    },
                    {
                        "name": "Shangqian Gao"
                    },
                    {
                        "name": "Tingting Yu"
                    }
                ],
                "author_detail": {
                    "name": "Tingting Yu"
                },
                "author": "Tingting Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01473v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01473v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05606v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05606v1",
                "updated": "2025-08-07T17:45:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    45,
                    17,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T17:45:17Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    45,
                    17,
                    3,
                    219,
                    0
                ],
                "title": "Uni-cot: Towards Unified Chain-of-Thought Reasoning Across Text and\n  Vision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uni-cot: Towards Unified Chain-of-Thought Reasoning Across Text and\n  Vision"
                },
                "summary": "Chain-of-Thought (CoT) reasoning has been widely adopted to enhance Large\nLanguage Models (LLMs) by decomposing complex tasks into simpler, sequential\nsubtasks. However, extending CoT to vision-language reasoning tasks remains\nchallenging, as it often requires interpreting transitions of visual states to\nsupport reasoning. Existing methods often struggle with this due to limited\ncapacity of modeling visual state transitions or incoherent visual trajectories\ncaused by fragmented architectures.\n  To overcome these limitations, we propose Uni-CoT, a Unified Chain-of-Thought\nframework that enables coherent and grounded multimodal reasoning within a\nsingle unified model. The key idea is to leverage a model capable of both image\nunderstanding and generation to reason over visual content and model evolving\nvisual states. However, empowering a unified model to achieve that is\nnon-trivial, given the high computational cost and the burden of training. To\naddress this, Uni-CoT introduces a novel two-level reasoning paradigm: A\nMacro-Level CoT for high-level task planning and A Micro-Level CoT for subtask\nexecution. This design significantly reduces the computational overhead.\nFurthermore, we introduce a structured training paradigm that combines\ninterleaved image-text supervision for macro-level CoT with multi-task\nobjectives for micro-level CoT. Together, these innovations allow Uni-CoT to\nperform scalable and coherent multi-modal reasoning. Furthermore, thanks to our\ndesign, all experiments can be efficiently completed using only 8 A100 GPUs\nwith 80GB VRAM each. Experimental results on reasoning-driven image generation\nbenchmark (WISE) and editing benchmarks (RISE and KRIS) indicates that Uni-CoT\ndemonstrates SOTA performance and strong generalization, establishing Uni-CoT\nas a promising solution for multi-modal reasoning. Project Page and Code:\nhttps://sais-fuxi.github.io/projects/uni-cot/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) reasoning has been widely adopted to enhance Large\nLanguage Models (LLMs) by decomposing complex tasks into simpler, sequential\nsubtasks. However, extending CoT to vision-language reasoning tasks remains\nchallenging, as it often requires interpreting transitions of visual states to\nsupport reasoning. Existing methods often struggle with this due to limited\ncapacity of modeling visual state transitions or incoherent visual trajectories\ncaused by fragmented architectures.\n  To overcome these limitations, we propose Uni-CoT, a Unified Chain-of-Thought\nframework that enables coherent and grounded multimodal reasoning within a\nsingle unified model. The key idea is to leverage a model capable of both image\nunderstanding and generation to reason over visual content and model evolving\nvisual states. However, empowering a unified model to achieve that is\nnon-trivial, given the high computational cost and the burden of training. To\naddress this, Uni-CoT introduces a novel two-level reasoning paradigm: A\nMacro-Level CoT for high-level task planning and A Micro-Level CoT for subtask\nexecution. This design significantly reduces the computational overhead.\nFurthermore, we introduce a structured training paradigm that combines\ninterleaved image-text supervision for macro-level CoT with multi-task\nobjectives for micro-level CoT. Together, these innovations allow Uni-CoT to\nperform scalable and coherent multi-modal reasoning. Furthermore, thanks to our\ndesign, all experiments can be efficiently completed using only 8 A100 GPUs\nwith 80GB VRAM each. Experimental results on reasoning-driven image generation\nbenchmark (WISE) and editing benchmarks (RISE and KRIS) indicates that Uni-CoT\ndemonstrates SOTA performance and strong generalization, establishing Uni-CoT\nas a promising solution for multi-modal reasoning. Project Page and Code:\nhttps://sais-fuxi.github.io/projects/uni-cot/"
                },
                "authors": [
                    {
                        "name": "Luozheng Qin"
                    },
                    {
                        "name": "Jia Gong"
                    },
                    {
                        "name": "Yuqing Sun"
                    },
                    {
                        "name": "Tianjiao Li"
                    },
                    {
                        "name": "Mengping Yang"
                    },
                    {
                        "name": "Xiaomeng Yang"
                    },
                    {
                        "name": "Chao Qu"
                    },
                    {
                        "name": "Zhiyu Tan"
                    },
                    {
                        "name": "Hao Li"
                    }
                ],
                "author_detail": {
                    "name": "Hao Li"
                },
                "author": "Hao Li",
                "arxiv_comment": "https://sais-fuxi.github.io/projects/uni-cot/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05606v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05606v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05592v1",
                "updated": "2025-08-07T17:32:14Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    32,
                    14,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T17:32:14Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    32,
                    14,
                    3,
                    219,
                    0
                ],
                "title": "MathSmith: Towards Extremely Hard Mathematical Reasoning by Forging\n  Synthetic Problems with a Reinforced Policy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MathSmith: Towards Extremely Hard Mathematical Reasoning by Forging\n  Synthetic Problems with a Reinforced Policy"
                },
                "summary": "Large language models have achieved substantial progress in mathematical\nreasoning, yet their advancement is limited by the scarcity of high-quality,\nhigh-difficulty training data. Existing synthesis methods largely rely on\ntransforming human-written templates, limiting both diversity and scalability.\nWe propose MathSmith, a novel framework for synthesizing challenging\nmathematical problems to enhance LLM reasoning. Rather than modifying existing\nproblems, MathSmith constructs new ones from scratch by randomly sampling\nconcept-explanation pairs from PlanetMath, ensuring data independence and\navoiding contamination. To increase difficulty, we design nine predefined\nstrategies as soft constraints during rationales. We further adopts\nreinforcement learning to jointly optimize structural validity, reasoning\ncomplexity, and answer consistency. The length of the reasoning trace generated\nunder autoregressive prompting is used to reflect cognitive complexity,\nencouraging the creation of more demanding problems aligned with\nlong-chain-of-thought reasoning. Experiments across five benchmarks,\ncategorized as easy & medium (GSM8K, MATH-500) and hard (AIME2024, AIME2025,\nOlympiadBench), show that MathSmith consistently outperforms existing baselines\nunder both short and long CoT settings. Additionally, a weakness-focused\nvariant generation module enables targeted improvement on specific concepts.\nOverall, MathSmith exhibits strong scalability, generalization, and\ntransferability, highlighting the promise of high-difficulty synthetic data in\nadvancing LLM reasoning capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have achieved substantial progress in mathematical\nreasoning, yet their advancement is limited by the scarcity of high-quality,\nhigh-difficulty training data. Existing synthesis methods largely rely on\ntransforming human-written templates, limiting both diversity and scalability.\nWe propose MathSmith, a novel framework for synthesizing challenging\nmathematical problems to enhance LLM reasoning. Rather than modifying existing\nproblems, MathSmith constructs new ones from scratch by randomly sampling\nconcept-explanation pairs from PlanetMath, ensuring data independence and\navoiding contamination. To increase difficulty, we design nine predefined\nstrategies as soft constraints during rationales. We further adopts\nreinforcement learning to jointly optimize structural validity, reasoning\ncomplexity, and answer consistency. The length of the reasoning trace generated\nunder autoregressive prompting is used to reflect cognitive complexity,\nencouraging the creation of more demanding problems aligned with\nlong-chain-of-thought reasoning. Experiments across five benchmarks,\ncategorized as easy & medium (GSM8K, MATH-500) and hard (AIME2024, AIME2025,\nOlympiadBench), show that MathSmith consistently outperforms existing baselines\nunder both short and long CoT settings. Additionally, a weakness-focused\nvariant generation module enables targeted improvement on specific concepts.\nOverall, MathSmith exhibits strong scalability, generalization, and\ntransferability, highlighting the promise of high-difficulty synthetic data in\nadvancing LLM reasoning capabilities."
                },
                "authors": [
                    {
                        "name": "Shaoxiong Zhan"
                    },
                    {
                        "name": "Yanlin Lai"
                    },
                    {
                        "name": "Ziyu Lu"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Ziqing Yang"
                    },
                    {
                        "name": "Fei Tang"
                    }
                ],
                "author_detail": {
                    "name": "Fei Tang"
                },
                "author": "Fei Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05585v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05585v1",
                "updated": "2025-08-07T17:22:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    22,
                    33,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T17:22:33Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    22,
                    33,
                    3,
                    219,
                    0
                ],
                "title": "DART: Dual Adaptive Refinement Transfer for Open-Vocabulary Multi-Label\n  Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DART: Dual Adaptive Refinement Transfer for Open-Vocabulary Multi-Label\n  Recognition"
                },
                "summary": "Open-Vocabulary Multi-Label Recognition (OV-MLR) aims to identify multiple\nseen and unseen object categories within an image, requiring both precise\nintra-class localization to pinpoint objects and effective inter-class\nreasoning to model complex category dependencies. While Vision-Language\nPre-training (VLP) models offer a strong open-vocabulary foundation, they often\nstruggle with fine-grained localization under weak supervision and typically\nfail to explicitly leverage structured relational knowledge beyond basic\nsemantics, limiting performance especially for unseen classes. To overcome\nthese limitations, we propose the Dual Adaptive Refinement Transfer (DART)\nframework. DART enhances a frozen VLP backbone via two synergistic adaptive\nmodules. For intra-class refinement, an Adaptive Refinement Module (ARM)\nrefines patch features adaptively, coupled with a novel Weakly Supervised Patch\nSelecting (WPS) loss that enables discriminative localization using only\nimage-level labels. Concurrently, for inter-class transfer, an Adaptive\nTransfer Module (ATM) leverages a Class Relationship Graph (CRG), constructed\nusing structured knowledge mined from a Large Language Model (LLM), and employs\ngraph attention network to adaptively transfer relational information between\nclass representations. DART is the first framework, to our knowledge, to\nexplicitly integrate external LLM-derived relational knowledge for adaptive\ninter-class transfer while simultaneously performing adaptive intra-class\nrefinement under weak supervision for OV-MLR. Extensive experiments on\nchallenging benchmarks demonstrate that our DART achieves new state-of-the-art\nperformance, validating its effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-Vocabulary Multi-Label Recognition (OV-MLR) aims to identify multiple\nseen and unseen object categories within an image, requiring both precise\nintra-class localization to pinpoint objects and effective inter-class\nreasoning to model complex category dependencies. While Vision-Language\nPre-training (VLP) models offer a strong open-vocabulary foundation, they often\nstruggle with fine-grained localization under weak supervision and typically\nfail to explicitly leverage structured relational knowledge beyond basic\nsemantics, limiting performance especially for unseen classes. To overcome\nthese limitations, we propose the Dual Adaptive Refinement Transfer (DART)\nframework. DART enhances a frozen VLP backbone via two synergistic adaptive\nmodules. For intra-class refinement, an Adaptive Refinement Module (ARM)\nrefines patch features adaptively, coupled with a novel Weakly Supervised Patch\nSelecting (WPS) loss that enables discriminative localization using only\nimage-level labels. Concurrently, for inter-class transfer, an Adaptive\nTransfer Module (ATM) leverages a Class Relationship Graph (CRG), constructed\nusing structured knowledge mined from a Large Language Model (LLM), and employs\ngraph attention network to adaptively transfer relational information between\nclass representations. DART is the first framework, to our knowledge, to\nexplicitly integrate external LLM-derived relational knowledge for adaptive\ninter-class transfer while simultaneously performing adaptive intra-class\nrefinement under weak supervision for OV-MLR. Extensive experiments on\nchallenging benchmarks demonstrate that our DART achieves new state-of-the-art\nperformance, validating its effectiveness."
                },
                "authors": [
                    {
                        "name": "Haijing Liu"
                    },
                    {
                        "name": "Tao Pu"
                    },
                    {
                        "name": "Hefeng Wu"
                    },
                    {
                        "name": "Keze Wang"
                    },
                    {
                        "name": "Liang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Liang Lin"
                },
                "author": "Liang Lin",
                "arxiv_comment": "Accepted by ACM MM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05585v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05585v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05581v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05581v1",
                "updated": "2025-08-07T17:15:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    15,
                    17,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T17:15:17Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    15,
                    17,
                    3,
                    219,
                    0
                ],
                "title": "Iterative Learning of Computable Phenotypes for Treatment Resistant\n  Hypertension using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iterative Learning of Computable Phenotypes for Treatment Resistant\n  Hypertension using Large Language Models"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities for\nmedical question answering and programming, but their potential for generating\ninterpretable computable phenotypes (CPs) is under-explored. In this work, we\ninvestigate whether LLMs can generate accurate and concise CPs for six clinical\nphenotypes of varying complexity, which could be leveraged to enable scalable\nclinical decision support to improve care for patients with hypertension. In\naddition to evaluating zero-short performance, we propose and test a\nsynthesize, execute, debug, instruct strategy that uses LLMs to generate and\niteratively refine CPs using data-driven feedback. Our results show that LLMs,\ncoupled with iterative learning, can generate interpretable and reasonably\naccurate programs that approach the performance of state-of-the-art ML methods\nwhile requiring significantly fewer training examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities for\nmedical question answering and programming, but their potential for generating\ninterpretable computable phenotypes (CPs) is under-explored. In this work, we\ninvestigate whether LLMs can generate accurate and concise CPs for six clinical\nphenotypes of varying complexity, which could be leveraged to enable scalable\nclinical decision support to improve care for patients with hypertension. In\naddition to evaluating zero-short performance, we propose and test a\nsynthesize, execute, debug, instruct strategy that uses LLMs to generate and\niteratively refine CPs using data-driven feedback. Our results show that LLMs,\ncoupled with iterative learning, can generate interpretable and reasonably\naccurate programs that approach the performance of state-of-the-art ML methods\nwhile requiring significantly fewer training examples."
                },
                "authors": [
                    {
                        "name": "Guilherme Seidyo Imai Aldeia"
                    },
                    {
                        "name": "Daniel S. Herman"
                    },
                    {
                        "name": "William G. La Cava"
                    }
                ],
                "author_detail": {
                    "name": "William G. La Cava"
                },
                "author": "William G. La Cava",
                "arxiv_comment": "To appear in PMLR, Volume 298, Machine Learning for Healthcare, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05581v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05581v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20756v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20756v2",
                "updated": "2025-08-07T17:07:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    7,
                    31,
                    3,
                    219,
                    0
                ],
                "published": "2025-04-29T13:34:52Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    13,
                    34,
                    52,
                    1,
                    119,
                    0
                ],
                "title": "Graph-Based Fault Diagnosis for Rotating Machinery: Adaptive\n  Segmentation and Structural Feature Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-Based Fault Diagnosis for Rotating Machinery: Adaptive\n  Segmentation and Structural Feature Integration"
                },
                "summary": "This paper proposes a novel graph-based framework for robust and\ninterpretable multiclass fault diagnosis in rotating machinery. The method\nintegrates entropy-optimized signal segmentation, time-frequency feature\nextraction, and graph-theoretic modeling to transform vibration signals into\nstructured representations suitable for classification. Graph metrics, such as\naverage shortest path length, modularity, and spectral gap, are computed and\ncombined with local features to capture global and segment-level fault\ncharacteristics. The proposed method achieves high diagnostic accuracy when\nevaluated on two benchmark datasets, the CWRU bearing dataset (under 0-3 HP\nloads) and the SU gearbox and bearing datasets (under different speed-load\nconfigurations). Classification scores reach up to 99.8% accuracy on Case\nWestern Reserve University (CWRU) and 100% accuracy on the Southeast University\ndatasets using a logistic regression classifier. Furthermore, the model\nexhibits strong noise resilience, maintaining over 95.4% accuracy at high noise\nlevels (standard deviation = 0.5), and demonstrates excellent cross-domain\ntransferability with up to 99.7% F1-score in load-transfer scenarios. Compared\nto traditional techniques, this approach requires no deep learning\narchitecture, enabling lower complexity while ensuring interpretability. The\nresults confirm the method's scalability, reliability, and potential for\nreal-time deployment in industrial diagnostics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a novel graph-based framework for robust and\ninterpretable multiclass fault diagnosis in rotating machinery. The method\nintegrates entropy-optimized signal segmentation, time-frequency feature\nextraction, and graph-theoretic modeling to transform vibration signals into\nstructured representations suitable for classification. Graph metrics, such as\naverage shortest path length, modularity, and spectral gap, are computed and\ncombined with local features to capture global and segment-level fault\ncharacteristics. The proposed method achieves high diagnostic accuracy when\nevaluated on two benchmark datasets, the CWRU bearing dataset (under 0-3 HP\nloads) and the SU gearbox and bearing datasets (under different speed-load\nconfigurations). Classification scores reach up to 99.8% accuracy on Case\nWestern Reserve University (CWRU) and 100% accuracy on the Southeast University\ndatasets using a logistic regression classifier. Furthermore, the model\nexhibits strong noise resilience, maintaining over 95.4% accuracy at high noise\nlevels (standard deviation = 0.5), and demonstrates excellent cross-domain\ntransferability with up to 99.7% F1-score in load-transfer scenarios. Compared\nto traditional techniques, this approach requires no deep learning\narchitecture, enabling lower complexity while ensuring interpretability. The\nresults confirm the method's scalability, reliability, and potential for\nreal-time deployment in industrial diagnostics."
                },
                "authors": [
                    {
                        "name": "Moirangthem Tiken Singh"
                    }
                ],
                "author_detail": {
                    "name": "Moirangthem Tiken Singh"
                },
                "author": "Moirangthem Tiken Singh",
                "arxiv_doi": "10.1016/j.rineng.2025.106566.",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.rineng.2025.106566.",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.20756v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20756v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Results in Engineering, 106566 (2025)",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05571v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05571v1",
                "updated": "2025-08-07T17:02:23Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    2,
                    23,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T17:02:23Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    17,
                    2,
                    23,
                    3,
                    219,
                    0
                ],
                "title": "Fairy$\\pm i$: the First 2-bit Complex LLM with All Parameters in\n  $\\{\\pm1, \\pm i\\}$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fairy$\\pm i$: the First 2-bit Complex LLM with All Parameters in\n  $\\{\\pm1, \\pm i\\}$"
                },
                "summary": "Quantization-Aware Training (QAT) integrates quantization into the training\nloop, enabling LLMs to learn robust low-bit representations, and is widely\nrecognized as one of the most promising research directions. All current QAT\nresearch focuses on minimizing quantization error on full-precision models,\nwhere the full-precision accuracy acts as an upper bound (accuracy ceiling). No\nexisting method has even attempted to surpass this ceiling. To break this\nceiling, we propose a new paradigm: raising the ceiling (full-precision model),\nand then still quantizing it efficiently into 2 bits. We propose Fairy$\\pm i$,\nthe first 2-bit quantization framework for complex-valued LLMs. Specifically,\nour method leverages the representational advantages of the complex domain to\nboost full-precision accuracy. We map weights to the fourth roots of unity\n$\\{\\pm1, \\pm i\\}$, forming a perfectly symmetric and information-theoretically\noptimal 2-bit representation. Importantly, each quantized weight has either a\nzero real or imaginary part, enabling multiplication-free inference using only\nadditions and element swaps. Experimental results show that Fairy$\\pm i$\noutperforms the ceiling of existing 2-bit quantization approaches in terms of\nboth PPL and downstream tasks, while maintaining strict storage and compute\nefficiency. This work opens a new direction for building highly accurate and\npractical LLMs under extremely low-bit constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization-Aware Training (QAT) integrates quantization into the training\nloop, enabling LLMs to learn robust low-bit representations, and is widely\nrecognized as one of the most promising research directions. All current QAT\nresearch focuses on minimizing quantization error on full-precision models,\nwhere the full-precision accuracy acts as an upper bound (accuracy ceiling). No\nexisting method has even attempted to surpass this ceiling. To break this\nceiling, we propose a new paradigm: raising the ceiling (full-precision model),\nand then still quantizing it efficiently into 2 bits. We propose Fairy$\\pm i$,\nthe first 2-bit quantization framework for complex-valued LLMs. Specifically,\nour method leverages the representational advantages of the complex domain to\nboost full-precision accuracy. We map weights to the fourth roots of unity\n$\\{\\pm1, \\pm i\\}$, forming a perfectly symmetric and information-theoretically\noptimal 2-bit representation. Importantly, each quantized weight has either a\nzero real or imaginary part, enabling multiplication-free inference using only\nadditions and element swaps. Experimental results show that Fairy$\\pm i$\noutperforms the ceiling of existing 2-bit quantization approaches in terms of\nboth PPL and downstream tasks, while maintaining strict storage and compute\nefficiency. This work opens a new direction for building highly accurate and\npractical LLMs under extremely low-bit constraints."
                },
                "authors": [
                    {
                        "name": "Feiyu Wang"
                    },
                    {
                        "name": "Guoan Wang"
                    },
                    {
                        "name": "Yihao Zhang"
                    },
                    {
                        "name": "Shengfan Wang"
                    },
                    {
                        "name": "Weitao Li"
                    },
                    {
                        "name": "Bokai Huang"
                    },
                    {
                        "name": "Shimao Chen"
                    },
                    {
                        "name": "Zihan Jiang"
                    },
                    {
                        "name": "Rui Xu"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "arxiv_comment": "13 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05571v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05571v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02085v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02085v3",
                "updated": "2025-08-07T16:46:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    16,
                    46,
                    44,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-04T05:51:55Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    5,
                    51,
                    55,
                    0,
                    216,
                    0
                ],
                "title": "SE-Agent: Self-Evolution Trajectory Optimization in Multi-Step Reasoning\n  with LLM-Based Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SE-Agent: Self-Evolution Trajectory Optimization in Multi-Step Reasoning\n  with LLM-Based Agents"
                },
                "summary": "Large Language Model (LLM)-based agents have recently shown impressive\ncapabilities in complex reasoning and tool use via multi-step interactions with\ntheir environments. While these agents have the potential to tackle complicated\ntasks, their problem-solving process, i.e., agents' interaction trajectory\nleading to task completion, remains underexploited. These trajectories contain\nrich feedback that can navigate agents toward the right directions for solving\nproblems correctly. Although prevailing approaches, such as Monte Carlo Tree\nSearch (MCTS), can effectively balance exploration and exploitation, they\nignore the interdependence among various trajectories and lack the diversity of\nsearch spaces, which leads to redundant reasoning and suboptimal outcomes. To\naddress these challenges, we propose SE-Agent, a Self-Evolution framework that\nenables Agents to optimize their reasoning processes iteratively. Our approach\nrevisits and enhances former pilot trajectories through three key operations:\nrevision, recombination, and refinement. This evolutionary mechanism enables\ntwo critical advantages: (1) it expands the search space beyond local optima by\nintelligently exploring diverse solution paths guided by previous trajectories,\nand (2) it leverages cross-trajectory inspiration to efficiently enhance\nperformance while mitigating the impact of suboptimal reasoning paths. Through\nthese mechanisms, SE-Agent achieves continuous self-evolution that\nincrementally improves reasoning quality. We evaluate SE-Agent on SWE-bench\nVerified to resolve real-world GitHub issues. Experimental results across five\nstrong LLMs show that integrating SE-Agent delivers up to 55% relative\nimprovement, achieving state-of-the-art performance among all open-source\nagents on SWE-bench Verified. Our code and demonstration materials are publicly\navailable at https://github.com/JARVIS-Xs/SE-Agent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based agents have recently shown impressive\ncapabilities in complex reasoning and tool use via multi-step interactions with\ntheir environments. While these agents have the potential to tackle complicated\ntasks, their problem-solving process, i.e., agents' interaction trajectory\nleading to task completion, remains underexploited. These trajectories contain\nrich feedback that can navigate agents toward the right directions for solving\nproblems correctly. Although prevailing approaches, such as Monte Carlo Tree\nSearch (MCTS), can effectively balance exploration and exploitation, they\nignore the interdependence among various trajectories and lack the diversity of\nsearch spaces, which leads to redundant reasoning and suboptimal outcomes. To\naddress these challenges, we propose SE-Agent, a Self-Evolution framework that\nenables Agents to optimize their reasoning processes iteratively. Our approach\nrevisits and enhances former pilot trajectories through three key operations:\nrevision, recombination, and refinement. This evolutionary mechanism enables\ntwo critical advantages: (1) it expands the search space beyond local optima by\nintelligently exploring diverse solution paths guided by previous trajectories,\nand (2) it leverages cross-trajectory inspiration to efficiently enhance\nperformance while mitigating the impact of suboptimal reasoning paths. Through\nthese mechanisms, SE-Agent achieves continuous self-evolution that\nincrementally improves reasoning quality. We evaluate SE-Agent on SWE-bench\nVerified to resolve real-world GitHub issues. Experimental results across five\nstrong LLMs show that integrating SE-Agent delivers up to 55% relative\nimprovement, achieving state-of-the-art performance among all open-source\nagents on SWE-bench Verified. Our code and demonstration materials are publicly\navailable at https://github.com/JARVIS-Xs/SE-Agent."
                },
                "authors": [
                    {
                        "name": "Jiaye Lin"
                    },
                    {
                        "name": "Yifu Guo"
                    },
                    {
                        "name": "Yuzhen Han"
                    },
                    {
                        "name": "Sen Hu"
                    },
                    {
                        "name": "Ziyi Ni"
                    },
                    {
                        "name": "Licheng Wang"
                    },
                    {
                        "name": "Mingguang Chen"
                    },
                    {
                        "name": "Daxin Jiang"
                    },
                    {
                        "name": "Binxing Jiao"
                    },
                    {
                        "name": "Chen Hu"
                    },
                    {
                        "name": "Huacan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huacan Wang"
                },
                "author": "Huacan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02085v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02085v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05553v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05553v1",
                "updated": "2025-08-07T16:33:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    16,
                    33,
                    45,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T16:33:45Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    16,
                    33,
                    45,
                    3,
                    219,
                    0
                ],
                "title": "Do Political Opinions Transfer Between Western Languages? An Analysis of\n  Unaligned and Aligned Multilingual LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Political Opinions Transfer Between Western Languages? An Analysis of\n  Unaligned and Aligned Multilingual LLMs"
                },
                "summary": "Public opinion surveys show cross-cultural differences in political opinions\nbetween socio-cultural contexts. However, there is no clear evidence whether\nthese differences translate to cross-lingual differences in multilingual large\nlanguage models (MLLMs). We analyze whether opinions transfer between languages\nor whether there are separate opinions for each language in MLLMs of various\nsizes across five Western languages. We evaluate MLLMs' opinions by prompting\nthem to report their (dis)agreement with political statements from voting\nadvice applications. To better understand the interaction between languages in\nthe models, we evaluate them both before and after aligning them with more left\nor right views using direct preference optimization and English alignment data\nonly. Our findings reveal that unaligned models show only very few significant\ncross-lingual differences in the political opinions they reflect. The political\nalignment shifts opinions almost uniformly across all five languages. We\nconclude that in Western language contexts, political opinions transfer between\nlanguages, demonstrating the challenges in achieving explicit socio-linguistic,\ncultural, and political alignment of MLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Public opinion surveys show cross-cultural differences in political opinions\nbetween socio-cultural contexts. However, there is no clear evidence whether\nthese differences translate to cross-lingual differences in multilingual large\nlanguage models (MLLMs). We analyze whether opinions transfer between languages\nor whether there are separate opinions for each language in MLLMs of various\nsizes across five Western languages. We evaluate MLLMs' opinions by prompting\nthem to report their (dis)agreement with political statements from voting\nadvice applications. To better understand the interaction between languages in\nthe models, we evaluate them both before and after aligning them with more left\nor right views using direct preference optimization and English alignment data\nonly. Our findings reveal that unaligned models show only very few significant\ncross-lingual differences in the political opinions they reflect. The political\nalignment shifts opinions almost uniformly across all five languages. We\nconclude that in Western language contexts, political opinions transfer between\nlanguages, demonstrating the challenges in achieving explicit socio-linguistic,\ncultural, and political alignment of MLLMs."
                },
                "authors": [
                    {
                        "name": "Franziska Weeber"
                    },
                    {
                        "name": "Tanise Ceron"
                    },
                    {
                        "name": "Sebastian Pad"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Pad"
                },
                "author": "Sebastian Pad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05553v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05553v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; J.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00255v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00255v2",
                "updated": "2025-08-07T16:31:54Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    16,
                    31,
                    54,
                    3,
                    219,
                    0
                ],
                "published": "2025-03-31T22:02:24Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    22,
                    2,
                    24,
                    0,
                    90,
                    0
                ],
                "title": "SciReplicate-Bench: Benchmarking LLMs in Agent-driven Algorithmic\n  Reproduction from Research Papers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SciReplicate-Bench: Benchmarking LLMs in Agent-driven Algorithmic\n  Reproduction from Research Papers"
                },
                "summary": "This study evaluates large language models (LLMs) in generating code from\nalgorithm descriptions in recent NLP papers. The task requires two key\ncompetencies: (1) algorithm comprehension: synthesizing information from papers\nand academic literature to understand implementation logic, and (2) coding\nexpertise: identifying dependencies and correctly implementing necessary APIs.\nTo facilitate rigorous evaluation, we introduce SciReplicate-Bench, a benchmark\nof 100 tasks from 36 NLP papers published in 2024, featuring detailed\nannotations and comprehensive test cases. Building on SciReplicate-Bench, we\npropose Sci-Reproducer, a dual-agent framework consisting of a Paper Agent that\ninterprets algorithmic concepts from literature and a Code Agent that retrieves\ndependencies from repositories and implements solutions. To assess algorithm\nunderstanding, we introduce reasoning graph accuracy, which quantifies\nsimilarity between generated and reference reasoning graphs derived from code\ncomments and structure. For evaluating implementation quality, we employ\nexecution accuracy, CodeBLEU, and repository dependency/API recall metrics. In\nour experiments, we evaluate various powerful non-reasoning and reasoning LLMs\nas foundational models. The best-performing LLM using \\ModelName~achieves only\n39% execution accuracy, highlighting the benchmark's difficulty. Our analysis\nidentifies missing or inconsistent algorithm descriptions as key barriers to\nsuccessful reproduction. We make available our benchmark and code at\nhttps://github.com/xyzCS/SciReplicate-Bench and project homepage at\nhttps://xyzcs.github.io/scireplicate.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study evaluates large language models (LLMs) in generating code from\nalgorithm descriptions in recent NLP papers. The task requires two key\ncompetencies: (1) algorithm comprehension: synthesizing information from papers\nand academic literature to understand implementation logic, and (2) coding\nexpertise: identifying dependencies and correctly implementing necessary APIs.\nTo facilitate rigorous evaluation, we introduce SciReplicate-Bench, a benchmark\nof 100 tasks from 36 NLP papers published in 2024, featuring detailed\nannotations and comprehensive test cases. Building on SciReplicate-Bench, we\npropose Sci-Reproducer, a dual-agent framework consisting of a Paper Agent that\ninterprets algorithmic concepts from literature and a Code Agent that retrieves\ndependencies from repositories and implements solutions. To assess algorithm\nunderstanding, we introduce reasoning graph accuracy, which quantifies\nsimilarity between generated and reference reasoning graphs derived from code\ncomments and structure. For evaluating implementation quality, we employ\nexecution accuracy, CodeBLEU, and repository dependency/API recall metrics. In\nour experiments, we evaluate various powerful non-reasoning and reasoning LLMs\nas foundational models. The best-performing LLM using \\ModelName~achieves only\n39% execution accuracy, highlighting the benchmark's difficulty. Our analysis\nidentifies missing or inconsistent algorithm descriptions as key barriers to\nsuccessful reproduction. We make available our benchmark and code at\nhttps://github.com/xyzCS/SciReplicate-Bench and project homepage at\nhttps://xyzcs.github.io/scireplicate.github.io/."
                },
                "authors": [
                    {
                        "name": "Yanzheng Xiang"
                    },
                    {
                        "name": "Hanqi Yan"
                    },
                    {
                        "name": "Shuyin Ouyang"
                    },
                    {
                        "name": "Lin Gui"
                    },
                    {
                        "name": "Yulan He"
                    }
                ],
                "author_detail": {
                    "name": "Yulan He"
                },
                "author": "Yulan He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00255v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00255v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05548v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05548v1",
                "updated": "2025-08-07T16:27:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    16,
                    27,
                    57,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T16:27:57Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    16,
                    27,
                    57,
                    3,
                    219,
                    0
                ],
                "title": "Development of PANOSETI Telescopes for Ultra-High-Energy Gamma-Ray\n  Astronomy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development of PANOSETI Telescopes for Ultra-High-Energy Gamma-Ray\n  Astronomy"
                },
                "summary": "Ultra-High-Energy (UHE, E $>100$ TeV) gamma rays are one of the few channels\nto search for and study Galactic PeVatrons. Among the most promising PeVatron\ncandidates are the many UHE gamma-ray sources that have recently been\nidentified on the Galactic Plane. Ground-based particle detectors see these\nsources as extended rather than point-like, and current generation Imaging\nAtmospheric Cherenkov Telescopes (IACTs) struggle to study them with effective\nareas and background rejection that are suboptimal at UHE. A cost-efficient way\nof constructing an array of IACTs explicitly designed for UHE sensitivity is to\nsparsely separate many small telescopes. We have simulated, prototyped, and\ntwice deployed a pathfinder array that is instrumented with telescopes designed\nby the Panoramic Search for Extraterrestrial Intelligence (PANOSETI) team.\nThese 0.5-meter Fresnel lens telescopes are purpose-built for imaging optical\ntransients on nanosecond timescales and are equipped with a\n$10^\\circ\\times10^\\circ$ silicon photomultiplier camera. Three PANOSETI\ntelescopes were deployed twice in the same temporary configuration at Lick\nObservatory in March and October 2024. Here we give a brief description of the\ninstrument and present a comparison of simulations with the data collected,\nincluding an analysis of the Crab Nebula. We also report on the ongoing\ndeployment of PANOSETI telescopes for the Dark100 array that is planned to\noperate for five years at Palomar Observatory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultra-High-Energy (UHE, E $>100$ TeV) gamma rays are one of the few channels\nto search for and study Galactic PeVatrons. Among the most promising PeVatron\ncandidates are the many UHE gamma-ray sources that have recently been\nidentified on the Galactic Plane. Ground-based particle detectors see these\nsources as extended rather than point-like, and current generation Imaging\nAtmospheric Cherenkov Telescopes (IACTs) struggle to study them with effective\nareas and background rejection that are suboptimal at UHE. A cost-efficient way\nof constructing an array of IACTs explicitly designed for UHE sensitivity is to\nsparsely separate many small telescopes. We have simulated, prototyped, and\ntwice deployed a pathfinder array that is instrumented with telescopes designed\nby the Panoramic Search for Extraterrestrial Intelligence (PANOSETI) team.\nThese 0.5-meter Fresnel lens telescopes are purpose-built for imaging optical\ntransients on nanosecond timescales and are equipped with a\n$10^\\circ\\times10^\\circ$ silicon photomultiplier camera. Three PANOSETI\ntelescopes were deployed twice in the same temporary configuration at Lick\nObservatory in March and October 2024. Here we give a brief description of the\ninstrument and present a comparison of simulations with the data collected,\nincluding an analysis of the Crab Nebula. We also report on the ongoing\ndeployment of PANOSETI telescopes for the Dark100 array that is planned to\noperate for five years at Palomar Observatory."
                },
                "authors": [
                    {
                        "name": "Nikolas Korzoun"
                    }
                ],
                "author_detail": {
                    "name": "Nikolas Korzoun"
                },
                "arxiv_affiliation": "for the PANOSETI Collaboration",
                "author": "Nikolas Korzoun",
                "arxiv_comment": "Proceedings paper presented at the 39th International Cosmic Ray\n  Conference (ICRC2025), held 14--24 July, 2025, in Geneva, Switzerland",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05548v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05548v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12496v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12496v2",
                "updated": "2025-08-07T16:23:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    16,
                    23,
                    33,
                    3,
                    219,
                    0
                ],
                "published": "2025-06-14T13:17:27Z",
                "published_parsed": [
                    2025,
                    6,
                    14,
                    13,
                    17,
                    27,
                    5,
                    165,
                    0
                ],
                "title": "Improving Factuality for Dialogue Response Generation via Graph-Based\n  Knowledge Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Factuality for Dialogue Response Generation via Graph-Based\n  Knowledge Augmentation"
                },
                "summary": "Large Language Models (LLMs) succeed in many natural language processing\ntasks. However, their tendency to hallucinate - generate plausible but\ninconsistent or factually incorrect text - can cause significant problems in\ncertain tasks, including response generation in dialogue. To mitigate this\nissue, we propose two novel graph knowledge-augmented frameworks, Dialogue\nResponse Generation via Textualised Graphs (TG-DRG) and Graph-Aware Dialogue\nResponse Generation (GA-DRG), which combine reasoning-guided dialogue\nreformulation, dialogue sense knowledge selection, and graph-enhanced response\ngeneration to improve the factuality of dialogue responses. To evaluate the\nfactuality of generated responses, we propose a dialogue fact score that\naddresses the limitations of existing fact-score methods in dialogue settings,\nproviding a more reliable assessment of factual consistency. We evaluate our\nmethods using different baselines on the OpendialKG and HybriDialogue datasets.\nOur methods noticeably improve factuality compared to other graph\nknowledge-augmentation baselines, including the state-of-the-art G-retriever,\nachieving improvements of 3.47% on OpendialKG and 3.12% on HybriDialogue in\nterms of dialogue fact score. The code will be released on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) succeed in many natural language processing\ntasks. However, their tendency to hallucinate - generate plausible but\ninconsistent or factually incorrect text - can cause significant problems in\ncertain tasks, including response generation in dialogue. To mitigate this\nissue, we propose two novel graph knowledge-augmented frameworks, Dialogue\nResponse Generation via Textualised Graphs (TG-DRG) and Graph-Aware Dialogue\nResponse Generation (GA-DRG), which combine reasoning-guided dialogue\nreformulation, dialogue sense knowledge selection, and graph-enhanced response\ngeneration to improve the factuality of dialogue responses. To evaluate the\nfactuality of generated responses, we propose a dialogue fact score that\naddresses the limitations of existing fact-score methods in dialogue settings,\nproviding a more reliable assessment of factual consistency. We evaluate our\nmethods using different baselines on the OpendialKG and HybriDialogue datasets.\nOur methods noticeably improve factuality compared to other graph\nknowledge-augmentation baselines, including the state-of-the-art G-retriever,\nachieving improvements of 3.47% on OpendialKG and 3.12% on HybriDialogue in\nterms of dialogue fact score. The code will be released on GitHub."
                },
                "authors": [
                    {
                        "name": "Xiangyan Chen"
                    },
                    {
                        "name": "Yujian Gan"
                    },
                    {
                        "name": "Yimeng Gu"
                    },
                    {
                        "name": "Matthew Purver"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Purver"
                },
                "author": "Matthew Purver",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12496v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12496v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05544v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05544v1",
                "updated": "2025-08-07T16:22:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    16,
                    22,
                    49,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T16:22:49Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    16,
                    22,
                    49,
                    3,
                    219,
                    0
                ],
                "title": "Conformal Sets in Multiple-Choice Question Answering under Black-Box\n  Settings with Provable Coverage Guarantees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conformal Sets in Multiple-Choice Question Answering under Black-Box\n  Settings with Provable Coverage Guarantees"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable progress in\nmultiple-choice question answering (MCQA), but their inherent unreliability,\nsuch as hallucination and overconfidence, limits their application in high-risk\ndomains. To address this, we propose a frequency-based uncertainty\nquantification method under black-box settings, leveraging conformal prediction\n(CP) to ensure provable coverage guarantees. Our approach involves multiple\nindependent samplings of the model's output distribution for each input, with\nthe most frequent sample serving as a reference to calculate predictive entropy\n(PE). Experimental evaluations across six LLMs and four datasets (MedMCQA,\nMedQA, MMLU, MMLU-Pro) demonstrate that frequency-based PE outperforms\nlogit-based PE in distinguishing between correct and incorrect predictions, as\nmeasured by AUROC. Furthermore, the method effectively controls the empirical\nmiscoverage rate under user-specified risk levels, validating that sampling\nfrequency can serve as a viable substitute for logit-based probabilities in\nblack-box scenarios. This work provides a distribution-free model-agnostic\nframework for reliable uncertainty quantification in MCQA with guaranteed\ncoverage, enhancing the trustworthiness of LLMs in practical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable progress in\nmultiple-choice question answering (MCQA), but their inherent unreliability,\nsuch as hallucination and overconfidence, limits their application in high-risk\ndomains. To address this, we propose a frequency-based uncertainty\nquantification method under black-box settings, leveraging conformal prediction\n(CP) to ensure provable coverage guarantees. Our approach involves multiple\nindependent samplings of the model's output distribution for each input, with\nthe most frequent sample serving as a reference to calculate predictive entropy\n(PE). Experimental evaluations across six LLMs and four datasets (MedMCQA,\nMedQA, MMLU, MMLU-Pro) demonstrate that frequency-based PE outperforms\nlogit-based PE in distinguishing between correct and incorrect predictions, as\nmeasured by AUROC. Furthermore, the method effectively controls the empirical\nmiscoverage rate under user-specified risk levels, validating that sampling\nfrequency can serve as a viable substitute for logit-based probabilities in\nblack-box scenarios. This work provides a distribution-free model-agnostic\nframework for reliable uncertainty quantification in MCQA with guaranteed\ncoverage, enhancing the trustworthiness of LLMs in practical applications."
                },
                "authors": [
                    {
                        "name": "Guang Yang"
                    },
                    {
                        "name": "Xinyang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xinyang Liu"
                },
                "author": "Xinyang Liu",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05544v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05544v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05545v1",
                "updated": "2025-08-07T16:22:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    16,
                    22,
                    49,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T16:22:49Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    16,
                    22,
                    49,
                    3,
                    219,
                    0
                ],
                "title": "PRvL: Quantifying the Capabilities and Risks of Large Language Models\n  for PII Redaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRvL: Quantifying the Capabilities and Risks of Large Language Models\n  for PII Redaction"
                },
                "summary": "Redacting Personally Identifiable Information (PII) from unstructured text is\ncritical for ensuring data privacy in regulated domains. While earlier\napproaches have relied on rule-based systems and domain-specific Named Entity\nRecognition (NER) models, these methods fail to generalize across formats and\ncontexts. Recent advances in Large Language Models (LLMs) offer a promising\nalternative, yet the effect of architectural and training choices on redaction\nperformance remains underexplored. LLMs have demonstrated strong performance in\ntasks that require contextual language understanding, including the redaction\nof PII in free-form text. Prior work suggests that with appropriate adaptation,\nLLMs can become effective contextual privacy learners. However, the\nconsequences of architectural and training choices for PII Redaction remain\nunderexplored. In this work, we present a comprehensive analysis of LLMs as\nprivacy-preserving PII Redaction systems. We evaluate a range of LLM\narchitectures and training strategies for their effectiveness in PII Redaction.\nOur analysis measures redaction performance, semantic preservation, and PII\nleakage, and compares these outcomes against latency and computational cost.\nThe results provide practical guidance for configuring LLM-based redactors that\nare accurate, efficient, and privacy-aware. To support reproducibility and\nreal-world deployment, we release PRvL, an open-source suite of fine-tuned\nmodels, and evaluation tools for general-purpose PII Redaction. PRvL is built\nentirely on open-source LLMs and supports multiple inference settings for\nflexibility and compliance. It is designed to be easily customized for\ndifferent domains and fully operable within secure, self-managed environments.\nThis enables data owners to perform redactions without relying on third-party\nservices or exposing sensitive content beyond their own infrastructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Redacting Personally Identifiable Information (PII) from unstructured text is\ncritical for ensuring data privacy in regulated domains. While earlier\napproaches have relied on rule-based systems and domain-specific Named Entity\nRecognition (NER) models, these methods fail to generalize across formats and\ncontexts. Recent advances in Large Language Models (LLMs) offer a promising\nalternative, yet the effect of architectural and training choices on redaction\nperformance remains underexplored. LLMs have demonstrated strong performance in\ntasks that require contextual language understanding, including the redaction\nof PII in free-form text. Prior work suggests that with appropriate adaptation,\nLLMs can become effective contextual privacy learners. However, the\nconsequences of architectural and training choices for PII Redaction remain\nunderexplored. In this work, we present a comprehensive analysis of LLMs as\nprivacy-preserving PII Redaction systems. We evaluate a range of LLM\narchitectures and training strategies for their effectiveness in PII Redaction.\nOur analysis measures redaction performance, semantic preservation, and PII\nleakage, and compares these outcomes against latency and computational cost.\nThe results provide practical guidance for configuring LLM-based redactors that\nare accurate, efficient, and privacy-aware. To support reproducibility and\nreal-world deployment, we release PRvL, an open-source suite of fine-tuned\nmodels, and evaluation tools for general-purpose PII Redaction. PRvL is built\nentirely on open-source LLMs and supports multiple inference settings for\nflexibility and compliance. It is designed to be easily customized for\ndifferent domains and fully operable within secure, self-managed environments.\nThis enables data owners to perform redactions without relying on third-party\nservices or exposing sensitive content beyond their own infrastructure."
                },
                "authors": [
                    {
                        "name": "Leon Garza"
                    },
                    {
                        "name": "Anantaa Kotal"
                    },
                    {
                        "name": "Aritran Piplai"
                    },
                    {
                        "name": "Lavanya Elluri"
                    },
                    {
                        "name": "Prajit Das"
                    },
                    {
                        "name": "Aman Chadha"
                    }
                ],
                "author_detail": {
                    "name": "Aman Chadha"
                },
                "author": "Aman Chadha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05543v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05543v1",
                "updated": "2025-08-07T16:20:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    16,
                    20,
                    31,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T16:20:31Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    16,
                    20,
                    31,
                    3,
                    219,
                    0
                ],
                "title": "CleanUpBench: Embodied Sweeping and Grasping Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CleanUpBench: Embodied Sweeping and Grasping Benchmark"
                },
                "summary": "Embodied AI benchmarks have advanced navigation, manipulation, and reasoning,\nbut most target complex humanoid agents or large-scale simulations that are far\nfrom real-world deployment. In contrast, mobile cleaning robots with dual mode\ncapabilities, such as sweeping and grasping, are rapidly emerging as realistic\nand commercially viable platforms. However, no benchmark currently exists that\nsystematically evaluates these agents in structured, multi-target cleaning\ntasks, revealing a critical gap between academic research and real-world\napplications. We introduce CleanUpBench, a reproducible and extensible\nbenchmark for evaluating embodied agents in realistic indoor cleaning\nscenarios. Built on NVIDIA Isaac Sim, CleanUpBench simulates a mobile service\nrobot equipped with a sweeping mechanism and a six-degree-of-freedom robotic\narm, enabling interaction with heterogeneous objects. The benchmark includes\nmanually designed environments and one procedurally generated layout to assess\ngeneralization, along with a comprehensive evaluation suite covering task\ncompletion, spatial efficiency, motion quality, and control performance. To\nsupport comparative studies, we provide baseline agents based on heuristic\nstrategies and map-based planning. CleanUpBench bridges the gap between\nlow-level skill evaluation and full-scene testing, offering a scalable testbed\nfor grounded, embodied intelligence in everyday settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied AI benchmarks have advanced navigation, manipulation, and reasoning,\nbut most target complex humanoid agents or large-scale simulations that are far\nfrom real-world deployment. In contrast, mobile cleaning robots with dual mode\ncapabilities, such as sweeping and grasping, are rapidly emerging as realistic\nand commercially viable platforms. However, no benchmark currently exists that\nsystematically evaluates these agents in structured, multi-target cleaning\ntasks, revealing a critical gap between academic research and real-world\napplications. We introduce CleanUpBench, a reproducible and extensible\nbenchmark for evaluating embodied agents in realistic indoor cleaning\nscenarios. Built on NVIDIA Isaac Sim, CleanUpBench simulates a mobile service\nrobot equipped with a sweeping mechanism and a six-degree-of-freedom robotic\narm, enabling interaction with heterogeneous objects. The benchmark includes\nmanually designed environments and one procedurally generated layout to assess\ngeneralization, along with a comprehensive evaluation suite covering task\ncompletion, spatial efficiency, motion quality, and control performance. To\nsupport comparative studies, we provide baseline agents based on heuristic\nstrategies and map-based planning. CleanUpBench bridges the gap between\nlow-level skill evaluation and full-scene testing, offering a scalable testbed\nfor grounded, embodied intelligence in everyday settings."
                },
                "authors": [
                    {
                        "name": "Wenbo Li"
                    },
                    {
                        "name": "Guanting Chen"
                    },
                    {
                        "name": "Tao Zhao"
                    },
                    {
                        "name": "Jiyao Wang"
                    },
                    {
                        "name": "Tianxin Hu"
                    },
                    {
                        "name": "Yuwen Liao"
                    },
                    {
                        "name": "Weixiang Guo"
                    },
                    {
                        "name": "Shenghai Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Shenghai Yuan"
                },
                "author": "Shenghai Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05543v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05543v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05535v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05535v1",
                "updated": "2025-08-07T16:09:12Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    16,
                    9,
                    12,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T16:09:12Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    16,
                    9,
                    12,
                    3,
                    219,
                    0
                ],
                "title": "Mixed-Initiative Dialog for Human-Robot Collaborative Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixed-Initiative Dialog for Human-Robot Collaborative Manipulation"
                },
                "summary": "Effective robotic systems for long-horizon human-robot collaboration must\nadapt to a wide range of human partners, whose physical behavior, willingness\nto assist, and understanding of the robot's capabilities may change over time.\nThis demands a tightly coupled communication loop that grants both agents the\nflexibility to propose, accept, or decline requests as they coordinate toward\ncompleting the task effectively. We apply a Mixed-Initiative dialog paradigm to\nCollaborative human-roBot teaming and propose MICoBot, a system that handles\nthe common scenario where both agents, using natural language, take initiative\nin formulating, accepting, or rejecting proposals on who can best complete\ndifferent steps of a task. To handle diverse, task-directed dialog, and find\nsuccessful collaborative strategies that minimize human effort, MICoBot makes\ndecisions at three levels: (1) a meta-planner considers human dialog to\nformulate and code a high-level collaboration strategy, (2) a planner optimally\nallocates the remaining steps to either agent based on the robot's capabilities\n(measured by a simulation-pretrained affordance model) and the human's\nestimated availability to help, and (3) an action executor decides the\nlow-level actions to perform or words to say to the human. Our extensive\nevaluations in simulation and real-world -- on a physical robot with 18 unique\nhuman participants over 27 hours -- demonstrate the ability of our method to\neffectively collaborate with diverse human users, yielding significantly\nimproved task success and user experience than a pure LLM baseline and other\nagent allocation models. See additional videos and materials at\nhttps://robin-lab.cs.utexas.edu/MicoBot/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective robotic systems for long-horizon human-robot collaboration must\nadapt to a wide range of human partners, whose physical behavior, willingness\nto assist, and understanding of the robot's capabilities may change over time.\nThis demands a tightly coupled communication loop that grants both agents the\nflexibility to propose, accept, or decline requests as they coordinate toward\ncompleting the task effectively. We apply a Mixed-Initiative dialog paradigm to\nCollaborative human-roBot teaming and propose MICoBot, a system that handles\nthe common scenario where both agents, using natural language, take initiative\nin formulating, accepting, or rejecting proposals on who can best complete\ndifferent steps of a task. To handle diverse, task-directed dialog, and find\nsuccessful collaborative strategies that minimize human effort, MICoBot makes\ndecisions at three levels: (1) a meta-planner considers human dialog to\nformulate and code a high-level collaboration strategy, (2) a planner optimally\nallocates the remaining steps to either agent based on the robot's capabilities\n(measured by a simulation-pretrained affordance model) and the human's\nestimated availability to help, and (3) an action executor decides the\nlow-level actions to perform or words to say to the human. Our extensive\nevaluations in simulation and real-world -- on a physical robot with 18 unique\nhuman participants over 27 hours -- demonstrate the ability of our method to\neffectively collaborate with diverse human users, yielding significantly\nimproved task success and user experience than a pure LLM baseline and other\nagent allocation models. See additional videos and materials at\nhttps://robin-lab.cs.utexas.edu/MicoBot/."
                },
                "authors": [
                    {
                        "name": "Albert Yu"
                    },
                    {
                        "name": "Chengshu Li"
                    },
                    {
                        "name": "Luca Macesanu"
                    },
                    {
                        "name": "Arnav Balaji"
                    },
                    {
                        "name": "Ruchira Ray"
                    },
                    {
                        "name": "Raymond Mooney"
                    },
                    {
                        "name": "Roberto Martn-Martn"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Martn-Martn"
                },
                "author": "Roberto Martn-Martn",
                "arxiv_comment": "Project website at https://robin-lab.cs.utexas.edu/MicoBot/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05535v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05535v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.9; I.2.7; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05534v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05534v1",
                "updated": "2025-08-07T16:06:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    16,
                    6,
                    58,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T16:06:58Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    16,
                    6,
                    58,
                    3,
                    219,
                    0
                ],
                "title": "CoCoLex: Confidence-guided Copy-based Decoding for Grounded Legal Text\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoCoLex: Confidence-guided Copy-based Decoding for Grounded Legal Text\n  Generation"
                },
                "summary": "Due to their ability to process long and complex contexts, LLMs can offer key\nbenefits to the Legal domain, but their adoption has been hindered by their\ntendency to generate unfaithful, ungrounded, or hallucinatory outputs. While\nRetrieval-Augmented Generation offers a promising solution by grounding\ngenerations in external knowledge, it offers no guarantee that the provided\ncontext will be effectively integrated. To address this, context-aware decoding\nstrategies have been proposed to amplify the influence of relevant context, but\nthey usually do not explicitly enforce faithfulness to the context. In this\nwork, we introduce Confidence-guided Copy-based Decoding for Legal Text\nGeneration (CoCoLex)-a decoding strategy that dynamically interpolates the\nmodel produced vocabulary distribution with a distribution derived based on\ncopying from the context. CoCoLex encourages direct copying based on the\nmodel's confidence, ensuring greater fidelity to the source. Experimental\nresults on five legal benchmarks demonstrate that CoCoLex outperforms existing\ncontext-aware decoding methods, particularly in long-form generation tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to their ability to process long and complex contexts, LLMs can offer key\nbenefits to the Legal domain, but their adoption has been hindered by their\ntendency to generate unfaithful, ungrounded, or hallucinatory outputs. While\nRetrieval-Augmented Generation offers a promising solution by grounding\ngenerations in external knowledge, it offers no guarantee that the provided\ncontext will be effectively integrated. To address this, context-aware decoding\nstrategies have been proposed to amplify the influence of relevant context, but\nthey usually do not explicitly enforce faithfulness to the context. In this\nwork, we introduce Confidence-guided Copy-based Decoding for Legal Text\nGeneration (CoCoLex)-a decoding strategy that dynamically interpolates the\nmodel produced vocabulary distribution with a distribution derived based on\ncopying from the context. CoCoLex encourages direct copying based on the\nmodel's confidence, ensuring greater fidelity to the source. Experimental\nresults on five legal benchmarks demonstrate that CoCoLex outperforms existing\ncontext-aware decoding methods, particularly in long-form generation tasks."
                },
                "authors": [
                    {
                        "name": "Santosh T. Y. S. S"
                    },
                    {
                        "name": "Youssef Tarek Elkhayat"
                    },
                    {
                        "name": "Oana Ichim"
                    },
                    {
                        "name": "Pranav Shetty"
                    },
                    {
                        "name": "Dongsheng Wang"
                    },
                    {
                        "name": "Zhiqiang Ma"
                    },
                    {
                        "name": "Armineh Nourbakhsh"
                    },
                    {
                        "name": "Xiaomo Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaomo Liu"
                },
                "author": "Xiaomo Liu",
                "arxiv_doi": "10.18653/v1/2025.acl-long.931",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.18653/v1/2025.acl-long.931",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.05534v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05534v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to ACL 2025-Main Conference",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09032v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09032v2",
                "updated": "2025-08-07T16:01:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    16,
                    1,
                    43,
                    3,
                    219,
                    0
                ],
                "published": "2025-03-12T03:45:53Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    3,
                    45,
                    53,
                    2,
                    71,
                    0
                ],
                "title": "Teaching LLMs How to Learn with Contextual Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaching LLMs How to Learn with Contextual Fine-Tuning"
                },
                "summary": "Prompting Large Language Models (LLMs), or providing context on the expected\nmodel of operation, is an effective way to steer the outputs of such models to\nsatisfy human desiderata after they have been trained. But in rapidly evolving\ndomains, there is often need to fine-tune LLMs to improve either the kind of\nknowledge in their memory or their abilities to perform open ended reasoning in\nnew domains. When human's learn new concepts, we often do so by linking the new\nmaterial that we are studying to concepts we have already learned before. To\nthat end, we ask, \"can prompting help us teach LLMs how to learn\". In this\nwork, we study a novel generalization of instruction tuning, called contextual\nfine-tuning, to fine-tune LLMs. Our method leverages instructional prompts\ndesigned to mimic human cognitive strategies in learning and problem-solving to\nguide the learning process during training, aiming to improve the model's\ninterpretation and understanding of domain-specific knowledge. We empirically\ndemonstrate that this simple yet effective modification improves the ability of\nLLMs to be fine-tuned rapidly on new datasets both within the medical and\nfinancial domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompting Large Language Models (LLMs), or providing context on the expected\nmodel of operation, is an effective way to steer the outputs of such models to\nsatisfy human desiderata after they have been trained. But in rapidly evolving\ndomains, there is often need to fine-tune LLMs to improve either the kind of\nknowledge in their memory or their abilities to perform open ended reasoning in\nnew domains. When human's learn new concepts, we often do so by linking the new\nmaterial that we are studying to concepts we have already learned before. To\nthat end, we ask, \"can prompting help us teach LLMs how to learn\". In this\nwork, we study a novel generalization of instruction tuning, called contextual\nfine-tuning, to fine-tune LLMs. Our method leverages instructional prompts\ndesigned to mimic human cognitive strategies in learning and problem-solving to\nguide the learning process during training, aiming to improve the model's\ninterpretation and understanding of domain-specific knowledge. We empirically\ndemonstrate that this simple yet effective modification improves the ability of\nLLMs to be fine-tuned rapidly on new datasets both within the medical and\nfinancial domains."
                },
                "authors": [
                    {
                        "name": "Younwoo Choi"
                    },
                    {
                        "name": "Muhammad Adil Asif"
                    },
                    {
                        "name": "Ziwen Han"
                    },
                    {
                        "name": "John Willes"
                    },
                    {
                        "name": "Rahul G. Krishnan"
                    }
                ],
                "author_detail": {
                    "name": "Rahul G. Krishnan"
                },
                "author": "Rahul G. Krishnan",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09032v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09032v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02583v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02583v2",
                "updated": "2025-08-07T15:57:23Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    57,
                    23,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-04T16:39:24Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    39,
                    24,
                    0,
                    216,
                    0
                ],
                "title": "CAMA: Enhancing Mathematical Reasoning in Large Language Models with\n  Causal Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAMA: Enhancing Mathematical Reasoning in Large Language Models with\n  Causal Knowledge"
                },
                "summary": "Large Language Models (LLMs) have demonstrated strong performance across a\nwide range of tasks, yet they still struggle with complex mathematical\nreasoning, a challenge fundamentally rooted in deep structural dependencies. To\naddress this challenge, we propose \\textbf{CA}usal \\textbf{MA}thematician\n(\\textbf{CAMA}), a two-stage causal framework that equips LLMs with explicit,\nreusable mathematical structure. In the learning stage, CAMA first constructs\nthe \\textbf{M}athematical \\textbf{C}ausal \\textbf{G}raph (\\textbf{MCG}), a\nhigh-level representation of solution strategies, by combining LLM priors with\ncausal discovery algorithms applied to a corpus of question-solution pairs. The\nresulting MCG encodes essential knowledge points and their causal dependencies.\nTo better align the graph with downstream reasoning tasks, CAMA further refines\nthe MCG through iterative feedback derived from a selected subset of the\nquestion-solution pairs. In the reasoning stage, given a new question, CAMA\ndynamically extracts a task-relevant subgraph from the MCG, conditioned on both\nthe question content and the LLM's intermediate reasoning trace. This subgraph,\nwhich encodes the most pertinent knowledge points and their causal\ndependencies, is then injected back into the LLM to guide its reasoning\nprocess. Empirical results on real-world datasets show that CAMA significantly\nimproves LLM performance on challenging mathematical problems. Furthermore, our\nexperiments demonstrate that structured guidance consistently outperforms\nunstructured alternatives, and that incorporating asymmetric causal\nrelationships yields greater improvements than using symmetric associations\nalone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated strong performance across a\nwide range of tasks, yet they still struggle with complex mathematical\nreasoning, a challenge fundamentally rooted in deep structural dependencies. To\naddress this challenge, we propose \\textbf{CA}usal \\textbf{MA}thematician\n(\\textbf{CAMA}), a two-stage causal framework that equips LLMs with explicit,\nreusable mathematical structure. In the learning stage, CAMA first constructs\nthe \\textbf{M}athematical \\textbf{C}ausal \\textbf{G}raph (\\textbf{MCG}), a\nhigh-level representation of solution strategies, by combining LLM priors with\ncausal discovery algorithms applied to a corpus of question-solution pairs. The\nresulting MCG encodes essential knowledge points and their causal dependencies.\nTo better align the graph with downstream reasoning tasks, CAMA further refines\nthe MCG through iterative feedback derived from a selected subset of the\nquestion-solution pairs. In the reasoning stage, given a new question, CAMA\ndynamically extracts a task-relevant subgraph from the MCG, conditioned on both\nthe question content and the LLM's intermediate reasoning trace. This subgraph,\nwhich encodes the most pertinent knowledge points and their causal\ndependencies, is then injected back into the LLM to guide its reasoning\nprocess. Empirical results on real-world datasets show that CAMA significantly\nimproves LLM performance on challenging mathematical problems. Furthermore, our\nexperiments demonstrate that structured guidance consistently outperforms\nunstructured alternatives, and that incorporating asymmetric causal\nrelationships yields greater improvements than using symmetric associations\nalone."
                },
                "authors": [
                    {
                        "name": "Lei Zan"
                    },
                    {
                        "name": "Keli Zhang"
                    },
                    {
                        "name": "Ruichu Cai"
                    },
                    {
                        "name": "Lujia Pan"
                    }
                ],
                "author_detail": {
                    "name": "Lujia Pan"
                },
                "author": "Lujia Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02583v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02583v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05527v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05527v1",
                "updated": "2025-08-07T15:55:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    55,
                    46,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T15:55:46Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    55,
                    46,
                    3,
                    219,
                    0
                ],
                "title": "AI vs. Human Moderators: A Comparative Evaluation of Multimodal LLMs in\n  Content Moderation for Brand Safety",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI vs. Human Moderators: A Comparative Evaluation of Multimodal LLMs in\n  Content Moderation for Brand Safety"
                },
                "summary": "As the volume of video content online grows exponentially, the demand for\nmoderation of unsafe videos has surpassed human capabilities, posing both\noperational and mental health challenges. While recent studies demonstrated the\nmerits of Multimodal Large Language Models (MLLMs) in various video\nunderstanding tasks, their application to multimodal content moderation, a\ndomain that requires nuanced understanding of both visual and textual cues,\nremains relatively underexplored. In this work, we benchmark the capabilities\nof MLLMs in brand safety classification, a critical subset of content\nmoderation for safe-guarding advertising integrity. To this end, we introduce a\nnovel, multimodal and multilingual dataset, meticulously labeled by\nprofessional reviewers in a multitude of risk categories. Through a detailed\ncomparative analysis, we demonstrate the effectiveness of MLLMs such as Gemini,\nGPT, and Llama in multimodal brand safety, and evaluate their accuracy and cost\nefficiency compared to professional human reviewers. Furthermore, we present an\nin-depth discussion shedding light on limitations of MLLMs and failure cases.\nWe are releasing our dataset alongside this paper to facilitate future research\non effective and responsible brand safety and content moderation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the volume of video content online grows exponentially, the demand for\nmoderation of unsafe videos has surpassed human capabilities, posing both\noperational and mental health challenges. While recent studies demonstrated the\nmerits of Multimodal Large Language Models (MLLMs) in various video\nunderstanding tasks, their application to multimodal content moderation, a\ndomain that requires nuanced understanding of both visual and textual cues,\nremains relatively underexplored. In this work, we benchmark the capabilities\nof MLLMs in brand safety classification, a critical subset of content\nmoderation for safe-guarding advertising integrity. To this end, we introduce a\nnovel, multimodal and multilingual dataset, meticulously labeled by\nprofessional reviewers in a multitude of risk categories. Through a detailed\ncomparative analysis, we demonstrate the effectiveness of MLLMs such as Gemini,\nGPT, and Llama in multimodal brand safety, and evaluate their accuracy and cost\nefficiency compared to professional human reviewers. Furthermore, we present an\nin-depth discussion shedding light on limitations of MLLMs and failure cases.\nWe are releasing our dataset alongside this paper to facilitate future research\non effective and responsible brand safety and content moderation."
                },
                "authors": [
                    {
                        "name": "Adi Levi"
                    },
                    {
                        "name": "Or Levi"
                    },
                    {
                        "name": "Sardhendu Mishra"
                    },
                    {
                        "name": "Jonathan Morra"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Morra"
                },
                "author": "Jonathan Morra",
                "arxiv_comment": "Accepted to the Computer Vision in Advertising and Marketing (CVAM)\n  workshop at ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05527v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05527v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.10; I.2.7; H.3.3; H.4.3; K.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05526v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05526v1",
                "updated": "2025-08-07T15:55:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    55,
                    13,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T15:55:13Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    55,
                    13,
                    3,
                    219,
                    0
                ],
                "title": "When Deepfake Detection Meets Graph Neural Network:a Unified and\n  Lightweight Learning Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Deepfake Detection Meets Graph Neural Network:a Unified and\n  Lightweight Learning Framework"
                },
                "summary": "The proliferation of generative video models has made detecting AI-generated\nand manipulated videos an urgent challenge. Existing detection approaches often\nfail to generalize across diverse manipulation types due to their reliance on\nisolated spatial, temporal, or spectral information, and typically require\nlarge models to perform well. This paper introduces SSTGNN, a lightweight\nSpatial-Spectral-Temporal Graph Neural Network framework that represents videos\nas structured graphs, enabling joint reasoning over spatial inconsistencies,\ntemporal artifacts, and spectral distortions. SSTGNN incorporates learnable\nspectral filters and temporal differential modeling into a graph-based\narchitecture, capturing subtle manipulation traces more effectively. Extensive\nexperiments on diverse benchmark datasets demonstrate that SSTGNN not only\nachieves superior performance in both in-domain and cross-domain settings, but\nalso offers strong robustness against unseen manipulations. Remarkably, SSTGNN\naccomplishes these results with up to 42.4$\\times$ fewer parameters than\nstate-of-the-art models, making it highly lightweight and scalable for\nreal-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of generative video models has made detecting AI-generated\nand manipulated videos an urgent challenge. Existing detection approaches often\nfail to generalize across diverse manipulation types due to their reliance on\nisolated spatial, temporal, or spectral information, and typically require\nlarge models to perform well. This paper introduces SSTGNN, a lightweight\nSpatial-Spectral-Temporal Graph Neural Network framework that represents videos\nas structured graphs, enabling joint reasoning over spatial inconsistencies,\ntemporal artifacts, and spectral distortions. SSTGNN incorporates learnable\nspectral filters and temporal differential modeling into a graph-based\narchitecture, capturing subtle manipulation traces more effectively. Extensive\nexperiments on diverse benchmark datasets demonstrate that SSTGNN not only\nachieves superior performance in both in-domain and cross-domain settings, but\nalso offers strong robustness against unseen manipulations. Remarkably, SSTGNN\naccomplishes these results with up to 42.4$\\times$ fewer parameters than\nstate-of-the-art models, making it highly lightweight and scalable for\nreal-world deployment."
                },
                "authors": [
                    {
                        "name": "Haoyu Liu"
                    },
                    {
                        "name": "Chaoyu Gong"
                    },
                    {
                        "name": "Mengke He"
                    },
                    {
                        "name": "Jiate Li"
                    },
                    {
                        "name": "Kai Han"
                    },
                    {
                        "name": "Siqiang Luo"
                    }
                ],
                "author_detail": {
                    "name": "Siqiang Luo"
                },
                "author": "Siqiang Luo",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05526v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05526v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05525v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05525v1",
                "updated": "2025-08-07T15:53:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    53,
                    30,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T15:53:30Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    53,
                    30,
                    3,
                    219,
                    0
                ],
                "title": "The World According to LLMs: How Geographic Origin Influences LLMs'\n  Entity Deduction Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The World According to LLMs: How Geographic Origin Influences LLMs'\n  Entity Deduction Capabilities"
                },
                "summary": "Large Language Models (LLMs) have been extensively tuned to mitigate explicit\nbiases, yet they often exhibit subtle implicit biases rooted in their\npre-training data. Rather than directly probing LLMs with human-crafted\nquestions that may trigger guardrails, we propose studying how models behave\nwhen they proactively ask questions themselves. The 20 Questions game, a\nmulti-turn deduction task, serves as an ideal testbed for this purpose. We\nsystematically evaluate geographic performance disparities in entity deduction\nusing a new dataset, Geo20Q+, consisting of both notable people and culturally\nsignificant objects (e.g., foods, landmarks, animals) from diverse regions. We\ntest popular LLMs across two gameplay configurations (canonical 20-question and\nunlimited turns) and in seven languages (English, Hindi, Mandarin, Japanese,\nFrench, Spanish, and Turkish). Our results reveal geographic disparities: LLMs\nare substantially more successful at deducing entities from the Global North\nthan the Global South, and the Global West than the Global East. While\nWikipedia pageviews and pre-training corpus frequency correlate mildly with\nperformance, they fail to fully explain these disparities. Notably, the\nlanguage in which the game is played has minimal impact on performance gaps.\nThese findings demonstrate the value of creative, free-form evaluation\nframeworks for uncovering subtle biases in LLMs that remain hidden in standard\nprompting setups. By analyzing how models initiate and pursue reasoning goals\nover multiple turns, we find geographic and cultural disparities embedded in\ntheir reasoning processes. We release the dataset (Geo20Q+) and code at\nhttps://sites.google.com/view/llmbias20q/home.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been extensively tuned to mitigate explicit\nbiases, yet they often exhibit subtle implicit biases rooted in their\npre-training data. Rather than directly probing LLMs with human-crafted\nquestions that may trigger guardrails, we propose studying how models behave\nwhen they proactively ask questions themselves. The 20 Questions game, a\nmulti-turn deduction task, serves as an ideal testbed for this purpose. We\nsystematically evaluate geographic performance disparities in entity deduction\nusing a new dataset, Geo20Q+, consisting of both notable people and culturally\nsignificant objects (e.g., foods, landmarks, animals) from diverse regions. We\ntest popular LLMs across two gameplay configurations (canonical 20-question and\nunlimited turns) and in seven languages (English, Hindi, Mandarin, Japanese,\nFrench, Spanish, and Turkish). Our results reveal geographic disparities: LLMs\nare substantially more successful at deducing entities from the Global North\nthan the Global South, and the Global West than the Global East. While\nWikipedia pageviews and pre-training corpus frequency correlate mildly with\nperformance, they fail to fully explain these disparities. Notably, the\nlanguage in which the game is played has minimal impact on performance gaps.\nThese findings demonstrate the value of creative, free-form evaluation\nframeworks for uncovering subtle biases in LLMs that remain hidden in standard\nprompting setups. By analyzing how models initiate and pursue reasoning goals\nover multiple turns, we find geographic and cultural disparities embedded in\ntheir reasoning processes. We release the dataset (Geo20Q+) and code at\nhttps://sites.google.com/view/llmbias20q/home."
                },
                "authors": [
                    {
                        "name": "Harsh Nishant Lalai"
                    },
                    {
                        "name": "Raj Sanjay Shah"
                    },
                    {
                        "name": "Jiaxin Pei"
                    },
                    {
                        "name": "Sashank Varma"
                    },
                    {
                        "name": "Yi-Chia Wang"
                    },
                    {
                        "name": "Ali Emami"
                    }
                ],
                "author_detail": {
                    "name": "Ali Emami"
                },
                "author": "Ali Emami",
                "arxiv_comment": "Conference on Language Modeling 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05525v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05525v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05512v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05512v1",
                "updated": "2025-08-07T15:46:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    46,
                    53,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T15:46:53Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    46,
                    53,
                    3,
                    219,
                    0
                ],
                "title": "RankArena: A Unified Platform for Evaluating Retrieval, Reranking and\n  RAG with Human and LLM Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RankArena: A Unified Platform for Evaluating Retrieval, Reranking and\n  RAG with Human and LLM Feedback"
                },
                "summary": "Evaluating the quality of retrieval-augmented generation (RAG) and document\nreranking systems remains challenging due to the lack of scalable,\nuser-centric, and multi-perspective evaluation tools. We introduce RankArena, a\nunified platform for comparing and analysing the performance of retrieval\npipelines, rerankers, and RAG systems using structured human and LLM-based\nfeedback as well as for collecting such feedback. RankArena supports multiple\nevaluation modes: direct reranking visualisation, blind pairwise comparisons\nwith human or LLM voting, supervised manual document annotation, and end-to-end\nRAG answer quality assessment. It captures fine-grained relevance feedback\nthrough both pairwise preferences and full-list annotations, along with\nauxiliary metadata such as movement metrics, annotation time, and quality\nratings. The platform also integrates LLM-as-a-judge evaluation, enabling\ncomparison between model-generated rankings and human ground truth annotations.\nAll interactions are stored as structured evaluation datasets that can be used\nto train rerankers, reward models, judgment agents, or retrieval strategy\nselectors. Our platform is publicly available at https://rankarena.ngrok.io/,\nand the Demo video is provided https://youtu.be/jIYAP4PaSSI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the quality of retrieval-augmented generation (RAG) and document\nreranking systems remains challenging due to the lack of scalable,\nuser-centric, and multi-perspective evaluation tools. We introduce RankArena, a\nunified platform for comparing and analysing the performance of retrieval\npipelines, rerankers, and RAG systems using structured human and LLM-based\nfeedback as well as for collecting such feedback. RankArena supports multiple\nevaluation modes: direct reranking visualisation, blind pairwise comparisons\nwith human or LLM voting, supervised manual document annotation, and end-to-end\nRAG answer quality assessment. It captures fine-grained relevance feedback\nthrough both pairwise preferences and full-list annotations, along with\nauxiliary metadata such as movement metrics, annotation time, and quality\nratings. The platform also integrates LLM-as-a-judge evaluation, enabling\ncomparison between model-generated rankings and human ground truth annotations.\nAll interactions are stored as structured evaluation datasets that can be used\nto train rerankers, reward models, judgment agents, or retrieval strategy\nselectors. Our platform is publicly available at https://rankarena.ngrok.io/,\nand the Demo video is provided https://youtu.be/jIYAP4PaSSI."
                },
                "authors": [
                    {
                        "name": "Abdelrahman Abdallah"
                    },
                    {
                        "name": "Mahmoud Abdalla"
                    },
                    {
                        "name": "Bhawna Piryani"
                    },
                    {
                        "name": "Jamshid Mozafari"
                    },
                    {
                        "name": "Mohammed Ali"
                    },
                    {
                        "name": "Adam Jatowt"
                    }
                ],
                "author_detail": {
                    "name": "Adam Jatowt"
                },
                "author": "Adam Jatowt",
                "arxiv_comment": "Accept at CIKM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05512v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05512v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05509v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05509v1",
                "updated": "2025-08-07T15:42:00Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    42,
                    0,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T15:42:00Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    42,
                    0,
                    3,
                    219,
                    0
                ],
                "title": "LAG: Logic-Augmented Generation from a Cartesian Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAG: Logic-Augmented Generation from a Cartesian Perspective"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\na wide range of tasks, yet exhibit critical limitations in knowledge-intensive\ntasks, often generating hallucinations when faced with questions requiring\nspecialized expertise. While retrieval-augmented generation (RAG) mitigates\nthis by integrating external knowledge, it struggles with complex reasoning\nscenarios due to its reliance on direct semantic retrieval and lack of\nstructured logical organization. Inspired by Cartesian principles from\n\\textit{Discours de la m\\'ethode}, this paper introduces Logic-Augmented\nGeneration (LAG), a novel paradigm that reframes knowledge augmentation through\nsystematic question decomposition and dependency-aware reasoning. Specifically,\nLAG first decomposes complex questions into atomic sub-questions ordered by\nlogical dependencies. It then resolves these sequentially, using prior answers\nto guide context retrieval for subsequent sub-questions, ensuring stepwise\ngrounding in logical chain. To prevent error propagation, LAG incorporates a\nlogical termination mechanism that halts inference upon encountering\nunanswerable sub-questions and reduces wasted computation on excessive\nreasoning. Finally, it synthesizes all sub-resolutions to generate verified\nresponses. Experiments on four benchmark datasets demonstrate that LAG\nsignificantly enhances reasoning robustness, reduces hallucination, and aligns\nLLM problem-solving with human cognition, offering a principled alternative to\nexisting RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities across\na wide range of tasks, yet exhibit critical limitations in knowledge-intensive\ntasks, often generating hallucinations when faced with questions requiring\nspecialized expertise. While retrieval-augmented generation (RAG) mitigates\nthis by integrating external knowledge, it struggles with complex reasoning\nscenarios due to its reliance on direct semantic retrieval and lack of\nstructured logical organization. Inspired by Cartesian principles from\n\\textit{Discours de la m\\'ethode}, this paper introduces Logic-Augmented\nGeneration (LAG), a novel paradigm that reframes knowledge augmentation through\nsystematic question decomposition and dependency-aware reasoning. Specifically,\nLAG first decomposes complex questions into atomic sub-questions ordered by\nlogical dependencies. It then resolves these sequentially, using prior answers\nto guide context retrieval for subsequent sub-questions, ensuring stepwise\ngrounding in logical chain. To prevent error propagation, LAG incorporates a\nlogical termination mechanism that halts inference upon encountering\nunanswerable sub-questions and reduces wasted computation on excessive\nreasoning. Finally, it synthesizes all sub-resolutions to generate verified\nresponses. Experiments on four benchmark datasets demonstrate that LAG\nsignificantly enhances reasoning robustness, reduces hallucination, and aligns\nLLM problem-solving with human cognition, offering a principled alternative to\nexisting RAG systems."
                },
                "authors": [
                    {
                        "name": "Yilin Xiao"
                    },
                    {
                        "name": "Chuang Zhou"
                    },
                    {
                        "name": "Qinggang Zhang"
                    },
                    {
                        "name": "Su Dong"
                    },
                    {
                        "name": "Shengyuan Chen"
                    },
                    {
                        "name": "Xiao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xiao Huang"
                },
                "author": "Xiao Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05509v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05509v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04094v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04094v2",
                "updated": "2025-08-07T15:40:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    40,
                    48,
                    3,
                    219,
                    0
                ],
                "published": "2024-10-05T09:27:52Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    9,
                    27,
                    52,
                    5,
                    279,
                    0
                ],
                "title": "BloomWise: Enhancing Problem-Solving capabilities of Large Language\n  Models using Bloom's-Taxonomy-Inspired Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BloomWise: Enhancing Problem-Solving capabilities of Large Language\n  Models using Bloom's-Taxonomy-Inspired Prompts"
                },
                "summary": "Despite the remarkable capabilities of large language models (LLMs) across a\nrange of tasks, mathematical reasoning remains a challenging frontier.\nMotivated by the observation that humans learn more effectively when prompted\nnot what to think but how to think, we introduce BloomWise, a\ncognitively-inspired prompting technique designed to enhance LLMs' performance\non mathematical problem solving while making their solutions more explainable.\nBloomWise encourages LLMs to generate solutions - in the form of explanations -\nby progressing through a sequence of cognitive operations-from basic (e.g.,\nremembering) to more advanced reasoning skills (e.g., evaluating) - mirroring\nhow humans build understanding. The process iterates through these levels,\nhalting early if a convergence criterion is met: specifically, if two or more\nconsecutive levels yield the same answer, the solution from the earliest such\nlevel is output; otherwise, the process continues until all levels are\ncompleted. Through extensive experiments across five popular math reasoning\ndatasets, we demonstrate the effectiveness of BloomWise. We also present\ncomprehensive ablation studies to analyze the strengths of each component\nwithin our system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable capabilities of large language models (LLMs) across a\nrange of tasks, mathematical reasoning remains a challenging frontier.\nMotivated by the observation that humans learn more effectively when prompted\nnot what to think but how to think, we introduce BloomWise, a\ncognitively-inspired prompting technique designed to enhance LLMs' performance\non mathematical problem solving while making their solutions more explainable.\nBloomWise encourages LLMs to generate solutions - in the form of explanations -\nby progressing through a sequence of cognitive operations-from basic (e.g.,\nremembering) to more advanced reasoning skills (e.g., evaluating) - mirroring\nhow humans build understanding. The process iterates through these levels,\nhalting early if a convergence criterion is met: specifically, if two or more\nconsecutive levels yield the same answer, the solution from the earliest such\nlevel is output; otherwise, the process continues until all levels are\ncompleted. Through extensive experiments across five popular math reasoning\ndatasets, we demonstrate the effectiveness of BloomWise. We also present\ncomprehensive ablation studies to analyze the strengths of each component\nwithin our system."
                },
                "authors": [
                    {
                        "name": "Maria-Eleni Zoumpoulidi"
                    },
                    {
                        "name": "Georgios Paraskevopoulos"
                    },
                    {
                        "name": "Alexandros Potamianos"
                    }
                ],
                "author_detail": {
                    "name": "Alexandros Potamianos"
                },
                "author": "Alexandros Potamianos",
                "arxiv_comment": "16 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04094v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04094v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05508v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05508v1",
                "updated": "2025-08-07T15:39:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    39,
                    48,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T15:39:48Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    39,
                    48,
                    3,
                    219,
                    0
                ],
                "title": "Auto-Eval Judge: Towards a General Agentic Framework for Task Completion\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-Eval Judge: Towards a General Agentic Framework for Task Completion\n  Evaluation"
                },
                "summary": "The increasing adoption of foundation models as agents across diverse domains\nnecessitates a robust evaluation framework. Current methods, such as\nLLM-as-a-Judge, focus only on final outputs, overlooking the step-by-step\nreasoning that drives agentic decision-making. Meanwhile, existing\nAgent-as-a-Judge systems, where one agent evaluates another's task completion,\nare typically designed for narrow, domain-specific settings. To address this\ngap, we propose a generalizable, modular framework for evaluating agent task\ncompletion independent of the task domain. The framework emulates human-like\nevaluation by decomposing tasks into sub-tasks and validating each step using\navailable information, such as the agent's output and reasoning. Each module\ncontributes to a specific aspect of the evaluation process, and their outputs\nare aggregated to produce a final verdict on task completion. We validate our\nframework by evaluating the Magentic-One Actor Agent on two benchmarks, GAIA\nand BigCodeBench. Our Judge Agent predicts task success with closer agreement\nto human evaluations, achieving 4.76% and 10.52% higher alignment accuracy,\nrespectively, compared to the GPT-4o based LLM-as-a-Judge baseline. This\ndemonstrates the potential of our proposed general-purpose evaluation\nframework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing adoption of foundation models as agents across diverse domains\nnecessitates a robust evaluation framework. Current methods, such as\nLLM-as-a-Judge, focus only on final outputs, overlooking the step-by-step\nreasoning that drives agentic decision-making. Meanwhile, existing\nAgent-as-a-Judge systems, where one agent evaluates another's task completion,\nare typically designed for narrow, domain-specific settings. To address this\ngap, we propose a generalizable, modular framework for evaluating agent task\ncompletion independent of the task domain. The framework emulates human-like\nevaluation by decomposing tasks into sub-tasks and validating each step using\navailable information, such as the agent's output and reasoning. Each module\ncontributes to a specific aspect of the evaluation process, and their outputs\nare aggregated to produce a final verdict on task completion. We validate our\nframework by evaluating the Magentic-One Actor Agent on two benchmarks, GAIA\nand BigCodeBench. Our Judge Agent predicts task success with closer agreement\nto human evaluations, achieving 4.76% and 10.52% higher alignment accuracy,\nrespectively, compared to the GPT-4o based LLM-as-a-Judge baseline. This\ndemonstrates the potential of our proposed general-purpose evaluation\nframework."
                },
                "authors": [
                    {
                        "name": "Roshita Bhonsle"
                    },
                    {
                        "name": "Rishav Dutta"
                    },
                    {
                        "name": "Sneha Vavilapalli"
                    },
                    {
                        "name": "Harsh Seth"
                    },
                    {
                        "name": "Abubakarr Jaye"
                    },
                    {
                        "name": "Yapei Chang"
                    },
                    {
                        "name": "Mukund Rungta"
                    },
                    {
                        "name": "Emmanuel Aboah Boateng"
                    },
                    {
                        "name": "Sadid Hasan"
                    },
                    {
                        "name": "Ehi Nosakhare"
                    },
                    {
                        "name": "Soundar Srinivasan"
                    }
                ],
                "author_detail": {
                    "name": "Soundar Srinivasan"
                },
                "author": "Soundar Srinivasan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05508v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05508v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05503v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05503v1",
                "updated": "2025-08-07T15:36:38Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    36,
                    38,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T15:36:38Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    36,
                    38,
                    3,
                    219,
                    0
                ],
                "title": "AutoIAD: Manager-Driven Multi-Agent Collaboration for Automated\n  Industrial Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoIAD: Manager-Driven Multi-Agent Collaboration for Automated\n  Industrial Anomaly Detection"
                },
                "summary": "Industrial anomaly detection (IAD) is critical for manufacturing quality\ncontrol, but conventionally requires significant manual effort for various\napplication scenarios. This paper introduces AutoIAD, a multi-agent\ncollaboration framework, specifically designed for end-to-end automated\ndevelopment of industrial visual anomaly detection. AutoIAD leverages a\nManager-Driven central agent to orchestrate specialized sub-agents (including\nData Preparation, Data Loader, Model Designer, Trainer) and integrates a\ndomain-specific knowledge base, which intelligently handles the entire pipeline\nusing raw industrial image data to develop a trained anomaly detection model.\nWe construct a comprehensive benchmark using MVTec AD datasets to evaluate\nAutoIAD across various LLM backends. Extensive experiments demonstrate that\nAutoIAD significantly outperforms existing general-purpose agentic\ncollaboration frameworks and traditional AutoML frameworks in task completion\nrate and model performance (AUROC), while effectively mitigating issues like\nhallucination through iterative refinement. Ablation studies further confirm\nthe crucial roles of the Manager central agent and the domain knowledge base\nmodule in producing robust and high-quality IAD solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Industrial anomaly detection (IAD) is critical for manufacturing quality\ncontrol, but conventionally requires significant manual effort for various\napplication scenarios. This paper introduces AutoIAD, a multi-agent\ncollaboration framework, specifically designed for end-to-end automated\ndevelopment of industrial visual anomaly detection. AutoIAD leverages a\nManager-Driven central agent to orchestrate specialized sub-agents (including\nData Preparation, Data Loader, Model Designer, Trainer) and integrates a\ndomain-specific knowledge base, which intelligently handles the entire pipeline\nusing raw industrial image data to develop a trained anomaly detection model.\nWe construct a comprehensive benchmark using MVTec AD datasets to evaluate\nAutoIAD across various LLM backends. Extensive experiments demonstrate that\nAutoIAD significantly outperforms existing general-purpose agentic\ncollaboration frameworks and traditional AutoML frameworks in task completion\nrate and model performance (AUROC), while effectively mitigating issues like\nhallucination through iterative refinement. Ablation studies further confirm\nthe crucial roles of the Manager central agent and the domain knowledge base\nmodule in producing robust and high-quality IAD solutions."
                },
                "authors": [
                    {
                        "name": "Dongwei Ji"
                    },
                    {
                        "name": "Bingzhang Hu"
                    },
                    {
                        "name": "Yi Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yi Zhou"
                },
                "author": "Yi Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05503v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05503v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05498v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05498v1",
                "updated": "2025-08-07T15:34:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    34,
                    41,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T15:34:41Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    34,
                    41,
                    3,
                    219,
                    0
                ],
                "title": "GRAIL:Learning to Interact with Large Knowledge Graphs for Retrieval\n  Augmented Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GRAIL:Learning to Interact with Large Knowledge Graphs for Retrieval\n  Augmented Reasoning"
                },
                "summary": "Large Language Models (LLMs) integrated with Retrieval-Augmented Generation\n(RAG) techniques have exhibited remarkable performance across a wide range of\ndomains. However, existing RAG approaches primarily operate on unstructured\ndata and demonstrate limited capability in handling structured knowledge such\nas knowledge graphs. Meanwhile, current graph retrieval methods fundamentally\nstruggle to capture holistic graph structures while simultaneously facing\nprecision control challenges that manifest as either critical information gaps\nor excessive redundant connections, collectively undermining reasoning\nperformance. To address this challenge, we propose GRAIL: Graph-Retrieval\nAugmented Interactive Learning, a framework designed to interact with\nlarge-scale graphs for retrieval-augmented reasoning. Specifically, GRAIL\nintegrates LLM-guided random exploration with path filtering to establish a\ndata synthesis pipeline, where a fine-grained reasoning trajectory is\nautomatically generated for each task. Based on the synthesized data, we then\nemploy a two-stage training process to learn a policy that dynamically decides\nthe optimal actions at each reasoning step. The overall objective of\nprecision-conciseness balance in graph retrieval is decoupled into fine-grained\nprocess-supervised rewards to enhance data efficiency and training stability.\nIn practical deployment, GRAIL adopts an interactive retrieval paradigm,\nenabling the model to autonomously explore graph paths while dynamically\nbalancing retrieval breadth and precision. Extensive experiments have shown\nthat GRAIL achieves an average accuracy improvement of 21.01% and F1\nimprovement of 22.43% on three knowledge graph question-answering datasets. Our\nsource code and datasets is available at https://github.com/Changgeww/GRAIL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) integrated with Retrieval-Augmented Generation\n(RAG) techniques have exhibited remarkable performance across a wide range of\ndomains. However, existing RAG approaches primarily operate on unstructured\ndata and demonstrate limited capability in handling structured knowledge such\nas knowledge graphs. Meanwhile, current graph retrieval methods fundamentally\nstruggle to capture holistic graph structures while simultaneously facing\nprecision control challenges that manifest as either critical information gaps\nor excessive redundant connections, collectively undermining reasoning\nperformance. To address this challenge, we propose GRAIL: Graph-Retrieval\nAugmented Interactive Learning, a framework designed to interact with\nlarge-scale graphs for retrieval-augmented reasoning. Specifically, GRAIL\nintegrates LLM-guided random exploration with path filtering to establish a\ndata synthesis pipeline, where a fine-grained reasoning trajectory is\nautomatically generated for each task. Based on the synthesized data, we then\nemploy a two-stage training process to learn a policy that dynamically decides\nthe optimal actions at each reasoning step. The overall objective of\nprecision-conciseness balance in graph retrieval is decoupled into fine-grained\nprocess-supervised rewards to enhance data efficiency and training stability.\nIn practical deployment, GRAIL adopts an interactive retrieval paradigm,\nenabling the model to autonomously explore graph paths while dynamically\nbalancing retrieval breadth and precision. Extensive experiments have shown\nthat GRAIL achieves an average accuracy improvement of 21.01% and F1\nimprovement of 22.43% on three knowledge graph question-answering datasets. Our\nsource code and datasets is available at https://github.com/Changgeww/GRAIL."
                },
                "authors": [
                    {
                        "name": "Ge Chang"
                    },
                    {
                        "name": "Jinbo Su"
                    },
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Pengfei Yang"
                    },
                    {
                        "name": "Yuhao Shang"
                    },
                    {
                        "name": "Huiwen Zheng"
                    },
                    {
                        "name": "Hongli Ma"
                    },
                    {
                        "name": "Yan Liang"
                    },
                    {
                        "name": "Yuanchun Li"
                    },
                    {
                        "name": "Yunxin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yunxin Liu"
                },
                "author": "Yunxin Liu",
                "arxiv_comment": "9 pages,3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05498v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05498v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05496v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05496v1",
                "updated": "2025-08-07T15:34:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    34,
                    6,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T15:34:06Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    34,
                    6,
                    3,
                    219,
                    0
                ],
                "title": "InfiAlign: A Scalable and Sample-Efficient Framework for Aligning LLMs\n  to Enhance Reasoning Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiAlign: A Scalable and Sample-Efficient Framework for Aligning LLMs\n  to Enhance Reasoning Capabilities"
                },
                "summary": "Large language models (LLMs) have exhibited impressive reasoning abilities on\na wide range of complex tasks. However, enhancing these capabilities through\npost-training remains resource intensive, particularly in terms of data and\ncomputational cost. Although recent efforts have sought to improve sample\nefficiency through selective data curation, existing methods often rely on\nheuristic or task-specific strategies that hinder scalability. In this work, we\nintroduce InfiAlign, a scalable and sample-efficient post-training framework\nthat integrates supervised fine-tuning (SFT) with Direct Preference\nOptimization (DPO) to align LLMs for enhanced reasoning. At the core of\nInfiAlign is a robust data selection pipeline that automatically curates\nhigh-quality alignment data from open-source reasoning datasets using\nmultidimensional quality metrics. This pipeline enables significant performance\ngains while drastically reducing data requirements and remains extensible to\nnew data sources. When applied to the Qwen2.5-Math-7B-Base model, our SFT model\nachieves performance on par with DeepSeek-R1-Distill-Qwen-7B, while using only\napproximately 12% of the training data, and demonstrates strong generalization\nacross diverse reasoning tasks. Additional improvements are obtained through\nthe application of DPO, with particularly notable gains in mathematical\nreasoning tasks. The model achieves an average improvement of 3.89% on AIME\n24/25 benchmarks. Our results highlight the effectiveness of combining\nprincipled data selection with full-stage post-training, offering a practical\nsolution for aligning large reasoning models in a scalable and data-efficient\nmanner. The model checkpoints are available at\nhttps://huggingface.co/InfiX-ai/InfiAlign-Qwen-7B-SFT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have exhibited impressive reasoning abilities on\na wide range of complex tasks. However, enhancing these capabilities through\npost-training remains resource intensive, particularly in terms of data and\ncomputational cost. Although recent efforts have sought to improve sample\nefficiency through selective data curation, existing methods often rely on\nheuristic or task-specific strategies that hinder scalability. In this work, we\nintroduce InfiAlign, a scalable and sample-efficient post-training framework\nthat integrates supervised fine-tuning (SFT) with Direct Preference\nOptimization (DPO) to align LLMs for enhanced reasoning. At the core of\nInfiAlign is a robust data selection pipeline that automatically curates\nhigh-quality alignment data from open-source reasoning datasets using\nmultidimensional quality metrics. This pipeline enables significant performance\ngains while drastically reducing data requirements and remains extensible to\nnew data sources. When applied to the Qwen2.5-Math-7B-Base model, our SFT model\nachieves performance on par with DeepSeek-R1-Distill-Qwen-7B, while using only\napproximately 12% of the training data, and demonstrates strong generalization\nacross diverse reasoning tasks. Additional improvements are obtained through\nthe application of DPO, with particularly notable gains in mathematical\nreasoning tasks. The model achieves an average improvement of 3.89% on AIME\n24/25 benchmarks. Our results highlight the effectiveness of combining\nprincipled data selection with full-stage post-training, offering a practical\nsolution for aligning large reasoning models in a scalable and data-efficient\nmanner. The model checkpoints are available at\nhttps://huggingface.co/InfiX-ai/InfiAlign-Qwen-7B-SFT."
                },
                "authors": [
                    {
                        "name": "Shuo Cai"
                    },
                    {
                        "name": "Su Lu"
                    },
                    {
                        "name": "Qi Zhou"
                    },
                    {
                        "name": "Kejing Yang"
                    },
                    {
                        "name": "Zhijie Sang"
                    },
                    {
                        "name": "Congkai Xie"
                    },
                    {
                        "name": "Hongxia Yang"
                    }
                ],
                "author_detail": {
                    "name": "Hongxia Yang"
                },
                "author": "Hongxia Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05496v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05496v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05492v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05492v1",
                "updated": "2025-08-07T15:28:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    28,
                    34,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T15:28:34Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    28,
                    34,
                    3,
                    219,
                    0
                ],
                "title": "MoMA: A Mixture-of-Multimodal-Agents Architecture for Enhancing Clinical\n  Prediction Modelling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoMA: A Mixture-of-Multimodal-Agents Architecture for Enhancing Clinical\n  Prediction Modelling"
                },
                "summary": "Multimodal electronic health record (EHR) data provide richer, complementary\ninsights into patient health compared to single-modality data. However,\neffectively integrating diverse data modalities for clinical prediction\nmodeling remains challenging due to the substantial data requirements. We\nintroduce a novel architecture, Mixture-of-Multimodal-Agents (MoMA), designed\nto leverage multiple large language model (LLM) agents for clinical prediction\ntasks using multimodal EHR data. MoMA employs specialized LLM agents\n(\"specialist agents\") to convert non-textual modalities, such as medical images\nand laboratory results, into structured textual summaries. These summaries,\ntogether with clinical notes, are combined by another LLM (\"aggregator agent\")\nto generate a unified multimodal summary, which is then used by a third LLM\n(\"predictor agent\") to produce clinical predictions. Evaluating MoMA on three\nprediction tasks using real-world datasets with different modality combinations\nand prediction settings, MoMA outperforms current state-of-the-art methods,\nhighlighting its enhanced accuracy and flexibility across various tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal electronic health record (EHR) data provide richer, complementary\ninsights into patient health compared to single-modality data. However,\neffectively integrating diverse data modalities for clinical prediction\nmodeling remains challenging due to the substantial data requirements. We\nintroduce a novel architecture, Mixture-of-Multimodal-Agents (MoMA), designed\nto leverage multiple large language model (LLM) agents for clinical prediction\ntasks using multimodal EHR data. MoMA employs specialized LLM agents\n(\"specialist agents\") to convert non-textual modalities, such as medical images\nand laboratory results, into structured textual summaries. These summaries,\ntogether with clinical notes, are combined by another LLM (\"aggregator agent\")\nto generate a unified multimodal summary, which is then used by a third LLM\n(\"predictor agent\") to produce clinical predictions. Evaluating MoMA on three\nprediction tasks using real-world datasets with different modality combinations\nand prediction settings, MoMA outperforms current state-of-the-art methods,\nhighlighting its enhanced accuracy and flexibility across various tasks."
                },
                "authors": [
                    {
                        "name": "Jifan Gao"
                    },
                    {
                        "name": "Mahmudur Rahman"
                    },
                    {
                        "name": "John Caskey"
                    },
                    {
                        "name": "Madeline Oguss"
                    },
                    {
                        "name": "Ann O'Rourke"
                    },
                    {
                        "name": "Randy Brown"
                    },
                    {
                        "name": "Anne Stey"
                    },
                    {
                        "name": "Anoop Mayampurath"
                    },
                    {
                        "name": "Matthew M. Churpek"
                    },
                    {
                        "name": "Guanhua Chen"
                    },
                    {
                        "name": "Majid Afshar"
                    }
                ],
                "author_detail": {
                    "name": "Majid Afshar"
                },
                "author": "Majid Afshar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05492v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05492v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04610v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04610v2",
                "updated": "2025-08-07T15:23:54Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    23,
                    54,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-06T16:29:59Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    16,
                    29,
                    59,
                    2,
                    218,
                    0
                ],
                "title": "Neuromorphic Cybersecurity with Semi-supervised Lifelong Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuromorphic Cybersecurity with Semi-supervised Lifelong Learning"
                },
                "summary": "Inspired by the brain's hierarchical processing and energy efficiency, this\npaper presents a Spiking Neural Network (SNN) architecture for lifelong Network\nIntrusion Detection System (NIDS). The proposed system first employs an\nefficient static SNN to identify potential intrusions, which then activates an\nadaptive dynamic SNN responsible for classifying the specific attack type.\nMimicking biological adaptation, the dynamic classifier utilizes Grow When\nRequired (GWR)-inspired structural plasticity and a novel Adaptive\nSpike-Timing-Dependent Plasticity (Ad-STDP) learning rule. These bio-plausible\nmechanisms enable the network to learn new threats incrementally while\npreserving existing knowledge. Tested on the UNSW-NB15 benchmark in a continual\nlearning setting, the architecture demonstrates robust adaptation, reduced\ncatastrophic forgetting, and achieves $85.3$\\% overall accuracy. Furthermore,\nsimulations using the Intel Lava framework confirm high operational sparsity,\nhighlighting the potential for low-power deployment on neuromorphic hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inspired by the brain's hierarchical processing and energy efficiency, this\npaper presents a Spiking Neural Network (SNN) architecture for lifelong Network\nIntrusion Detection System (NIDS). The proposed system first employs an\nefficient static SNN to identify potential intrusions, which then activates an\nadaptive dynamic SNN responsible for classifying the specific attack type.\nMimicking biological adaptation, the dynamic classifier utilizes Grow When\nRequired (GWR)-inspired structural plasticity and a novel Adaptive\nSpike-Timing-Dependent Plasticity (Ad-STDP) learning rule. These bio-plausible\nmechanisms enable the network to learn new threats incrementally while\npreserving existing knowledge. Tested on the UNSW-NB15 benchmark in a continual\nlearning setting, the architecture demonstrates robust adaptation, reduced\ncatastrophic forgetting, and achieves $85.3$\\% overall accuracy. Furthermore,\nsimulations using the Intel Lava framework confirm high operational sparsity,\nhighlighting the potential for low-power deployment on neuromorphic hardware."
                },
                "authors": [
                    {
                        "name": "Md Zesun Ahmed Mia"
                    },
                    {
                        "name": "Malyaban Bal"
                    },
                    {
                        "name": "Sen Lu"
                    },
                    {
                        "name": "George M. Nishibuchi"
                    },
                    {
                        "name": "Suhas Chelian"
                    },
                    {
                        "name": "Srini Vasan"
                    },
                    {
                        "name": "Abhronil Sengupta"
                    }
                ],
                "author_detail": {
                    "name": "Abhronil Sengupta"
                },
                "author": "Abhronil Sengupta",
                "arxiv_comment": "Accepted at ACM International Conference on Neuromorphic Systems\n  (ICONS) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04610v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04610v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08754v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08754v5",
                "updated": "2025-08-07T15:23:38Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    23,
                    38,
                    3,
                    219,
                    0
                ],
                "published": "2025-03-28T15:49:52Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    15,
                    49,
                    52,
                    4,
                    87,
                    0
                ],
                "title": "Towards Personalized Conversational Sales Agents: Contextual User\n  Profiling for Strategic Action",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Personalized Conversational Sales Agents: Contextual User\n  Profiling for Strategic Action"
                },
                "summary": "Conversational Recommender Systems (CRSs)aim to engage users in dialogue to\nprovide tailored recommendations. While traditional CRSs focus on eliciting\npreferences and retrieving items, real-world e-commerce interactions involve\nmore complex decision-making, where users consider multiple factors beyond\nsimple attributes. To capture this complexity, we introduce Conversational\nSales (CSALES), a novel task that integrates preference elicitation,\nrecommendation, and persuasion within a unified conversational framework. To\nsupport realistic and systematic evaluation, we present CSUSER, an evaluation\nprotocol with LLM-based user simulator grounded in real-world behavioral data\nby modeling fine-grained user profiles for personalized interaction. We also\npropose CSI, a conversational sales agent that proactively infers contextual\nuser profiles and strategically selects actions through conversation.\nComprehensive experiments show that CSI significantly improves both\nrecommendation success and persuasive effectiveness across diverse user\nprofiles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational Recommender Systems (CRSs)aim to engage users in dialogue to\nprovide tailored recommendations. While traditional CRSs focus on eliciting\npreferences and retrieving items, real-world e-commerce interactions involve\nmore complex decision-making, where users consider multiple factors beyond\nsimple attributes. To capture this complexity, we introduce Conversational\nSales (CSALES), a novel task that integrates preference elicitation,\nrecommendation, and persuasion within a unified conversational framework. To\nsupport realistic and systematic evaluation, we present CSUSER, an evaluation\nprotocol with LLM-based user simulator grounded in real-world behavioral data\nby modeling fine-grained user profiles for personalized interaction. We also\npropose CSI, a conversational sales agent that proactively infers contextual\nuser profiles and strategically selects actions through conversation.\nComprehensive experiments show that CSI significantly improves both\nrecommendation success and persuasive effectiveness across diverse user\nprofiles."
                },
                "authors": [
                    {
                        "name": "Tongyoung Kim"
                    },
                    {
                        "name": "Jeongeun Lee"
                    },
                    {
                        "name": "Soojin Yoon"
                    },
                    {
                        "name": "Sunghwan Kim"
                    },
                    {
                        "name": "Dongha Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongha Lee"
                },
                "author": "Dongha Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08754v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08754v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05474v1",
                "updated": "2025-08-07T15:13:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    13,
                    55,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T15:13:55Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    13,
                    55,
                    3,
                    219,
                    0
                ],
                "title": "Can Large Language Models Generate Effective Datasets for Emotion\n  Recognition in Conversations?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Large Language Models Generate Effective Datasets for Emotion\n  Recognition in Conversations?"
                },
                "summary": "Emotion recognition in conversations (ERC) focuses on identifying emotion\nshifts within interactions, representing a significant step toward advancing\nmachine intelligence. However, ERC data remains scarce, and existing datasets\nface numerous challenges due to their highly biased sources and the inherent\nsubjectivity of soft labels. Even though Large Language Models (LLMs) have\ndemonstrated their quality in many affective tasks, they are typically\nexpensive to train, and their application to ERC tasks--particularly in data\ngeneration--remains limited. To address these challenges, we employ a small,\nresource-efficient, and general-purpose LLM to synthesize ERC datasets with\ndiverse properties, supplementing the three most widely used ERC benchmarks. We\ngenerate six novel datasets, with two tailored to enhance each benchmark. We\nevaluate the utility of these datasets to (1) supplement existing datasets for\nERC classification, and (2) analyze the effects of label imbalance in ERC. Our\nexperimental results indicate that ERC classifier models trained on the\ngenerated datasets exhibit strong robustness and consistently achieve\nstatistically significant performance improvements on existing ERC benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emotion recognition in conversations (ERC) focuses on identifying emotion\nshifts within interactions, representing a significant step toward advancing\nmachine intelligence. However, ERC data remains scarce, and existing datasets\nface numerous challenges due to their highly biased sources and the inherent\nsubjectivity of soft labels. Even though Large Language Models (LLMs) have\ndemonstrated their quality in many affective tasks, they are typically\nexpensive to train, and their application to ERC tasks--particularly in data\ngeneration--remains limited. To address these challenges, we employ a small,\nresource-efficient, and general-purpose LLM to synthesize ERC datasets with\ndiverse properties, supplementing the three most widely used ERC benchmarks. We\ngenerate six novel datasets, with two tailored to enhance each benchmark. We\nevaluate the utility of these datasets to (1) supplement existing datasets for\nERC classification, and (2) analyze the effects of label imbalance in ERC. Our\nexperimental results indicate that ERC classifier models trained on the\ngenerated datasets exhibit strong robustness and consistently achieve\nstatistically significant performance improvements on existing ERC benchmarks."
                },
                "authors": [
                    {
                        "name": "Burak Can Kaplan"
                    },
                    {
                        "name": "Hugo Cesar De Castro Carneiro"
                    },
                    {
                        "name": "Stefan Wermter"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Wermter"
                },
                "author": "Stefan Wermter",
                "arxiv_comment": "8 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05473v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05473v1",
                "updated": "2025-08-07T15:13:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    13,
                    42,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T15:13:42Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    13,
                    42,
                    3,
                    219,
                    0
                ],
                "title": "Embedding Alignment in Code Generation for Audio",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embedding Alignment in Code Generation for Audio"
                },
                "summary": "LLM-powered code generation has the potential to revolutionize creative\ncoding endeavors, such as live-coding, by enabling users to focus on structural\nmotifs over syntactic details. In such domains, when prompting an LLM, users\nmay benefit from considering multiple varied code candidates to better realize\ntheir musical intentions. Code generation models, however, struggle to present\nunique and diverse code candidates, with no direct insight into the code's\naudio output. To better establish a relationship between code candidates and\nproduced audio, we investigate the topology of the mapping between code and\naudio embedding spaces. We find that code and audio embeddings do not exhibit a\nsimple linear relationship, but supplement this with a constructed predictive\nmodel that shows an embedding alignment map could be learned. Supplementing the\naim for musically diverse output, we present a model that given code predicts\noutput audio embedding, constructing a code-audio embedding alignment map.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-powered code generation has the potential to revolutionize creative\ncoding endeavors, such as live-coding, by enabling users to focus on structural\nmotifs over syntactic details. In such domains, when prompting an LLM, users\nmay benefit from considering multiple varied code candidates to better realize\ntheir musical intentions. Code generation models, however, struggle to present\nunique and diverse code candidates, with no direct insight into the code's\naudio output. To better establish a relationship between code candidates and\nproduced audio, we investigate the topology of the mapping between code and\naudio embedding spaces. We find that code and audio embeddings do not exhibit a\nsimple linear relationship, but supplement this with a constructed predictive\nmodel that shows an embedding alignment map could be learned. Supplementing the\naim for musically diverse output, we present a model that given code predicts\noutput audio embedding, constructing a code-audio embedding alignment map."
                },
                "authors": [
                    {
                        "name": "Sam Kouteili"
                    },
                    {
                        "name": "Hiren Madhu"
                    },
                    {
                        "name": "George Typaldos"
                    },
                    {
                        "name": "Mark Santolucito"
                    }
                ],
                "author_detail": {
                    "name": "Mark Santolucito"
                },
                "author": "Mark Santolucito",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05473v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05473v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05470v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05470v1",
                "updated": "2025-08-07T15:11:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    11,
                    48,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T15:11:48Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    11,
                    48,
                    3,
                    219,
                    0
                ],
                "title": "Rethinking Creativity Evaluation: A Critical Analysis of Existing\n  Creativity Evaluations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Creativity Evaluation: A Critical Analysis of Existing\n  Creativity Evaluations"
                },
                "summary": "We systematically examine, analyze, and compare representative creativity\nmeasures--creativity index, perplexity, syntactic templates, and\nLLM-as-a-Judge--across diverse creative domains, including creative writing,\nunconventional problem-solving, and research ideation. Our analyses reveal that\nthese metrics exhibit limited consistency, capturing different dimensions of\ncreativity. We highlight key limitations, including the creativity index's\nfocus on lexical diversity, perplexity's sensitivity to model confidence, and\nsyntactic templates' inability to capture conceptual creativity. Additionally,\nLLM-as-a-Judge shows instability and bias. Our findings underscore the need for\nmore robust, generalizable evaluation frameworks that better align with human\njudgments of creativity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We systematically examine, analyze, and compare representative creativity\nmeasures--creativity index, perplexity, syntactic templates, and\nLLM-as-a-Judge--across diverse creative domains, including creative writing,\nunconventional problem-solving, and research ideation. Our analyses reveal that\nthese metrics exhibit limited consistency, capturing different dimensions of\ncreativity. We highlight key limitations, including the creativity index's\nfocus on lexical diversity, perplexity's sensitivity to model confidence, and\nsyntactic templates' inability to capture conceptual creativity. Additionally,\nLLM-as-a-Judge shows instability and bias. Our findings underscore the need for\nmore robust, generalizable evaluation frameworks that better align with human\njudgments of creativity."
                },
                "authors": [
                    {
                        "name": "Li-Chun Lu"
                    },
                    {
                        "name": "Miri Liu"
                    },
                    {
                        "name": "Pin-Chun Lu"
                    },
                    {
                        "name": "Yufei Tian"
                    },
                    {
                        "name": "Shao-Hua Sun"
                    },
                    {
                        "name": "Nanyun Peng"
                    }
                ],
                "author_detail": {
                    "name": "Nanyun Peng"
                },
                "author": "Nanyun Peng",
                "arxiv_comment": "15 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05470v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05470v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05469v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05469v1",
                "updated": "2025-08-07T15:11:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    11,
                    43,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T15:11:43Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    11,
                    43,
                    3,
                    219,
                    0
                ],
                "title": "Let's Measure Information Step-by-Step: LLM-Based Evaluation Beyond\n  Vibes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let's Measure Information Step-by-Step: LLM-Based Evaluation Beyond\n  Vibes"
                },
                "summary": "We develop mechanisms for evaluating AI systems without ground truth by\nexploiting a connection between gaming resistance and output quality. The data\nprocessing inequality ensures post-hoc attempts to game a metric degrades both\ninformation content and task performance. We prove that f-mutual information\nmeasures are the unique gaming resistant mechanisms under natural conditions,\nwith the overseer acting as an agent. While Shannon mutual information faces\nexponential sample complexity, bounded measures like total variation distance\nremain tractable. Empirically, across ten domains from translation to peer\nreview, all information-theoretic mechanisms achieve perfect discrimination (d\n> 0.5) between faithful and strategic agents. In contrast, LLM judges exhibit\nsystematic evaluation inversion, preferring fabricated content over accurate\nsummaries. Our mechanisms show 10-100x better robustness to adversarial\nmanipulation than current practices. We also find performance follows an\ninverted-U curve with compression ratio, peaking at 10:1 where agent responses\nexhibit optimal information diversity (3 effective dimensions), giving a\nbias-variance perspective on when our approach is expected to be most\neffective.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop mechanisms for evaluating AI systems without ground truth by\nexploiting a connection between gaming resistance and output quality. The data\nprocessing inequality ensures post-hoc attempts to game a metric degrades both\ninformation content and task performance. We prove that f-mutual information\nmeasures are the unique gaming resistant mechanisms under natural conditions,\nwith the overseer acting as an agent. While Shannon mutual information faces\nexponential sample complexity, bounded measures like total variation distance\nremain tractable. Empirically, across ten domains from translation to peer\nreview, all information-theoretic mechanisms achieve perfect discrimination (d\n> 0.5) between faithful and strategic agents. In contrast, LLM judges exhibit\nsystematic evaluation inversion, preferring fabricated content over accurate\nsummaries. Our mechanisms show 10-100x better robustness to adversarial\nmanipulation than current practices. We also find performance follows an\ninverted-U curve with compression ratio, peaking at 10:1 where agent responses\nexhibit optimal information diversity (3 effective dimensions), giving a\nbias-variance perspective on when our approach is expected to be most\neffective."
                },
                "authors": [
                    {
                        "name": "Zachary Robertson"
                    },
                    {
                        "name": "Sanmi Koyejo"
                    }
                ],
                "author_detail": {
                    "name": "Sanmi Koyejo"
                },
                "author": "Sanmi Koyejo",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05469v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05469v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05468v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05468v1",
                "updated": "2025-08-07T15:11:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    11,
                    17,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T15:11:17Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    11,
                    17,
                    3,
                    219,
                    0
                ],
                "title": "TASE: Token Awareness and Structured Evaluation for Multilingual\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TASE: Token Awareness and Structured Evaluation for Multilingual\n  Language Models"
                },
                "summary": "While large language models (LLMs) have demonstrated remarkable performance\non high-level semantic tasks, they often struggle with fine-grained,\ntoken-level understanding and structural reasoning--capabilities that are\nessential for applications requiring precision and control. We introduce TASE,\na comprehensive benchmark designed to evaluate LLMs' ability to perceive and\nreason about token-level information across languages. TASE covers 10 tasks\nunder two core categories: token awareness and structural understanding,\nspanning Chinese, English, and Korean, with a 35,927-instance evaluation set\nand a scalable synthetic data generation pipeline for training. Tasks include\ncharacter counting, token alignment, syntactic structure parsing, and length\nconstraint satisfaction. We evaluate over 30 leading commercial and open-source\nLLMs, including O3, Claude 4, Gemini 2.5 Pro, and DeepSeek-R1, and train a\ncustom Qwen2.5-14B model using the GRPO training method. Results show that\nhuman performance significantly outpaces current LLMs, revealing persistent\nweaknesses in token-level reasoning. TASE sheds light on these limitations and\nprovides a new diagnostic lens for future improvements in low-level language\nunderstanding and cross-lingual generalization. Our code and dataset are\npublicly available at https://github.com/cyzcz/Tase .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have demonstrated remarkable performance\non high-level semantic tasks, they often struggle with fine-grained,\ntoken-level understanding and structural reasoning--capabilities that are\nessential for applications requiring precision and control. We introduce TASE,\na comprehensive benchmark designed to evaluate LLMs' ability to perceive and\nreason about token-level information across languages. TASE covers 10 tasks\nunder two core categories: token awareness and structural understanding,\nspanning Chinese, English, and Korean, with a 35,927-instance evaluation set\nand a scalable synthetic data generation pipeline for training. Tasks include\ncharacter counting, token alignment, syntactic structure parsing, and length\nconstraint satisfaction. We evaluate over 30 leading commercial and open-source\nLLMs, including O3, Claude 4, Gemini 2.5 Pro, and DeepSeek-R1, and train a\ncustom Qwen2.5-14B model using the GRPO training method. Results show that\nhuman performance significantly outpaces current LLMs, revealing persistent\nweaknesses in token-level reasoning. TASE sheds light on these limitations and\nprovides a new diagnostic lens for future improvements in low-level language\nunderstanding and cross-lingual generalization. Our code and dataset are\npublicly available at https://github.com/cyzcz/Tase ."
                },
                "authors": [
                    {
                        "name": "Chenzhuo Zhao"
                    },
                    {
                        "name": "Xinda Wang"
                    },
                    {
                        "name": "Yue Huang"
                    },
                    {
                        "name": "Junting Lu"
                    },
                    {
                        "name": "Ziqian Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ziqian Liu"
                },
                "author": "Ziqian Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05468v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05468v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01545v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01545v2",
                "updated": "2025-08-07T15:04:14Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    4,
                    14,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-03T01:58:38Z",
                "published_parsed": [
                    2025,
                    8,
                    3,
                    1,
                    58,
                    38,
                    6,
                    215,
                    0
                ],
                "title": "Getting out of the Big-Muddy: Escalation of Commitment in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Getting out of the Big-Muddy: Escalation of Commitment in LLMs"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed in autonomous\ndecision-making roles across high-stakes domains. However, since models are\ntrained on human-generated data, they may inherit cognitive biases that\nsystematically distort human judgment, including escalation of commitment,\nwhere decision-makers continue investing in failing courses of action due to\nprior investment. Understanding when LLMs exhibit such biases presents a unique\nchallenge. While these biases are well-documented in humans, it remains unclear\nwhether they manifest consistently in LLMs or require specific triggering\nconditions. This paper investigates this question using a two-stage investment\ntask across four experimental conditions: model as investor, model as advisor,\nmulti-agent deliberation, and compound pressure scenario. Across N = 6,500\ntrials, we find that bias manifestation in LLMs is highly context-dependent. In\nindividual decision-making contexts (Studies 1-2, N = 4,000), LLMs demonstrate\nstrong rational cost-benefit logic with minimal escalation of commitment.\nHowever, multi-agent deliberation reveals a striking hierarchy effect (Study 3,\nN = 500): while asymmetrical hierarchies show moderate escalation rates\n(46.2%), symmetrical peer-based decision-making produces near-universal\nescalation (99.2%). Similarly, when subjected to compound organizational and\npersonal pressures (Study 4, N = 2,000), models exhibit high degrees of\nescalation of commitment (68.95% average allocation to failing divisions).\nThese findings reveal that LLM bias manifestation depends critically on social\nand organizational context rather than being inherent, with significant\nimplications for the deployment of multi-agent systems and unsupervised\noperations where such conditions may emerge naturally.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in autonomous\ndecision-making roles across high-stakes domains. However, since models are\ntrained on human-generated data, they may inherit cognitive biases that\nsystematically distort human judgment, including escalation of commitment,\nwhere decision-makers continue investing in failing courses of action due to\nprior investment. Understanding when LLMs exhibit such biases presents a unique\nchallenge. While these biases are well-documented in humans, it remains unclear\nwhether they manifest consistently in LLMs or require specific triggering\nconditions. This paper investigates this question using a two-stage investment\ntask across four experimental conditions: model as investor, model as advisor,\nmulti-agent deliberation, and compound pressure scenario. Across N = 6,500\ntrials, we find that bias manifestation in LLMs is highly context-dependent. In\nindividual decision-making contexts (Studies 1-2, N = 4,000), LLMs demonstrate\nstrong rational cost-benefit logic with minimal escalation of commitment.\nHowever, multi-agent deliberation reveals a striking hierarchy effect (Study 3,\nN = 500): while asymmetrical hierarchies show moderate escalation rates\n(46.2%), symmetrical peer-based decision-making produces near-universal\nescalation (99.2%). Similarly, when subjected to compound organizational and\npersonal pressures (Study 4, N = 2,000), models exhibit high degrees of\nescalation of commitment (68.95% average allocation to failing divisions).\nThese findings reveal that LLM bias manifestation depends critically on social\nand organizational context rather than being inherent, with significant\nimplications for the deployment of multi-agent systems and unsupervised\noperations where such conditions may emerge naturally."
                },
                "authors": [
                    {
                        "name": "Emilio Barkett"
                    },
                    {
                        "name": "Olivia Long"
                    },
                    {
                        "name": "Paul Krger"
                    }
                ],
                "author_detail": {
                    "name": "Paul Krger"
                },
                "author": "Paul Krger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01545v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01545v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05464v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05464v1",
                "updated": "2025-08-07T15:03:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    3,
                    39,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T15:03:39Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    15,
                    3,
                    39,
                    3,
                    219,
                    0
                ],
                "title": "Bench-2-CoP: Can We Trust Benchmarking for EU AI Compliance?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bench-2-CoP: Can We Trust Benchmarking for EU AI Compliance?"
                },
                "summary": "The rapid advancement of General Purpose AI (GPAI) models necessitates robust\nevaluation frameworks, especially with emerging regulations like the EU AI Act\nand its associated Code of Practice (CoP). Current AI evaluation practices\ndepend heavily on established benchmarks, but these tools were not designed to\nmeasure the systemic risks that are the focus of the new regulatory landscape.\nThis research addresses the urgent need to quantify this \"benchmark-regulation\ngap.\" We introduce Bench-2-CoP, a novel, systematic framework that uses\nvalidated LLM-as-judge analysis to map the coverage of 194,955 questions from\nwidely-used benchmarks against the EU AI Act's taxonomy of model capabilities\nand propensities. Our findings reveal a profound misalignment: the evaluation\necosystem is overwhelmingly focused on a narrow set of behavioral propensities,\nsuch as \"Tendency to hallucinate\" (53.7% of the corpus) and \"Discriminatory\nbias\" (28.9%), while critical functional capabilities are dangerously\nneglected. Crucially, capabilities central to loss-of-control scenarios,\nincluding evading human oversight, self-replication, and autonomous AI\ndevelopment, receive zero coverage in the entire benchmark corpus. This\ntranslates to a near-total evaluation gap for systemic risks like \"Loss of\nControl\" (0.4% coverage) and \"Cyber Offence\" (0.8% coverage). This study\nprovides the first comprehensive, quantitative analysis of this gap, offering\ncritical insights for policymakers to refine the CoP and for developers to\nbuild the next generation of evaluation tools, ultimately fostering safer and\nmore compliant AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of General Purpose AI (GPAI) models necessitates robust\nevaluation frameworks, especially with emerging regulations like the EU AI Act\nand its associated Code of Practice (CoP). Current AI evaluation practices\ndepend heavily on established benchmarks, but these tools were not designed to\nmeasure the systemic risks that are the focus of the new regulatory landscape.\nThis research addresses the urgent need to quantify this \"benchmark-regulation\ngap.\" We introduce Bench-2-CoP, a novel, systematic framework that uses\nvalidated LLM-as-judge analysis to map the coverage of 194,955 questions from\nwidely-used benchmarks against the EU AI Act's taxonomy of model capabilities\nand propensities. Our findings reveal a profound misalignment: the evaluation\necosystem is overwhelmingly focused on a narrow set of behavioral propensities,\nsuch as \"Tendency to hallucinate\" (53.7% of the corpus) and \"Discriminatory\nbias\" (28.9%), while critical functional capabilities are dangerously\nneglected. Crucially, capabilities central to loss-of-control scenarios,\nincluding evading human oversight, self-replication, and autonomous AI\ndevelopment, receive zero coverage in the entire benchmark corpus. This\ntranslates to a near-total evaluation gap for systemic risks like \"Loss of\nControl\" (0.4% coverage) and \"Cyber Offence\" (0.8% coverage). This study\nprovides the first comprehensive, quantitative analysis of this gap, offering\ncritical insights for policymakers to refine the CoP and for developers to\nbuild the next generation of evaluation tools, ultimately fostering safer and\nmore compliant AI."
                },
                "authors": [
                    {
                        "name": "Matteo Prandi"
                    },
                    {
                        "name": "Vincenzo Suriani"
                    },
                    {
                        "name": "Federico Pierucci"
                    },
                    {
                        "name": "Marcello Galisai"
                    },
                    {
                        "name": "Daniele Nardi"
                    },
                    {
                        "name": "Piercosma Bisconti"
                    }
                ],
                "author_detail": {
                    "name": "Piercosma Bisconti"
                },
                "author": "Piercosma Bisconti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05464v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05464v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19168v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19168v2",
                "updated": "2025-08-07T14:58:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    14,
                    58,
                    9,
                    3,
                    219,
                    0
                ],
                "published": "2025-03-24T21:43:47Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    21,
                    43,
                    47,
                    0,
                    83,
                    0
                ],
                "title": "Language Model Uncertainty Quantification with Attention Chain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Model Uncertainty Quantification with Attention Chain"
                },
                "summary": "Accurately quantifying a large language model's (LLM) predictive uncertainty\nis crucial for judging the reliability of its answers. While most existing\nresearch focuses on short, directly answerable questions with closed-form\noutputs (e.g., multiple-choice), involving intermediate reasoning steps in LLM\nresponses is increasingly important. This added complexity complicates\nuncertainty quantification (UQ) because the probabilities assigned to answer\ntokens are conditioned on a vast space of preceding reasoning tokens. Direct\nmarginalization is infeasible, and the dependency inflates probability\nestimates, causing overconfidence in UQ. To address this, we propose UQAC, an\nefficient method that narrows the reasoning space to a tractable size for\nmarginalization. UQAC iteratively constructs an \"attention chain\" of tokens\ndeemed \"semantically crucial\" to the final answer via a backtracking procedure.\nStarting from the answer tokens, it uses attention weights to identify the most\ninfluential predecessors, then iterates this process until reaching the input\ntokens. The resulting chain is further refined with similarity filtering and\nprobability thresholding, which reduce the reasoning space, facilitating the\napproximation of the marginal answer token probabilities. We validate UQAC on\nmultiple reasoning benchmarks with advanced open-source LLMs, demonstrating\nthat it consistently delivers reliable UQ estimates with high computational\nefficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately quantifying a large language model's (LLM) predictive uncertainty\nis crucial for judging the reliability of its answers. While most existing\nresearch focuses on short, directly answerable questions with closed-form\noutputs (e.g., multiple-choice), involving intermediate reasoning steps in LLM\nresponses is increasingly important. This added complexity complicates\nuncertainty quantification (UQ) because the probabilities assigned to answer\ntokens are conditioned on a vast space of preceding reasoning tokens. Direct\nmarginalization is infeasible, and the dependency inflates probability\nestimates, causing overconfidence in UQ. To address this, we propose UQAC, an\nefficient method that narrows the reasoning space to a tractable size for\nmarginalization. UQAC iteratively constructs an \"attention chain\" of tokens\ndeemed \"semantically crucial\" to the final answer via a backtracking procedure.\nStarting from the answer tokens, it uses attention weights to identify the most\ninfluential predecessors, then iterates this process until reaching the input\ntokens. The resulting chain is further refined with similarity filtering and\nprobability thresholding, which reduce the reasoning space, facilitating the\napproximation of the marginal answer token probabilities. We validate UQAC on\nmultiple reasoning benchmarks with advanced open-source LLMs, demonstrating\nthat it consistently delivers reliable UQ estimates with high computational\nefficiency."
                },
                "authors": [
                    {
                        "name": "Yinghao Li"
                    },
                    {
                        "name": "Rushi Qiang"
                    },
                    {
                        "name": "Lama Moukheiber"
                    },
                    {
                        "name": "Chao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Zhang"
                },
                "author": "Chao Zhang",
                "arxiv_comment": "36 pages, 7 figures, 36 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19168v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19168v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11105v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11105v3",
                "updated": "2025-08-07T14:57:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    14,
                    57,
                    45,
                    3,
                    219,
                    0
                ],
                "published": "2025-06-07T01:37:42Z",
                "published_parsed": [
                    2025,
                    6,
                    7,
                    1,
                    37,
                    42,
                    5,
                    158,
                    0
                ],
                "title": "Enabling On-Device Medical AI Assistants via Input-Driven Saliency\n  Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling On-Device Medical AI Assistants via Input-Driven Saliency\n  Adaptation"
                },
                "summary": "Large Language Models (LLMs) have significant impact on the healthcare\nscenarios but remain prohibitively large for deployment in real-time,\nresource-constrained environments such as edge devices. In this work, we\nintroduce a novel medical assistant system, optimized through our\ngeneral-purpose compression framework, which tailors Large Language Models\n(LLMs) for deployment in specialized domains. By measuring neuron saliency on\ndomain-specific data, our method can aggressively prune irrelevant neurons,\nreducing model size while preserving performance. Following pruning, we apply\npost-training quantization to further reduce the memory footprint, and evaluate\nthe compressed model across medical benchmarks including MedMCQA, MedQA, and\nPubMedQA. We also deploy the 50\\% compressed Gemma and the 67\\% compressed\nLLaMA3 models on Jetson Orin Nano (18.7W peak) and Raspberry Pi 5 (6.3W peak),\nachieving real-time, energy-efficient inference under hardware constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have significant impact on the healthcare\nscenarios but remain prohibitively large for deployment in real-time,\nresource-constrained environments such as edge devices. In this work, we\nintroduce a novel medical assistant system, optimized through our\ngeneral-purpose compression framework, which tailors Large Language Models\n(LLMs) for deployment in specialized domains. By measuring neuron saliency on\ndomain-specific data, our method can aggressively prune irrelevant neurons,\nreducing model size while preserving performance. Following pruning, we apply\npost-training quantization to further reduce the memory footprint, and evaluate\nthe compressed model across medical benchmarks including MedMCQA, MedQA, and\nPubMedQA. We also deploy the 50\\% compressed Gemma and the 67\\% compressed\nLLaMA3 models on Jetson Orin Nano (18.7W peak) and Raspberry Pi 5 (6.3W peak),\nachieving real-time, energy-efficient inference under hardware constraints."
                },
                "authors": [
                    {
                        "name": "Uttej Kallakurik"
                    },
                    {
                        "name": "Edward Humes"
                    },
                    {
                        "name": "Rithvik Jonna"
                    },
                    {
                        "name": "Xiaomin Lin"
                    },
                    {
                        "name": "Tinoosh Mohsenin"
                    }
                ],
                "author_detail": {
                    "name": "Tinoosh Mohsenin"
                },
                "author": "Tinoosh Mohsenin",
                "arxiv_comment": "Accepted for publication in the Proceedings of IEEE BioCAS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11105v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11105v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05871v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05871v2",
                "updated": "2025-08-07T14:50:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    14,
                    50,
                    30,
                    3,
                    219,
                    0
                ],
                "published": "2025-04-08T09:54:49Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    54,
                    49,
                    1,
                    98,
                    0
                ],
                "title": "Agent Guide: A Simple Agent Behavioral Watermarking Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent Guide: A Simple Agent Behavioral Watermarking Framework"
                },
                "summary": "The increasing deployment of intelligent agents in digital ecosystems, such\nas social media platforms, has raised significant concerns about traceability\nand accountability, particularly in cybersecurity and digital content\nprotection. Traditional large language model (LLM) watermarking techniques,\nwhich rely on token-level manipulations, are ill-suited for agents due to the\nchallenges of behavior tokenization and information loss during\nbehavior-to-action translation. To address these issues, we propose Agent\nGuide, a novel behavioral watermarking framework that embeds watermarks by\nguiding the agent's high-level decisions (behavior) through probability biases,\nwhile preserving the naturalness of specific executions (action). Our approach\ndecouples agent behavior into two levels, behavior (e.g., choosing to bookmark)\nand action (e.g., bookmarking with specific tags), and applies watermark-guided\nbiases to the behavior probability distribution. We employ a z-statistic-based\nstatistical analysis to detect the watermark, ensuring reliable extraction over\nmultiple rounds. Experiments in a social media scenario with diverse agent\nprofiles demonstrate that Agent Guide achieves effective watermark detection\nwith a low false positive rate. Our framework provides a practical and robust\nsolution for agent watermarking, with applications in identifying malicious\nagents and protecting proprietary agent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing deployment of intelligent agents in digital ecosystems, such\nas social media platforms, has raised significant concerns about traceability\nand accountability, particularly in cybersecurity and digital content\nprotection. Traditional large language model (LLM) watermarking techniques,\nwhich rely on token-level manipulations, are ill-suited for agents due to the\nchallenges of behavior tokenization and information loss during\nbehavior-to-action translation. To address these issues, we propose Agent\nGuide, a novel behavioral watermarking framework that embeds watermarks by\nguiding the agent's high-level decisions (behavior) through probability biases,\nwhile preserving the naturalness of specific executions (action). Our approach\ndecouples agent behavior into two levels, behavior (e.g., choosing to bookmark)\nand action (e.g., bookmarking with specific tags), and applies watermark-guided\nbiases to the behavior probability distribution. We employ a z-statistic-based\nstatistical analysis to detect the watermark, ensuring reliable extraction over\nmultiple rounds. Experiments in a social media scenario with diverse agent\nprofiles demonstrate that Agent Guide achieves effective watermark detection\nwith a low false positive rate. Our framework provides a practical and robust\nsolution for agent watermarking, with applications in identifying malicious\nagents and protecting proprietary agent systems."
                },
                "authors": [
                    {
                        "name": "Kaibo Huang"
                    },
                    {
                        "name": "Zipei Zhang"
                    },
                    {
                        "name": "Zhongliang Yang"
                    },
                    {
                        "name": "Linna Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Linna Zhou"
                },
                "author": "Linna Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05871v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05871v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.00708v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.00708v2",
                "updated": "2025-08-07T14:48:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    14,
                    48,
                    25,
                    3,
                    219,
                    0
                ],
                "published": "2024-04-23T19:57:03Z",
                "published_parsed": [
                    2024,
                    4,
                    23,
                    19,
                    57,
                    3,
                    1,
                    114,
                    0
                ],
                "title": "Understanding Large Language Model Behaviors through Interactive\n  Counterfactual Generation and Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding Large Language Model Behaviors through Interactive\n  Counterfactual Generation and Analysis"
                },
                "summary": "Understanding the behavior of large language models (LLMs) is crucial for\nensuring their safe and reliable use. However, existing explainable AI (XAI)\nmethods for LLMs primarily rely on word-level explanations, which are often\ncomputationally inefficient and misaligned with human reasoning processes.\nMoreover, these methods often treat explanation as a one-time output,\noverlooking its inherently interactive and iterative nature. In this paper, we\npresent LLM Analyzer, an interactive visualization system that addresses these\nlimitations by enabling intuitive and efficient exploration of LLM behaviors\nthrough counterfactual analysis. Our system features a novel algorithm that\ngenerates fluent and semantically meaningful counterfactuals via targeted\nremoval and replacement operations at user-defined levels of granularity. These\ncounterfactuals are used to compute feature attribution scores, which are then\nintegrated with concrete examples in a table-based visualization, supporting\ndynamic analysis of model behavior. A user study with LLM practitioners and\ninterviews with experts demonstrate the system's usability and effectiveness,\nemphasizing the importance of involving humans in the explanation process as\nactive participants rather than passive recipients.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the behavior of large language models (LLMs) is crucial for\nensuring their safe and reliable use. However, existing explainable AI (XAI)\nmethods for LLMs primarily rely on word-level explanations, which are often\ncomputationally inefficient and misaligned with human reasoning processes.\nMoreover, these methods often treat explanation as a one-time output,\noverlooking its inherently interactive and iterative nature. In this paper, we\npresent LLM Analyzer, an interactive visualization system that addresses these\nlimitations by enabling intuitive and efficient exploration of LLM behaviors\nthrough counterfactual analysis. Our system features a novel algorithm that\ngenerates fluent and semantically meaningful counterfactuals via targeted\nremoval and replacement operations at user-defined levels of granularity. These\ncounterfactuals are used to compute feature attribution scores, which are then\nintegrated with concrete examples in a table-based visualization, supporting\ndynamic analysis of model behavior. A user study with LLM practitioners and\ninterviews with experts demonstrate the system's usability and effectiveness,\nemphasizing the importance of involving humans in the explanation process as\nactive participants rather than passive recipients."
                },
                "authors": [
                    {
                        "name": "Furui Cheng"
                    },
                    {
                        "name": "Vilm Zouhar"
                    },
                    {
                        "name": "Robin Shing Moon Chan"
                    },
                    {
                        "name": "Daniel Frst"
                    },
                    {
                        "name": "Hendrik Strobelt"
                    },
                    {
                        "name": "Mennatallah El-Assady"
                    }
                ],
                "author_detail": {
                    "name": "Mennatallah El-Assady"
                },
                "author": "Mennatallah El-Assady",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.00708v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.00708v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; H.5.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05452v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05452v1",
                "updated": "2025-08-07T14:46:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    14,
                    46,
                    30,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T14:46:30Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    14,
                    46,
                    30,
                    3,
                    219,
                    0
                ],
                "title": "LLMEval-3: A Large-Scale Longitudinal Study on Robust and Fair\n  Evaluation of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMEval-3: A Large-Scale Longitudinal Study on Robust and Fair\n  Evaluation of Large Language Models"
                },
                "summary": "Existing evaluation of Large Language Models (LLMs) on static benchmarks is\nvulnerable to data contamination and leaderboard overfitting, critical issues\nthat obscure true model capabilities. To address this, we introduce LLMEval-3,\na framework for dynamic evaluation of LLMs. LLMEval-3 is built on a proprietary\nbank of 220k graduate-level questions, from which it dynamically samples unseen\ntest sets for each evaluation run. Its automated pipeline ensures integrity via\ncontamination-resistant data curation, a novel anti-cheating architecture, and\na calibrated LLM-as-a-judge process achieving 90% agreement with human experts,\ncomplemented by a relative ranking system for fair comparison. An 20-month\nlongitudinal study of nearly 50 leading models reveals a performance ceiling on\nknowledge memorization and exposes data contamination vulnerabilities\nundetectable by static benchmarks. The framework demonstrates exceptional\nrobustness in ranking stability and consistency, providing strong empirical\nvalidation for the dynamic evaluation paradigm. LLMEval-3 offers a robust and\ncredible methodology for assessing the true capabilities of LLMs beyond\nleaderboard scores, promoting the development of more trustworthy evaluation\nstandards.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing evaluation of Large Language Models (LLMs) on static benchmarks is\nvulnerable to data contamination and leaderboard overfitting, critical issues\nthat obscure true model capabilities. To address this, we introduce LLMEval-3,\na framework for dynamic evaluation of LLMs. LLMEval-3 is built on a proprietary\nbank of 220k graduate-level questions, from which it dynamically samples unseen\ntest sets for each evaluation run. Its automated pipeline ensures integrity via\ncontamination-resistant data curation, a novel anti-cheating architecture, and\na calibrated LLM-as-a-judge process achieving 90% agreement with human experts,\ncomplemented by a relative ranking system for fair comparison. An 20-month\nlongitudinal study of nearly 50 leading models reveals a performance ceiling on\nknowledge memorization and exposes data contamination vulnerabilities\nundetectable by static benchmarks. The framework demonstrates exceptional\nrobustness in ranking stability and consistency, providing strong empirical\nvalidation for the dynamic evaluation paradigm. LLMEval-3 offers a robust and\ncredible methodology for assessing the true capabilities of LLMs beyond\nleaderboard scores, promoting the development of more trustworthy evaluation\nstandards."
                },
                "authors": [
                    {
                        "name": "Ming Zhang"
                    },
                    {
                        "name": "Yujiong Shen"
                    },
                    {
                        "name": "Jingyi Deng"
                    },
                    {
                        "name": "Yuhui Wang"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Junzhe Wang"
                    },
                    {
                        "name": "Shichun Liu"
                    },
                    {
                        "name": "Shihan Dou"
                    },
                    {
                        "name": "Huayu Sha"
                    },
                    {
                        "name": "Qiyuan Peng"
                    },
                    {
                        "name": "Changhao Jiang"
                    },
                    {
                        "name": "Jingqi Tong"
                    },
                    {
                        "name": "Yilong Wu"
                    },
                    {
                        "name": "Zhihao Zhang"
                    },
                    {
                        "name": "Mingqi Wu"
                    },
                    {
                        "name": "Zhiheng Xi"
                    },
                    {
                        "name": "Mingxu Chai"
                    },
                    {
                        "name": "Tao Liang"
                    },
                    {
                        "name": "Zhihui Fei"
                    },
                    {
                        "name": "Zhen Wang"
                    },
                    {
                        "name": "Mingyang Wan"
                    },
                    {
                        "name": "Guojun Ma"
                    },
                    {
                        "name": "Tao Gui"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05452v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05452v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12106v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12106v4",
                "updated": "2025-08-07T14:43:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    14,
                    43,
                    9,
                    3,
                    219,
                    0
                ],
                "published": "2025-01-21T12:56:47Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    12,
                    56,
                    47,
                    1,
                    21,
                    0
                ],
                "title": "Can open source large language models be used for tumor documentation in\n  Germany? -- An evaluation on urological doctors' notes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can open source large language models be used for tumor documentation in\n  Germany? -- An evaluation on urological doctors' notes"
                },
                "summary": "Tumor documentation in Germany is largely done manually, requiring reading\npatient records and entering data into structured databases. Large language\nmodels (LLMs) could potentially enhance this process by improving efficiency\nand reliability. This evaluation tests eleven different open source LLMs with\nsizes ranging from 1-70 billion model parameters on three basic tasks of the\ntumor documentation process: identifying tumor diagnoses, assigning ICD-10\ncodes, and extracting the date of first diagnosis. For evaluating the LLMs on\nthese tasks, a dataset of annotated text snippets based on anonymized doctors'\nnotes from urology was prepared. Different prompting strategies were used to\ninvestigate the effect of the number of examples in few-shot prompting and to\nexplore the capabilities of the LLMs in general. The models Llama 3.1 8B,\nMistral 7B, and Mistral NeMo 12 B performed comparably well in the tasks.\nModels with less extensive training data or having fewer than 7 billion\nparameters showed notably lower performance, while larger models did not\ndisplay performance gains. Examples from a different medical domain than\nurology could also improve the outcome in few-shot prompting, which\ndemonstrates the ability of LLMs to handle tasks needed for tumor\ndocumentation. Open source LLMs show a strong potential for automating tumor\ndocumentation. Models from 7-12 billion parameters could offer an optimal\nbalance between performance and resource efficiency. With tailored fine-tuning\nand well-designed prompting, these models might become important tools for\nclinical documentation in the future. The code for the evaluation is available\nfrom https://github.com/stefan-m-lenz/UroLlmEval. We also release the dataset\nas a new valuable resource that addresses the shortage of authentic and easily\naccessible benchmarks in German-language medical NLP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tumor documentation in Germany is largely done manually, requiring reading\npatient records and entering data into structured databases. Large language\nmodels (LLMs) could potentially enhance this process by improving efficiency\nand reliability. This evaluation tests eleven different open source LLMs with\nsizes ranging from 1-70 billion model parameters on three basic tasks of the\ntumor documentation process: identifying tumor diagnoses, assigning ICD-10\ncodes, and extracting the date of first diagnosis. For evaluating the LLMs on\nthese tasks, a dataset of annotated text snippets based on anonymized doctors'\nnotes from urology was prepared. Different prompting strategies were used to\ninvestigate the effect of the number of examples in few-shot prompting and to\nexplore the capabilities of the LLMs in general. The models Llama 3.1 8B,\nMistral 7B, and Mistral NeMo 12 B performed comparably well in the tasks.\nModels with less extensive training data or having fewer than 7 billion\nparameters showed notably lower performance, while larger models did not\ndisplay performance gains. Examples from a different medical domain than\nurology could also improve the outcome in few-shot prompting, which\ndemonstrates the ability of LLMs to handle tasks needed for tumor\ndocumentation. Open source LLMs show a strong potential for automating tumor\ndocumentation. Models from 7-12 billion parameters could offer an optimal\nbalance between performance and resource efficiency. With tailored fine-tuning\nand well-designed prompting, these models might become important tools for\nclinical documentation in the future. The code for the evaluation is available\nfrom https://github.com/stefan-m-lenz/UroLlmEval. We also release the dataset\nas a new valuable resource that addresses the shortage of authentic and easily\naccessible benchmarks in German-language medical NLP."
                },
                "authors": [
                    {
                        "name": "Stefan Lenz"
                    },
                    {
                        "name": "Arsenij Ustjanzew"
                    },
                    {
                        "name": "Marco Jeray"
                    },
                    {
                        "name": "Meike Ressing"
                    },
                    {
                        "name": "Torsten Panholzer"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Panholzer"
                },
                "author": "Torsten Panholzer",
                "arxiv_doi": "10.1186/s13040-025-00463-8",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1186/s13040-025-00463-8",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.12106v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12106v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "53 pages, 5 figures",
                "arxiv_journal_ref": "BioData Mining volume 18, Article number 48 (2025)",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05433v1",
                "updated": "2025-08-07T14:24:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    14,
                    24,
                    3,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T14:24:03Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    14,
                    24,
                    3,
                    3,
                    219,
                    0
                ],
                "title": "Discovering Interpretable Programmatic Policies via Multimodal\n  LLM-assisted Evolutionary Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discovering Interpretable Programmatic Policies via Multimodal\n  LLM-assisted Evolutionary Search"
                },
                "summary": "Interpretability and high performance are essential goals in designing\ncontrol policies, particularly for safety-critical tasks. Deep reinforcement\nlearning has greatly enhanced performance, yet its inherent lack of\ninterpretability often undermines trust and hinders real-world deployment. This\nwork addresses these dual challenges by introducing a novel approach for\nprogrammatic policy discovery, called Multimodal Large Language Model-assisted\nEvolutionary Search (MLES). MLES utilizes multimodal large language models as\npolicy generators, combining them with evolutionary mechanisms for automatic\npolicy optimization. It integrates visual feedback-driven behavior analysis\nwithin the policy generation process to identify failure patterns and\nfacilitate targeted improvements, enhancing the efficiency of policy discovery\nand producing adaptable, human-aligned policies. Experimental results show that\nMLES achieves policy discovery capabilities and efficiency comparable to\nProximal Policy Optimization (PPO) across two control tasks, while offering\ntransparent control logic and traceable design processes. This paradigm\novercomes the limitations of predefined domain-specific languages, facilitates\nknowledge transfer and reuse, and is scalable across various control tasks.\nMLES shows promise as a leading approach for the next generation of\ninterpretable control policy discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpretability and high performance are essential goals in designing\ncontrol policies, particularly for safety-critical tasks. Deep reinforcement\nlearning has greatly enhanced performance, yet its inherent lack of\ninterpretability often undermines trust and hinders real-world deployment. This\nwork addresses these dual challenges by introducing a novel approach for\nprogrammatic policy discovery, called Multimodal Large Language Model-assisted\nEvolutionary Search (MLES). MLES utilizes multimodal large language models as\npolicy generators, combining them with evolutionary mechanisms for automatic\npolicy optimization. It integrates visual feedback-driven behavior analysis\nwithin the policy generation process to identify failure patterns and\nfacilitate targeted improvements, enhancing the efficiency of policy discovery\nand producing adaptable, human-aligned policies. Experimental results show that\nMLES achieves policy discovery capabilities and efficiency comparable to\nProximal Policy Optimization (PPO) across two control tasks, while offering\ntransparent control logic and traceable design processes. This paradigm\novercomes the limitations of predefined domain-specific languages, facilitates\nknowledge transfer and reuse, and is scalable across various control tasks.\nMLES shows promise as a leading approach for the next generation of\ninterpretable control policy discovery."
                },
                "authors": [
                    {
                        "name": "Qinglong Hu"
                    },
                    {
                        "name": "Xialiang Tong"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    },
                    {
                        "name": "Fei Liu"
                    },
                    {
                        "name": "Zhichao Lu"
                    },
                    {
                        "name": "Qingfu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qingfu Zhang"
                },
                "author": "Qingfu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05429v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05429v1",
                "updated": "2025-08-07T14:17:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    14,
                    17,
                    43,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T14:17:43Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    14,
                    17,
                    43,
                    3,
                    219,
                    0
                ],
                "title": "MyCulture: Exploring Malaysia's Diverse Culture under Low-Resource\n  Language Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MyCulture: Exploring Malaysia's Diverse Culture under Low-Resource\n  Language Constraints"
                },
                "summary": "Large Language Models (LLMs) often exhibit cultural biases due to training\ndata dominated by high-resource languages like English and Chinese. This poses\nchallenges for accurately representing and evaluating diverse cultural\ncontexts, particularly in low-resource language settings. To address this, we\nintroduce MyCulture, a benchmark designed to comprehensively evaluate LLMs on\nMalaysian culture across six pillars: arts, attire, customs, entertainment,\nfood, and religion presented in Bahasa Melayu. Unlike conventional benchmarks,\nMyCulture employs a novel open-ended multiple-choice question format without\npredefined options, thereby reducing guessing and mitigating format bias. We\nprovide a theoretical justification for the effectiveness of this open-ended\nstructure in improving both fairness and discriminative power. Furthermore, we\nanalyze structural bias by comparing model performance on structured versus\nfree-form outputs, and assess language bias through multilingual prompt\nvariations. Our evaluation across a range of regional and international LLMs\nreveals significant disparities in cultural comprehension, highlighting the\nurgent need for culturally grounded and linguistically inclusive benchmarks in\nthe development and assessment of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often exhibit cultural biases due to training\ndata dominated by high-resource languages like English and Chinese. This poses\nchallenges for accurately representing and evaluating diverse cultural\ncontexts, particularly in low-resource language settings. To address this, we\nintroduce MyCulture, a benchmark designed to comprehensively evaluate LLMs on\nMalaysian culture across six pillars: arts, attire, customs, entertainment,\nfood, and religion presented in Bahasa Melayu. Unlike conventional benchmarks,\nMyCulture employs a novel open-ended multiple-choice question format without\npredefined options, thereby reducing guessing and mitigating format bias. We\nprovide a theoretical justification for the effectiveness of this open-ended\nstructure in improving both fairness and discriminative power. Furthermore, we\nanalyze structural bias by comparing model performance on structured versus\nfree-form outputs, and assess language bias through multilingual prompt\nvariations. Our evaluation across a range of regional and international LLMs\nreveals significant disparities in cultural comprehension, highlighting the\nurgent need for culturally grounded and linguistically inclusive benchmarks in\nthe development and assessment of LLMs."
                },
                "authors": [
                    {
                        "name": "Zhong Ken Hew"
                    },
                    {
                        "name": "Jia Xin Low"
                    },
                    {
                        "name": "Sze Jue Yang"
                    },
                    {
                        "name": "Chee Seng chan"
                    }
                ],
                "author_detail": {
                    "name": "Chee Seng chan"
                },
                "author": "Chee Seng chan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05429v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05429v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11790v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11790v3",
                "updated": "2025-08-07T14:17:38Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    14,
                    17,
                    38,
                    3,
                    219,
                    0
                ],
                "published": "2025-05-17T02:28:12Z",
                "published_parsed": [
                    2025,
                    5,
                    17,
                    2,
                    28,
                    12,
                    5,
                    137,
                    0
                ],
                "title": "JULI: Jailbreak Large Language Models by Self-Introspection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JULI: Jailbreak Large Language Models by Self-Introspection"
                },
                "summary": "Large Language Models (LLMs) are trained with safety alignment to prevent\ngenerating malicious content. Although some attacks have highlighted\nvulnerabilities in these safety-aligned LLMs, they typically have limitations,\nsuch as necessitating access to the model weights or the generation process.\nSince proprietary models through API-calling do not grant users such\npermissions, these attacks find it challenging to compromise them. In this\npaper, we propose Jailbreaking Using LLM Introspection (JULI), which jailbreaks\nLLMs by manipulating the token log probabilities, using a tiny plug-in block,\nBiasNet. JULI relies solely on the knowledge of the target LLM's predicted\ntoken log probabilities. It can effectively jailbreak API-calling LLMs under a\nblack-box setting and knowing only top-$5$ token log probabilities. Our\napproach demonstrates superior effectiveness, outperforming existing\nstate-of-the-art (SOTA) approaches across multiple metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are trained with safety alignment to prevent\ngenerating malicious content. Although some attacks have highlighted\nvulnerabilities in these safety-aligned LLMs, they typically have limitations,\nsuch as necessitating access to the model weights or the generation process.\nSince proprietary models through API-calling do not grant users such\npermissions, these attacks find it challenging to compromise them. In this\npaper, we propose Jailbreaking Using LLM Introspection (JULI), which jailbreaks\nLLMs by manipulating the token log probabilities, using a tiny plug-in block,\nBiasNet. JULI relies solely on the knowledge of the target LLM's predicted\ntoken log probabilities. It can effectively jailbreak API-calling LLMs under a\nblack-box setting and knowing only top-$5$ token log probabilities. Our\napproach demonstrates superior effectiveness, outperforming existing\nstate-of-the-art (SOTA) approaches across multiple metrics."
                },
                "authors": [
                    {
                        "name": "Jesson Wang"
                    },
                    {
                        "name": "Zhanhao Hu"
                    },
                    {
                        "name": "David Wagner"
                    }
                ],
                "author_detail": {
                    "name": "David Wagner"
                },
                "author": "David Wagner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11790v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11790v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05428v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05428v1",
                "updated": "2025-08-07T14:17:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    14,
                    17,
                    28,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T14:17:28Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    14,
                    17,
                    28,
                    3,
                    219,
                    0
                ],
                "title": "Group Causal Policy Optimization for Post-Training Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Group Causal Policy Optimization for Post-Training Large Language Models"
                },
                "summary": "Recent advances in large language models (LLMs) have broadened their\napplicability across diverse tasks, yet specialized domains still require\ntargeted post training. Among existing methods, Group Relative Policy\nOptimization (GRPO) stands out for its efficiency, leveraging groupwise\nrelative rewards while avoiding costly value function learning. However, GRPO\ntreats candidate responses as independent, overlooking semantic interactions\nsuch as complementarity and contradiction. To address this challenge, we first\nintroduce a Structural Causal Model (SCM) that reveals hidden dependencies\namong candidate responses induced by conditioning on a final integrated output\nforming a collider structure. Then, our causal analysis leads to two insights:\n(1) projecting responses onto a causally informed subspace improves prediction\nquality, and (2) this projection yields a better baseline than query only\nconditioning. Building on these insights, we propose Group Causal Policy\nOptimization (GCPO), which integrates causal structure into optimization\nthrough two key components: a causally informed reward adjustment and a novel\nKL regularization term that aligns the policy with a causally projected\nreference distribution. Comprehensive experimental evaluations demonstrate that\nGCPO consistently surpasses existing methods, including GRPO across multiple\nreasoning benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have broadened their\napplicability across diverse tasks, yet specialized domains still require\ntargeted post training. Among existing methods, Group Relative Policy\nOptimization (GRPO) stands out for its efficiency, leveraging groupwise\nrelative rewards while avoiding costly value function learning. However, GRPO\ntreats candidate responses as independent, overlooking semantic interactions\nsuch as complementarity and contradiction. To address this challenge, we first\nintroduce a Structural Causal Model (SCM) that reveals hidden dependencies\namong candidate responses induced by conditioning on a final integrated output\nforming a collider structure. Then, our causal analysis leads to two insights:\n(1) projecting responses onto a causally informed subspace improves prediction\nquality, and (2) this projection yields a better baseline than query only\nconditioning. Building on these insights, we propose Group Causal Policy\nOptimization (GCPO), which integrates causal structure into optimization\nthrough two key components: a causally informed reward adjustment and a novel\nKL regularization term that aligns the policy with a causally projected\nreference distribution. Comprehensive experimental evaluations demonstrate that\nGCPO consistently surpasses existing methods, including GRPO across multiple\nreasoning benchmarks."
                },
                "authors": [
                    {
                        "name": "Ziyin Gu"
                    },
                    {
                        "name": "Jingyao Wang"
                    },
                    {
                        "name": "Ran Zuo"
                    },
                    {
                        "name": "Chuxiong Sun"
                    },
                    {
                        "name": "Zeen Song"
                    },
                    {
                        "name": "Changwen Zheng"
                    },
                    {
                        "name": "Wenwen Qiang"
                    }
                ],
                "author_detail": {
                    "name": "Wenwen Qiang"
                },
                "author": "Wenwen Qiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05428v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05428v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05427v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05427v1",
                "updated": "2025-08-07T14:17:23Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    14,
                    17,
                    23,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T14:17:23Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    14,
                    17,
                    23,
                    3,
                    219,
                    0
                ],
                "title": "Large Language Models Transform Organic Synthesis From Reaction\n  Prediction to Automation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Transform Organic Synthesis From Reaction\n  Prediction to Automation"
                },
                "summary": "Large language models (LLMs) are beginning to reshape how chemists plan and\nrun reactions in organic synthesis. Trained on millions of reported\ntransformations, these text-based models can propose synthetic routes, forecast\nreaction outcomes and even instruct robots that execute experiments without\nhuman supervision. Here we survey the milestones that turned LLMs from\nspeculative tools into practical lab partners. We show how coupling LLMs with\ngraph neural networks, quantum calculations and real-time spectroscopy shrinks\ndiscovery cycles and supports greener, data-driven chemistry. We discuss\nlimitations, including biased datasets, opaque reasoning and the need for\nsafety gates that prevent unintentional hazards. Finally, we outline community\ninitiatives open benchmarks, federated learning and explainable interfaces that\naim to democratize access while keeping humans firmly in control. These\nadvances chart a path towards rapid, reliable and inclusive molecular\ninnovation powered by artificial intelligence and automation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are beginning to reshape how chemists plan and\nrun reactions in organic synthesis. Trained on millions of reported\ntransformations, these text-based models can propose synthetic routes, forecast\nreaction outcomes and even instruct robots that execute experiments without\nhuman supervision. Here we survey the milestones that turned LLMs from\nspeculative tools into practical lab partners. We show how coupling LLMs with\ngraph neural networks, quantum calculations and real-time spectroscopy shrinks\ndiscovery cycles and supports greener, data-driven chemistry. We discuss\nlimitations, including biased datasets, opaque reasoning and the need for\nsafety gates that prevent unintentional hazards. Finally, we outline community\ninitiatives open benchmarks, federated learning and explainable interfaces that\naim to democratize access while keeping humans firmly in control. These\nadvances chart a path towards rapid, reliable and inclusive molecular\ninnovation powered by artificial intelligence and automation."
                },
                "authors": [
                    {
                        "name": "Kartar Kumar Lohana Tharwani"
                    },
                    {
                        "name": "Rajesh Kumar"
                    },
                    {
                        "name": "Sumita"
                    },
                    {
                        "name": "Numan Ahmed"
                    },
                    {
                        "name": "Yong Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yong Tang"
                },
                "author": "Yong Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05427v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05427v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05425v1",
                "updated": "2025-08-07T14:15:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    14,
                    15,
                    18,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T14:15:18Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    14,
                    15,
                    18,
                    3,
                    219,
                    0
                ],
                "title": "Categorising SME Bank Transactions with Machine Learning and Synthetic\n  Data Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Categorising SME Bank Transactions with Machine Learning and Synthetic\n  Data Generation"
                },
                "summary": "Despite their significant economic contributions, Small and Medium\nEnterprises (SMEs) face persistent barriers to securing traditional financing\ndue to information asymmetries. Cash flow lending has emerged as a promising\nalternative, but its effectiveness depends on accurate modelling of\ntransaction-level data. The main challenge in SME transaction analysis lies in\nthe unstructured nature of textual descriptions, characterised by extreme\nabbreviations, limited context, and imbalanced label distributions. While\nconsumer transaction descriptions often show significant commonalities across\nindividuals, SME transaction descriptions are typically nonstandard and\ninconsistent across businesses and industries. To address some of these\nchallenges, we propose a bank categorisation pipeline that leverages synthetic\ndata generation to augment existing transaction data sets. Our approach\ncomprises three core components: (1) a synthetic data generation module that\nreplicates transaction properties while preserving context and semantic\nmeaning; (2) a fine-tuned classification model trained on this enriched\ndataset; and (3) a calibration methodology that aligns model outputs with\nreal-world label distributions. Experimental results demonstrate that our\napproach achieves 73.49% (+-5.09) standard accuracy on held-out data, with\nhigh-confidence predictions reaching 90.36% (+-6.52) accuracy. The model\nexhibits robust generalisation across different types of SMEs and transactions,\nwhich makes it suitable for practical deployment in cash-flow lending\napplications. By addressing core data challenges, namely, scarcity, noise, and\nimbalance, our framework provides a practical solution to build robust\nclassification systems in data-sparse SME lending contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their significant economic contributions, Small and Medium\nEnterprises (SMEs) face persistent barriers to securing traditional financing\ndue to information asymmetries. Cash flow lending has emerged as a promising\nalternative, but its effectiveness depends on accurate modelling of\ntransaction-level data. The main challenge in SME transaction analysis lies in\nthe unstructured nature of textual descriptions, characterised by extreme\nabbreviations, limited context, and imbalanced label distributions. While\nconsumer transaction descriptions often show significant commonalities across\nindividuals, SME transaction descriptions are typically nonstandard and\ninconsistent across businesses and industries. To address some of these\nchallenges, we propose a bank categorisation pipeline that leverages synthetic\ndata generation to augment existing transaction data sets. Our approach\ncomprises three core components: (1) a synthetic data generation module that\nreplicates transaction properties while preserving context and semantic\nmeaning; (2) a fine-tuned classification model trained on this enriched\ndataset; and (3) a calibration methodology that aligns model outputs with\nreal-world label distributions. Experimental results demonstrate that our\napproach achieves 73.49% (+-5.09) standard accuracy on held-out data, with\nhigh-confidence predictions reaching 90.36% (+-6.52) accuracy. The model\nexhibits robust generalisation across different types of SMEs and transactions,\nwhich makes it suitable for practical deployment in cash-flow lending\napplications. By addressing core data challenges, namely, scarcity, noise, and\nimbalance, our framework provides a practical solution to build robust\nclassification systems in data-sparse SME lending contexts."
                },
                "authors": [
                    {
                        "name": "Aluffi Pietro Alessandro"
                    },
                    {
                        "name": "Brandi Jess"
                    },
                    {
                        "name": "Marya Bazzi"
                    },
                    {
                        "name": "Kate Kennedy"
                    },
                    {
                        "name": "Matt Arderne"
                    },
                    {
                        "name": "Daniel Rodrigues"
                    },
                    {
                        "name": "Martin Lotz"
                    }
                ],
                "author_detail": {
                    "name": "Martin Lotz"
                },
                "author": "Martin Lotz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.09994v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.09994v2",
                "updated": "2025-08-07T14:15:07Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    14,
                    15,
                    7,
                    3,
                    219,
                    0
                ],
                "published": "2025-03-13T03:05:11Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    3,
                    5,
                    11,
                    3,
                    72,
                    0
                ],
                "title": "TIME: Temporal-Sensitive Multi-Dimensional Instruction Tuning and Robust\n  Benchmarking for Video-LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TIME: Temporal-Sensitive Multi-Dimensional Instruction Tuning and Robust\n  Benchmarking for Video-LLMs"
                },
                "summary": "Video large language models have achieved remarkable performance in tasks\nsuch as video question answering, however, their temporal understanding remains\nsuboptimal. To address this limitation, we curate a dedicated instruction\nfine-tuning dataset that focuses on enhancing temporal comprehension across\nfive key dimensions. In order to reduce reliance on costly temporal\nannotations, we introduce a multi-task prompt fine-tuning approach that\nseamlessly integrates temporal-sensitive tasks into existing instruction\ndatasets without requiring additional annotations. Furthermore, we develop a\nnovel benchmark for temporal-sensitive video understanding that not only fills\nthe gaps in dimension coverage left by existing benchmarks but also rigorously\nfilters out potential shortcuts, ensuring a more accurate evaluation. Extensive\nexperimental results demonstrate that our approach significantly enhances the\ntemporal understanding of video-LLMs while avoiding reliance on shortcuts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models have achieved remarkable performance in tasks\nsuch as video question answering, however, their temporal understanding remains\nsuboptimal. To address this limitation, we curate a dedicated instruction\nfine-tuning dataset that focuses on enhancing temporal comprehension across\nfive key dimensions. In order to reduce reliance on costly temporal\nannotations, we introduce a multi-task prompt fine-tuning approach that\nseamlessly integrates temporal-sensitive tasks into existing instruction\ndatasets without requiring additional annotations. Furthermore, we develop a\nnovel benchmark for temporal-sensitive video understanding that not only fills\nthe gaps in dimension coverage left by existing benchmarks but also rigorously\nfilters out potential shortcuts, ensuring a more accurate evaluation. Extensive\nexperimental results demonstrate that our approach significantly enhances the\ntemporal understanding of video-LLMs while avoiding reliance on shortcuts."
                },
                "authors": [
                    {
                        "name": "Yunxiao Wang"
                    },
                    {
                        "name": "Meng Liu"
                    },
                    {
                        "name": "Wenqi Liu"
                    },
                    {
                        "name": "Xuemeng Song"
                    },
                    {
                        "name": "Bin Wen"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Tingting Gao"
                    },
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Guorui Zhou"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.09994v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.09994v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05421v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05421v1",
                "updated": "2025-08-07T14:14:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    14,
                    14,
                    8,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T14:14:08Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    14,
                    14,
                    8,
                    3,
                    219,
                    0
                ],
                "title": "LLM-based Multi-Agent Copilot for Quantum Sensor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based Multi-Agent Copilot for Quantum Sensor"
                },
                "summary": "Large language models (LLM) exhibit broad utility but face limitations in\nquantum sensor development, stemming from interdisciplinary knowledge barriers\nand involving complex optimization processes. Here we present QCopilot, an\nLLM-based multi-agent framework integrating external knowledge access, active\nlearning, and uncertainty quantification for quantum sensor design and\ndiagnosis. Comprising commercial LLMs with few-shot prompt engineering and\nvector knowledge base, QCopilot employs specialized agents to adaptively select\noptimization methods, automate modeling analysis, and independently perform\nproblem diagnosis. Applying QCopilot to atom cooling experiments, we generated\n10${}^{\\rm{8}}$ sub-$\\rm{\\mu}$K atoms without any human intervention within a\nfew hours, representing $\\sim$100$\\times$ speedup over manual experimentation.\nNotably, by continuously accumulating prior knowledge and enabling dynamic\nmodeling, QCopilot can autonomously identify anomalous parameters in\nmulti-parameter experimental settings. Our work reduces barriers to large-scale\nquantum sensor deployment and readily extends to other quantum information\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLM) exhibit broad utility but face limitations in\nquantum sensor development, stemming from interdisciplinary knowledge barriers\nand involving complex optimization processes. Here we present QCopilot, an\nLLM-based multi-agent framework integrating external knowledge access, active\nlearning, and uncertainty quantification for quantum sensor design and\ndiagnosis. Comprising commercial LLMs with few-shot prompt engineering and\nvector knowledge base, QCopilot employs specialized agents to adaptively select\noptimization methods, automate modeling analysis, and independently perform\nproblem diagnosis. Applying QCopilot to atom cooling experiments, we generated\n10${}^{\\rm{8}}$ sub-$\\rm{\\mu}$K atoms without any human intervention within a\nfew hours, representing $\\sim$100$\\times$ speedup over manual experimentation.\nNotably, by continuously accumulating prior knowledge and enabling dynamic\nmodeling, QCopilot can autonomously identify anomalous parameters in\nmulti-parameter experimental settings. Our work reduces barriers to large-scale\nquantum sensor deployment and readily extends to other quantum information\nsystems."
                },
                "authors": [
                    {
                        "name": "Rong Sha"
                    },
                    {
                        "name": "Binglin Wang"
                    },
                    {
                        "name": "Jun Yang"
                    },
                    {
                        "name": "Xiaoxiao Ma"
                    },
                    {
                        "name": "Chengkun Wu"
                    },
                    {
                        "name": "Liang Yan"
                    },
                    {
                        "name": "Chao Zhou"
                    },
                    {
                        "name": "Jixun Liu"
                    },
                    {
                        "name": "Guochao Wang"
                    },
                    {
                        "name": "Shuhua Yan"
                    },
                    {
                        "name": "Lingxiao Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Lingxiao Zhu"
                },
                "author": "Lingxiao Zhu",
                "arxiv_comment": "13 pages,4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05421v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05421v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04377v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04377v2",
                "updated": "2025-08-07T14:05:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    14,
                    5,
                    19,
                    3,
                    219,
                    0
                ],
                "published": "2025-04-06T06:09:21Z",
                "published_parsed": [
                    2025,
                    4,
                    6,
                    6,
                    9,
                    21,
                    6,
                    96,
                    0
                ],
                "title": "PolyGuard: A Multilingual Safety Moderation Tool for 17 Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PolyGuard: A Multilingual Safety Moderation Tool for 17 Languages"
                },
                "summary": "Truly multilingual safety moderation efforts for Large Language Models (LLMs)\nhave been hindered by a narrow focus on a small set of languages (e.g.,\nEnglish, Chinese) as well as a limited scope of safety definition, resulting in\nsignificant gaps in moderation capabilities. To bridge these gaps, we release\nPOLYGUARD, a new state-of-the-art multilingual safety model for safeguarding\nLLM generations, and the corresponding training and evaluation datasets.\nPOLYGUARD is trained on POLYGUARDMIX, the largest multilingual safety training\ncorpus to date containing 1.91M samples across 17 languages (e.g., Chinese,\nCzech, English, Hindi). We also introduce POLYGUARDPROMPTS, a high quality\nmultilingual benchmark with 29K samples for the evaluation of safety\nguardrails. Created by combining naturally occurring multilingual human-LLM\ninteractions and human-verified machine translations of an English-only safety\ndataset (WildGuardMix; Han et al., 2024), our datasets contain prompt-output\npairs with labels of prompt harmfulness, response harmfulness, and response\nrefusal. Through extensive evaluations across multiple safety and toxicity\nbenchmarks, we demonstrate that POLYGUARD outperforms existing state-of-the-art\nopen-weight and commercial safety classifiers by 5.5%. Our contributions\nadvance efforts toward safer multilingual LLMs for all global users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Truly multilingual safety moderation efforts for Large Language Models (LLMs)\nhave been hindered by a narrow focus on a small set of languages (e.g.,\nEnglish, Chinese) as well as a limited scope of safety definition, resulting in\nsignificant gaps in moderation capabilities. To bridge these gaps, we release\nPOLYGUARD, a new state-of-the-art multilingual safety model for safeguarding\nLLM generations, and the corresponding training and evaluation datasets.\nPOLYGUARD is trained on POLYGUARDMIX, the largest multilingual safety training\ncorpus to date containing 1.91M samples across 17 languages (e.g., Chinese,\nCzech, English, Hindi). We also introduce POLYGUARDPROMPTS, a high quality\nmultilingual benchmark with 29K samples for the evaluation of safety\nguardrails. Created by combining naturally occurring multilingual human-LLM\ninteractions and human-verified machine translations of an English-only safety\ndataset (WildGuardMix; Han et al., 2024), our datasets contain prompt-output\npairs with labels of prompt harmfulness, response harmfulness, and response\nrefusal. Through extensive evaluations across multiple safety and toxicity\nbenchmarks, we demonstrate that POLYGUARD outperforms existing state-of-the-art\nopen-weight and commercial safety classifiers by 5.5%. Our contributions\nadvance efforts toward safer multilingual LLMs for all global users."
                },
                "authors": [
                    {
                        "name": "Priyanshu Kumar"
                    },
                    {
                        "name": "Devansh Jain"
                    },
                    {
                        "name": "Akhila Yerukola"
                    },
                    {
                        "name": "Liwei Jiang"
                    },
                    {
                        "name": "Himanshu Beniwal"
                    },
                    {
                        "name": "Thomas Hartvigsen"
                    },
                    {
                        "name": "Maarten Sap"
                    }
                ],
                "author_detail": {
                    "name": "Maarten Sap"
                },
                "author": "Maarten Sap",
                "arxiv_comment": "Accepted to COLM 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04377v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04377v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05387v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05387v1",
                "updated": "2025-08-07T13:37:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    13,
                    37,
                    4,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T13:37:04Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    13,
                    37,
                    4,
                    3,
                    219,
                    0
                ],
                "title": "Echo: Decoupling Inference and Training for Large-Scale RL Alignment on\n  Heterogeneous Swarms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Echo: Decoupling Inference and Training for Large-Scale RL Alignment on\n  Heterogeneous Swarms"
                },
                "summary": "Modern RL-based post-training for large language models (LLMs) co-locate\ntrajectory sampling and policy optimisation on the same GPU cluster, forcing\nthe system to switch between inference and training workloads. This serial\ncontext switching violates the single-program-multiple-data (SPMD) assumption\nunderlying today's distributed training systems. We present Echo, the RL system\nthat cleanly decouples these two phases across heterogeneous \"inference\" and\n\"training\" swarms while preserving statistical efficiency. Echo introduces two\nlightweight synchronization protocols: a sequential pull mode that refreshes\nsampler weights on every API call for minimal bias, and an asynchronous\npush-pull mode that streams version-tagged rollouts through a replay buffer to\nmaximise hardware utilisation. Training three representative RL workloads with\nQwen3-4B, Qwen2.5-7B and Qwen3-32B on a geographically distributed cluster,\nEcho matches a fully co-located Verl baseline in convergence speed and final\nreward while off-loading trajectory generation to commodity edge hardware.\nThese promising results demonstrate that large-scale RL for LLMs could achieve\ndatacentre-grade performance using decentralised, heterogeneous resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern RL-based post-training for large language models (LLMs) co-locate\ntrajectory sampling and policy optimisation on the same GPU cluster, forcing\nthe system to switch between inference and training workloads. This serial\ncontext switching violates the single-program-multiple-data (SPMD) assumption\nunderlying today's distributed training systems. We present Echo, the RL system\nthat cleanly decouples these two phases across heterogeneous \"inference\" and\n\"training\" swarms while preserving statistical efficiency. Echo introduces two\nlightweight synchronization protocols: a sequential pull mode that refreshes\nsampler weights on every API call for minimal bias, and an asynchronous\npush-pull mode that streams version-tagged rollouts through a replay buffer to\nmaximise hardware utilisation. Training three representative RL workloads with\nQwen3-4B, Qwen2.5-7B and Qwen3-32B on a geographically distributed cluster,\nEcho matches a fully co-located Verl baseline in convergence speed and final\nreward while off-loading trajectory generation to commodity edge hardware.\nThese promising results demonstrate that large-scale RL for LLMs could achieve\ndatacentre-grade performance using decentralised, heterogeneous resources."
                },
                "authors": [
                    {
                        "name": "Jie Xiao"
                    },
                    {
                        "name": "Shaoduo Gan"
                    },
                    {
                        "name": "Changyuan Fan"
                    },
                    {
                        "name": "Qingnan Ren"
                    },
                    {
                        "name": "Alfred Long"
                    },
                    {
                        "name": "Yuchen Zhang"
                    },
                    {
                        "name": "Rymon Yu"
                    },
                    {
                        "name": "Eric Yang"
                    },
                    {
                        "name": "Lynn Ai"
                    }
                ],
                "author_detail": {
                    "name": "Lynn Ai"
                },
                "author": "Lynn Ai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05387v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05387v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03703v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03703v4",
                "updated": "2025-08-07T13:34:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    13,
                    34,
                    51,
                    3,
                    219,
                    0
                ],
                "published": "2025-07-04T16:38:09Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    16,
                    38,
                    9,
                    4,
                    185,
                    0
                ],
                "title": "Sign Spotting Disambiguation using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sign Spotting Disambiguation using Large Language Models"
                },
                "summary": "Sign spotting, the task of identifying and localizing individual signs within\ncontinuous sign language video, plays a pivotal role in scaling dataset\nannotations and addressing the severe data scarcity issue in sign language\ntranslation. While automatic sign spotting holds great promise for enabling\nframe-level supervision at scale, it grapples with challenges such as\nvocabulary inflexibility and ambiguity inherent in continuous sign streams.\nHence, we introduce a novel, training-free framework that integrates Large\nLanguage Models (LLMs) to significantly enhance sign spotting quality. Our\napproach extracts global spatio-temporal and hand shape features, which are\nthen matched against a large-scale sign dictionary using dynamic time warping\nand cosine similarity. This dictionary-based matching inherently offers\nsuperior vocabulary flexibility without requiring model retraining. To mitigate\nnoise and ambiguity from the matching process, an LLM performs context-aware\ngloss disambiguation via beam search, notably without fine-tuning. Extensive\nexperiments on both synthetic and real-world sign language datasets demonstrate\nour method's superior accuracy and sentence fluency compared to traditional\napproaches, highlighting the potential of LLMs in advancing sign spotting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sign spotting, the task of identifying and localizing individual signs within\ncontinuous sign language video, plays a pivotal role in scaling dataset\nannotations and addressing the severe data scarcity issue in sign language\ntranslation. While automatic sign spotting holds great promise for enabling\nframe-level supervision at scale, it grapples with challenges such as\nvocabulary inflexibility and ambiguity inherent in continuous sign streams.\nHence, we introduce a novel, training-free framework that integrates Large\nLanguage Models (LLMs) to significantly enhance sign spotting quality. Our\napproach extracts global spatio-temporal and hand shape features, which are\nthen matched against a large-scale sign dictionary using dynamic time warping\nand cosine similarity. This dictionary-based matching inherently offers\nsuperior vocabulary flexibility without requiring model retraining. To mitigate\nnoise and ambiguity from the matching process, an LLM performs context-aware\ngloss disambiguation via beam search, notably without fine-tuning. Extensive\nexperiments on both synthetic and real-world sign language datasets demonstrate\nour method's superior accuracy and sentence fluency compared to traditional\napproaches, highlighting the potential of LLMs in advancing sign spotting."
                },
                "authors": [
                    {
                        "name": "JianHe Low"
                    },
                    {
                        "name": "Ozge Mercanoglu Sincan"
                    },
                    {
                        "name": "Richard Bowden"
                    }
                ],
                "author_detail": {
                    "name": "Richard Bowden"
                },
                "author": "Richard Bowden",
                "arxiv_comment": "Accepted in the international conference on Intelligent Virtual\n  Agents (IVA Adjunct)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03703v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03703v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03751v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03751v4",
                "updated": "2025-08-07T13:29:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    13,
                    29,
                    43,
                    3,
                    219,
                    0
                ],
                "published": "2024-10-01T21:48:12Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    21,
                    48,
                    12,
                    1,
                    275,
                    0
                ],
                "title": "Recent Advances in Speech Language Models: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Advances in Speech Language Models: A Survey"
                },
                "summary": "Large Language Models (LLMs) have recently garnered significant attention,\nprimarily for their capabilities in text-based interactions. However, natural\nhuman interaction often relies on speech, necessitating a shift towards\nvoice-based models. A straightforward approach to achieve this involves a\npipeline of ``Automatic Speech Recognition (ASR) + LLM + Text-to-Speech (TTS)\",\nwhere input speech is transcribed to text, processed by an LLM, and then\nconverted back to speech. Despite being straightforward, this method suffers\nfrom inherent limitations, such as information loss during modality conversion,\nsignificant latency due to the complex pipeline, and error accumulation across\nthe three stages. To address these issues, Speech Language Models (SpeechLMs)\n-- end-to-end models that generate speech without converting from text -- have\nemerged as a promising alternative. This survey paper provides the first\ncomprehensive overview of recent methodologies for constructing SpeechLMs,\ndetailing the key components of their architecture and the various training\nrecipes integral to their development. Additionally, we systematically survey\nthe various capabilities of SpeechLMs, categorize their evaluation metrics, and\ndiscuss the challenges and future research directions in this rapidly evolving\nfield. The GitHub repository is available at\nhttps://github.com/dreamtheater123/Awesome-SpeechLM-Survey",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently garnered significant attention,\nprimarily for their capabilities in text-based interactions. However, natural\nhuman interaction often relies on speech, necessitating a shift towards\nvoice-based models. A straightforward approach to achieve this involves a\npipeline of ``Automatic Speech Recognition (ASR) + LLM + Text-to-Speech (TTS)\",\nwhere input speech is transcribed to text, processed by an LLM, and then\nconverted back to speech. Despite being straightforward, this method suffers\nfrom inherent limitations, such as information loss during modality conversion,\nsignificant latency due to the complex pipeline, and error accumulation across\nthe three stages. To address these issues, Speech Language Models (SpeechLMs)\n-- end-to-end models that generate speech without converting from text -- have\nemerged as a promising alternative. This survey paper provides the first\ncomprehensive overview of recent methodologies for constructing SpeechLMs,\ndetailing the key components of their architecture and the various training\nrecipes integral to their development. Additionally, we systematically survey\nthe various capabilities of SpeechLMs, categorize their evaluation metrics, and\ndiscuss the challenges and future research directions in this rapidly evolving\nfield. The GitHub repository is available at\nhttps://github.com/dreamtheater123/Awesome-SpeechLM-Survey"
                },
                "authors": [
                    {
                        "name": "Wenqian Cui"
                    },
                    {
                        "name": "Dianzhi Yu"
                    },
                    {
                        "name": "Xiaoqi Jiao"
                    },
                    {
                        "name": "Ziqiao Meng"
                    },
                    {
                        "name": "Guangyan Zhang"
                    },
                    {
                        "name": "Qichao Wang"
                    },
                    {
                        "name": "Yiwen Guo"
                    },
                    {
                        "name": "Irwin King"
                    }
                ],
                "author_detail": {
                    "name": "Irwin King"
                },
                "author": "Irwin King",
                "arxiv_comment": "The reduced version of this paper has been accepted at ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03751v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03751v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05370v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05370v1",
                "updated": "2025-08-07T13:15:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    13,
                    15,
                    59,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T13:15:59Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    13,
                    15,
                    59,
                    3,
                    219,
                    0
                ],
                "title": "Simulating LLM training workloads for heterogeneous compute and network\n  infrastructure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating LLM training workloads for heterogeneous compute and network\n  infrastructure"
                },
                "summary": "The growing demand for large-scale GPU clusters in distributed model training\npresents a significant barrier to innovation, particularly in model\noptimization, performance tuning, and system-level enhancements. To address\nthis challenge, LLM training simulators are employed to estimate training time\nand guide design decisions. However, the state-of-the-art LLM training\nsimulators assume homogeneous compute and network infrastructure. In practice,\ndevice heterogeneity is inevitable due to resource sharing in cloud\nenvironments, frequent shifts in device generations, and inherent intra-chip\ninterconnect heterogeneity. To address the gap between state-of-the-art and\npractical requirements, we propose the design of a heterogeneity-aware\ndistributed LLM simulator capable of predicting training time while enabling\nabstractions to specify custom configurations for device groups and\ndevice-to-parallelism mapping. We present the design requirements and\nchallenges in building a heterogeneity-aware distributed ML training simulator,\nand design components such as non-uniform workload partitioning. Our initial\nsimulation results demonstrate the impact of heterogeneity on the model\ncomputation and communication time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing demand for large-scale GPU clusters in distributed model training\npresents a significant barrier to innovation, particularly in model\noptimization, performance tuning, and system-level enhancements. To address\nthis challenge, LLM training simulators are employed to estimate training time\nand guide design decisions. However, the state-of-the-art LLM training\nsimulators assume homogeneous compute and network infrastructure. In practice,\ndevice heterogeneity is inevitable due to resource sharing in cloud\nenvironments, frequent shifts in device generations, and inherent intra-chip\ninterconnect heterogeneity. To address the gap between state-of-the-art and\npractical requirements, we propose the design of a heterogeneity-aware\ndistributed LLM simulator capable of predicting training time while enabling\nabstractions to specify custom configurations for device groups and\ndevice-to-parallelism mapping. We present the design requirements and\nchallenges in building a heterogeneity-aware distributed ML training simulator,\nand design components such as non-uniform workload partitioning. Our initial\nsimulation results demonstrate the impact of heterogeneity on the model\ncomputation and communication time."
                },
                "authors": [
                    {
                        "name": "Sumit Kumar"
                    },
                    {
                        "name": "Arjun Temura"
                    },
                    {
                        "name": "Naman Sharma"
                    },
                    {
                        "name": "Ramanjeet Singh"
                    },
                    {
                        "name": "Meet Dadhania"
                    },
                    {
                        "name": "Praveen Tammana"
                    },
                    {
                        "name": "Satananda Burla"
                    },
                    {
                        "name": "Abed Mohammad Kamaluddin"
                    },
                    {
                        "name": "Rinku Shah"
                    }
                ],
                "author_detail": {
                    "name": "Rinku Shah"
                },
                "author": "Rinku Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05370v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05370v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05366v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05366v1",
                "updated": "2025-08-07T13:13:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    13,
                    13,
                    19,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T13:13:19Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    13,
                    13,
                    19,
                    3,
                    219,
                    0
                ],
                "title": "Can Language Models Critique Themselves? Investigating Self-Feedback for\n  Retrieval Augmented Generation at BioASQ 2025",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Language Models Critique Themselves? Investigating Self-Feedback for\n  Retrieval Augmented Generation at BioASQ 2025"
                },
                "summary": "Agentic Retrieval Augmented Generation (RAG) and 'deep research' systems aim\nto enable autonomous search processes where Large Language Models (LLMs)\niteratively refine outputs. However, applying these systems to domain-specific\nprofessional search, such as biomedical research, presents challenges, as\nautomated systems may reduce user involvement and misalign with expert\ninformation needs. Professional search tasks often demand high levels of user\nexpertise and transparency. The BioASQ CLEF 2025 challenge, using\nexpert-formulated questions, can serve as a platform to study these issues. We\nexplored the performance of current reasoning and nonreasoning LLMs like\nGemini-Flash 2.0, o3-mini, o4-mini and DeepSeek-R1. A key aspect of our\nmethodology was a self-feedback mechanism where LLMs generated, evaluated, and\nthen refined their outputs for query expansion and for multiple answer types\n(yes/no, factoid, list, ideal). We investigated whether this iterative\nself-correction improves performance and if reasoning models are more capable\nof generating useful feedback. Preliminary results indicate varied performance\nfor the self-feedback strategy across models and tasks. This work offers\ninsights into LLM self-correction and informs future work on comparing the\neffectiveness of LLM-generated feedback with direct human expert input in these\nsearch systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic Retrieval Augmented Generation (RAG) and 'deep research' systems aim\nto enable autonomous search processes where Large Language Models (LLMs)\niteratively refine outputs. However, applying these systems to domain-specific\nprofessional search, such as biomedical research, presents challenges, as\nautomated systems may reduce user involvement and misalign with expert\ninformation needs. Professional search tasks often demand high levels of user\nexpertise and transparency. The BioASQ CLEF 2025 challenge, using\nexpert-formulated questions, can serve as a platform to study these issues. We\nexplored the performance of current reasoning and nonreasoning LLMs like\nGemini-Flash 2.0, o3-mini, o4-mini and DeepSeek-R1. A key aspect of our\nmethodology was a self-feedback mechanism where LLMs generated, evaluated, and\nthen refined their outputs for query expansion and for multiple answer types\n(yes/no, factoid, list, ideal). We investigated whether this iterative\nself-correction improves performance and if reasoning models are more capable\nof generating useful feedback. Preliminary results indicate varied performance\nfor the self-feedback strategy across models and tasks. This work offers\ninsights into LLM self-correction and informs future work on comparing the\neffectiveness of LLM-generated feedback with direct human expert input in these\nsearch systems."
                },
                "authors": [
                    {
                        "name": "Samy Ateia"
                    },
                    {
                        "name": "Udo Kruschwitz"
                    }
                ],
                "author_detail": {
                    "name": "Udo Kruschwitz"
                },
                "author": "Udo Kruschwitz",
                "arxiv_comment": "Version as accepted at the BioASQ Lab at CLEF 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05366v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05366v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01215v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01215v3",
                "updated": "2025-08-07T13:10:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    13,
                    10,
                    30,
                    3,
                    219,
                    0
                ],
                "published": "2024-10-02T03:57:21Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    3,
                    57,
                    21,
                    2,
                    276,
                    0
                ],
                "title": "From Code to Correctness: Closing the Last Mile of Code Generation with\n  Hierarchical Debugging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Code to Correctness: Closing the Last Mile of Code Generation with\n  Hierarchical Debugging"
                },
                "summary": "While large language models have made significant strides in code generation,\nthe pass rate of the generated code is bottlenecked on subtle errors, often\nrequiring human intervention to pass tests, especially for complex problems.\nExisting LLM-based debugging systems treat generated programs as monolithic\nunits, failing to address bugs at multiple levels of granularity, from\nlow-level syntax errors to high-level algorithmic flaws. In this paper, we\nintroduce Multi-Granularity Debugger (MGDebugger), a hierarchical code debugger\nby isolating, identifying, and resolving bugs at various levels of granularity.\nMGDebugger decomposes problematic code into a hierarchical tree structure of\nsubfunctions, with each level representing a particular granularity of error.\nDuring debugging, it analyzes each subfunction and iteratively resolves bugs in\na bottom-up manner. To effectively test each subfunction, we propose an\nLLM-simulated Python executor, which traces code execution and tracks important\nvariable states to pinpoint errors accurately. Extensive experiments\ndemonstrate that MGDebugger outperforms existing debugging systems, achieving\nan 18.9% improvement in accuracy over seed generations in HumanEval and a 97.6%\nrepair success rate in HumanEvalFix. Furthermore, MGDebugger effectively fixes\nbugs across different categories and difficulty levels, demonstrating its\nrobustness and effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models have made significant strides in code generation,\nthe pass rate of the generated code is bottlenecked on subtle errors, often\nrequiring human intervention to pass tests, especially for complex problems.\nExisting LLM-based debugging systems treat generated programs as monolithic\nunits, failing to address bugs at multiple levels of granularity, from\nlow-level syntax errors to high-level algorithmic flaws. In this paper, we\nintroduce Multi-Granularity Debugger (MGDebugger), a hierarchical code debugger\nby isolating, identifying, and resolving bugs at various levels of granularity.\nMGDebugger decomposes problematic code into a hierarchical tree structure of\nsubfunctions, with each level representing a particular granularity of error.\nDuring debugging, it analyzes each subfunction and iteratively resolves bugs in\na bottom-up manner. To effectively test each subfunction, we propose an\nLLM-simulated Python executor, which traces code execution and tracks important\nvariable states to pinpoint errors accurately. Extensive experiments\ndemonstrate that MGDebugger outperforms existing debugging systems, achieving\nan 18.9% improvement in accuracy over seed generations in HumanEval and a 97.6%\nrepair success rate in HumanEvalFix. Furthermore, MGDebugger effectively fixes\nbugs across different categories and difficulty levels, demonstrating its\nrobustness and effectiveness."
                },
                "authors": [
                    {
                        "name": "Yuling Shi"
                    },
                    {
                        "name": "Songsong Wang"
                    },
                    {
                        "name": "Chengcheng Wan"
                    },
                    {
                        "name": "Min Wang"
                    },
                    {
                        "name": "Xiaodong Gu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaodong Gu"
                },
                "author": "Xiaodong Gu",
                "arxiv_comment": "Code and data available at https://github.com/YerbaPage/MGDebugger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01215v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01215v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05344v1",
                "updated": "2025-08-07T12:49:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    12,
                    49,
                    44,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T12:49:44Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    12,
                    49,
                    44,
                    3,
                    219,
                    0
                ],
                "title": "NomicLaw: Emergent Trust and Strategic Argumentation in LLMs During\n  Collaborative Law-Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NomicLaw: Emergent Trust and Strategic Argumentation in LLMs During\n  Collaborative Law-Making"
                },
                "summary": "Recent advancements in large language models (LLMs) have extended their\ncapabilities from basic text processing to complex reasoning tasks, including\nlegal interpretation, argumentation, and strategic interaction. However,\nempirical understanding of LLM behavior in open-ended, multi-agent settings\nespecially those involving deliberation over legal and ethical dilemmas remains\nlimited. We introduce NomicLaw, a structured multi-agent simulation where LLMs\nengage in collaborative law-making, responding to complex legal vignettes by\nproposing rules, justifying them, and voting on peer proposals. We\nquantitatively measure trust and reciprocity via voting patterns and\nqualitatively assess how agents use strategic language to justify proposals and\ninfluence outcomes. Experiments involving homogeneous and heterogeneous LLM\ngroups demonstrate how agents spontaneously form alliances, betray trust, and\nadapt their rhetoric to shape collective decisions. Our results highlight the\nlatent social reasoning and persuasive capabilities of ten open-source LLMs and\nprovide insights into the design of future AI systems capable of autonomous\nnegotiation, coordination and drafting legislation in legal settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have extended their\ncapabilities from basic text processing to complex reasoning tasks, including\nlegal interpretation, argumentation, and strategic interaction. However,\nempirical understanding of LLM behavior in open-ended, multi-agent settings\nespecially those involving deliberation over legal and ethical dilemmas remains\nlimited. We introduce NomicLaw, a structured multi-agent simulation where LLMs\nengage in collaborative law-making, responding to complex legal vignettes by\nproposing rules, justifying them, and voting on peer proposals. We\nquantitatively measure trust and reciprocity via voting patterns and\nqualitatively assess how agents use strategic language to justify proposals and\ninfluence outcomes. Experiments involving homogeneous and heterogeneous LLM\ngroups demonstrate how agents spontaneously form alliances, betray trust, and\nadapt their rhetoric to shape collective decisions. Our results highlight the\nlatent social reasoning and persuasive capabilities of ten open-source LLMs and\nprovide insights into the design of future AI systems capable of autonomous\nnegotiation, coordination and drafting legislation in legal settings."
                },
                "authors": [
                    {
                        "name": "Asutosh Hota"
                    },
                    {
                        "name": "Jussi P. P. Jokinen"
                    }
                ],
                "author_detail": {
                    "name": "Jussi P. P. Jokinen"
                },
                "author": "Jussi P. P. Jokinen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05342v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05342v1",
                "updated": "2025-08-07T12:48:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    12,
                    48,
                    9,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T12:48:09Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    12,
                    48,
                    9,
                    3,
                    219,
                    0
                ],
                "title": "Information-Theoretic Graph Fusion with Vision-Language-Action Model for\n  Policy Reasoning and Dual Robotic Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information-Theoretic Graph Fusion with Vision-Language-Action Model for\n  Policy Reasoning and Dual Robotic Control"
                },
                "summary": "Teaching robots dexterous skills from human videos remains challenging due to\nthe reliance on low-level trajectory imitation, which fails to generalize\nacross object types, spatial layouts, and manipulator configurations. We\npropose Graph-Fused Vision-Language-Action (GF-VLA), a framework that enables\ndual-arm robotic systems to perform task-level reasoning and execution directly\nfrom RGB and Depth human demonstrations. GF-VLA first extracts\nShannon-information-based cues to identify hands and objects with the highest\ntask relevance, then encodes these cues into temporally ordered scene graphs\nthat capture both hand-object and object-object interactions. These graphs are\nfused with a language-conditioned transformer that generates hierarchical\nbehavior trees and interpretable Cartesian motion commands. To improve\nexecution efficiency in bimanual settings, we further introduce a cross-hand\nselection policy that infers optimal gripper assignment without explicit\ngeometric reasoning. We evaluate GF-VLA on four structured dual-arm block\nassembly tasks involving symbolic shape construction and spatial\ngeneralization. Experimental results show that the information-theoretic scene\nrepresentation achieves over 95 percent graph accuracy and 93 percent subtask\nsegmentation, supporting the LLM planner in generating reliable and\nhuman-readable task policies. When executed by the dual-arm robot, these\npolicies yield 94 percent grasp success, 89 percent placement accuracy, and 90\npercent overall task success across stacking, letter-building, and geometric\nreconfiguration scenarios, demonstrating strong generalization and robustness\nacross diverse spatial and semantic variations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaching robots dexterous skills from human videos remains challenging due to\nthe reliance on low-level trajectory imitation, which fails to generalize\nacross object types, spatial layouts, and manipulator configurations. We\npropose Graph-Fused Vision-Language-Action (GF-VLA), a framework that enables\ndual-arm robotic systems to perform task-level reasoning and execution directly\nfrom RGB and Depth human demonstrations. GF-VLA first extracts\nShannon-information-based cues to identify hands and objects with the highest\ntask relevance, then encodes these cues into temporally ordered scene graphs\nthat capture both hand-object and object-object interactions. These graphs are\nfused with a language-conditioned transformer that generates hierarchical\nbehavior trees and interpretable Cartesian motion commands. To improve\nexecution efficiency in bimanual settings, we further introduce a cross-hand\nselection policy that infers optimal gripper assignment without explicit\ngeometric reasoning. We evaluate GF-VLA on four structured dual-arm block\nassembly tasks involving symbolic shape construction and spatial\ngeneralization. Experimental results show that the information-theoretic scene\nrepresentation achieves over 95 percent graph accuracy and 93 percent subtask\nsegmentation, supporting the LLM planner in generating reliable and\nhuman-readable task policies. When executed by the dual-arm robot, these\npolicies yield 94 percent grasp success, 89 percent placement accuracy, and 90\npercent overall task success across stacking, letter-building, and geometric\nreconfiguration scenarios, demonstrating strong generalization and robustness\nacross diverse spatial and semantic variations."
                },
                "authors": [
                    {
                        "name": "Shunlei Li"
                    },
                    {
                        "name": "Longsen Gao"
                    },
                    {
                        "name": "Jin Wang"
                    },
                    {
                        "name": "Chang Che"
                    },
                    {
                        "name": "Xi Xiao"
                    },
                    {
                        "name": "Jiuwen Cao"
                    },
                    {
                        "name": "Yingbai Hu"
                    },
                    {
                        "name": "Hamid Reza Karimi"
                    }
                ],
                "author_detail": {
                    "name": "Hamid Reza Karimi"
                },
                "author": "Hamid Reza Karimi",
                "arxiv_comment": "Journal under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05342v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05342v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02253v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02253v3",
                "updated": "2025-08-07T12:29:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    12,
                    29,
                    47,
                    3,
                    219,
                    0
                ],
                "published": "2025-07-03T03:02:49Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    3,
                    2,
                    49,
                    3,
                    184,
                    0
                ],
                "title": "Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and\n  Rigorous Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and\n  Rigorous Evaluation"
                },
                "summary": "Effective agent performance relies on the ability to compose tools and agents\ninto effective workflows. However, progress in Large Language Model (LLM)\nplanning and reasoning is limited by the scarcity of scalable, reliable\nevaluation data. This study addresses this limitation by identifying a suitable\nworkflow domain for LLM application. I introduce NL2Flow, a fully automated\nsystem for parametrically generating planning problems, which are expressed in\nnatural language, a structured intermediate representation, and formal PDDL,\nand rigorously evaluating the quality of generated plans. NL2Flow generates a\ndataset of 2296 low-difficulty problems in automated workflow generation and\nevaluates multiple open-sourced, instruct-tuned LLMs without task-specific\noptimization or architectural modifications. Results reveal that the highest\nperforming model achieved 86% success in generating valid plans and 69% in\ngenerating optimal plans, specifically for problems with feasible plans.\nRegression analysis shows that the influence of problem characteristics on plan\ngeneration is contingent on both model and prompt design. To investigate the\npotential of LLMs as natural language-to-JSON translators for workflow\ndefinition, and to facilitate integration with downstream symbolic computation\ntools and a symbolic planner, I evaluated the LLM's translation performance on\nnatural language workflow descriptions. I observed that translating natural\nlanguage into a JSON representation of a workflow problem yielded a lower\nsuccess rate than generating a plan directly, suggesting that unnecessary\ndecomposition of the reasoning task may degrade performance and highlighting\nthe benefit of models capable of reasoning directly from natural language to\naction. As LLM reasoning scales to increasingly complex problems, understanding\nthe shifting bottlenecks and sources of error within these systems will be\ncrucial.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective agent performance relies on the ability to compose tools and agents\ninto effective workflows. However, progress in Large Language Model (LLM)\nplanning and reasoning is limited by the scarcity of scalable, reliable\nevaluation data. This study addresses this limitation by identifying a suitable\nworkflow domain for LLM application. I introduce NL2Flow, a fully automated\nsystem for parametrically generating planning problems, which are expressed in\nnatural language, a structured intermediate representation, and formal PDDL,\nand rigorously evaluating the quality of generated plans. NL2Flow generates a\ndataset of 2296 low-difficulty problems in automated workflow generation and\nevaluates multiple open-sourced, instruct-tuned LLMs without task-specific\noptimization or architectural modifications. Results reveal that the highest\nperforming model achieved 86% success in generating valid plans and 69% in\ngenerating optimal plans, specifically for problems with feasible plans.\nRegression analysis shows that the influence of problem characteristics on plan\ngeneration is contingent on both model and prompt design. To investigate the\npotential of LLMs as natural language-to-JSON translators for workflow\ndefinition, and to facilitate integration with downstream symbolic computation\ntools and a symbolic planner, I evaluated the LLM's translation performance on\nnatural language workflow descriptions. I observed that translating natural\nlanguage into a JSON representation of a workflow problem yielded a lower\nsuccess rate than generating a plan directly, suggesting that unnecessary\ndecomposition of the reasoning task may degrade performance and highlighting\nthe benefit of models capable of reasoning directly from natural language to\naction. As LLM reasoning scales to increasingly complex problems, understanding\nthe shifting bottlenecks and sources of error within these systems will be\ncrucial."
                },
                "authors": [
                    {
                        "name": "Jungkoo Kang"
                    }
                ],
                "author_detail": {
                    "name": "Jungkoo Kang"
                },
                "author": "Jungkoo Kang",
                "arxiv_comment": "26 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02253v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02253v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06608v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06608v5",
                "updated": "2025-08-07T12:26:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    12,
                    26,
                    15,
                    3,
                    219,
                    0
                ],
                "published": "2025-07-09T07:27:18Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    7,
                    27,
                    18,
                    2,
                    190,
                    0
                ],
                "title": "Nexus:Proactive Intra-GPU Disaggregation of Prefill and Decode in LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nexus:Proactive Intra-GPU Disaggregation of Prefill and Decode in LLM\n  Serving"
                },
                "summary": "Monolithic serving with chunked prefill improves GPU utilization by batching\nprefill and decode together, but suffers from fine-grained phase interference.\nEngine-level prefill-decode (PD) disaggregation avoids interference but incurs\nhigher hardware and coordination overhead. Prior intra-GPU disaggregation\napproaches multiplex prefill and decode within a single GPU, using SLO-based\ntuning guided by heuristics from offline profiling or reactive feedback loops.\nHowever, these methods respond reactively to performance issues rather than\nanticipating them, limiting adaptability under dynamic workloads.\n  We ask: can we achieve proactive intra-GPU disaggregation that adapts\neffectively to dynamic workloads? The key challenge lies in managing the\nconflicting resource demands of prefill and decode under varying conditions. We\nfirst show that GPU resources exhibit diminishing returns -- beyond a\nsaturation point, more allocation yields minimal latency benefit. Second, we\nobserve that memory bandwidth contention becomes a critical bottleneck. These\ninsights motivate a design that dynamically partitions GPU resources across\nprefill and decode phases, while jointly considering compute capacity, memory\nfootprint, and bandwidth contention.\n  Evaluated on diverse LLMs and workloads, our system Nexus achieves up to 2.2x\nhigher throughput, 20x lower TTFT, and 2.5x lower TBT than vLLM; outperforms\nSGLang by up to 2x; and matches or exceeds disaggregated vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monolithic serving with chunked prefill improves GPU utilization by batching\nprefill and decode together, but suffers from fine-grained phase interference.\nEngine-level prefill-decode (PD) disaggregation avoids interference but incurs\nhigher hardware and coordination overhead. Prior intra-GPU disaggregation\napproaches multiplex prefill and decode within a single GPU, using SLO-based\ntuning guided by heuristics from offline profiling or reactive feedback loops.\nHowever, these methods respond reactively to performance issues rather than\nanticipating them, limiting adaptability under dynamic workloads.\n  We ask: can we achieve proactive intra-GPU disaggregation that adapts\neffectively to dynamic workloads? The key challenge lies in managing the\nconflicting resource demands of prefill and decode under varying conditions. We\nfirst show that GPU resources exhibit diminishing returns -- beyond a\nsaturation point, more allocation yields minimal latency benefit. Second, we\nobserve that memory bandwidth contention becomes a critical bottleneck. These\ninsights motivate a design that dynamically partitions GPU resources across\nprefill and decode phases, while jointly considering compute capacity, memory\nfootprint, and bandwidth contention.\n  Evaluated on diverse LLMs and workloads, our system Nexus achieves up to 2.2x\nhigher throughput, 20x lower TTFT, and 2.5x lower TBT than vLLM; outperforms\nSGLang by up to 2x; and matches or exceeds disaggregated vLLM."
                },
                "authors": [
                    {
                        "name": "Xiaoxiang Shi"
                    },
                    {
                        "name": "Colin Cai"
                    },
                    {
                        "name": "Junjia Du"
                    },
                    {
                        "name": "Zhihao Jia"
                    }
                ],
                "author_detail": {
                    "name": "Zhihao Jia"
                },
                "author": "Zhihao Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06608v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06608v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05311v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05311v1",
                "updated": "2025-08-07T12:11:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    12,
                    11,
                    53,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T12:11:53Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    12,
                    11,
                    53,
                    3,
                    219,
                    0
                ],
                "title": "A Novel Architecture for Symbolic Reasoning with Decision Trees and LLM\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Novel Architecture for Symbolic Reasoning with Decision Trees and LLM\n  Agents"
                },
                "summary": "We propose a hybrid architecture that integrates decision tree-based symbolic\nreasoning with the generative capabilities of large language models (LLMs)\nwithin a coordinated multi-agent framework. Unlike prior approaches that\nloosely couple symbolic and neural modules, our design embeds decision trees\nand random forests as callable oracles within a unified reasoning system.\nTree-based modules enable interpretable rule inference and causal logic, while\nLLM agents handle abductive reasoning, generalization, and interactive\nplanning. A central orchestrator maintains belief state consistency and\nmediates communication across agents and external tools, enabling reasoning\nover both structured and unstructured inputs.\n  The system achieves strong performance on reasoning benchmarks. On\n\\textit{ProofWriter}, it improves entailment consistency by +7.2\\% through\nlogic-grounded tree validation. On GSM8k, it achieves +5.3\\% accuracy gains in\nmultistep mathematical problems via symbolic augmentation. On \\textit{ARC}, it\nboosts abstraction accuracy by +6.0\\% through integration of symbolic oracles.\nApplications in clinical decision support and scientific discovery show how the\nsystem encodes domain rules symbolically while leveraging LLMs for contextual\ninference and hypothesis generation. This architecture offers a robust,\ninterpretable, and extensible solution for general-purpose neuro-symbolic\nreasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a hybrid architecture that integrates decision tree-based symbolic\nreasoning with the generative capabilities of large language models (LLMs)\nwithin a coordinated multi-agent framework. Unlike prior approaches that\nloosely couple symbolic and neural modules, our design embeds decision trees\nand random forests as callable oracles within a unified reasoning system.\nTree-based modules enable interpretable rule inference and causal logic, while\nLLM agents handle abductive reasoning, generalization, and interactive\nplanning. A central orchestrator maintains belief state consistency and\nmediates communication across agents and external tools, enabling reasoning\nover both structured and unstructured inputs.\n  The system achieves strong performance on reasoning benchmarks. On\n\\textit{ProofWriter}, it improves entailment consistency by +7.2\\% through\nlogic-grounded tree validation. On GSM8k, it achieves +5.3\\% accuracy gains in\nmultistep mathematical problems via symbolic augmentation. On \\textit{ARC}, it\nboosts abstraction accuracy by +6.0\\% through integration of symbolic oracles.\nApplications in clinical decision support and scientific discovery show how the\nsystem encodes domain rules symbolically while leveraging LLMs for contextual\ninference and hypothesis generation. This architecture offers a robust,\ninterpretable, and extensible solution for general-purpose neuro-symbolic\nreasoning."
                },
                "authors": [
                    {
                        "name": "Andrew Kiruluta"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Kiruluta"
                },
                "author": "Andrew Kiruluta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05311v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05311v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03930v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03930v2",
                "updated": "2025-08-07T12:06:29Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    12,
                    6,
                    29,
                    3,
                    219,
                    0
                ],
                "published": "2024-12-05T07:12:53Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    7,
                    12,
                    53,
                    3,
                    340,
                    0
                ],
                "title": "GuARD: Effective Anomaly Detection through a Text-Rich and\n  Graph-Informed Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GuARD: Effective Anomaly Detection through a Text-Rich and\n  Graph-Informed Language Model"
                },
                "summary": "Anomaly detection on text-rich graphs is widely prevalent in real life, such\nas detecting incorrectly assigned academic papers to authors and detecting bots\nin social networks. The remarkable capabilities of large language models (LLMs)\npave a new revenue by utilizing rich-text information for effective anomaly\ndetection. However, simply introducing rich texts into LLMs can obscure\nessential detection cues and introduce high fine-tuning costs. Moreover, LLMs\noften overlook the intrinsic structural bias of graphs which is vital for\ndistinguishing normal from abnormal node patterns. To this end, this paper\nintroduces GuARD, a text-rich and graph-informed language model that combines\nkey structural features from graph-based methods with fine-grained semantic\nattributes extracted via small language models for effective anomaly detection\non text-rich graphs. GuARD is optimized with the progressive multi-modal\nmulti-turn instruction tuning framework in the task-guided instruction tuning\nregime tailed to incorporate both rich-text and structural modalities.\nExtensive experiments on four datasets reveal that GuARD outperforms\ngraph-based and LLM-based anomaly detection methods, while offering up to\n5$\\times$ times speedup in training and 5$\\times$ times speedup in inference\nover vanilla long-context LLMs on the large-scale WhoIsWho dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anomaly detection on text-rich graphs is widely prevalent in real life, such\nas detecting incorrectly assigned academic papers to authors and detecting bots\nin social networks. The remarkable capabilities of large language models (LLMs)\npave a new revenue by utilizing rich-text information for effective anomaly\ndetection. However, simply introducing rich texts into LLMs can obscure\nessential detection cues and introduce high fine-tuning costs. Moreover, LLMs\noften overlook the intrinsic structural bias of graphs which is vital for\ndistinguishing normal from abnormal node patterns. To this end, this paper\nintroduces GuARD, a text-rich and graph-informed language model that combines\nkey structural features from graph-based methods with fine-grained semantic\nattributes extracted via small language models for effective anomaly detection\non text-rich graphs. GuARD is optimized with the progressive multi-modal\nmulti-turn instruction tuning framework in the task-guided instruction tuning\nregime tailed to incorporate both rich-text and structural modalities.\nExtensive experiments on four datasets reveal that GuARD outperforms\ngraph-based and LLM-based anomaly detection methods, while offering up to\n5$\\times$ times speedup in training and 5$\\times$ times speedup in inference\nover vanilla long-context LLMs on the large-scale WhoIsWho dataset."
                },
                "authors": [
                    {
                        "name": "Yunhe Pang"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Fanjin Zhang"
                    },
                    {
                        "name": "Yanghui Rao"
                    },
                    {
                        "name": "Evgeny Kharlamov"
                    },
                    {
                        "name": "Jie Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Tang"
                },
                "author": "Jie Tang",
                "arxiv_doi": "10.1145/3711896.3736993",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3711896.3736993",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.03930v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03930v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at KDD 2025",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05305v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05305v1",
                "updated": "2025-08-07T12:03:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    12,
                    3,
                    44,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T12:03:44Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    12,
                    3,
                    44,
                    3,
                    219,
                    0
                ],
                "title": "SONAR-LLM: Autoregressive Transformer that Thinks in Sentence Embeddings\n  and Speaks in Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SONAR-LLM: Autoregressive Transformer that Thinks in Sentence Embeddings\n  and Speaks in Tokens"
                },
                "summary": "The recently proposed Large Concept Model (LCM) generates text by predicting\na sequence of sentence-level embeddings and training with either mean-squared\nerror or diffusion objectives. We present SONAR-LLM, a decoder-only transformer\nthat \"thinks\" in the same continuous SONAR embedding space, yet is supervised\nthrough token-level cross-entropy propagated via the frozen SONAR decoder. This\nhybrid objective retains the semantic abstraction of LCM while eliminating its\ndiffusion sampler and restoring a likelihood-based training signal. Across\nmodel sizes from 39M to 1.3B parameters, SONAR-LLM attains competitive\ngeneration quality. We report scaling trends, ablations, benchmark results, and\nrelease the complete training code and all pretrained checkpoints to foster\nreproducibility and future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recently proposed Large Concept Model (LCM) generates text by predicting\na sequence of sentence-level embeddings and training with either mean-squared\nerror or diffusion objectives. We present SONAR-LLM, a decoder-only transformer\nthat \"thinks\" in the same continuous SONAR embedding space, yet is supervised\nthrough token-level cross-entropy propagated via the frozen SONAR decoder. This\nhybrid objective retains the semantic abstraction of LCM while eliminating its\ndiffusion sampler and restoring a likelihood-based training signal. Across\nmodel sizes from 39M to 1.3B parameters, SONAR-LLM attains competitive\ngeneration quality. We report scaling trends, ablations, benchmark results, and\nrelease the complete training code and all pretrained checkpoints to foster\nreproducibility and future research."
                },
                "authors": [
                    {
                        "name": "Nikita Dragunov"
                    },
                    {
                        "name": "Temurbek Rahmatullaev"
                    },
                    {
                        "name": "Elizaveta Goncharova"
                    },
                    {
                        "name": "Andrey Kuznetsov"
                    },
                    {
                        "name": "Anton Razzhigaev"
                    }
                ],
                "author_detail": {
                    "name": "Anton Razzhigaev"
                },
                "author": "Anton Razzhigaev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05305v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05305v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05299v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05299v1",
                "updated": "2025-08-07T11:59:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    11,
                    59,
                    50,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T11:59:50Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    11,
                    59,
                    50,
                    3,
                    219,
                    0
                ],
                "title": "VS-LLM: Visual-Semantic Depression Assessment based on LLM for Drawing\n  Projection Test",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VS-LLM: Visual-Semantic Depression Assessment based on LLM for Drawing\n  Projection Test"
                },
                "summary": "The Drawing Projection Test (DPT) is an essential tool in art therapy,\nallowing psychologists to assess participants' mental states through their\nsketches. Specifically, through sketches with the theme of \"a person picking an\napple from a tree (PPAT)\", it can be revealed whether the participants are in\nmental states such as depression. Compared with scales, the DPT can enrich\npsychologists' understanding of an individual's mental state. However, the\ninterpretation of the PPAT is laborious and depends on the experience of the\npsychologists. To address this issue, we propose an effective identification\nmethod to support psychologists in conducting a large-scale automatic DPT.\nUnlike traditional sketch recognition, DPT more focus on the overall evaluation\nof the sketches, such as color usage and space utilization. Moreover, PPAT\nimposes a time limit and prohibits verbal reminders, resulting in low drawing\naccuracy and a lack of detailed depiction. To address these challenges, we\npropose the following efforts: (1) Providing an experimental environment for\nautomated analysis of PPAT sketches for depression assessment; (2) Offering a\nVisual-Semantic depression assessment based on LLM (VS-LLM) method; (3)\nExperimental results demonstrate that our method improves by 17.6% compared to\nthe psychologist assessment method. We anticipate that this work will\ncontribute to the research in mental state assessment based on PPAT sketches'\nelements recognition. Our datasets and codes are available at\nhttps://github.com/wmeiqi/VS-LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Drawing Projection Test (DPT) is an essential tool in art therapy,\nallowing psychologists to assess participants' mental states through their\nsketches. Specifically, through sketches with the theme of \"a person picking an\napple from a tree (PPAT)\", it can be revealed whether the participants are in\nmental states such as depression. Compared with scales, the DPT can enrich\npsychologists' understanding of an individual's mental state. However, the\ninterpretation of the PPAT is laborious and depends on the experience of the\npsychologists. To address this issue, we propose an effective identification\nmethod to support psychologists in conducting a large-scale automatic DPT.\nUnlike traditional sketch recognition, DPT more focus on the overall evaluation\nof the sketches, such as color usage and space utilization. Moreover, PPAT\nimposes a time limit and prohibits verbal reminders, resulting in low drawing\naccuracy and a lack of detailed depiction. To address these challenges, we\npropose the following efforts: (1) Providing an experimental environment for\nautomated analysis of PPAT sketches for depression assessment; (2) Offering a\nVisual-Semantic depression assessment based on LLM (VS-LLM) method; (3)\nExperimental results demonstrate that our method improves by 17.6% compared to\nthe psychologist assessment method. We anticipate that this work will\ncontribute to the research in mental state assessment based on PPAT sketches'\nelements recognition. Our datasets and codes are available at\nhttps://github.com/wmeiqi/VS-LLM."
                },
                "authors": [
                    {
                        "name": "Meiqi Wu"
                    },
                    {
                        "name": "Yaxuan Kang"
                    },
                    {
                        "name": "Xuchen Li"
                    },
                    {
                        "name": "Shiyu Hu"
                    },
                    {
                        "name": "Xiaotang Chen"
                    },
                    {
                        "name": "Yunfeng Kang"
                    },
                    {
                        "name": "Weiqiang Wang"
                    },
                    {
                        "name": "Kaiqi Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kaiqi Huang"
                },
                "author": "Kaiqi Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05299v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05299v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05298v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05298v1",
                "updated": "2025-08-07T11:55:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    11,
                    55,
                    46,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T11:55:46Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    11,
                    55,
                    46,
                    3,
                    219,
                    0
                ],
                "title": "GhostShell: Streaming LLM Function Calls for Concurrent Embodied\n  Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GhostShell: Streaming LLM Function Calls for Concurrent Embodied\n  Programming"
                },
                "summary": "We present GhostShell, a novel approach that leverages Large Language Models\n(LLMs) to enable streaming and concurrent behavioral programming for embodied\nsystems. In contrast to conventional methods that rely on pre-scheduled action\nsequences or behavior trees, GhostShell drives embodied systems to act\non-the-fly by issuing function calls incrementally as tokens are streamed from\nthe LLM. GhostShell features a streaming XML function token parser, a dynamic\nfunction interface mapper, and a multi-channel scheduler that orchestrates\nintra-channel synchronous and inter-channel asynchronous function calls,\nthereby coordinating serial-parallel embodied actions across multiple robotic\ncomponents as directed by the LLM. We evaluate GhostShell on our robot\nprototype COCO through comprehensive grounded experiments across 34 real-world\ninteraction tasks and multiple LLMs. The results demonstrate that our approach\nachieves state-of-the-art Behavioral Correctness Metric of 0.85 with Claude-4\nSonnet and up to 66X faster response times compared to LLM native function\ncalling APIs. GhostShell also proves effective in long-horizon multimodal\ntasks, demonstrating strong robustness and generalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present GhostShell, a novel approach that leverages Large Language Models\n(LLMs) to enable streaming and concurrent behavioral programming for embodied\nsystems. In contrast to conventional methods that rely on pre-scheduled action\nsequences or behavior trees, GhostShell drives embodied systems to act\non-the-fly by issuing function calls incrementally as tokens are streamed from\nthe LLM. GhostShell features a streaming XML function token parser, a dynamic\nfunction interface mapper, and a multi-channel scheduler that orchestrates\nintra-channel synchronous and inter-channel asynchronous function calls,\nthereby coordinating serial-parallel embodied actions across multiple robotic\ncomponents as directed by the LLM. We evaluate GhostShell on our robot\nprototype COCO through comprehensive grounded experiments across 34 real-world\ninteraction tasks and multiple LLMs. The results demonstrate that our approach\nachieves state-of-the-art Behavioral Correctness Metric of 0.85 with Claude-4\nSonnet and up to 66X faster response times compared to LLM native function\ncalling APIs. GhostShell also proves effective in long-horizon multimodal\ntasks, demonstrating strong robustness and generalization."
                },
                "authors": [
                    {
                        "name": "Jian Gong"
                    },
                    {
                        "name": "Youwei Huang"
                    },
                    {
                        "name": "Bo Yuan"
                    },
                    {
                        "name": "Ming Zhu"
                    },
                    {
                        "name": "Juncheng Zhan"
                    },
                    {
                        "name": "Jinke Wang"
                    },
                    {
                        "name": "Hang Shu"
                    },
                    {
                        "name": "Mingyue Xiong"
                    },
                    {
                        "name": "Yanjun Ye"
                    },
                    {
                        "name": "Yufan Zu"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Yihan Ding"
                    },
                    {
                        "name": "Xuannian Chen"
                    },
                    {
                        "name": "Xingyu Lu"
                    },
                    {
                        "name": "Runjie Ban"
                    },
                    {
                        "name": "Bingchao Huang"
                    },
                    {
                        "name": "Fusen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Fusen Liu"
                },
                "author": "Fusen Liu",
                "arxiv_comment": "17 pages, 5 figures, conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05298v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05298v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05294v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05294v1",
                "updated": "2025-08-07T11:48:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    11,
                    48,
                    3,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T11:48:03Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    11,
                    48,
                    3,
                    3,
                    219,
                    0
                ],
                "title": "Towards Embodied Agentic AI: Review and Classification of LLM- and\n  VLM-Driven Robot Autonomy and Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Embodied Agentic AI: Review and Classification of LLM- and\n  VLM-Driven Robot Autonomy and Interaction"
                },
                "summary": "Foundation models, including large language models (LLMs) and vision-language\nmodels (VLMs), have recently enabled novel approaches to robot autonomy and\nhuman-robot interfaces. In parallel, vision-language-action models (VLAs) or\nlarge behavior models (BLMs) are increasing the dexterity and capabilities of\nrobotic systems. This survey paper focuses on those words advancing towards\nagentic applications and architectures. This includes initial efforts exploring\nGPT-style interfaces to tooling, as well as more complex system where AI agents\nare coordinators, planners, perception actors, or generalist interfaces. Such\nagentic architectures allow robots to reason over natural language\ninstructions, invoke APIs, plan task sequences, or assist in operations and\ndiagnostics. In addition to peer-reviewed research, due to the fast-evolving\nnature of the field, we highlight and include community-driven projects, ROS\npackages, and industrial frameworks that show emerging trends. We propose a\ntaxonomy for classifying model integration approaches and present a comparative\nanalysis of the role that agents play in different solutions in today's\nliterature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation models, including large language models (LLMs) and vision-language\nmodels (VLMs), have recently enabled novel approaches to robot autonomy and\nhuman-robot interfaces. In parallel, vision-language-action models (VLAs) or\nlarge behavior models (BLMs) are increasing the dexterity and capabilities of\nrobotic systems. This survey paper focuses on those words advancing towards\nagentic applications and architectures. This includes initial efforts exploring\nGPT-style interfaces to tooling, as well as more complex system where AI agents\nare coordinators, planners, perception actors, or generalist interfaces. Such\nagentic architectures allow robots to reason over natural language\ninstructions, invoke APIs, plan task sequences, or assist in operations and\ndiagnostics. In addition to peer-reviewed research, due to the fast-evolving\nnature of the field, we highlight and include community-driven projects, ROS\npackages, and industrial frameworks that show emerging trends. We propose a\ntaxonomy for classifying model integration approaches and present a comparative\nanalysis of the role that agents play in different solutions in today's\nliterature."
                },
                "authors": [
                    {
                        "name": "Sahar Salimpour"
                    },
                    {
                        "name": "Lei Fu"
                    },
                    {
                        "name": "Farhad Keramat"
                    },
                    {
                        "name": "Leonardo Militano"
                    },
                    {
                        "name": "Giovanni Toffetti"
                    },
                    {
                        "name": "Harry Edelman"
                    },
                    {
                        "name": "Jorge Pea Queralta"
                    }
                ],
                "author_detail": {
                    "name": "Jorge Pea Queralta"
                },
                "author": "Jorge Pea Queralta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05294v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05294v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05289v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05289v1",
                "updated": "2025-08-07T11:36:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    11,
                    36,
                    55,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T11:36:55Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    11,
                    36,
                    55,
                    3,
                    219,
                    0
                ],
                "title": "RLHF Fine-Tuning of LLMs for Alignment with Implicit User Feedback in\n  Conversational Recommenders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RLHF Fine-Tuning of LLMs for Alignment with Implicit User Feedback in\n  Conversational Recommenders"
                },
                "summary": "Conversational recommender systems (CRS) based on Large Language Models\n(LLMs) need to constantly be aligned to the user preferences to provide\nsatisfying and context-relevant item recommendations. The traditional\nsupervised fine-tuning cannot capture the implicit feedback signal, e.g., dwell\ntime, sentiment polarity, or engagement patterns. In this paper, we share a\nfine-tuning solution using human feedback reinforcement learning (RLHF) to\nmaximize implied user feedback (IUF) in a multi-turn recommendation context. We\nspecify a reward model $R_{\\phi}$ learnt on weakly-labelled engagement\ninformation and maximize user-centric utility by optimizing the foundational\nLLM M_{\\theta} through a proximal policy optimization (PPO) approach. The\narchitecture models conversational state transitions $s_t \\to a_t \\to s_{t\n+1}$, where the action $a_t$ is associated with LLM-generated item suggestions\nonly on condition of conversation history in the past. The evaluation across\nsynthetic and real-world datasets (e.g.REDIAL, OpenDialKG) demonstrates that\nour RLHF-fine-tuned models can perform better in terms of top-$k$\nrecommendation accuracy, coherence, and user satisfaction compared to\n(arrow-zero-cmwrquca-teja-falset ensuite 2Round group-deca States penalty give\nup This paper shows that implicit signal alignment can be efficient in\nachieving scalable and user-adaptive design of CRS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conversational recommender systems (CRS) based on Large Language Models\n(LLMs) need to constantly be aligned to the user preferences to provide\nsatisfying and context-relevant item recommendations. The traditional\nsupervised fine-tuning cannot capture the implicit feedback signal, e.g., dwell\ntime, sentiment polarity, or engagement patterns. In this paper, we share a\nfine-tuning solution using human feedback reinforcement learning (RLHF) to\nmaximize implied user feedback (IUF) in a multi-turn recommendation context. We\nspecify a reward model $R_{\\phi}$ learnt on weakly-labelled engagement\ninformation and maximize user-centric utility by optimizing the foundational\nLLM M_{\\theta} through a proximal policy optimization (PPO) approach. The\narchitecture models conversational state transitions $s_t \\to a_t \\to s_{t\n+1}$, where the action $a_t$ is associated with LLM-generated item suggestions\nonly on condition of conversation history in the past. The evaluation across\nsynthetic and real-world datasets (e.g.REDIAL, OpenDialKG) demonstrates that\nour RLHF-fine-tuned models can perform better in terms of top-$k$\nrecommendation accuracy, coherence, and user satisfaction compared to\n(arrow-zero-cmwrquca-teja-falset ensuite 2Round group-deca States penalty give\nup This paper shows that implicit signal alignment can be efficient in\nachieving scalable and user-adaptive design of CRS."
                },
                "authors": [
                    {
                        "name": "Zhongheng Yang"
                    },
                    {
                        "name": "Aijia Sun"
                    },
                    {
                        "name": "Yushang Zhao"
                    },
                    {
                        "name": "Yinuo Yang"
                    },
                    {
                        "name": "Dannier Li"
                    },
                    {
                        "name": "Chengrui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Chengrui Zhou"
                },
                "author": "Chengrui Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05289v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05289v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04632v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04632v2",
                "updated": "2025-08-07T11:30:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    11,
                    30,
                    20,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-06T17:00:54Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    17,
                    0,
                    54,
                    2,
                    218,
                    0
                ],
                "title": "IFDECORATOR: Wrapping Instruction Following Reinforcement Learning with\n  Verifiable Rewards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IFDECORATOR: Wrapping Instruction Following Reinforcement Learning with\n  Verifiable Rewards"
                },
                "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) improves instruction\nfollowing capabilities of large language models (LLMs), but suffers from\ntraining inefficiency due to inadequate difficulty assessment. Moreover, RLVR\nis prone to over-optimization, where LLMs exploit verification shortcuts\nwithout aligning to the actual intent of user instructions. We introduce\nInstruction Following Decorator (IFDecorator}, a framework that wraps RLVR\ntraining into a robust and sample-efficient pipeline. It consists of three\ncomponents: (1) a cooperative-adversarial data flywheel that co-evolves\ninstructions and hybrid verifications, generating progressively more\nchallenging instruction-verification pairs; (2) IntentCheck, a bypass module\nenforcing intent alignment; and (3) trip wires, a diagnostic mechanism that\ndetects reward hacking via trap instructions, which trigger and capture\nshortcut exploitation behaviors. Our Qwen2.5-32B-Instruct-IFDecorator achieves\n87.43% accuracy on IFEval, outperforming larger proprietary models such as\nGPT-4o. Additionally, we demonstrate substantial improvements on FollowBench\nwhile preserving general capabilities. Our trip wires show significant\nreductions in reward hacking rates. We will release models, code, and data for\nfuture research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Verifiable Rewards (RLVR) improves instruction\nfollowing capabilities of large language models (LLMs), but suffers from\ntraining inefficiency due to inadequate difficulty assessment. Moreover, RLVR\nis prone to over-optimization, where LLMs exploit verification shortcuts\nwithout aligning to the actual intent of user instructions. We introduce\nInstruction Following Decorator (IFDecorator}, a framework that wraps RLVR\ntraining into a robust and sample-efficient pipeline. It consists of three\ncomponents: (1) a cooperative-adversarial data flywheel that co-evolves\ninstructions and hybrid verifications, generating progressively more\nchallenging instruction-verification pairs; (2) IntentCheck, a bypass module\nenforcing intent alignment; and (3) trip wires, a diagnostic mechanism that\ndetects reward hacking via trap instructions, which trigger and capture\nshortcut exploitation behaviors. Our Qwen2.5-32B-Instruct-IFDecorator achieves\n87.43% accuracy on IFEval, outperforming larger proprietary models such as\nGPT-4o. Additionally, we demonstrate substantial improvements on FollowBench\nwhile preserving general capabilities. Our trip wires show significant\nreductions in reward hacking rates. We will release models, code, and data for\nfuture research."
                },
                "authors": [
                    {
                        "name": "Xu Guo"
                    },
                    {
                        "name": "Tianyi Liang"
                    },
                    {
                        "name": "Tong Jian"
                    },
                    {
                        "name": "Xiaogui Yang"
                    },
                    {
                        "name": "Ling-I Wu"
                    },
                    {
                        "name": "Chenhui Li"
                    },
                    {
                        "name": "Zhihui Lu"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "arxiv_comment": "7 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04632v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04632v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05283v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05283v1",
                "updated": "2025-08-07T11:27:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    11,
                    27,
                    43,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T11:27:43Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    11,
                    27,
                    43,
                    3,
                    219,
                    0
                ],
                "title": "Decision-Making with Deliberation: Meta-reviewing as a Document-grounded\n  Dialogue",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decision-Making with Deliberation: Meta-reviewing as a Document-grounded\n  Dialogue"
                },
                "summary": "Meta-reviewing is a pivotal stage in the peer-review process, serving as the\nfinal step in determining whether a paper is recommended for acceptance. Prior\nresearch on meta-reviewing has treated this as a summarization problem over\nreview reports. However, complementary to this perspective, meta-reviewing is a\ndecision-making process that requires weighing reviewer arguments and placing\nthem within a broader context. Prior research has demonstrated that\ndecision-makers can be effectively assisted in such scenarios via dialogue\nagents. In line with this framing, we explore the practical challenges for\nrealizing dialog agents that can effectively assist meta-reviewers. Concretely,\nwe first address the issue of data scarcity for training dialogue agents by\ngenerating synthetic data using Large Language Models (LLMs) based on a\nself-refinement strategy to improve the relevance of these dialogues to expert\ndomains. Our experiments demonstrate that this method produces higher-quality\nsynthetic data and can serve as a valuable resource towards training\nmeta-reviewing assistants. Subsequently, we utilize this data to train dialogue\nagents tailored for meta-reviewing and find that these agents outperform\n\\emph{off-the-shelf} LLM-based assistants for this task. Finally, we apply our\nagents in real-world meta-reviewing scenarios and confirm their effectiveness\nin enhancing the efficiency of meta-reviewing.\\footnote{Code and Data:\nhttps://github.com/UKPLab/arxiv2025-meta-review-as-dialog",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meta-reviewing is a pivotal stage in the peer-review process, serving as the\nfinal step in determining whether a paper is recommended for acceptance. Prior\nresearch on meta-reviewing has treated this as a summarization problem over\nreview reports. However, complementary to this perspective, meta-reviewing is a\ndecision-making process that requires weighing reviewer arguments and placing\nthem within a broader context. Prior research has demonstrated that\ndecision-makers can be effectively assisted in such scenarios via dialogue\nagents. In line with this framing, we explore the practical challenges for\nrealizing dialog agents that can effectively assist meta-reviewers. Concretely,\nwe first address the issue of data scarcity for training dialogue agents by\ngenerating synthetic data using Large Language Models (LLMs) based on a\nself-refinement strategy to improve the relevance of these dialogues to expert\ndomains. Our experiments demonstrate that this method produces higher-quality\nsynthetic data and can serve as a valuable resource towards training\nmeta-reviewing assistants. Subsequently, we utilize this data to train dialogue\nagents tailored for meta-reviewing and find that these agents outperform\n\\emph{off-the-shelf} LLM-based assistants for this task. Finally, we apply our\nagents in real-world meta-reviewing scenarios and confirm their effectiveness\nin enhancing the efficiency of meta-reviewing.\\footnote{Code and Data:\nhttps://github.com/UKPLab/arxiv2025-meta-review-as-dialog"
                },
                "authors": [
                    {
                        "name": "Sukannya Purkayastha"
                    },
                    {
                        "name": "Nils Dycke"
                    },
                    {
                        "name": "Anne Lauscher"
                    },
                    {
                        "name": "Iryna Gurevych"
                    }
                ],
                "author_detail": {
                    "name": "Iryna Gurevych"
                },
                "author": "Iryna Gurevych",
                "arxiv_comment": "36 pages, 16 tables, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05283v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05283v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05282v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05282v1",
                "updated": "2025-08-07T11:26:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    11,
                    26,
                    40,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T11:26:40Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    11,
                    26,
                    40,
                    3,
                    219,
                    0
                ],
                "title": "ASCoT: An Adaptive Self-Correction Chain-of-Thought Method for\n  Late-Stage Fragility in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASCoT: An Adaptive Self-Correction Chain-of-Thought Method for\n  Late-Stage Fragility in LLMs"
                },
                "summary": "Chain-of-Thought (CoT) prompting has significantly advanced the reasoning\ncapabilities of Large Language Models (LLMs), yet the reliability of these\nreasoning chains remains a critical challenge. A widely held \"cascading\nfailure\" hypothesis suggests that errors are most detrimental when they occur\nearly in the reasoning process. This paper challenges that assumption through\nsystematic error-injection experiments, revealing a counter-intuitive\nphenomenon we term \"Late-Stage Fragility\": errors introduced in the later\nstages of a CoT chain are significantly more likely to corrupt the final answer\nthan identical errors made at the beginning. To address this specific\nvulnerability, we introduce the Adaptive Self-Correction Chain-of-Thought\n(ASCoT) method. ASCoT employs a modular pipeline in which an Adaptive\nVerification Manager (AVM) operates first, followed by the Multi-Perspective\nSelf-Correction Engine (MSCE). The AVM leverages a Positional Impact Score\nfunction I(k) that assigns different weights based on the position within the\nreasoning chains, addressing the Late-Stage Fragility issue by identifying and\nprioritizing high-risk, late-stage steps. Once these critical steps are\nidentified, the MSCE applies robust, dual-path correction specifically to the\nfailure parts. Extensive experiments on benchmarks such as GSM8K and MATH\ndemonstrate that ASCoT achieves outstanding accuracy, outperforming strong\nbaselines, including standard CoT. Our work underscores the importance of\ndiagnosing specific failure modes in LLM reasoning and advocates for a shift\nfrom uniform verification strategies to adaptive, vulnerability-aware\ncorrection mechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) prompting has significantly advanced the reasoning\ncapabilities of Large Language Models (LLMs), yet the reliability of these\nreasoning chains remains a critical challenge. A widely held \"cascading\nfailure\" hypothesis suggests that errors are most detrimental when they occur\nearly in the reasoning process. This paper challenges that assumption through\nsystematic error-injection experiments, revealing a counter-intuitive\nphenomenon we term \"Late-Stage Fragility\": errors introduced in the later\nstages of a CoT chain are significantly more likely to corrupt the final answer\nthan identical errors made at the beginning. To address this specific\nvulnerability, we introduce the Adaptive Self-Correction Chain-of-Thought\n(ASCoT) method. ASCoT employs a modular pipeline in which an Adaptive\nVerification Manager (AVM) operates first, followed by the Multi-Perspective\nSelf-Correction Engine (MSCE). The AVM leverages a Positional Impact Score\nfunction I(k) that assigns different weights based on the position within the\nreasoning chains, addressing the Late-Stage Fragility issue by identifying and\nprioritizing high-risk, late-stage steps. Once these critical steps are\nidentified, the MSCE applies robust, dual-path correction specifically to the\nfailure parts. Extensive experiments on benchmarks such as GSM8K and MATH\ndemonstrate that ASCoT achieves outstanding accuracy, outperforming strong\nbaselines, including standard CoT. Our work underscores the importance of\ndiagnosing specific failure modes in LLM reasoning and advocates for a shift\nfrom uniform verification strategies to adaptive, vulnerability-aware\ncorrection mechanisms."
                },
                "authors": [
                    {
                        "name": "Dongxu Zhang"
                    },
                    {
                        "name": "Ning Yang"
                    },
                    {
                        "name": "Jihua Zhu"
                    },
                    {
                        "name": "Jinnan Yang"
                    },
                    {
                        "name": "Miao Xin"
                    },
                    {
                        "name": "Baoliang Tian"
                    }
                ],
                "author_detail": {
                    "name": "Baoliang Tian"
                },
                "author": "Baoliang Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05282v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05282v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05281v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05281v1",
                "updated": "2025-08-07T11:25:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    11,
                    25,
                    44,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T11:25:44Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    11,
                    25,
                    44,
                    3,
                    219,
                    0
                ],
                "title": "A Methodological Framework and Questionnaire for Investigating Perceived\n  Algorithmic Fairness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Methodological Framework and Questionnaire for Investigating Perceived\n  Algorithmic Fairness"
                },
                "summary": "This study explores perceptions of fairness in algorithmic decision-making\namong users in Bangladesh through a comprehensive mixed-methods approach. By\nintegrating quantitative survey data with qualitative interview insights, we\nexamine how cultural, social, and contextual factors influence users'\nunderstanding of fairness, transparency, and accountability in AI systems. Our\nfindings reveal nuanced attitudes toward human oversight, explanation\nmechanisms, and contestability, highlighting the importance of culturally aware\ndesign principles for equitable and trustworthy algorithmic systems. These\ninsights contribute to ongoing discussions on algorithmic fairness by\nforegrounding perspectives from a non-Western context, thus broadening the\nglobal dialogue on ethical AI deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores perceptions of fairness in algorithmic decision-making\namong users in Bangladesh through a comprehensive mixed-methods approach. By\nintegrating quantitative survey data with qualitative interview insights, we\nexamine how cultural, social, and contextual factors influence users'\nunderstanding of fairness, transparency, and accountability in AI systems. Our\nfindings reveal nuanced attitudes toward human oversight, explanation\nmechanisms, and contestability, highlighting the importance of culturally aware\ndesign principles for equitable and trustworthy algorithmic systems. These\ninsights contribute to ongoing discussions on algorithmic fairness by\nforegrounding perspectives from a non-Western context, thus broadening the\nglobal dialogue on ethical AI deployment."
                },
                "authors": [
                    {
                        "name": "Ahmed Abdal Shafi Rasel"
                    },
                    {
                        "name": "Ahmed Mustafa Amlan"
                    },
                    {
                        "name": "Tasmim Shajahan Mim"
                    },
                    {
                        "name": "Tanvir Hasan"
                    }
                ],
                "author_detail": {
                    "name": "Tanvir Hasan"
                },
                "author": "Tanvir Hasan",
                "arxiv_comment": "34 pages, Submitted for review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05281v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05281v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01188v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01188v3",
                "updated": "2025-08-07T11:20:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    11,
                    20,
                    47,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-02T04:21:07Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    4,
                    21,
                    7,
                    5,
                    214,
                    0
                ],
                "title": "SpectrumWorld: Artificial Intelligence Foundation for Spectroscopy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpectrumWorld: Artificial Intelligence Foundation for Spectroscopy"
                },
                "summary": "Deep learning holds immense promise for spectroscopy, yet research and\nevaluation in this emerging field often lack standardized formulations. To\naddress this issue, we introduce SpectrumLab, a pioneering unified platform\ndesigned to systematize and accelerate deep learning research in spectroscopy.\nSpectrumLab integrates three core components: a comprehensive Python library\nfeaturing essential data processing and evaluation tools, along with\nleaderboards; an innovative SpectrumAnnotator module that generates\nhigh-quality benchmarks from limited seed data; and SpectrumBench, a\nmulti-layered benchmark suite covering 14 spectroscopic tasks and over 10\nspectrum types, featuring spectra curated from over 1.2 million distinct\nchemical substances. Thorough empirical studies on SpectrumBench with 18\ncutting-edge multimodal LLMs reveal critical limitations of current approaches.\nWe hope SpectrumLab will serve as a crucial foundation for future advancements\nin deep learning-driven spectroscopy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning holds immense promise for spectroscopy, yet research and\nevaluation in this emerging field often lack standardized formulations. To\naddress this issue, we introduce SpectrumLab, a pioneering unified platform\ndesigned to systematize and accelerate deep learning research in spectroscopy.\nSpectrumLab integrates three core components: a comprehensive Python library\nfeaturing essential data processing and evaluation tools, along with\nleaderboards; an innovative SpectrumAnnotator module that generates\nhigh-quality benchmarks from limited seed data; and SpectrumBench, a\nmulti-layered benchmark suite covering 14 spectroscopic tasks and over 10\nspectrum types, featuring spectra curated from over 1.2 million distinct\nchemical substances. Thorough empirical studies on SpectrumBench with 18\ncutting-edge multimodal LLMs reveal critical limitations of current approaches.\nWe hope SpectrumLab will serve as a crucial foundation for future advancements\nin deep learning-driven spectroscopy."
                },
                "authors": [
                    {
                        "name": "Zhuo Yang"
                    },
                    {
                        "name": "Jiaqing Xie"
                    },
                    {
                        "name": "Shuaike Shen"
                    },
                    {
                        "name": "Daolang Wang"
                    },
                    {
                        "name": "Yeyun Chen"
                    },
                    {
                        "name": "Ben Gao"
                    },
                    {
                        "name": "Shuzhou Sun"
                    },
                    {
                        "name": "Biqing Qi"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    },
                    {
                        "name": "Lei Bai"
                    },
                    {
                        "name": "Linjiang Chen"
                    },
                    {
                        "name": "Shufei Zhang"
                    },
                    {
                        "name": "Jun Jiang"
                    },
                    {
                        "name": "Tianfan Fu"
                    },
                    {
                        "name": "Yuqiang Li"
                    }
                ],
                "author_detail": {
                    "name": "Yuqiang Li"
                },
                "author": "Yuqiang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01188v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01188v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08567v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08567v2",
                "updated": "2025-08-07T11:18:14Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    11,
                    18,
                    14,
                    3,
                    219,
                    0
                ],
                "published": "2025-07-11T13:11:11Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    13,
                    11,
                    11,
                    4,
                    192,
                    0
                ],
                "title": "AbbIE: Autoregressive Block-Based Iterative Encoder for Efficient\n  Sequence Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AbbIE: Autoregressive Block-Based Iterative Encoder for Efficient\n  Sequence Modeling"
                },
                "summary": "We introduce the Autoregressive Block-Based Iterative Encoder (AbbIE), a\nnovel recursive generalization of the encoder-only Transformer architecture,\nwhich achieves better perplexity than a standard Transformer and allows for the\ndynamic scaling of compute resources at test time. This simple, recursive\napproach is a complement to scaling large language model (LLM) performance\nthrough parameter and token counts. AbbIE performs its iterations in latent\nspace, but unlike latent reasoning models, does not require a specialized\ndataset or training protocol. We show that AbbIE upward generalizes (ability to\ngeneralize to arbitrary iteration lengths) at test time by only using 2\niterations during train time, far outperforming alternative iterative methods.\nAbbIE's ability to scale its computational expenditure based on the complexity\nof the task gives it an up to \\textbf{12\\%} improvement in zero-shot in-context\nlearning tasks versus other iterative and standard methods and up to 5\\%\nimprovement in language perplexity. The results from this study open a new\navenue to Transformer performance scaling. We perform all of our evaluations on\nmodel sizes up to 350M parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the Autoregressive Block-Based Iterative Encoder (AbbIE), a\nnovel recursive generalization of the encoder-only Transformer architecture,\nwhich achieves better perplexity than a standard Transformer and allows for the\ndynamic scaling of compute resources at test time. This simple, recursive\napproach is a complement to scaling large language model (LLM) performance\nthrough parameter and token counts. AbbIE performs its iterations in latent\nspace, but unlike latent reasoning models, does not require a specialized\ndataset or training protocol. We show that AbbIE upward generalizes (ability to\ngeneralize to arbitrary iteration lengths) at test time by only using 2\niterations during train time, far outperforming alternative iterative methods.\nAbbIE's ability to scale its computational expenditure based on the complexity\nof the task gives it an up to \\textbf{12\\%} improvement in zero-shot in-context\nlearning tasks versus other iterative and standard methods and up to 5\\%\nimprovement in language perplexity. The results from this study open a new\navenue to Transformer performance scaling. We perform all of our evaluations on\nmodel sizes up to 350M parameters."
                },
                "authors": [
                    {
                        "name": "Preslav Aleksandrov"
                    },
                    {
                        "name": "Meghdad Kurmanji"
                    },
                    {
                        "name": "Fernando Garcia Redondo"
                    },
                    {
                        "name": "David O'Shea"
                    },
                    {
                        "name": "William Shen"
                    },
                    {
                        "name": "Alex Iacob"
                    },
                    {
                        "name": "Lorenzo Sani"
                    },
                    {
                        "name": "Xinchi Qiu"
                    },
                    {
                        "name": "Nicola Cancedda"
                    },
                    {
                        "name": "Nicholas D. Lane"
                    }
                ],
                "author_detail": {
                    "name": "Nicholas D. Lane"
                },
                "author": "Nicholas D. Lane",
                "arxiv_comment": "14 pages and 6 figures. Submitted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08567v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08567v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05269v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05269v1",
                "updated": "2025-08-07T11:11:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    11,
                    11,
                    56,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T11:11:56Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    11,
                    11,
                    56,
                    3,
                    219,
                    0
                ],
                "title": "B4DL: A Benchmark for 4D LiDAR LLM in Spatio-Temporal Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "B4DL: A Benchmark for 4D LiDAR LLM in Spatio-Temporal Understanding"
                },
                "summary": "Understanding dynamic outdoor environments requires capturing complex object\ninteractions and their evolution over time. LiDAR-based 4D point clouds provide\nprecise spatial geometry and rich temporal cues, making them ideal for\nrepresenting real-world scenes. However, despite their potential, 4D LiDAR\nremains underexplored in the context of Multimodal Large Language Models\n(MLLMs) due to the absence of high-quality, modality-specific annotations and\nthe lack of MLLM architectures capable of processing its high-dimensional\ncomposition. To address these challenges, we introduce B4DL, a new benchmark\nspecifically designed for training and evaluating MLLMs on 4D LiDAR\nunderstanding. In addition, we propose a scalable data generation pipeline and\nan MLLM model that, for the first time, directly processes raw 4D LiDAR by\nbridging it with language understanding. Combined with our dataset and\nbenchmark, our model offers a unified solution for spatio-temporal reasoning in\ndynamic outdoor environments. We provide rendered 4D LiDAR videos, generated\ndataset, and inference outputs on diverse scenarios at:\nhttps://mmb4dl.github.io/mmb4dl/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding dynamic outdoor environments requires capturing complex object\ninteractions and their evolution over time. LiDAR-based 4D point clouds provide\nprecise spatial geometry and rich temporal cues, making them ideal for\nrepresenting real-world scenes. However, despite their potential, 4D LiDAR\nremains underexplored in the context of Multimodal Large Language Models\n(MLLMs) due to the absence of high-quality, modality-specific annotations and\nthe lack of MLLM architectures capable of processing its high-dimensional\ncomposition. To address these challenges, we introduce B4DL, a new benchmark\nspecifically designed for training and evaluating MLLMs on 4D LiDAR\nunderstanding. In addition, we propose a scalable data generation pipeline and\nan MLLM model that, for the first time, directly processes raw 4D LiDAR by\nbridging it with language understanding. Combined with our dataset and\nbenchmark, our model offers a unified solution for spatio-temporal reasoning in\ndynamic outdoor environments. We provide rendered 4D LiDAR videos, generated\ndataset, and inference outputs on diverse scenarios at:\nhttps://mmb4dl.github.io/mmb4dl/"
                },
                "authors": [
                    {
                        "name": "Changho Choi"
                    },
                    {
                        "name": "Youngwoo Shin"
                    },
                    {
                        "name": "Gyojin Han"
                    },
                    {
                        "name": "Dong-Jae Lee"
                    },
                    {
                        "name": "Junmo Kim"
                    }
                ],
                "author_detail": {
                    "name": "Junmo Kim"
                },
                "author": "Junmo Kim",
                "arxiv_doi": "10.1145/3746027.3755074",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746027.3755074",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.05269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05269v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at ACM MM 2025",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02088v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02088v2",
                "updated": "2025-08-07T11:09:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    11,
                    9,
                    6,
                    3,
                    219,
                    0
                ],
                "published": "2025-07-02T19:04:56Z",
                "published_parsed": [
                    2025,
                    7,
                    2,
                    19,
                    4,
                    56,
                    2,
                    183,
                    0
                ],
                "title": "McBE: A Multi-task Chinese Bias Evaluation Benchmark for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "McBE: A Multi-task Chinese Bias Evaluation Benchmark for Large Language\n  Models"
                },
                "summary": "As large language models (LLMs) are increasingly applied to various NLP\ntasks, their inherent biases are gradually disclosed. Therefore, measuring\nbiases in LLMs is crucial to mitigate its ethical risks. However, most existing\nbias evaluation datasets focus on English and North American culture, and their\nbias categories are not fully applicable to other cultures. The datasets\ngrounded in the Chinese language and culture are scarce. More importantly,\nthese datasets usually only support single evaluation tasks and cannot evaluate\nthe bias from multiple aspects in LLMs. To address these issues, we present a\nMulti-task Chinese Bias Evaluation Benchmark (McBE) that includes 4,077 bias\nevaluation instances, covering 12 single bias categories, 82 subcategories and\nintroducing 5 evaluation tasks, providing extensive category coverage, content\ndiversity, and measuring comprehensiveness. Additionally, we evaluate several\npopular LLMs from different series and with parameter sizes. In general, all\nthese LLMs demonstrated varying degrees of bias. We conduct an in-depth\nanalysis of results, offering novel insights into bias in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) are increasingly applied to various NLP\ntasks, their inherent biases are gradually disclosed. Therefore, measuring\nbiases in LLMs is crucial to mitigate its ethical risks. However, most existing\nbias evaluation datasets focus on English and North American culture, and their\nbias categories are not fully applicable to other cultures. The datasets\ngrounded in the Chinese language and culture are scarce. More importantly,\nthese datasets usually only support single evaluation tasks and cannot evaluate\nthe bias from multiple aspects in LLMs. To address these issues, we present a\nMulti-task Chinese Bias Evaluation Benchmark (McBE) that includes 4,077 bias\nevaluation instances, covering 12 single bias categories, 82 subcategories and\nintroducing 5 evaluation tasks, providing extensive category coverage, content\ndiversity, and measuring comprehensiveness. Additionally, we evaluate several\npopular LLMs from different series and with parameter sizes. In general, all\nthese LLMs demonstrated varying degrees of bias. We conduct an in-depth\nanalysis of results, offering novel insights into bias in LLMs."
                },
                "authors": [
                    {
                        "name": "Tian Lan"
                    },
                    {
                        "name": "Xiangdong Su"
                    },
                    {
                        "name": "Xu Liu"
                    },
                    {
                        "name": "Ruirui Wang"
                    },
                    {
                        "name": "Ke Chang"
                    },
                    {
                        "name": "Jiang Li"
                    },
                    {
                        "name": "Guanglai Gao"
                    }
                ],
                "author_detail": {
                    "name": "Guanglai Gao"
                },
                "author": "Guanglai Gao",
                "arxiv_doi": "10.18653/v1/2025.findings-acl.313",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.18653/v1/2025.findings-acl.313",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.02088v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02088v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by ACL2025 Findings",
                "arxiv_journal_ref": "In Findings of the Association for Computational Linguistics: ACL\n  2025, pages 6033-6056, Vienna, Austria. Association for Computational\n  Linguistics",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05267v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05267v1",
                "updated": "2025-08-07T11:02:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    11,
                    2,
                    40,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T11:02:40Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    11,
                    2,
                    40,
                    3,
                    219,
                    0
                ],
                "title": "An Explainable Natural Language Framework for Identifying and Notifying\n  Target Audiences In Enterprise Communication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Explainable Natural Language Framework for Identifying and Notifying\n  Target Audiences In Enterprise Communication"
                },
                "summary": "In large-scale maintenance organizations, identifying subject matter experts\nand managing communications across complex entities relationships poses\nsignificant challenges -- including information overload and longer response\ntimes -- that traditional communication approaches fail to address effectively.\nWe propose a novel framework that combines RDF graph databases with LLMs to\nprocess natural language queries for precise audience targeting, while\nproviding transparent reasoning through a planning-orchestration architecture.\nOur solution enables communication owners to formulate intuitive queries\ncombining concepts such as equipment, manufacturers, maintenance engineers, and\nfacilities, delivering explainable results that maintain trust in the system\nwhile improving communication efficiency across the organization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large-scale maintenance organizations, identifying subject matter experts\nand managing communications across complex entities relationships poses\nsignificant challenges -- including information overload and longer response\ntimes -- that traditional communication approaches fail to address effectively.\nWe propose a novel framework that combines RDF graph databases with LLMs to\nprocess natural language queries for precise audience targeting, while\nproviding transparent reasoning through a planning-orchestration architecture.\nOur solution enables communication owners to formulate intuitive queries\ncombining concepts such as equipment, manufacturers, maintenance engineers, and\nfacilities, delivering explainable results that maintain trust in the system\nwhile improving communication efficiency across the organization."
                },
                "authors": [
                    {
                        "name": "Vtor N. Loureno"
                    },
                    {
                        "name": "Mohnish Dubey"
                    },
                    {
                        "name": "Yunfei Bai"
                    },
                    {
                        "name": "Audrey Depeige"
                    },
                    {
                        "name": "Vivek Jain"
                    }
                ],
                "author_detail": {
                    "name": "Vivek Jain"
                },
                "author": "Vivek Jain",
                "arxiv_comment": "Accepted to publication at the 24th International Semantic Web\n  Conference Industry Track, ISWC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05267v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05267v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05266v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05266v1",
                "updated": "2025-08-07T11:02:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    11,
                    2,
                    32,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T11:02:32Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    11,
                    2,
                    32,
                    3,
                    219,
                    0
                ],
                "title": "Understanding and Mitigating Errors of LLM-Generated RTL Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and Mitigating Errors of LLM-Generated RTL Code"
                },
                "summary": "Despite the promising potential of large language model (LLM) based\nregister-transfer-level (RTL) code generation, the overall success rate remains\nunsatisfactory. Errors arise from various factors, with limited understanding\nof specific failure causes hindering improvement. To address this, we conduct a\ncomprehensive error analysis and manual categorization. Our findings reveal\nthat most errors stem not from LLM reasoning limitations, but from insufficient\nRTL programming knowledge, poor understanding of circuit concepts, ambiguous\ndesign descriptions, or misinterpretation of complex multimodal inputs.\nLeveraging in-context learning, we propose targeted error correction\ntechniques. Specifically, we construct a domain-specific knowledge base and\nemploy retrieval-augmented generation (RAG) to supply necessary RTL knowledge.\nTo mitigate ambiguity errors, we introduce design description rules and\nimplement a rule-checking mechanism. For multimodal misinterpretation, we\nintegrate external tools to convert inputs into LLM-compatible meta-formats.\nFor remaining errors, we adopt an iterative debugging loop (simulation-error\nlocalization-correction). Integrating these techniques into an LLM-based\nframework significantly improves performance. We incorporate these error\ncorrection techniques into a foundational LLM-based RTL code generation\nframework, resulting in significantly improved performance. Experimental\nresults show that our enhanced framework achieves 91.0\\% accuracy on the\nVerilogEval benchmark, surpassing the baseline code generation approach by\n32.7\\%, demonstrating the effectiveness of our methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the promising potential of large language model (LLM) based\nregister-transfer-level (RTL) code generation, the overall success rate remains\nunsatisfactory. Errors arise from various factors, with limited understanding\nof specific failure causes hindering improvement. To address this, we conduct a\ncomprehensive error analysis and manual categorization. Our findings reveal\nthat most errors stem not from LLM reasoning limitations, but from insufficient\nRTL programming knowledge, poor understanding of circuit concepts, ambiguous\ndesign descriptions, or misinterpretation of complex multimodal inputs.\nLeveraging in-context learning, we propose targeted error correction\ntechniques. Specifically, we construct a domain-specific knowledge base and\nemploy retrieval-augmented generation (RAG) to supply necessary RTL knowledge.\nTo mitigate ambiguity errors, we introduce design description rules and\nimplement a rule-checking mechanism. For multimodal misinterpretation, we\nintegrate external tools to convert inputs into LLM-compatible meta-formats.\nFor remaining errors, we adopt an iterative debugging loop (simulation-error\nlocalization-correction). Integrating these techniques into an LLM-based\nframework significantly improves performance. We incorporate these error\ncorrection techniques into a foundational LLM-based RTL code generation\nframework, resulting in significantly improved performance. Experimental\nresults show that our enhanced framework achieves 91.0\\% accuracy on the\nVerilogEval benchmark, surpassing the baseline code generation approach by\n32.7\\%, demonstrating the effectiveness of our methods."
                },
                "authors": [
                    {
                        "name": "Jiazheng Zhang"
                    },
                    {
                        "name": "Cheng Liu"
                    },
                    {
                        "name": "Huawei Li"
                    }
                ],
                "author_detail": {
                    "name": "Huawei Li"
                },
                "author": "Huawei Li",
                "arxiv_comment": "14 pages, 26 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05266v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05266v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05257v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05257v1",
                "updated": "2025-08-07T10:48:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    10,
                    48,
                    24,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T10:48:24Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    10,
                    48,
                    24,
                    3,
                    219,
                    0
                ],
                "title": "MoBE: Mixture-of-Basis-Experts for Compressing MoE-based LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoBE: Mixture-of-Basis-Experts for Compressing MoE-based LLMs"
                },
                "summary": "The Mixture-of-Experts (MoE) architecture has become a predominant paradigm\nfor scaling large language models (LLMs). Despite offering strong performance\nand computational efficiency, large MoE-based LLMs like DeepSeek-V3-0324 and\nKimi-K2-Instruct present serious challenges due to substantial memory\nrequirements in deployment. While recent works have explored MoE compression to\naddress this issue, existing methods often suffer from considerable accuracy\ndrops (e.g., 7-14% relatively) even at modest compression rates. This paper\nintroduces a novel Mixture-of-Basis-Experts (MoBE) method that achieves model\ncompression while incurring minimal accuracy drops. Specifically, each up/gate\nmatrix in an expert is decomposed via a rank decomposition as W = AB, where\nmatrix A is unique to each expert. The relatively larger matrix B is further\nre-parameterized as a linear combination of basis matrices {Bi} shared across\nall experts within a given MoE layer. The factorization is learned by\nminimizing the reconstruction error relative to the original weight matrices.\nExperiments demonstrate that MoBE achieves notably lower accuracy drops\ncompared to prior works. For instance, MoBE can reduce the parameter counts of\nQwen3-235B-A22B-2507, DeepSeek-V3-0324 (671B) and Kimi-K2-Instruct (1T) by\n24%-30% with only 1%-2% accuracy drop (about 2% drops when measured\nrelatively).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mixture-of-Experts (MoE) architecture has become a predominant paradigm\nfor scaling large language models (LLMs). Despite offering strong performance\nand computational efficiency, large MoE-based LLMs like DeepSeek-V3-0324 and\nKimi-K2-Instruct present serious challenges due to substantial memory\nrequirements in deployment. While recent works have explored MoE compression to\naddress this issue, existing methods often suffer from considerable accuracy\ndrops (e.g., 7-14% relatively) even at modest compression rates. This paper\nintroduces a novel Mixture-of-Basis-Experts (MoBE) method that achieves model\ncompression while incurring minimal accuracy drops. Specifically, each up/gate\nmatrix in an expert is decomposed via a rank decomposition as W = AB, where\nmatrix A is unique to each expert. The relatively larger matrix B is further\nre-parameterized as a linear combination of basis matrices {Bi} shared across\nall experts within a given MoE layer. The factorization is learned by\nminimizing the reconstruction error relative to the original weight matrices.\nExperiments demonstrate that MoBE achieves notably lower accuracy drops\ncompared to prior works. For instance, MoBE can reduce the parameter counts of\nQwen3-235B-A22B-2507, DeepSeek-V3-0324 (671B) and Kimi-K2-Instruct (1T) by\n24%-30% with only 1%-2% accuracy drop (about 2% drops when measured\nrelatively)."
                },
                "authors": [
                    {
                        "name": "Xiaodong Chen"
                    },
                    {
                        "name": "Mingming Ha"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Jianguo Li"
                    }
                ],
                "author_detail": {
                    "name": "Jianguo Li"
                },
                "author": "Jianguo Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05257v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05257v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15384v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15384v2",
                "updated": "2025-08-07T10:39:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    10,
                    39,
                    28,
                    3,
                    219,
                    0
                ],
                "published": "2025-01-26T03:51:56Z",
                "published_parsed": [
                    2025,
                    1,
                    26,
                    3,
                    51,
                    56,
                    6,
                    26,
                    0
                ],
                "title": "MetaOcc: Spatio-Temporal Fusion of Surround-View 4D Radar and Camera for\n  3D Occupancy Prediction with Dual Training Strategies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MetaOcc: Spatio-Temporal Fusion of Surround-View 4D Radar and Camera for\n  3D Occupancy Prediction with Dual Training Strategies"
                },
                "summary": "Robust 3D occupancy prediction is essential for autonomous driving,\nparticularly under adverse weather conditions where traditional vision-only\nsystems struggle. While the fusion of surround-view 4D radar and cameras offers\na promising low-cost solution, effectively extracting and integrating features\nfrom these heterogeneous sensors remains challenging. This paper introduces\nMetaOcc, a novel multi-modal framework for omnidirectional 3D occupancy\nprediction that leverages both multi-view 4D radar and images. To address the\nlimitations of directly applying LiDAR-oriented encoders to sparse radar data,\nwe propose a Radar Height Self-Attention module that enhances vertical spatial\nreasoning and feature extraction. Additionally, a Hierarchical Multi-scale\nMulti-modal Fusion strategy is developed to perform adaptive local-global\nfusion across modalities and time, mitigating spatio-temporal misalignments and\nenriching fused feature representations. To reduce reliance on expensive point\ncloud annotations, we further propose a pseudo-label generation pipeline based\non an open-set segmentor. This enables a semi-supervised strategy that achieves\n90% of the fully supervised performance using only 50% of the ground truth\nlabels, offering an effective trade-off between annotation cost and accuracy.\nExtensive experiments demonstrate that MetaOcc under full supervision achieves\nstate-of-the-art performance, outperforming previous methods by +0.47 SC IoU\nand +4.02 mIoU on the OmniHD-Scenes dataset, and by +1.16 SC IoU and +1.24 mIoU\non the SurroundOcc-nuScenes dataset. These results demonstrate the scalability\nand robustness of MetaOcc across sensor domains and training conditions, paving\nthe way for practical deployment in real-world autonomous systems. Code and\ndata are available at https://github.com/LucasYang567/MetaOcc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust 3D occupancy prediction is essential for autonomous driving,\nparticularly under adverse weather conditions where traditional vision-only\nsystems struggle. While the fusion of surround-view 4D radar and cameras offers\na promising low-cost solution, effectively extracting and integrating features\nfrom these heterogeneous sensors remains challenging. This paper introduces\nMetaOcc, a novel multi-modal framework for omnidirectional 3D occupancy\nprediction that leverages both multi-view 4D radar and images. To address the\nlimitations of directly applying LiDAR-oriented encoders to sparse radar data,\nwe propose a Radar Height Self-Attention module that enhances vertical spatial\nreasoning and feature extraction. Additionally, a Hierarchical Multi-scale\nMulti-modal Fusion strategy is developed to perform adaptive local-global\nfusion across modalities and time, mitigating spatio-temporal misalignments and\nenriching fused feature representations. To reduce reliance on expensive point\ncloud annotations, we further propose a pseudo-label generation pipeline based\non an open-set segmentor. This enables a semi-supervised strategy that achieves\n90% of the fully supervised performance using only 50% of the ground truth\nlabels, offering an effective trade-off between annotation cost and accuracy.\nExtensive experiments demonstrate that MetaOcc under full supervision achieves\nstate-of-the-art performance, outperforming previous methods by +0.47 SC IoU\nand +4.02 mIoU on the OmniHD-Scenes dataset, and by +1.16 SC IoU and +1.24 mIoU\non the SurroundOcc-nuScenes dataset. These results demonstrate the scalability\nand robustness of MetaOcc across sensor domains and training conditions, paving\nthe way for practical deployment in real-world autonomous systems. Code and\ndata are available at https://github.com/LucasYang567/MetaOcc."
                },
                "authors": [
                    {
                        "name": "Long Yang"
                    },
                    {
                        "name": "Lianqing Zheng"
                    },
                    {
                        "name": "Wenjin Ai"
                    },
                    {
                        "name": "Minghao Liu"
                    },
                    {
                        "name": "Sen Li"
                    },
                    {
                        "name": "Qunshu Lin"
                    },
                    {
                        "name": "Shengyu Yan"
                    },
                    {
                        "name": "Jie Bai"
                    },
                    {
                        "name": "Zhixiong Ma"
                    },
                    {
                        "name": "Tao Huang"
                    },
                    {
                        "name": "Xichan Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Xichan Zhu"
                },
                "author": "Xichan Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15384v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15384v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05242v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05242v1",
                "updated": "2025-08-07T10:31:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    10,
                    31,
                    24,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T10:31:24Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    10,
                    31,
                    24,
                    3,
                    219,
                    0
                ],
                "title": "CodeBoost: Boosting Code LLMs by Squeezing Knowledge from Code Snippets\n  with RL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeBoost: Boosting Code LLMs by Squeezing Knowledge from Code Snippets\n  with RL"
                },
                "summary": "Code large language models (LLMs) have become indispensable tools for\nbuilding efficient and automated coding pipelines. Existing models are\ntypically post-trained using reinforcement learning (RL) from general-purpose\nLLMs using \"human instruction-final answer\" pairs, where the instructions are\nusually from manual annotations. However, collecting high-quality coding\ninstructions is both labor-intensive and difficult to scale. On the other hand,\ncode snippets are abundantly available from various sources. This imbalance\npresents a major bottleneck in instruction-based post-training. We propose\nCodeBoost, a post-training framework that enhances code LLMs purely from code\nsnippets, without relying on human-annotated instructions. CodeBoost introduces\nthe following key components: (1) maximum-clique curation, which selects a\nrepresentative and diverse training corpus from code; (2) bi-directional\nprediction, which enables the model to learn from both forward and backward\nprediction objectives; (3) error-aware prediction, which incorporates learning\nsignals from both correct and incorrect outputs; (4) heterogeneous\naugmentation, which diversifies the training distribution to enrich code\nsemantics; and (5) heterogeneous rewarding, which guides model learning through\nmultiple reward types including format correctness and execution feedback from\nboth successes and failures. Extensive experiments across several code LLMs and\nbenchmarks verify that CodeBoost consistently improves performance,\ndemonstrating its effectiveness as a scalable and effective training pipeline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code large language models (LLMs) have become indispensable tools for\nbuilding efficient and automated coding pipelines. Existing models are\ntypically post-trained using reinforcement learning (RL) from general-purpose\nLLMs using \"human instruction-final answer\" pairs, where the instructions are\nusually from manual annotations. However, collecting high-quality coding\ninstructions is both labor-intensive and difficult to scale. On the other hand,\ncode snippets are abundantly available from various sources. This imbalance\npresents a major bottleneck in instruction-based post-training. We propose\nCodeBoost, a post-training framework that enhances code LLMs purely from code\nsnippets, without relying on human-annotated instructions. CodeBoost introduces\nthe following key components: (1) maximum-clique curation, which selects a\nrepresentative and diverse training corpus from code; (2) bi-directional\nprediction, which enables the model to learn from both forward and backward\nprediction objectives; (3) error-aware prediction, which incorporates learning\nsignals from both correct and incorrect outputs; (4) heterogeneous\naugmentation, which diversifies the training distribution to enrich code\nsemantics; and (5) heterogeneous rewarding, which guides model learning through\nmultiple reward types including format correctness and execution feedback from\nboth successes and failures. Extensive experiments across several code LLMs and\nbenchmarks verify that CodeBoost consistently improves performance,\ndemonstrating its effectiveness as a scalable and effective training pipeline."
                },
                "authors": [
                    {
                        "name": "Sijie Wang"
                    },
                    {
                        "name": "Quanjiang Guo"
                    },
                    {
                        "name": "Kai Zhao"
                    },
                    {
                        "name": "Yawei Zhang"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Siqi Li"
                    },
                    {
                        "name": "Rui She"
                    },
                    {
                        "name": "Shangshu Yu"
                    },
                    {
                        "name": "Wee Peng Tay"
                    }
                ],
                "author_detail": {
                    "name": "Wee Peng Tay"
                },
                "author": "Wee Peng Tay",
                "arxiv_comment": "Technical report. Project page: https://github.com/sijieaaa/CodeBoost",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05242v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05242v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02317v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02317v3",
                "updated": "2025-08-07T10:31:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    10,
                    31,
                    9,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-04T11:33:04Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    11,
                    33,
                    4,
                    0,
                    216,
                    0
                ],
                "title": "VeOmni: Scaling Any Modality Model Training with Model-Centric\n  Distributed Recipe Zoo",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VeOmni: Scaling Any Modality Model Training with Model-Centric\n  Distributed Recipe Zoo"
                },
                "summary": "Recent advances in large language models (LLMs) have driven impressive\nprogress in omni-modal understanding and generation. However, training\nomni-modal LLMs remains a significant challenge due to the heterogeneous model\narchitectures required to process diverse modalities, necessitating\nsophisticated system design for efficient large-scale training. Existing\nframeworks typically entangle model definition with parallel logic, incurring\nlimited scalability and substantial engineering overhead for end-to-end\nomni-modal training. We present VeOmni, a modular and efficient training\nframework to accelerate the development of omni-modal LLMs. VeOmni introduces\nmodel-centric distributed recipes that decouples communication from\ncomputation, enabling efficient 3D parallelism on omni-modal LLMs. VeOmni also\nfeatures a flexible configuration interface supporting seamless integration of\nnew modalities with minimal code change. Using VeOmni, a omni-modal\nmixture-of-experts (MoE) model with 30B parameters can be trained with over\n2,800 tokens/sec/GPU throughput and scale to 160K context lengths via 3D\nparallelism on 128 GPUs, showcasing its superior efficiency and scalability for\ntraining large omni-modal LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have driven impressive\nprogress in omni-modal understanding and generation. However, training\nomni-modal LLMs remains a significant challenge due to the heterogeneous model\narchitectures required to process diverse modalities, necessitating\nsophisticated system design for efficient large-scale training. Existing\nframeworks typically entangle model definition with parallel logic, incurring\nlimited scalability and substantial engineering overhead for end-to-end\nomni-modal training. We present VeOmni, a modular and efficient training\nframework to accelerate the development of omni-modal LLMs. VeOmni introduces\nmodel-centric distributed recipes that decouples communication from\ncomputation, enabling efficient 3D parallelism on omni-modal LLMs. VeOmni also\nfeatures a flexible configuration interface supporting seamless integration of\nnew modalities with minimal code change. Using VeOmni, a omni-modal\nmixture-of-experts (MoE) model with 30B parameters can be trained with over\n2,800 tokens/sec/GPU throughput and scale to 160K context lengths via 3D\nparallelism on 128 GPUs, showcasing its superior efficiency and scalability for\ntraining large omni-modal LLMs."
                },
                "authors": [
                    {
                        "name": "Qianli Ma"
                    },
                    {
                        "name": "Yaowei Zheng"
                    },
                    {
                        "name": "Zhelun Shi"
                    },
                    {
                        "name": "Zhongkai Zhao"
                    },
                    {
                        "name": "Bin Jia"
                    },
                    {
                        "name": "Ziyue Huang"
                    },
                    {
                        "name": "Zhiqi Lin"
                    },
                    {
                        "name": "Youjie Li"
                    },
                    {
                        "name": "Jiacheng Yang"
                    },
                    {
                        "name": "Yanghua Peng"
                    },
                    {
                        "name": "Zhi Zhang"
                    },
                    {
                        "name": "Xin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xin Liu"
                },
                "author": "Xin Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02317v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02317v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05239v1",
                "updated": "2025-08-07T10:27:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    10,
                    27,
                    1,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T10:27:01Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    10,
                    27,
                    1,
                    3,
                    219,
                    0
                ],
                "title": "Pruning Large Language Models by Identifying and Preserving Functional\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pruning Large Language Models by Identifying and Preserving Functional\n  Networks"
                },
                "summary": "Structured pruning is one of the representative techniques for compressing\nlarge language models (LLMs) to reduce GPU memory consumption and accelerate\ninference speed. It offers significant practical value in improving the\nefficiency of LLMs in real-world applications. Current structured pruning\nmethods typically rely on assessment of the importance of the structure units\nand pruning the units with less importance. Most of them overlooks the\ninteraction and collaboration among artificial neurons that are crucial for the\nfunctionalities of LLMs, leading to a disruption in the macro functional\narchitecture of LLMs and consequently a pruning performance degradation.\nInspired by the inherent similarities between artificial neural networks and\nfunctional neural networks in the human brain, we alleviate this challenge and\npropose to prune LLMs by identifying and preserving functional networks within\nLLMs in this study. To achieve this, we treat an LLM as a digital brain and\ndecompose the LLM into functional networks, analogous to identifying functional\nbrain networks in neuroimaging data. Afterwards, an LLM is pruned by preserving\nthe key neurons within these functional networks. Experimental results\ndemonstrate that the proposed method can successfully identify and locate\nfunctional networks and key neurons in LLMs, enabling efficient model pruning.\nOur code is available at https://github.com/WhatAboutMyStar/LLM_ACTIVATION.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured pruning is one of the representative techniques for compressing\nlarge language models (LLMs) to reduce GPU memory consumption and accelerate\ninference speed. It offers significant practical value in improving the\nefficiency of LLMs in real-world applications. Current structured pruning\nmethods typically rely on assessment of the importance of the structure units\nand pruning the units with less importance. Most of them overlooks the\ninteraction and collaboration among artificial neurons that are crucial for the\nfunctionalities of LLMs, leading to a disruption in the macro functional\narchitecture of LLMs and consequently a pruning performance degradation.\nInspired by the inherent similarities between artificial neural networks and\nfunctional neural networks in the human brain, we alleviate this challenge and\npropose to prune LLMs by identifying and preserving functional networks within\nLLMs in this study. To achieve this, we treat an LLM as a digital brain and\ndecompose the LLM into functional networks, analogous to identifying functional\nbrain networks in neuroimaging data. Afterwards, an LLM is pruned by preserving\nthe key neurons within these functional networks. Experimental results\ndemonstrate that the proposed method can successfully identify and locate\nfunctional networks and key neurons in LLMs, enabling efficient model pruning.\nOur code is available at https://github.com/WhatAboutMyStar/LLM_ACTIVATION."
                },
                "authors": [
                    {
                        "name": "Yiheng Liu"
                    },
                    {
                        "name": "Junhao Ning"
                    },
                    {
                        "name": "Sichen Xia"
                    },
                    {
                        "name": "Xiaohui Gao"
                    },
                    {
                        "name": "Ning Qiang"
                    },
                    {
                        "name": "Bao Ge"
                    },
                    {
                        "name": "Junwei Han"
                    },
                    {
                        "name": "Xintao Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xintao Hu"
                },
                "author": "Xintao Hu",
                "arxiv_comment": "9 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05238v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05238v1",
                "updated": "2025-08-07T10:26:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    10,
                    26,
                    28,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T10:26:28Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    10,
                    26,
                    28,
                    3,
                    219,
                    0
                ],
                "title": "Driver Assistant: Persuading Drivers to Adjust Secondary Tasks Using\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Driver Assistant: Persuading Drivers to Adjust Secondary Tasks Using\n  Large Language Models"
                },
                "summary": "Level 3 automated driving systems allows drivers to engage in secondary tasks\nwhile diminishing their perception of risk. In the event of an emergency\nnecessitating driver intervention, the system will alert the driver with a\nlimited window for reaction and imposing a substantial cognitive burden. To\naddress this challenge, this study employs a Large Language Model (LLM) to\nassist drivers in maintaining an appropriate attention on road conditions\nthrough a \"humanized\" persuasive advice. Our tool leverages the road conditions\nencountered by Level 3 systems as triggers, proactively steering driver\nbehavior via both visual and auditory routes. Empirical study indicates that\nour tool is effective in sustaining driver attention with reduced cognitive\nload and coordinating secondary tasks with takeover behavior. Our work provides\ninsights into the potential of using LLMs to support drivers during multi-task\nautomated driving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Level 3 automated driving systems allows drivers to engage in secondary tasks\nwhile diminishing their perception of risk. In the event of an emergency\nnecessitating driver intervention, the system will alert the driver with a\nlimited window for reaction and imposing a substantial cognitive burden. To\naddress this challenge, this study employs a Large Language Model (LLM) to\nassist drivers in maintaining an appropriate attention on road conditions\nthrough a \"humanized\" persuasive advice. Our tool leverages the road conditions\nencountered by Level 3 systems as triggers, proactively steering driver\nbehavior via both visual and auditory routes. Empirical study indicates that\nour tool is effective in sustaining driver attention with reduced cognitive\nload and coordinating secondary tasks with takeover behavior. Our work provides\ninsights into the potential of using LLMs to support drivers during multi-task\nautomated driving."
                },
                "authors": [
                    {
                        "name": "Wei Xiang"
                    },
                    {
                        "name": "Muchen Li"
                    },
                    {
                        "name": "Jie Yan"
                    },
                    {
                        "name": "Manling Zheng"
                    },
                    {
                        "name": "Hanfei Zhu"
                    },
                    {
                        "name": "Mengyun Jiang"
                    },
                    {
                        "name": "Lingyun Sun"
                    }
                ],
                "author_detail": {
                    "name": "Lingyun Sun"
                },
                "author": "Lingyun Sun",
                "arxiv_comment": "6 pages, 4 figures, 2025 IEEE International Conference on Systems,\n  Man, and Cybernetics (SMC)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05238v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05238v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05234v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05234v1",
                "updated": "2025-08-07T10:23:14Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    10,
                    23,
                    14,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T10:23:14Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    10,
                    23,
                    14,
                    3,
                    219,
                    0
                ],
                "title": "Resource-Limited Joint Multimodal Sentiment Reasoning and Classification\n  via Chain-of-Thought Enhancement and Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource-Limited Joint Multimodal Sentiment Reasoning and Classification\n  via Chain-of-Thought Enhancement and Distillation"
                },
                "summary": "The surge in rich multimodal content on social media platforms has greatly\nadvanced Multimodal Sentiment Analysis (MSA), with Large Language Models (LLMs)\nfurther accelerating progress in this field. Current approaches primarily\nleverage the knowledge and reasoning capabilities of parameter-heavy\n(Multimodal) LLMs for sentiment classification, overlooking autonomous\nmultimodal sentiment reasoning generation in resource-constrained environments.\nTherefore, we focus on the Resource-Limited Joint Multimodal Sentiment\nReasoning and Classification task, JMSRC, which simultaneously performs\nmultimodal sentiment reasoning chain generation and sentiment classification\nonly with a lightweight model. We propose a Multimodal Chain-of-Thought\nReasoning Distillation model, MulCoT-RD, designed for JMSRC that employs a\n\"Teacher-Assistant-Student\" distillation paradigm to address deployment\nconstraints in resource-limited environments. We first leverage a\nhigh-performance Multimodal Large Language Model (MLLM) to generate the initial\nreasoning dataset and train a medium-sized assistant model with a multi-task\nlearning mechanism. A lightweight student model is jointly trained to perform\nefficient multimodal sentiment reasoning generation and classification.\nExtensive experiments on four datasets demonstrate that MulCoT-RD with only 3B\nparameters achieves strong performance on JMSRC, while exhibiting robust\ngeneralization and enhanced interpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The surge in rich multimodal content on social media platforms has greatly\nadvanced Multimodal Sentiment Analysis (MSA), with Large Language Models (LLMs)\nfurther accelerating progress in this field. Current approaches primarily\nleverage the knowledge and reasoning capabilities of parameter-heavy\n(Multimodal) LLMs for sentiment classification, overlooking autonomous\nmultimodal sentiment reasoning generation in resource-constrained environments.\nTherefore, we focus on the Resource-Limited Joint Multimodal Sentiment\nReasoning and Classification task, JMSRC, which simultaneously performs\nmultimodal sentiment reasoning chain generation and sentiment classification\nonly with a lightweight model. We propose a Multimodal Chain-of-Thought\nReasoning Distillation model, MulCoT-RD, designed for JMSRC that employs a\n\"Teacher-Assistant-Student\" distillation paradigm to address deployment\nconstraints in resource-limited environments. We first leverage a\nhigh-performance Multimodal Large Language Model (MLLM) to generate the initial\nreasoning dataset and train a medium-sized assistant model with a multi-task\nlearning mechanism. A lightweight student model is jointly trained to perform\nefficient multimodal sentiment reasoning generation and classification.\nExtensive experiments on four datasets demonstrate that MulCoT-RD with only 3B\nparameters achieves strong performance on JMSRC, while exhibiting robust\ngeneralization and enhanced interpretability."
                },
                "authors": [
                    {
                        "name": "Haonan Shangguan"
                    },
                    {
                        "name": "Xiaocui Yang"
                    },
                    {
                        "name": "Shi Feng"
                    },
                    {
                        "name": "Daling Wang"
                    },
                    {
                        "name": "Yifei Zhang"
                    },
                    {
                        "name": "Ge Yu"
                    }
                ],
                "author_detail": {
                    "name": "Ge Yu"
                },
                "author": "Ge Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05234v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05234v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05232v1",
                "updated": "2025-08-07T10:21:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    10,
                    21,
                    8,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T10:21:08Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    10,
                    21,
                    8,
                    3,
                    219,
                    0
                ],
                "title": "Cross-LoRA: A Data-Free LoRA Transfer Framework across Heterogeneous\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-LoRA: A Data-Free LoRA Transfer Framework across Heterogeneous\n  LLMs"
                },
                "summary": "Traditional parameter-efficient fine-tuning (PEFT) methods such as LoRA are\ntightly coupled with the base model architecture, which constrains their\napplicability across heterogeneous pretrained large language models (LLMs). To\naddress this limitation, we introduce Cross-LoRA, a data-free framework for\ntransferring LoRA modules between diverse base models without requiring\nadditional training data. Cross-LoRA consists of two key components: (a)\nLoRA-Align, which performs subspace alignment between source and target base\nmodels through rank-truncated singular value decomposition (SVD) and\nFrobenius-optimal linear transformation, ensuring compatibility under dimension\nmismatch; and (b) LoRA-Shift, which applies the aligned subspaces to project\nsource LoRA weight updates into the target model parameter space. Both\ncomponents are data-free, training-free, and enable lightweight adaptation on a\ncommodity GPU in 20 minutes. Experiments on ARCs, OBOA and HellaSwag show that\nCross-LoRA achieves relative gains of up to 5.26% over base models. Across\nother commonsense reasoning benchmarks, Cross-LoRA maintains performance\ncomparable to that of directly trained LoRA adapters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional parameter-efficient fine-tuning (PEFT) methods such as LoRA are\ntightly coupled with the base model architecture, which constrains their\napplicability across heterogeneous pretrained large language models (LLMs). To\naddress this limitation, we introduce Cross-LoRA, a data-free framework for\ntransferring LoRA modules between diverse base models without requiring\nadditional training data. Cross-LoRA consists of two key components: (a)\nLoRA-Align, which performs subspace alignment between source and target base\nmodels through rank-truncated singular value decomposition (SVD) and\nFrobenius-optimal linear transformation, ensuring compatibility under dimension\nmismatch; and (b) LoRA-Shift, which applies the aligned subspaces to project\nsource LoRA weight updates into the target model parameter space. Both\ncomponents are data-free, training-free, and enable lightweight adaptation on a\ncommodity GPU in 20 minutes. Experiments on ARCs, OBOA and HellaSwag show that\nCross-LoRA achieves relative gains of up to 5.26% over base models. Across\nother commonsense reasoning benchmarks, Cross-LoRA maintains performance\ncomparable to that of directly trained LoRA adapters."
                },
                "authors": [
                    {
                        "name": "Feifan Xia"
                    },
                    {
                        "name": "Mingyang Liao"
                    },
                    {
                        "name": "Yuyang Fang"
                    },
                    {
                        "name": "Defang Li"
                    },
                    {
                        "name": "Yantong Xie"
                    },
                    {
                        "name": "Weikang Li"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Deguo Xia"
                    },
                    {
                        "name": "Jizhou Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jizhou Huang"
                },
                "author": "Jizhou Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19595v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19595v2",
                "updated": "2025-08-07T10:08:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    10,
                    8,
                    17,
                    3,
                    219,
                    0
                ],
                "published": "2025-07-25T18:08:10Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    18,
                    8,
                    10,
                    4,
                    206,
                    0
                ],
                "title": "Efficient Attention Mechanisms for Large Language Models: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Attention Mechanisms for Large Language Models: A Survey"
                },
                "summary": "Transformer-based architectures have become the prevailing backbone of large\nlanguage models. However, the quadratic time and memory complexity of\nself-attention remains a fundamental obstacle to efficient long-context\nmodeling. To address this limitation, recent research has introduced two\nprincipal categories of efficient attention mechanisms. Linear attention\nmethods achieve linear complexity through kernel approximations, recurrent\nformulations, or fastweight dynamics, thereby enabling scalable inference with\nreduced computational overhead. Sparse attention techniques, in contrast, limit\nattention computation to selected subsets of tokens based on fixed patterns,\nblock-wise routing, or clustering strategies, enhancing efficiency while\npreserving contextual coverage. This survey provides a systematic and\ncomprehensive overview of these developments, integrating both algorithmic\ninnovations and hardware-level considerations. In addition, we analyze the\nincorporation of efficient attention into largescale pre-trained language\nmodels, including both architectures built entirely on efficient attention and\nhybrid designs that combine local and global components. By aligning\ntheoretical foundations with practical deployment strategies, this work aims to\nserve as a foundational reference for advancing the design of scalable and\nefficient language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based architectures have become the prevailing backbone of large\nlanguage models. However, the quadratic time and memory complexity of\nself-attention remains a fundamental obstacle to efficient long-context\nmodeling. To address this limitation, recent research has introduced two\nprincipal categories of efficient attention mechanisms. Linear attention\nmethods achieve linear complexity through kernel approximations, recurrent\nformulations, or fastweight dynamics, thereby enabling scalable inference with\nreduced computational overhead. Sparse attention techniques, in contrast, limit\nattention computation to selected subsets of tokens based on fixed patterns,\nblock-wise routing, or clustering strategies, enhancing efficiency while\npreserving contextual coverage. This survey provides a systematic and\ncomprehensive overview of these developments, integrating both algorithmic\ninnovations and hardware-level considerations. In addition, we analyze the\nincorporation of efficient attention into largescale pre-trained language\nmodels, including both architectures built entirely on efficient attention and\nhybrid designs that combine local and global components. By aligning\ntheoretical foundations with practical deployment strategies, this work aims to\nserve as a foundational reference for advancing the design of scalable and\nefficient language models."
                },
                "authors": [
                    {
                        "name": "Yutao Sun"
                    },
                    {
                        "name": "Zhenyu Li"
                    },
                    {
                        "name": "Yike Zhang"
                    },
                    {
                        "name": "Tengyu Pan"
                    },
                    {
                        "name": "Bowen Dong"
                    },
                    {
                        "name": "Yuyi Guo"
                    },
                    {
                        "name": "Jianyong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jianyong Wang"
                },
                "author": "Jianyong Wang",
                "arxiv_comment": "work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19595v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19595v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05213v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05213v1",
                "updated": "2025-08-07T09:48:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    9,
                    48,
                    24,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T09:48:24Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    9,
                    48,
                    24,
                    3,
                    219,
                    0
                ],
                "title": "Textual and Visual Guided Task Adaptation for Source-Free Cross-Domain\n  Few-Shot Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Textual and Visual Guided Task Adaptation for Source-Free Cross-Domain\n  Few-Shot Segmentation"
                },
                "summary": "Few-Shot Segmentation(FSS) aims to efficient segmentation of new objects with\nfew labeled samples. However, its performance significantly degrades when\ndomain discrepancies exist between training and deployment. Cross-Domain\nFew-Shot Segmentation(CD-FSS) is proposed to mitigate such performance\ndegradation. Current CD-FSS methods primarily sought to develop segmentation\nmodels on a source domain capable of cross-domain generalization. However,\ndriven by escalating concerns over data privacy and the imperative to minimize\ndata transfer and training expenses, the development of source-free CD-FSS\napproaches has become essential. In this work, we propose a source-free CD-FSS\nmethod that leverages both textual and visual information to facilitate target\ndomain task adaptation without requiring source domain data. Specifically, we\nfirst append Task-Specific Attention Adapters (TSAA) to the feature pyramid of\na pretrained backbone, which adapt multi-level features extracted from the\nshared pre-trained backbone to the target task. Then, the parameters of the\nTSAA are trained through a Visual-Visual Embedding Alignment (VVEA) module and\na Text-Visual Embedding Alignment (TVEA) module. The VVEA module utilizes\nglobal-local visual features to align image features across different views,\nwhile the TVEA module leverages textual priors from pre-aligned multi-modal\nfeatures (e.g., from CLIP) to guide cross-modal adaptation. By combining the\noutputs of these modules through dense comparison operations and subsequent\nfusion via skip connections, our method produces refined prediction masks.\nUnder both 1-shot and 5-shot settings, the proposed approach achieves average\nsegmentation accuracy improvements of 2.18\\% and 4.11\\%, respectively, across\nfour cross-domain datasets, significantly outperforming state-of-the-art CD-FSS\nmethods. Code are available at https://github.com/ljm198134/TVGTANet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-Shot Segmentation(FSS) aims to efficient segmentation of new objects with\nfew labeled samples. However, its performance significantly degrades when\ndomain discrepancies exist between training and deployment. Cross-Domain\nFew-Shot Segmentation(CD-FSS) is proposed to mitigate such performance\ndegradation. Current CD-FSS methods primarily sought to develop segmentation\nmodels on a source domain capable of cross-domain generalization. However,\ndriven by escalating concerns over data privacy and the imperative to minimize\ndata transfer and training expenses, the development of source-free CD-FSS\napproaches has become essential. In this work, we propose a source-free CD-FSS\nmethod that leverages both textual and visual information to facilitate target\ndomain task adaptation without requiring source domain data. Specifically, we\nfirst append Task-Specific Attention Adapters (TSAA) to the feature pyramid of\na pretrained backbone, which adapt multi-level features extracted from the\nshared pre-trained backbone to the target task. Then, the parameters of the\nTSAA are trained through a Visual-Visual Embedding Alignment (VVEA) module and\na Text-Visual Embedding Alignment (TVEA) module. The VVEA module utilizes\nglobal-local visual features to align image features across different views,\nwhile the TVEA module leverages textual priors from pre-aligned multi-modal\nfeatures (e.g., from CLIP) to guide cross-modal adaptation. By combining the\noutputs of these modules through dense comparison operations and subsequent\nfusion via skip connections, our method produces refined prediction masks.\nUnder both 1-shot and 5-shot settings, the proposed approach achieves average\nsegmentation accuracy improvements of 2.18\\% and 4.11\\%, respectively, across\nfour cross-domain datasets, significantly outperforming state-of-the-art CD-FSS\nmethods. Code are available at https://github.com/ljm198134/TVGTANet."
                },
                "authors": [
                    {
                        "name": "Jianming Liu"
                    },
                    {
                        "name": "Wenlong Qiu"
                    },
                    {
                        "name": "Haitao Wei"
                    }
                ],
                "author_detail": {
                    "name": "Haitao Wei"
                },
                "author": "Haitao Wei",
                "arxiv_doi": "10.1145/3746027.3755772",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746027.3755772",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.05213v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05213v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "10 pages,Accepted at ACMMM2025",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05206v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05206v1",
                "updated": "2025-08-07T09:43:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    9,
                    43,
                    34,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T09:43:34Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    9,
                    43,
                    34,
                    3,
                    219,
                    0
                ],
                "title": "Bidding-Aware Retrieval for Multi-Stage Consistency in Online\n  Advertising",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bidding-Aware Retrieval for Multi-Stage Consistency in Online\n  Advertising"
                },
                "summary": "Online advertising systems typically use a cascaded architecture to manage\nmassive requests and candidate volumes, where the ranking stages allocate\ntraffic based on eCPM (predicted CTR $\\times$ Bid). With the increasing\npopularity of auto-bidding strategies, the inconsistency between the\ncomputationally sensitive retrieval stage and the ranking stages becomes more\npronounced, as the former cannot access precise, real-time bids for the vast ad\ncorpus. This discrepancy leads to sub-optimal platform revenue and advertiser\noutcomes. To tackle this problem, we propose Bidding-Aware Retrieval (BAR), a\nmodel-based retrieval framework that addresses multi-stage inconsistency by\nincorporating ad bid value into the retrieval scoring function. The core\ninnovation is Bidding-Aware Modeling, incorporating bid signals through\nmonotonicity-constrained learning and multi-task distillation to ensure\neconomically coherent representations, while Asynchronous Near-Line Inference\nenables real-time updates to the embedding for market responsiveness.\nFurthermore, the Task-Attentive Refinement module selectively enhances feature\ninteractions to disentangle user interest and commercial value signals.\nExtensive offline experiments and full-scale deployment across Alibaba's\ndisplay advertising platform validated BAR's efficacy: 4.32% platform revenue\nincrease with 22.2% impression lift for positively-operated advertisements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online advertising systems typically use a cascaded architecture to manage\nmassive requests and candidate volumes, where the ranking stages allocate\ntraffic based on eCPM (predicted CTR $\\times$ Bid). With the increasing\npopularity of auto-bidding strategies, the inconsistency between the\ncomputationally sensitive retrieval stage and the ranking stages becomes more\npronounced, as the former cannot access precise, real-time bids for the vast ad\ncorpus. This discrepancy leads to sub-optimal platform revenue and advertiser\noutcomes. To tackle this problem, we propose Bidding-Aware Retrieval (BAR), a\nmodel-based retrieval framework that addresses multi-stage inconsistency by\nincorporating ad bid value into the retrieval scoring function. The core\ninnovation is Bidding-Aware Modeling, incorporating bid signals through\nmonotonicity-constrained learning and multi-task distillation to ensure\neconomically coherent representations, while Asynchronous Near-Line Inference\nenables real-time updates to the embedding for market responsiveness.\nFurthermore, the Task-Attentive Refinement module selectively enhances feature\ninteractions to disentangle user interest and commercial value signals.\nExtensive offline experiments and full-scale deployment across Alibaba's\ndisplay advertising platform validated BAR's efficacy: 4.32% platform revenue\nincrease with 22.2% impression lift for positively-operated advertisements."
                },
                "authors": [
                    {
                        "name": "Bin Liu"
                    },
                    {
                        "name": "Yunfei Liu"
                    },
                    {
                        "name": "Ziru Xu"
                    },
                    {
                        "name": "Zhaoyu Zhou"
                    },
                    {
                        "name": "Zhi Kou"
                    },
                    {
                        "name": "Yeqiu Yang"
                    },
                    {
                        "name": "Han Zhu"
                    },
                    {
                        "name": "Jian Xu"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05206v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05206v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01330v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01330v2",
                "updated": "2025-08-07T09:42:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    9,
                    42,
                    28,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-02T11:53:41Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    11,
                    53,
                    41,
                    5,
                    214,
                    0
                ],
                "title": "NatureGAIA: Pushing the Frontiers of GUI Agents with a Challenging\n  Benchmark and High-Quality Trajectory Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NatureGAIA: Pushing the Frontiers of GUI Agents with a Challenging\n  Benchmark and High-Quality Trajectory Dataset"
                },
                "summary": "The rapid advancement of Large Language Model (LLM)-driven Graphical User\nInterface (GUI) agents is significantly hampered by the profound limitations of\nexisting evaluation benchmarks in terms of accuracy, reproducibility, and\nscalability. To address this critical gap, we introduce NaturalGAIA, a novel\nbenchmark engineered on the principle of Causal Pathways. This design paradigm\nstructures complex tasks into a series of programmatically verifiable atomic\nsteps, ensuring a rigorous, fully automated, and reproducible standard for\nassessment. Concurrently, to mitigate the inherent capability deficits of\nagents, we developed LightManus, a hierarchical agent architecture specifically\noptimized for long-horizon tasks. We leveraged this agent to generate a\nhigh-quality, human-verified trajectory dataset that uniquely captures diverse\nand even self-correcting interaction patterns of LLMs. We then utilized this\ndataset to perform Reinforcement Fine-Tuning (RFT) on the Qwen2.5-VL-7B model.\nOur experiments reveal that NaturalGAIA presents a formidable challenge to\ncurrent state-of-the-art LLMs; even the top-performing Claude-sonnet-4 achieved\na Weighted Pathway Success Rate (WPSR) of only 34.6%. Moreover, while RFT\nsubstantially improved the smaller model's GUI execution capabilities (WPSR\nincreased from 3.3% to 10.8%), its performance degraded sharply when handling\ncomplex scenarios. This outcome highlights the inherent capability ceiling of\nsmaller models when faced with comprehensive tasks that integrate perception,\ndecision-making, and execution. This research contributes a rigorous evaluation\nstandard and a high-quality dataset to the community, aiming to guide the\nfuture development of GUI agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Model (LLM)-driven Graphical User\nInterface (GUI) agents is significantly hampered by the profound limitations of\nexisting evaluation benchmarks in terms of accuracy, reproducibility, and\nscalability. To address this critical gap, we introduce NaturalGAIA, a novel\nbenchmark engineered on the principle of Causal Pathways. This design paradigm\nstructures complex tasks into a series of programmatically verifiable atomic\nsteps, ensuring a rigorous, fully automated, and reproducible standard for\nassessment. Concurrently, to mitigate the inherent capability deficits of\nagents, we developed LightManus, a hierarchical agent architecture specifically\noptimized for long-horizon tasks. We leveraged this agent to generate a\nhigh-quality, human-verified trajectory dataset that uniquely captures diverse\nand even self-correcting interaction patterns of LLMs. We then utilized this\ndataset to perform Reinforcement Fine-Tuning (RFT) on the Qwen2.5-VL-7B model.\nOur experiments reveal that NaturalGAIA presents a formidable challenge to\ncurrent state-of-the-art LLMs; even the top-performing Claude-sonnet-4 achieved\na Weighted Pathway Success Rate (WPSR) of only 34.6%. Moreover, while RFT\nsubstantially improved the smaller model's GUI execution capabilities (WPSR\nincreased from 3.3% to 10.8%), its performance degraded sharply when handling\ncomplex scenarios. This outcome highlights the inherent capability ceiling of\nsmaller models when faced with comprehensive tasks that integrate perception,\ndecision-making, and execution. This research contributes a rigorous evaluation\nstandard and a high-quality dataset to the community, aiming to guide the\nfuture development of GUI agents."
                },
                "authors": [
                    {
                        "name": "Zihan Zheng"
                    },
                    {
                        "name": "Tianle Cui"
                    },
                    {
                        "name": "Chuwen Xie"
                    },
                    {
                        "name": "Jiahui Zhang"
                    },
                    {
                        "name": "Jiahui Pan"
                    },
                    {
                        "name": "Lewei He"
                    },
                    {
                        "name": "Qianglong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Qianglong Chen"
                },
                "author": "Qianglong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01330v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01330v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05202v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05202v1",
                "updated": "2025-08-07T09:37:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    9,
                    37,
                    45,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T09:37:45Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    9,
                    37,
                    45,
                    3,
                    219,
                    0
                ],
                "title": "SPEX: A Vision-Language Model for Land Cover Extraction on Spectral\n  Remote Sensing Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPEX: A Vision-Language Model for Land Cover Extraction on Spectral\n  Remote Sensing Images"
                },
                "summary": "Spectral information has long been recognized as a critical cue in remote\nsensing observations. Although numerous vision-language models have been\ndeveloped for pixel-level interpretation, spectral information remains\nunderutilized, resulting in suboptimal performance, particularly in\nmultispectral scenarios. To address this limitation, we construct a\nvision-language instruction-following dataset named SPIE, which encodes\nspectral priors of land-cover objects into textual attributes recognizable by\nlarge language models (LLMs), based on classical spectral index computations.\nLeveraging this dataset, we propose SPEX, a multimodal LLM designed for\ninstruction-driven land cover extraction. To this end, we introduce several\ncarefully designed components and training strategies, including multiscale\nfeature aggregation, token context condensation, and multispectral visual\npre-training, to achieve precise and flexible pixel-level interpretation. To\nthe best of our knowledge, SPEX is the first multimodal vision-language model\ndedicated to land cover extraction in spectral remote sensing imagery.\nExtensive experiments on five public multispectral datasets demonstrate that\nSPEX consistently outperforms existing state-of-the-art methods in extracting\ntypical land cover categories such as vegetation, buildings, and water bodies.\nMoreover, SPEX is capable of generating textual explanations for its\npredictions, thereby enhancing interpretability and user-friendliness. Code\nwill be released at: https://github.com/MiliLab/SPEX.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spectral information has long been recognized as a critical cue in remote\nsensing observations. Although numerous vision-language models have been\ndeveloped for pixel-level interpretation, spectral information remains\nunderutilized, resulting in suboptimal performance, particularly in\nmultispectral scenarios. To address this limitation, we construct a\nvision-language instruction-following dataset named SPIE, which encodes\nspectral priors of land-cover objects into textual attributes recognizable by\nlarge language models (LLMs), based on classical spectral index computations.\nLeveraging this dataset, we propose SPEX, a multimodal LLM designed for\ninstruction-driven land cover extraction. To this end, we introduce several\ncarefully designed components and training strategies, including multiscale\nfeature aggregation, token context condensation, and multispectral visual\npre-training, to achieve precise and flexible pixel-level interpretation. To\nthe best of our knowledge, SPEX is the first multimodal vision-language model\ndedicated to land cover extraction in spectral remote sensing imagery.\nExtensive experiments on five public multispectral datasets demonstrate that\nSPEX consistently outperforms existing state-of-the-art methods in extracting\ntypical land cover categories such as vegetation, buildings, and water bodies.\nMoreover, SPEX is capable of generating textual explanations for its\npredictions, thereby enhancing interpretability and user-friendliness. Code\nwill be released at: https://github.com/MiliLab/SPEX."
                },
                "authors": [
                    {
                        "name": "Dongchen Si"
                    },
                    {
                        "name": "Di Wang"
                    },
                    {
                        "name": "Erzhong Gao"
                    },
                    {
                        "name": "Xiaolei Qin"
                    },
                    {
                        "name": "Liu Zhao"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Minqiang Xu"
                    },
                    {
                        "name": "Jianbo Zhan"
                    },
                    {
                        "name": "Jianshe Wang"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Bo Du"
                    },
                    {
                        "name": "Liangpei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Liangpei Zhang"
                },
                "author": "Liangpei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05202v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05202v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05201v1",
                "updated": "2025-08-07T09:37:14Z",
                "updated_parsed": [
                    2025,
                    8,
                    7,
                    9,
                    37,
                    14,
                    3,
                    219,
                    0
                ],
                "published": "2025-08-07T09:37:14Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    9,
                    37,
                    14,
                    3,
                    219,
                    0
                ],
                "title": "FAITH: A Framework for Assessing Intrinsic Tabular Hallucinations in\n  finance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAITH: A Framework for Assessing Intrinsic Tabular Hallucinations in\n  finance"
                },
                "summary": "Hallucination remains a critical challenge for deploying Large Language\nModels (LLMs) in finance. Accurate extraction and precise calculation from\ntabular data are essential for reliable financial analysis, since even minor\nnumerical errors can undermine decision-making and regulatory compliance.\nFinancial applications have unique requirements, often relying on\ncontext-dependent, numerical, and proprietary tabular data that existing\nhallucination benchmarks rarely capture. In this study, we develop a rigorous\nand scalable framework for evaluating intrinsic hallucinations in financial\nLLMs, conceptualized as a context-aware masked span prediction task over\nreal-world financial documents. Our main contributions are: (1) a novel,\nautomated dataset creation paradigm using a masking strategy; (2) a new\nhallucination evaluation dataset derived from S&P 500 annual reports; and (3) a\ncomprehensive evaluation of intrinsic hallucination patterns in\nstate-of-the-art LLMs on financial tabular data. Our work provides a robust\nmethodology for in-house LLM evaluation and serves as a critical step toward\nbuilding more trustworthy and reliable financial Generative AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucination remains a critical challenge for deploying Large Language\nModels (LLMs) in finance. Accurate extraction and precise calculation from\ntabular data are essential for reliable financial analysis, since even minor\nnumerical errors can undermine decision-making and regulatory compliance.\nFinancial applications have unique requirements, often relying on\ncontext-dependent, numerical, and proprietary tabular data that existing\nhallucination benchmarks rarely capture. In this study, we develop a rigorous\nand scalable framework for evaluating intrinsic hallucinations in financial\nLLMs, conceptualized as a context-aware masked span prediction task over\nreal-world financial documents. Our main contributions are: (1) a novel,\nautomated dataset creation paradigm using a masking strategy; (2) a new\nhallucination evaluation dataset derived from S&P 500 annual reports; and (3) a\ncomprehensive evaluation of intrinsic hallucination patterns in\nstate-of-the-art LLMs on financial tabular data. Our work provides a robust\nmethodology for in-house LLM evaluation and serves as a critical step toward\nbuilding more trustworthy and reliable financial Generative AI systems."
                },
                "authors": [
                    {
                        "name": "Mengao Zhang"
                    },
                    {
                        "name": "Jiayu Fu"
                    },
                    {
                        "name": "Tanya Warrier"
                    },
                    {
                        "name": "Yuwen Wang"
                    },
                    {
                        "name": "Tianhui Tan"
                    },
                    {
                        "name": "Ke-wei Huang"
                    }
                ],
                "author_detail": {
                    "name": "Ke-wei Huang"
                },
                "author": "Ke-wei Huang",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]